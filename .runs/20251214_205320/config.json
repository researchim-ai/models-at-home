{
  "hidden_size": 512,
  "num_layers": 8,
  "n_heads": 8,
  "seq_len": 512,
  "batch_size": 16,
  "gradient_accumulation": 4,
  "learning_rate": 0.0005,
  "warmup_steps": 1000,
  "epochs": 1,
  "mixed_precision": "bf16",
  "grad_checkpoint": false,
  "data_path": "datasets/fineweb_ru_1GB.jsonl",
  "output_dir": "out/training_run",
  "save_every": 5000,
  "log_every": 10,
  "tokenizer_path": "gpt2"
}