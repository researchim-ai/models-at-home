# LLM: что нужно знать о строении и обучении

Подробный учебный материал о больших языковых моделях (Large Language Models): из чего они состоят, как работают и как их тренируют.

---

## Что такое LLM

**LLM (Large Language Model)** — это нейросеть, обученная предсказывать следующий элемент текста — **токен** (подслово или слово). Формально это **модель условной вероятности**:

$$P(x_{t+1} \mid x_1, x_2, \ldots, x_t)$$

где $x_i$ — токены. Генерация идёт **авторегрессионно**: каждый следующий токен зависит от всех предыдущих; процесс повторяется до конца ответа или до спецтокена остановки.

```
  Авторегрессия (генерация по одному ТОКЕНУ; токен = подслово, не символ!):

  Промпт: "Столица Франции — "
       │
       ▼  токенизатор разбивает на подслова (пример для BPE/SentencePiece):
  ┌─────────────────────────────────────────────────────────────┐
  │  x₁        x₂        x₃      x₄       →   [ ? ]             │
  │  "Столица" "Франции" " —"   " "       │   следующий токен   │
  └─────────────────────────────────────────────────────────────┘
       │                                         │
       └────────── модель предсказывает ─────────┘
                         │
                         ▼
                    следующий токен: "Париж" (один токен — целое слово/подслово)
                         │
                         ▼
  "Столица Франции — Париж" → модель → "." → [EOS]
  (каждый шаг добавляет один токен; токены могут быть длиннее одного символа)
```

### Зачем «большая»

- С ростом числа параметров и объёма обучающих данных модель лучше усваивает грамматику, факты, рассуждения и стили (эффект **масштабирования**, scaling laws).
- «Понимание» и «знание» в LLM — это статистические закономерности в весах; модель не хранит факты в явном виде, а обобщает паттерны из данных.

### Окно контекста

Модель «видит» только последние **N** токенов (**длина контекста**, context length). Типичные значения: 2k, 4k, 8k, 32k, 128k. Всё, что было раньше, формально недоступно (хотя при обучении на длинных текстах модель учится сжимать важное в скрытом состоянии).

```
  Окно контекста (N токенов):

  ←────────── видимая часть (context window) ──────────→
  ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
  │ t₁  │ t₂  │ t₃  │ ... │ tₙ₋₂│ tₙ₋₁│ tₙ  │ ??? │
  └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘
    ↑                                               ↑
  модель видит только эти N токенов          следующий токен
  (всё слева от окна «забыто»)               предсказываем для этой позиции
```

---

## Токенизация (разбиение текста)

Текст перед подачей в модель превращается в последовательность целых чисел — **ид токенов**. Этим занимается **токенизатор**.

### Зачем не по словам

- Словарь по словам огромен (сотни тысяч слов, плюс словоформы, опечатки).
- Редкие слова пришлось бы кодировать как «неизвестный токен», потери информации.
- **Подсловная токенизация** (subword) даёт разумный размер словаря и умеет обрабатывать новые слова через кусочки.

### BPE (Byte Pair Encoding)

- Начинают с **символов** (или байтов в byte-level BPE). Изначально каждому символу соответствует свой «токен».
- На каждой **итерации** находят самую частую **пару** подряд идущих токенов в корпусе и объединяют её в один новый токен. Словарь растёт на 1.
- Повторяют до достижения целевого размера словаря (например 32k–50k).
- **Декодирование**: обратная операция — сливаем пары по правилам словаря, пока не получим строку символов.
- Итог: словарь из частых подслов и символов. Незнакомое слово разбивается на известные куски: «нейросеть» → `нейр`, `о`, `сеть`.

**Пример одного шага BPE (упрощённо):**

- Текущий словарь: символы + уже слитые пары, например «н» «е» «й» «р» «о» «с» «е» «т» «ь» и пара «е»+«р» → «ер».
- В корпусе чаще всего подряд идёт, скажем, «е»+«р». Добавляем правило: «ер» — один токен.
- После многих таких шагов появятся «нейр», «сеть» и т.д.

**Размер словаря — компромисс:** маленький словарь (8k–16k) — длинные последовательности (больше вычислений), большой (64k–128k) — короткие последовательности, но больше параметров в embedding и LM head и реже многие токены в обучении.

### SentencePiece и аналоги

- Работают на уровне **Unicode-символов** или **байтов**, что удобно для мультиязычности и редких символов.
- Могут быть **unigram** (вероятностная сегментация) или BPE-подобные.
- В современных LLM часто используют именно SentencePiece (Llama, Mistral) или BPE (GPT-2, многие другие).

### Специальные токены

В словарь добавляют служебные токены, не соответствующие «нормальному» тексту:

- **BOS / EOS** (beginning/end of sequence) — границы последовательности.
- **Pad** — выравнивание до одной длины в батче (при обучении).
- В чат-моделях: `<|im_start|>`, `<|im_end|>`, `<|user|>`, `<|assistant|>` и т.п. для разметки ролей в диалоге.

**Unigram (SentencePiece):** вместо слияния пар задаётся целевой размер словаря, и алгоритм итеративно переоценивает вероятности сегментации и обновляет словарь (удаляет маловероятные токены, добавляет новые кандидаты). Часто даёт более ровное качество по разным языкам.

От выбора токенизатора и размера словаря зависят длина последовательности (одно и то же предложение может дать разное число токенов) и качество на разных языках. Токенизатор обычно **обучают** на репрезентативной выборке текста до обучения модели, затем замораживают.

```
  Токенизация: текст → id токенов

  "Привет, мир!"                    словарь (vocab)
        │                                │
        ▼                                │
  ┌───────────┐   BPE / SentencePiece    │
  │ Токенизатор│ ────────────────────────►│  "При" → 1045
  └───────────┘                          │  "вет" → 3921
        │                                │  ","   → 11
        ▼                                │  "мир" → 482
  [ 1045, 3921, 11, 482, 0 ]  ◄──────────┘  "!"   → 0
   id₁   id₂   id₃  id₄  id₅
   (seq_len = 5)
```

---

## Embedding: от токенов к векторам

Модель работает не с целыми числами, а с векторами фиксированной размерности **hidden_size** (типично 768, 1024, 2048, 4096, 8192). Первый шаг — **превратить каждый токен в вектор**.

### Token embedding

- Матрица весов размера **vocab_size × hidden_size**.
- Токен с id $i$ кодируется как $i$-я строка этой матрицы (или эквивалентно: lookup по индексу).
- Эти веса обучаются вместе с остальной сетью.

### Positional embedding: зачем нужны позиции

Transformer по своей структуре **не чувствует порядок** токенов: если переставить слова, для «чистого» attention вход будет тем же. Порядок вносят **позиционные кодировки**.

**Варианты:**

1. **Синусоидальные (Sinusoidal)** — как в оригинальном Transformer и GPT-2. Позиция $pos$ кодируется функциями $\sin(pos/10000^{2i/d})$, $\cos(\ldots)$. Плюс: можно экстраполировать на длины, не виденные при обучении. Минус: модель часто хуже использует длинный контекст.

2. **Обучаемые (Learned)** — отдельный вектор для каждой позиции до max_length. Просто и эффективно на обученных длинах, но за пределом max_length модель не обучена.

3. **RoPE (Rotary Position Embedding)** — используется в LLaMA, Mistral, Qwen и др. Позиция вносится не сложением, а **поворотом** векторов Query и Key в зависимости от позиции. Формула (упрощённо): для пары (Q, K) на позициях $m$ и $n$ вводится относительный угол $\theta(m-n)$. Плюсы: хорошая экстраполяция на большие длины, естественное кодирование «расстояния» между токенами.

4. **ALiBi (Attention with Linear Biases)** — позиция не в эмбеддинге, а в виде **линейного штрафа** к attention-скорому: к логам внимания добавляется $-m \cdot |i - j|$ (с коэффициентом $m$, зависящим от головы). Просто и часто хорошо экстраполирует.

**RoPE подробнее:** для каждой пары измерений $(2k, 2k+1)$ вектора Q и K поворачиваются на угол $\theta \cdot \mathrm{pos}$, где $\theta = 10000^{-2k/d}$. В итоге скалярное произведение $Q_i \cdot K_j$ зависит только от **разности позиций** $i-j$, что даёт инвариантность к сдвигу и хорошую экстраполяцию на длинные контексты. В LLaMA и аналогах RoPE применяют к Q и K **до** вычисления attention scores.

Итог: на вход блокам Transformer приходит **сумма** (или конкатенация, в зависимости от схемы) токенного и позиционного представлений; размерность на каждой позиции — **hidden_size**.

```
  Embedding: id → векторы (hidden_size)

  [ id₁   id₂   id₃  ...  id_L ]     (L = seq_len)
       │     │     │          │
       ▼     ▼     ▼          ▼
  ┌─────────────────────────────────────────┐
  │  Token Embedding  [vocab_size × H]      │   lookup по id
  └─────────────────────────────────────────┘
       │     │     │          │
       ▼     ▼     ▼          ▼
  [ v₁   v₂   v₃  ...  v_L ]   каждый v ∈ ℝ^H
       │     │     │          │
       │     +     +          +
       │     │     │          │
  ┌────┴─────┴─────┴──────────┴────┐
  │  Positional Embedding / RoPE   │
  └────────────────────────────────┘
       │     │     │          │
       ▼     ▼     ▼          ▼
  [ x₁   x₂   x₃  ...  x_L ]   вход в Transformer  [L × H]
```

---

## Transformer-блок: внимание и FFN

Сердце LLM — **повторяющийся блок** (слой). Обычно в блоке два подуровня: **Self-Attention** и **Feed-Forward Network (FFN)**. Между ними и вокруг — **нормализация** и **остаточные связи**.

### Self-Attention (самовнимание)

Модель для каждой позиции «смотрит» на все доступные токены (в LLM — только на себя и предыдущие) и формирует новый вектор как взвешенную сумму их представлений.

#### Шаги по формулам

1. Из входа $X$ (форма `[batch, seq_len, hidden_size]`) получают три матрицы линейными преобразованиями:
   - $Q = X W_Q$ (Query),
   - $K = X W_K$ (Key),
   - $V = X W_V$ (Value).

2. **Attention scores**: для каждой пары позиций $(i, j)$ считают «схожесть»:
   $$\text{scores}_{ij} = \frac{Q_i \cdot K_j^\top}{\sqrt{d_k}}$$
   где $d_k$ — размерность ключа (обычно hidden_size / num_heads). Делитель $\sqrt{d_k}$ (**scaled** dot-product) нужен по двум причинам: (1) при больших $d_k$ скалярные произведения $Q_i \cdot K_j$ по величине растут как $\sqrt{d_k}$, и без масштабирования после softmax распределение весов становится почти «пиковым» (один элемент ≈1, остальные ≈0) — градиенты затухают; (2) численная стабильность (избегаем слишком больших экспонент в softmax).

3. **Causal mask**: в LLM токен не должен видеть будущее. Поэтому для $j > i$ полагают $\text{scores}_{ij} = -\infty$. После softmax такие позиции дают вес 0.

4. **Вес внимания**: по строкам применяют softmax:
   $$A = \mathrm{softmax}(\text{scores})$$

5. **Выход**: взвешенная сумма значений:
   $$\text{Attention}(Q,K,V) = A \cdot V$$

Размерность выхода — та же, что у $V$ (на голову: hidden_size / num_heads).

```
  Attention (одна голова): от X к взвешенной сумме по V

  Позиции:    1     2     3    ...    L
              │     │     │           │
  X ────────► Q,K,V для каждой позиции
              │     │     │           │
  scores:   [ s11   -∞    -∞   ...  -∞  ]   causal mask:
            [ s21  s22   -∞   ...  -∞  ]    j>i → -∞
            [ s31  s32  s33  ...  -∞  ]
            [ ...  ...  ...  ...  ... ]
            [ sL1  sL2  sL3  ...  sLL ]
              │
              ▼ softmax по строкам
  веса A:   каждая строка суммируется в 1
              │
              ▼  A · V
  выход:    новый вектор на каждой позиции (контекстное представление)
```

#### Multi-Head Attention

Вместо одного набора Q, K, V делают **num_heads** параллельных «голов». У каждой свои матрицы $W_Q, W_K, W_V$ (размерность на голову обычно hidden_size / num_heads). Выходы всех голов **конкатенируют** и пропускают через ещё один линейный слой $W_O$:

$$\text{MultiHead}(X) = \mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h) W_O$$

Так модель может одновременно учитывать разные типы связей (например, синтаксис и соседние слова).

```
  Multi-Head Attention (num_heads = h)

  X [L × H] ──┬──► head_1 (Q₁,K₁,V₁) ──► out_1 [L × H/h]
              ├──► head_2 (Q₂,K₂,V₂) ──► out_2 [L × H/h]
              ├──► ...
              └──► head_h (Qₕ,Kₕ,Vₕ) ──► out_h [L × H/h]
                              │
                              ▼  Concat по последней оси
                        [L × H]
                              │
                              ▼  Linear W_O
                        выход [L × H]
```

#### Размерности

- **hidden_size** должен делиться на **num_heads**.
- Часто $d_k = d_v = \text{hidden\_size} / \text{num\_heads}$ (например 128 при hidden_size=4096 и num_heads=32).

### Feed-Forward Network (FFN)

После attention идёт двухслойная сеть, применяемая **по позициям независимо** (одинаковые веса для всех позиций):

**Классический вариант (GPT-2):**
$$\text{FFN}(x) = \mathrm{ReLU}(x W_1 + b_1) W_2 + b_2$$

**SwiGLU (LLaMA, многие современные):**
$$\text{FFN}(x) = (\sigma(x W_1) \odot (x W_3)) W_2$$

где $\sigma$ — SiLU (Swish), $\odot$ — поэлементное умножение, $W_1, W_3$ — два «верхних» слоя (часто объединяют в один большой слой и потом режут по выходу). Размерность внутреннего слоя — **intermediate_size**, обычно **в 3–4 раза больше hidden_size** (например 11008 при hidden_size=4096).

FFN даёт модели «место» для хранения фактов и паттернов; внимание — для доступа к нужному контексту.

```
  FFN (по позициям независимо, одна и та же сеть для всех позиций):

  x [H] ──► Linear ──► [intermediate] ──► SiLU/ReLU ──► Linear ──► [H]
              W₁              │                            W₂
              (H → 4H)        │                            (4H → H)
                              │
  SwiGLU:  x ──► W₁ ──► σ ──┐
            x ──► W₃ ──►─────┴── * (поэлементно) ──► W₂ ──► выход
```

### Нормализация и остаточные связи

- **Остаток (residual)**: каждый подуровень добавляется к входу: $x_{\mathrm{out}} = x + \mathrm{Sublayer}(x)$. Это помогает градиентам проходить через много слоёв.

- **Нормализация**: обычно **Pre-LN** — нормализуют **до** подуровня: $\mathrm{Sublayer}(\mathrm{Norm}(x))$. В LLM часто используют **RMSNorm** (root mean square norm) вместо LayerNorm.

**LayerNorm:** по последней оси считают среднее $\mu$ и дисперсию $\sigma^2$, затем $y = \gamma \cdot (x - \mu) / \sqrt{\sigma^2 + \varepsilon} + \beta$. Два обучаемых параметра $\gamma$, $\beta$ на канал.

**RMSNorm:** среднее не вычитают: $\mathrm{RMS}(x) = \sqrt{\frac{1}{d}\sum_i x_i^2 + \varepsilon}$, затем $y = x / \mathrm{RMS}(x)$ (и опционально обучаемый масштаб). Меньше параметров и вычислений; в практиках LLaMA качества хватает. Формула: $\mathrm{RMSNorm}(x) = x / \mathrm{RMS}(x)$.

**Post-LN vs Pre-LN:** в старых Transformer часто делали нормализацию **после** подуровня (Post-LN). Pre-LN (нормализация **до** подуровня) даёт более стабильное обучение при большой глубине, поэтому в LLM почти везде Pre-LN.

Типичный порядок в блоке (LLaMA-стиль):

1. $x_1 = x + \mathrm{Attention}(\mathrm{RMSNorm}(x))$
2. $x_2 = x_1 + \mathrm{FFN}(\mathrm{RMSNorm}(x_1))$

```
  Один Transformer-блок (LLaMA-стиль):

  x [L×H] ──► RMSNorm ──► Attention ──► (+) ──► x₁
    │                        ▲            │
    └────────────────────────┘            │
                                           ▼
  x₁ [L×H] ──► RMSNorm ──► FFN ──► (+) ──► x₂  (выход блока)
    │                    ▲    │
    └────────────────────┘    ┘
```

Число таких блоков — **num_layers** (12, 24, 32, 40, 80 в зависимости от размера модели).

---

## Выходной слой (LM head)

После всех блоков для каждой позиции есть вектор размерности **hidden_size**. Финальный шаг — предсказание распределения по словарю:

- Линейный слой: **hidden_size → vocab_size**. Получаем **логиты** для каждого токена в словаре.
- Часто веса этого слоя **связаны** с матрицей token embedding (weight tying): одна и та же матрица для входа и выхода (экономит параметры и часто улучшает обучение).

При **обучении** для каждой позиции считают cross-entropy между предсказанным распределением (softmax от логитов) и целевым токеном (one-hot). Усредняют по позициям и батчу.

При **инференсе** из логитов последней позиции (или нужной) получают распределение и **сэмплируют** следующий токен (см. ниже).

```
  Полный конвейер модели (от токенов к логитам):

  [id₁ ... id_L] ──► Embedding ──► [x₁ ... x_L]  (L×H)
                                        │
        ┌───────────────────────────────┘
        ▼
  ┌─────────────┐
  │ Block 1     │  RMSNorm → Attn → + → RMSNorm → FFN → +
  └──────┬──────┘
         ▼
  ┌─────────────┐
  │ Block 2     │  ...
  └──────┬──────┘
         │
         ⋮  (num_layers блоков)
         │
         ▼
  ┌─────────────┐
  │ Block N     │
  └──────┬──────┘
         ▼
  [h₁ ... h_L]  (контекстные представления)
         │
         ▼  берём последнюю позицию h_L
  LM head (Linear H → vocab_size)
         │
         ▼
  логиты [vocab_size] ──► softmax ──► P(токен) ──► сэмплирование
```

---

## Ключевые гиперпараметры и объём модели

| Параметр | Смысл | Примеры значений |
|----------|--------|-------------------|
| **vocab_size** | Размер словаря токенов | 32k, 50k, 128k |
| **hidden_size** | Размер скрытого представления (эмбеддинги, внимание, FFN) | 768, 2048, 4096, 8192 |
| **num_layers** | Число transformer-блоков | 12, 24, 32, 80 |
| **num_heads** | Число голов внимания | 12, 32, 64; hidden_size кратно num_heads |
| **num_kv_heads** | В GQA: число голов для K,V (меньше num_heads) | 8, 32 |
| **intermediate_size** | Внутренний размер в FFN | 3–4 × hidden_size |
| **max_position_embeddings** | Максимальная длина контекста | 2048, 8192, 128k |

**Оценка числа параметров** (приближённо, для стиля LLaMA):

- Embedding + LM head: $\approx 2 \cdot V \cdot H$ (если weight tying), где $V$ = vocab_size, $H$ = hidden_size.
- Один блок: внимание (4 матрицы Q,K,V,O по $H^2$) + FFN (3 матрицы: $H \to$ intermediate, intermediate $\to H$). Итого порядка $8 H^2 + 3 \cdot H \cdot \text{intermediate\_size}$.
- Суммируя по слоям и добавляя эмбеддинги, для 7B параметров типичны hidden_size ≈ 4096, num_layers ≈ 32, intermediate_size ≈ 11008.

**GQA (Grouped-Query Attention)** — в части современных моделей ключи и значения разделяют между несколькими «головами» (num_kv_heads < num_heads). У каждой головы внимания свой Query, но несколько голов **делят** одни и те же K, V. Например num_heads=32, num_kv_heads=8: 8 наборов (K,V), каждый используется для 4 голов Q. Это уменьшает память под KV-cache в 4 раза при инференсе и ускоряет генерацию при небольшой потере качества. **MQA** (Multi-Query) — крайний случай: одна общая пара (K,V) на все головы.

**Типичные конфигурации (для ориентира):**

| Модель  | Параметры | hidden | layers | heads | intermediate | контекст |
|---------|-----------|--------|--------|-------|---------------|----------|
| LLaMA 7B  | ~7B   | 4096 | 32 | 32 | 11008 | 2k–8k   |
| LLaMA 70B | ~70B  | 8192 | 80 | 64 | 28672 | 4k–8k   |
| Mistral 7B| ~7B   | 4096 | 32 | 32 (8 kv) | 14336 | 32k    |
| GPT-2 small | 124M | 768  | 12 | 12 | 3072  | 1024   |

---

## Как тренируют LLM: три этапа

### Этап 1: Pretraining (обучение с нуля)

**Цель:** научить модель предсказывать следующий токен в «сыром» тексте, чтобы она усвоила язык, факты и стили.

**Данные:**

- Огромные корпуса: веб (Common Crawl), книги, код, статьи, форумы и т.д.
- **Очистка и качество:** удаление мусора (повторы символов, битая разметка), фильтр по языку, классификаторы качества. Качество данных часто важнее объёма сверх некоторого порога.
- **Дедупликация:** точная (одинаковые строки) и приближённая (n-граммы, MinHash и т.п.) снижают запоминание копий и улучшают обобщение. На больших корпусах дедуп по n-граммам обязателен.
- **Токенизация на лету или заранее:** токенизатор применяют к тексту; либо сохраняют уже закодированные id (экономия CPU при обучении), либо токенизуют на лету при загрузке батча.
- Разбивают на последовательности фиксированной длины (например 2048 токенов) или по границам документов с паддингом/упаковкой. **Packing** — склеивание коротких документов в одну длинную последовательность с разделителями, чтобы не тратить позиции на pad.

```
  Подготовка батча для pretraining:

  Сырой текст (документы)
        │
        ▼  разбиение на чанки по seq_len токенов
  ┌─────────────────────────────────────────────────┐
  │  Sample 1: [t₁ t₂ t₃ ... t_L]   (L = 2048)      │
  │  Sample 2: [t₁ t₂ t₃ ... t_L]                  │
  │  ...                                             │
  │  Sample B: [t₁ t₂ t₃ ... t_L]   (batch_size=B)  │
  └─────────────────────────────────────────────────┘
        │
        ▼  вход модели
  Для каждой позиции i=1..L-1: цель = токен в позиции i+1
  (causal: при предсказании t_{i+1} модель видит только t_1..t_i)
```

#### Функция потерь (Cross-Entropy) — в деталях

В LLM для языкового моделирования используется **кросс-энтропия** между распределением модели по словарю и «истинным» токеном (one-hot).

**На одной позиции:**

- Модель выдаёт **логиты** $z \in \mathbb{R}^{V}$ (V = vocab_size).
- **Softmax**: $p_k = \frac{e^{z_k}}{\sum_j e^{z_j}}$ — вероятность $k$-го токена.
- Истинный токен в этой позиции имеет id $y$ (целое от 0 до V−1).
- **Cross-Entropy** для одной позиции:
  $$L_{\mathrm{CE}} = -\log p_y = -\log \frac{e^{z_y}}{\sum_j e^{z_j}} = -z_y + \log\sum_j e^{z_j}$$

Чем выше вероятность $p_y$, которую модель дала правильному токену, тем меньше loss. Градиент по логитам: $\frac{\partial L}{\partial z_k} = p_k - \mathbb{1}[k=y]$ (разность между предсказанием и one-hot).

**По всему батчу и последовательности:**

- Считают CE для **каждой позиции** в каждой последовательности батча (кроме позиций паддинга, если они маскируются).
- Усредняют по числу учтённых позиций:
  $$L = -\frac{1}{N_{\mathrm{pos}}} \sum_{\mathrm{pos}} \log P(x_{\mathrm{next}} \mid x_1, \ldots, x_{\mathrm{curr}})$$

где сумма идёт по всем (batch, position) парам, $N_{\mathrm{pos}}$ — их количество. Это и есть **next-token prediction loss**.

**Зачем именно Cross-Entropy:**

- Соответствует задаче классификации на V классов (выбор следующего токена).
- Градиенты хорошо ведут себя при softmax: не взрываются, дают чёткий сигнал «увеличить логит правильного класса».
- Эквивалентность: минимизация CE эквивалентна максимизации likelihood данных (MLE).

**Маскирование при обучении:**

- **Pad-токены**: позиции с pad_id часто **исключают** из суммы loss (mask), чтобы модель не училась предсказывать паддинг.
- **SFT**: при дообучении на диалогах loss считают **только по токенам ответа ассистента**; токены промпта и спецтокены маскируют (weight=0 в loss).

```
  Вычисление loss (один батч, одна последовательность):

  Позиция:     1    2    3    4   ...   L-1   L
  Вход:       t₁   t₂   t₃   t₄  ...  tₙ₋₁   tₙ
  Цель:       t₂   t₃   t₄   t₅  ...  tₙ    (не учим)
       │      │    │    │         │
       ▼      ▼    ▼    ▼         ▼
  Модель предсказывает распределение P(·|контекст) на каждой позиции
       │      │    │    │         │
       ▼      ▼    ▼    ▼         ▼
  Loss:   L₁   L₂   L₃   L₄  ...  Lₙ₋₁
       │
       ▼  усреднение по позициям и батчу
  L = (L₁ + L₂ + ... + Lₙ₋₁) / N_pos
```

#### Оптимизаторы

Параметры модели $\theta$ обновляются по градиенту loss. Основные варианты:

**SGD (Stochastic Gradient Descent):**
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta L$$
где $\eta$ — learning rate. Просто, но в глубоких сетях сходится медленно; чувствителен к масштабу и выбору $\eta$.

**SGD с моментом (Momentum):**
$$v_{t+1} = \mu v_t + \nabla_\theta L, \qquad \theta_{t+1} = \theta_t - \eta v_{t+1}$$
$\mu$ — коэффициент момента (0.9, 0.99). Сглаживает градиенты и ускоряет сходимость.

**Adam (Adaptive Moment Estimation)** — самый распространённый в LLM:

- Хранит **первый момент** (среднее градиентов) $m$ и **второй момент** (среднее квадратов) $v$.
- Обновление (упрощённо):
  $$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t, \quad v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$
  $$\hat{m}_t = m_t/(1-\beta_1^t), \quad \hat{v}_t = v_t/(1-\beta_2^t)$$
  $$\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}$$

Типично $\beta_1=0.9$, $\beta_2=0.999$, $\varepsilon=10^{-8}$. Adam адаптирует шаг к каждому параметру (маленький шаг для параметров с большим градиентом по величине), что стабилизирует обучение.

**AdamW (Adam + Weight Decay):**

- В Adam часто добавляют **L2-регуляризацию** через **decoupled weight decay**: после шага Adam делают
  $$\theta_{t+1} \leftarrow \theta_{t+1} - \eta \lambda \theta_t$$
  где $\lambda$ — weight decay. Это эквивалентно штрафу за большие веса и помогает обобщению. В большинстве репозиториев LLM используют именно **AdamW**.

**Сравнение (кратко):**

| Оптимизатор | Плюсы | Минусы |
|-------------|--------|--------|
| SGD | Простота, мало памяти | Медленно, нужен тщательный подбор LR |
| Adam | Быстрая сходимость, мало подбора | Доп. память (2 момента на параметр) |
| AdamW | Как Adam + лучшее обобщение | Стандарт для pretrain/SFT |

**Память оптимизатора:** у Adam/AdamW на каждый параметр хранятся $m$ и $v$ (те же размеры, что и параметр). Итого ~2× параметры в fp32; при mixed precision веса могут быть в fp16/bf16, моменты часто в fp32.

#### Learning rate: warmup и decay

- **Warmup**: в начале обучения LR линейно (или по другой схеме) поднимают от 0 (или малого значения) до целевого за несколько тысяч шагов. Это избегает слишком больших шагов при нестабильных градиентах в начале.
- **Decay**: после warmup LR уменьшают. Часто **cosine decay** до минимального значения (например 10% от пика):
  $$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{\pi \cdot \mathrm{step}}{\mathrm{total\_steps}}\right)\right)$$
  Линейный decay тоже распространён.

Типичные порядки: пик LR 1e-4 … 3e-4 для pretraining; для SFT часто меньше (1e-5 … 5e-5). Total steps задаётся заранее или через эпохи и размер датасета.

#### Gradient clipping

Градиенты в глубоких сетях могут **взрываться** (очень большие значения). Чтобы шаг обновления не был катастрофическим:

- Считают **глобальную норму** градиента: $\|g\| = \sqrt{\sum_i g_i^2}$ по всем параметрам.
- Если $\|g\| > \mathrm{max\_norm}$ (например 1.0), масштабируют все градиенты: $g \leftarrow g \cdot \mathrm{max\_norm} / \|g\|$.

Так направление градиента сохраняется, но шаг ограничен. В LLM почти везде используют **max_grad_norm** порядка 1.0.

#### Итог по оптимизации pretraining

- **Loss**: усреднённый cross-entropy по всем (batch, position) с маскированием pad (и при SFT — маскированием неподходящих токенов).
- **Оптимизатор**: AdamW с $\beta_1=0.9$, $\beta_2=0.999$, weight decay 0.01–0.1.
- **LR**: warmup + cosine (или linear) decay; пик 1e-4 … 3e-4.
- **Gradient clipping**: max_norm ≈ 1.0.
- **Mixed precision** (FP16/BF16) и при необходимости **gradient checkpointing** для экономии памяти.

**Batch size и gradient accumulation:**

- **Batch size** — сколько последовательностей обрабатывается за один forward/backward. Больший батч стабильнее градиенты, но требует больше VRAM.
- **Gradient accumulation**: если батч не помещается в память, делают несколько маленьких forward/backward без обновления весов, накапливают градиенты, затем один шаг оптимизатора. Эффективный батч = batch_size × accumulation_steps.

```
  Gradient accumulation (accumulation_steps = 4):

  Micro-batch 1 ──► forward ──► backward ──► grad₁ (сохраняем, θ не меняем)
  Micro-batch 2 ──► forward ──► backward ──► grad₂
  Micro-batch 3 ──► forward ──► backward ──► grad₃
  Micro-batch 4 ──► forward ──► backward ──► grad₄
                        │
                        ▼  grad = (grad₁ + grad₂ + grad₃ + grad₄) / 4
  Optimizer.step(θ, grad)  ──► θ обновлён один раз за 4 микро-батча
```

**Fused Cross-Entropy:** в больших моделях матрица логитов `[batch, seq, vocab_size]` огромна. «Fused» реализация считает loss без материализации всей матрицы в памяти (сразу по одному правильному классу на позицию), что сильно экономит VRAM.

**Масштаб:**

- Тренируют на триллионах токенов; модели — от сотен миллионов до сотен миллиардов параметров.
- **Scaling laws** (Chinchilla и др.): качество растёт с объёмом данных и размером модели. Оптимальное соотношение «токены на параметр» — порядка 15–20 (Chinchilla); переобучение на меньших данных или недобор данных ухудшают результат.

**Perplexity** — метрика качества языковой модели на отложенных данных. Определяется как $\mathrm{PPL} = \exp(L)$, где $L$ — усреднённый cross-entropy loss (в натсах на токен). Чем ниже loss, тем ниже PPL: модель «меньше удивлена» данными. PPL = 100 условно означает, что модель в среднем «выбирает» из ~100 равновероятных вариантов; типичные значения после pretraining — от единиц до сотен в зависимости от задачи и домена.

После pretraining модель умеет дополнять текст и «знает» язык, но не обязательно отвечает на вопросы в формате диалога или следует инструкциям.

---

### Этап 2: SFT (Supervised Fine-Tuning)

**Цель:** научить модель формату диалога/инструкций: на вход — запрос пользователя (и опционально системный промпт), на выход — ответ ассистента.

**Данные:**

- Наборы пар (инструкция/вопрос → ответ) или целые диалоги (user/assistant/system).
- Часто микс датасетов: общие инструкции, код, рассуждения, безопасность и т.д.

**Формат:**

- Текст собирают в одну последовательность по шаблону (chat template): например `<|im_start|>user\n...<|im_end|>\n<|im_start|>assistant\n...`.
- При подсчёте loss **маскируют** токены, которые не должны влиять на градиент: обычно считают loss только по токенам **ответа ассистента** (вопрос пользователя и спецтокены можно обнулять в loss).

```
  SFT: маскирование loss (считаем только по ответу ассистента)

  Последовательность:
  [ system | user | assistant | ... ]
     │        │         │
     ▼        ▼         ▼
  mask=0   mask=0   mask=1  (loss только здесь)
     │        │         │
  ┌──┴──┐  ┌──┴──┐  ┌──┴──────────────────────┐
  │ не  │  │ не  │  │ да: CE по каждому токену   │
  │ учим│  │ учим│  │ ответа ассистента         │
  └─────┘  └─────┘  └───────────────────────────┘
  L = среднее только по позициям с mask=1
```

**Объём и риски:**

- Данных обычно намного меньше, чем при pretraining (миллионы примеров).
- Риск **переобучения** и **катастрофического забывания** (модель «забывает» базовые знания). Поэтому часто:
  - используют небольшой learning rate,
  - ограничивают число эпох,
  - применяют **LoRA** (или QLoRA): дообучают только малую часть параметров (низкоранговые матрицы), остальные заморожены.

После SFT модель уже полезна как чат-бот и выполняет инструкции, но может не рассуждать пошагово или не оптимальна по сложным критериям (корректность, полнота, стиль).

---

### Этап 3: RL и выравнивание (GRPO, PPO, DPO и др.)

**Цель:** улучшить поведение по критериям, которые трудно описать только размеченными примерами (например «правильный ответ», «хорошее рассуждение», «безопасный ответ»).

**Идея:**

- Модель генерирует **несколько ответов** на один промпт (сэмплирование).
- Каждый ответ получает **награду** $r$ (от отдельной модели, правила, человека или комбинации).
- Обучение увеличивает вероятность ответов с высокой наградой и уменьшает — с низкой (policy gradient).

**GRPO (Group Relative Policy Optimization):**

- Ответы обрабатываются **группами** (несколько ответов на один промпт).
- Advantage для ответа в группе: $A_i = (r_i - \bar{r}) / \sigma$ или $A_i = r_i - \bar{r}$ (без деления на $\sigma$ в вариантах вроде Dr.GRPO).
- Policy gradient с ограничением шага (clipping), как в PPO, чтобы обновления не были слишком большими.

**Регуляризация:**

- **KL penalty**: штраф за отклонение от референсной модели (часто SFT-модель). Это не даёт политике «уехать» в экстремальные решения.
- **Clip range**: ограничение отношения новой и старой политики (например в диапазоне [0.8, 1.2]).

В итоге получают модели, которые лучше рассуждают (chain-of-thought), проходят тесты, пишут код и т.д. Другие варианты этапа выравнивания — **DPO** (Direct Preference Optimization), **KTO** и др., где явного reward-модели может не быть, а используются только предпочтения (лучший/худший ответ).

```
  Один шаг оптимизации (обобщённо):

  Батч данных ──► Forward ──► Loss L(θ)
                    │
                    ▼
  Backward ──► градиенты ∂L/∂θ
                    │
                    ▼  (опционально) gradient clipping
  θ_new = Optimizer.step(θ, grad)   (SGD / Adam / AdamW)
                    │
                    ▼
  θ ← θ_new  ;  следующий батч
```

---

## Инференс: как модель генерирует ответ

При генерации мы по одному токену дополняем последовательность. На каждом шаге:

1. Текущая последовательность (промпт + уже сгенерированные токены) подаётся в модель.
2. Считаются логиты для **последней** позиции (или для всех, но используют только последнюю).
3. Из распределения **сэмплируют** следующий токен.
4. Токен добавляется к последовательности; повторяем до EOS или лимита длины.

### Сэмплирование

- **Жадно (greedy)**: берём токен с максимальным логитом. Детерминировано, ответы часто «плоские».
- **Temperature**: перед softmax логиты делят на $T$. $T \to 0$ — почти жадно; $T$ большое — почти равномерное распределение, много случайности. Типично 0.7–1.0 для творческих задач, 0.1–0.3 для точных ответов.
- **Top-k**: оставляют только $k$ токенов с наибольшими логитами, остальные обнуляют (или делают $-\infty$), затем softmax и сэмплирование. Ограничивает «глупые» редкие токены.
- **Top-p (nucleus)**: сортируют токены по убыванию вероятности и берут минимальное множество с суммой вероятностей $\ge p$, остальные отбрасывают, затем перенормируют и сэмплируют. Адаптивно к форме распределения.

**Repetition penalty:** во время генерации логиты уже сгенерированных токенов (в окне или во всём ответе) штрафуют: уменьшают на коэффициент или обнуляют, чтобы модель реже повторяла одно и то же. Типичные значения 1.1–1.2.

**Stop sequences:** список строк (или id токенов), при появлении которых генерация останавливается (например `["\n\n", "<|im_end|>"]`). Позволяет не тратить лимит длины на «хвосты» после ответа.

**Max new tokens:** верхняя граница числа сгенерированных токенов после промпта; вместе с stop и EOS ограничивает длину вывода.

Обычно комбинируют temperature с top-p или top-k для баланса разнообразия и связности.

```
  Генерация по шагам (с KV-cache):

  Шаг 1:  [prompt tokens]        ──► модель ──► logits ──► sample ──► tok₁
              │                                                          │
              ▼                                                          ▼
  Шаг 2:  [prompt ... tok₁]      ──► модель (переиспользуем KV для prompt!) ──► sample ──► tok₂
              │
              ▼
  Шаг 3:  [prompt ... tok₁ tok₂] ──► модель ──► ...
  ...
  Память: KV-cache растёт на одну позицию за шаг (только для нового токена).
```

### KV-cache

При авторегрессии для одной и той же префикса (промпт + уже сгенерированное) attention каждый раз пересчитывает ключи и значения для **всех** предыдущих позиций. Чтобы не считать их заново, **ключи и значения** для всех слоёв и голов сохраняют в кэш (KV-cache). На следующем шаге добавляют только ключ и значение для нового токена. Это сильно ускоряет генерацию и снижает объём вычислений.

### Длина контекста при инференсе

Память и время растут с длиной последовательности (квадратично для внимания без оптимизаций, линейно при использовании Flash Attention и аналогов). Поэтому контекст ограничивают (например 4k, 8k, 32k токенов). Модели с длинным контекстом (128k+) часто используют специальные приёмы (разреженное внимание, группировка и т.д.).

---

## Память и скорость: что важно на практике

- **KV-cache**: объём растёт линейно с длиной последовательности и числом слоёв; при длинном контексте и больших batch-ах это основной потребитель VRAM при инференсе. На один слой: 2 (K и V) × batch × num_heads × seq_len × head_dim × sizeof(dtype). Для 7B, 32 слоя, seq=2048, bf16: порядка гигабайт на батч.
- **Flash Attention**: переформулировка attention так, чтобы не материализовать полную матрицу scores [L×L] в памяти — блочные вычисления и перезапись. Уменьшает объём обращений к памяти и лучше использует GPU. Даёт ускорение и снижение пиков памяти; для длинного контекста почти обязателен.
- **Квантизация**: веса и (опционально) активации хранят в пониженной точности (INT8, INT4, NF4). Это уменьшает размер модели (в 2–4 раза) и ускоряет инференс при небольшой потере качества. QLoRA: база в 4-bit, LoRA в fp16 — обучение больших моделей на малой VRAM.
- **Gradient checkpointing** (при обучении): не хранить все промежуточные активации в forward, а пересчитывать их при backward по мере надобности. Экономит память (иногда в разы) в обмен на ~20–30% времени обучения.

**Грубая оценка VRAM при обучении (fp16/bf16):** модель ~2 байта/параметр, оптимизатор (Adam) ~8 байт/параметр, градиенты ~2 байта/параметр, активации зависят от batch × seq × hidden × layers. Для 7B с batch=1, seq=2048 без checkpoint: десятки GB; с gradient checkpointing и небольшим батчем — умещается в 24 GB.

---

## Семейства архитектур (кратко)

- **GPT-2 / GPT-3 стиль**: decoder-only Transformer, обучаемые или синусоидальные позиции, классический MLP в FFN. Основа многих коммерческих и открытых моделей.
- **LLaMA / Mistral**: decoder-only, **RoPE**, **SwiGLU**, **RMSNorm**, **GQA**. Стали де-факто стандартом для открытых LLM.
- **Qwen, Yi, другие**: в целом похожи на LLaMA с вариациями (размер словаря, длина контекста, детали нормализации).

Различия в основном в позиционных кодировках, активациях в FFN, нормализации и в организации внимания (GQA и т.д.). Общая схема «эмбеддинги → N блоков (attention + FFN) → LM head» сохраняется.

---

## Типичные проблемы при обучении

- **Loss резко вырос (spike):** слишком большой learning rate, «плохой» батч, взрыв градиентов. Решение: уменьшить LR, проверить данные, включить/ужесточить gradient clipping.
- **Out of Memory (OOM):** не хватает VRAM. Решение: уменьшить batch size, включить gradient checkpointing, использовать LoRA/QLoRA, уменьшить seq_len, включить FSDP/DeepSpeed при multi-GPU.
- **Loss не падает / на плато:** слишком маленький LR, недостаточно данных, модель уже сошлась. Решение: увеличить LR (или перезапустить с warmup), проверить качество и объём данных.
- **Переобучение (SFT):** validation loss растёт при падающем train loss. Решение: меньше эпох, сильнее weight decay, больше данных, LoRA с малым рангом.
- **Катастрофическое забывание (SFT):** модель «забыла» базовые знания после дообучения. Решение: меньший LR, меньше шагов, LoRA вместо full fine-tune, микс данных (добавить часть pretrain-подобных данных).

---

## Что полезно помнить

- **LLM** — авторегрессионная модель над токенами; ядро — **Transformer** (causal multi-head attention + FFN, нормализация, остатки).
- **Токенизация** (BPE, SentencePiece) переводит текст в id; **эмбеддинги** — в векторы; **позиции** (RoPE, ALiBi и др.) задают порядок.
- **Обучение**: **pretraining** (next-token prediction, cross-entropy loss) → **SFT** (инструкции/диалоги, маскирование по ответу) → **RL/выравнивание** (награды, предпочтения). Оптимизатор — обычно AdamW, LR с warmup и decay, gradient clipping.
- **Инференс**: сэмплирование (temperature, top-k, top-p), repetition penalty, stop sequences, KV-cache, ограничение длины контекста.
- Для экономии ресурсов: **LoRA** / **QLoRA**, **квантизация**, **Flash Attention**, **gradient checkpointing**, **gradient accumulation**.

**Метрики:** loss (cross-entropy), perplexity = exp(loss). При SFT/RL — ещё человеческие оценки, reward, прохождение тестов.

Этот файл даёт развёрнутую базу; отдельные темы (детали RoPE, реализация attention, конкретные RL-алгоритмы, данные и очистка, оценка моделей) можно раскрывать в следующих материалах.
