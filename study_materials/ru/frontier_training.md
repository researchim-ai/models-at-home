# Обучение передовых (Frontier) LLM: Подробное руководство по созданию мыслящих ИИ и автономных агентов

Современный ландшафт больших языковых моделей (LLM) сместился от простых чат-ботов к мыслящим моделям (Thinking models) и автономным агентам. Такие модели, как **GLM-5**, **OLMo 3**, **Kimi K2.5**, **DeepSeek-R1** и **gpt-oss-120b**, демонстрируют переход от парадигмы *vibe coding* (написания кода по пошаговым подсказкам) к *agentic engineering* — агентной инженерии, где ИИ автономно планирует архитектуру, пишет код, исправляет ошибки в терминале и тестирует решения в реальной среде выполнения.

Этот масштабный учебный материал детально описывает весь жизненный цикл (Model Flow) создания передовых LLM: от архитектурных решений и оптимизаторов до многоэтапного сбора данных, RLVR (обучения с подкреплением на проверяемых наградах) и построения масштабируемых инфраструктур для симуляции агентных сред. 

Материал опирается на данные из технических отчетов **GLM-5**, **OLMo 3**, **Kimi K2.5**, **Hermes 4**, **SmolLM3**, **Intellect-3** и других открытых проектов.

---

## Оглавление
1. [Архитектура и стабильность](#1-архитектура-и-стабильность)
    - [Dense против MoE и балансировка нагрузки](#dense-против-moe-и-балансировка-нагрузки)
    - [Оптимизация внимания (MHA, GQA, MLA, DSA)](#оптимизация-внимания-mha-gqa-mla-dsa)
    - [Позиционные кодировки и длинный контекст](#позиционные-кодировки-и-длинный-контекст)
    - [Стабилизация логитов и функции потерь](#стабилизация-логитов-и-функции-потерь)
2. [Оптимизаторы и гиперпараметры](#2-оптимизаторы-и-гиперпараметры)
    - [AdamW против Muon](#adamw-против-muon)
    - [Расписание Learning Rate (WSD) и размер батча](#расписание-learning-rate-wsd-и-размер-батча)
3. [Этап 1: Предварительное обучение (Pre-Training)](#3-этап-1-предварительное-обучение-pre-training)
    - [Миксование и фильтрация данных](#миксование-и-фильтрация-данных)
    - [Качественный апсэмплинг (Quality-aware upsampling)](#качественный-апсэмплинг-quality-aware-upsampling)
4. [Этап 2: Промежуточное обучение (Mid-Training)](#4-этап-2-промежуточное-обучение-mid-training)
    - [Расширение контекста](#расширение-контекста)
    - [Синтетические данные и рассуждения](#синтетические-данные-и-рассуждения)
    - [Деконтаминация (защита от утечек)](#деконтаминация-защита-от-утечек)
5. [Этап 3: Постобучение (Post-Training)](#5-этап-3-постобучение-post-training)
    - [Supervised Fine-Tuning (SFT) и форматы чата](#supervised-fine-tuning-sft-и-форматы-чата)
    - [Оптимизация предпочтений (DPO) и Delta Learning](#оптимизация-предпочтений-dpo-и-delta-learning)
    - [Reinforcement Learning (RLVR и GRPO)](#reinforcement-learning-rlvr-и-grpo)
    - [Инфраструктура RL: Inflight Updates и Continuous Batching](#инфраструктура-rl-inflight-updates-и-continuous-batching)
    - [Cross-Stage Distillation (Дистилляция навыков)](#cross-stage-distillation-дистилляция-навыков)
6. [Агентная инженерия и масштабирование сред](#6-агентная-инженерия-и-масштабирование-сред)
    - [Программная инженерия (SWE) и терминалы](#программная-инженерия-swe-и-терминалы)
    - [Управление контекстом для агентов (Keep-recent-k)](#управление-контекстом-для-агентов-keep-recent-k)
7. [Инженерные трудности при обучении (Training Ops)](#7-инженерные-трудности-при-обучении-training-ops)
8. [Дополнительные детали из технических отчётов](#дополнительные-детали-из-технических-отчётов)

---

## 1. Архитектура и стабильность

Архитектурные решения закладывают фундамент производительности модели. Ошибки на этом этапе обходятся слишком дорого, поэтому индустрия стремится к проверенным подходам, меняя конфигурацию только после строгих абляционных исследований (ablation studies).

### Dense против MoE и балансировка нагрузки
*   **Dense (плотные сети):** Выбор по умолчанию, если у вас ограничена память GPU или сжаты сроки. Плотные модели легче обучать, у них меньше непредсказуемых скачков лосса (loss spikes).
*   **MoE (Mixture of Experts):** Необходимы для экстремального масштабирования (как у **GLM-5** с 744B параметрами или **Kimi K2** с 1T). MoE позволяет обрабатывать токены лишь малой частью экспертов (например, 8 из 256), снижая вычислительную нагрузку (FLOPs) при инференсе. 
*   **Sparsity (разреженность):** Модели увеличивают число экспертов (в Kimi K2 их 384) при уменьшении их размера (гранулярности). Это увеличивает "efficiency leverage". Для сохранения стабильности выделяют **Shared Experts** (общие эксперты), которые всегда активны и обрабатывают базовые паттерны языка, пока остальные (Routed Experts) специализируются на узких задачах.
*   **Load Balancing (Балансировка нагрузки):** Если эксперты нагружены неравномерно, модель деградирует. Исторически использовали *auxiliary loss* (штраф за неравномерность). Современные подходы включают:
    1. **Sequence-wise auxiliary loss:** Балансировка на уровне всей последовательности токенов, а не одного батча.
    2. **Auxiliary-loss free load balancing:** В **DeepSeek-V3** и **Kimi K2** вместо штрафов в loss-функцию вводят динамические векторы смещений (bias) для логитов маршрутизатора. Если эксперт "голодает", его bias увеличивается, притягивая токены.
    3. **SMEBU (Sequence-wise MoE Balancing with Uniformity):** Продвинутый алгоритм обновления bias через моментум и функцию `tanh`, гарантирующий стабильную балансировку.

### Оптимизация внимания (MHA, GQA, MLA, DSA)
*   **MHA (Multi-Head Attention)** — дает лучшее качество, но генерирует гигантский KV-кэш (истощитель видеопамяти при инференсе).
*   **GQA (Grouped Query Attention):** Компромисс. Разделяет ключи и значения на группы (например, 2, 4 или 8). При абляциях **GQA-8** обычно бьет MHA по соотношению "цена-качество".
*   **MLA (Multi-Latent Attention):** Инновация DeepSeek. Сильно сжимает KV-кэш, проецируя его в одно скрытое латентное пространство (например, 576 измерений). Однако **GLM-5** выявила, что базовый MLA проигрывает GQA-8. Проблема решилась применением раздельной оптимизации (**Muon Split**).
*   **DSA (DeepSeek Sparse Attention):** На сверхдлинных контекстах (128K+) даже GQA слишком дорог. В GLM-5 применено DSA, которое использует специальный индексатор для динамического выбора `top-k` самых важных токенов. Это снижает затраты на внимание в 1.5–2 раза без потери точности (что подтверждено тестами NIAH — Needle In A Haystack).
*   **Gated Attention:** Внедрение gating-слоев к выходам механизма внимания позволяет убрать «attention sinks» (токены-паразиты, вбирающие в себя лишнее внимание) и резко снизить спайки лосса.

### Позиционные кодировки и длинный контекст
Трансформеры не имеют встроенного понимания порядка слов. 
*   **RoPE (Rotary Position Embedding):** Стандарт де-факто. Кодирует позицию через вращение векторов Q и K в 2D-плоскостях. 
*   **YaRN и ABF (Adaptive Base Frequency):** Для расширения контекста (от 4K до 128K) базовую частоту вращения пропорционально увеличивают.
*   **RNoPE (Rotary + No Position Embedding):** Новейший гибрид (использован в SmolLM3). В некоторых слоях применяется RoPE (для локального контекста), а в других — NoPE (совсем без кодирования позиций) для абстрактного восприятия всей последовательности. 
*   **Document Masking:** При упаковке нескольких коротких документов в один длинный батч (Sequence Packing) необходимо запретить токенам одного документа "смотреть" на другой. Это не сильно влияет на коротком контексте, но **критически важно** при расширении окна до 64K+.

### Стабилизация логитов и функции потерь
Глубокие сети склонны к взрыву градиентов. 
*   **Logit Softcapping:** Впервые масштабно применен в Gemma 2 и стал индустриальным стандартом. Предварительные логиты механизма внимания и выходного слоя "сжимаются" с помощью функции `tanh`:
    $$ \text{logits} \leftarrow \text{soft\_cap} \cdot \tanh(\text{logits} / \text{soft\_cap}) $$
    Это плавно ограничивает значения логитов (например, диапазоном [-50, 50]), предотвращая фатальные скачки.
*   **Отключение Weight Decay для эмбеддингов:** Регуляризация эмбеддингов приводит к уменьшению их нормы, что математически вызывает взрыв градиентов на ранних слоях из-за свойств LayerNorm/RMSNorm. Оптимально — отключать Weight Decay (L2-регуляризацию) для эмбеддингов.

---

## 2. Оптимизаторы и гиперпараметры

### AdamW против Muon
*   **AdamW** с параметрами $\beta_1=0.9$, $\beta_2=0.95$ остается королем.
*   **Muon (Matrix update algorithm):** Новый оптимизатор, обновляющий параметры всей матрицей сразу через аппроксимацию знака матрицы методом Ньютона-Шульца. Он устраняет смещение по осям (axis-aligned bias) и демонстрирует лучшую "sample efficiency" на огромных батчах. В **GLM-5** применяется модификация **Muon Split** для MLA, где матрицы ап-проекций разбиваются на блоки для раздельной ортогонализации. 
*   Внедрение Muon требует сложной инженерной поддержки (например, All-to-All communication), так как алгоритму нужна полная матрица градиентов, а не ее шарды, как при обычном ZeRO / FSDP. Kimi K2 дополнила это техникой **MuonClip** для превентивного купирования взрывающихся логитов.

### Расписание Learning Rate (WSD) и размер батча
Классическое косинусное затухание (Cosine Decay) заменяется расписанием **WSD (Warmup-Stable-Decay)**. В WSD скорость обучения (LR) долго держится на высоком плато и стремительно падает лишь в последние 10-20% шагов. 
**Зачем это нужно?** WSD позволяет легко проводить абляции. Можно дообучить только финальную стадию (Decay) на разных миксах данных, экономя колоссальные объемы вычислительных ресурсов.

*Размер батча:* По мере стабилизации модели градиенты становятся менее шумными. Применяется **Batch Size Warmup** — на старте батч маленький, к концу претрейна он возрастает в несколько раз.

---

## 3. Этап 1: Предварительное обучение (Pre-Training)

На долю Pre-Training уходит 80-90% вычислительных ресурсов. Успех диктуется составом данных (Data Mixture). Модели (такие как OLMo 3 и GLM-5) тренируются на триллионах токенов (GLM-5 обучен на 28.5T токенах).

### Миксование и фильтрация данных
Сырые данные (Common Crawl, DCLM) подвергаются жесточайшей очистке:
1. **Эвристики и фильтры спама:** Очистка от HTML, списков, взрослого контента, оценка соотношения длины к количеству уникальных слов.
2. **Точная и нечеткая дедупликация:** Удаляются абсолютные клоны по хэшам и почти одинаковые тексты через **MinHash**. Для вычистки шаблонов и футеров применяются алгоритмы суффиксных массивов.
3. **Защита от утечек (Decontamination):** Важнейший шаг. Из обучающей выборки удаляются все 8-граммы, совпадающие с закрытыми бенчмарками (MMLU, HumanEval, GSM8K). Если этого не сделать, RL-обучение позже просто заставит модель "вытащить" зазубренные ответы.
4. **Многостадийное обучение (Multi-stage training):** Наиболее ценные домены (код и математика) добавляются не сразу, а сдвигаются ближе к концу обучения, чтобы свежие качественные паттерны сильнее закрепились в "памяти" модели.

### Качественный апсэмплинг (Quality-aware upsampling)
Простая сортировка и отсев "плохих" данных не работает: если брать только топ-5% текстов, данных не хватит на триллионы токенов. Вместо этого применяется качественный апсэмплинг: топ-5% высококачественных текстов дублируются (апсэмплятся) по 5–7 раз в итоговом датасете, тогда как документы из топ-40% встречаются только по одному разу, а всё, что ниже, удаляется.

---

## 4. Этап 2: Промежуточное обучение (Mid-Training)

Промежуточное обучение выступает мостом между базовым усвоением языка (Pre-Training) и тонкой настройкой (Post-Training). Объем мидтрейна — от 50 до 500 миллиардов токенов.

### Расширение контекста
Контекстное окно увеличивается поэтапно, например: `4K -> 32K -> 128K -> 200K`. 
На этом этапе в датасет подмешивают книги, длинные научные статьи (полученные через специализированные парсеры вроде olmOCR) и синтетические логические траектории. Для подавления эффекта "lost-in-the-middle" применяют *Interleaved packing* — перетасовку независимых текстов в рамках одного окна.

### Синтетические данные и рассуждения
В OLMo 3 на этапе Mid-Training активно используются "Дистиллированные токены рассуждений" (Distilled Reasoning Tokens). 
Создаются сотни тысяч синтетических промптов:
* **Программно проверяемые рассуждения (Program-verifiable data):** Решения сложных алгоритмических задач, сгенерированные сильной моделью (например, QwQ-32B или GPT-4) и верифицированные компилятором Python.
* **Мета-рассуждения (Meta-reasoning):** Данные, которые учат модель когнитивным паттернам: *бектрекингу* (backtracking, возвращению на шаг назад), самопроверке и дебаггингу кода.

**Вывод:** Внедрение цепочек рассуждений на этапе Mid-Training радикально повышает базовый математический и алгоритмический интеллект модели еще до начала полноценного RL-обучения.

---

## 5. Этап 3: Постобучение (Post-Training)

Именно здесь базовая математическо-языковая "губка" превращается в автономного мыслящего агента (Thinking model) или полезного ассистента (Instruct model). В GLM-5 и OLMo 3 этот пайплайн состоит из трех шагов.

### Supervised Fine-Tuning (SFT) и форматы чата
Модель учится понимать инструкции, форматы чата (`<|user|>`, `<|assistant|>`) и обучается базовому паттерну цепочки мыслей (`<think>...</think>`).
В GLM-5 и топовых моделях применяются продвинутые режимы:
* **Interleaved Thinking (Чередующееся мышление):** Модель обязана подумать перед КАЖДЫМ ответом или вызовом функции (tool call).
* **Preserved Thinking (Сохраняемое мышление):** В сложных задачах программирования история мыслей сохраняется на протяжении всех раундов диалога. Модель не начинает размышлять "с нуля" на каждом шаге, а опирается на выводы из предыдущего шага.

### Оптимизация предпочтений (DPO) и Delta Learning
Дальнейшее SFT на ответах от сильных моделей быстро достигает "насыщения". Имитация перестает работать. Модель должна научиться отличать *хорошее* от *плохого*.
Для этого применяется DPO (Direct Preference Optimization) с мощным механизмом **Delta Learning**:
1. Формируются пары ответов (Chosen и Rejected).
2. **Максимизация дельты:** Chosen берется от сильной модели (например, 32B), а Rejected генерируется намеренно слабой моделью (например, 0.5B).
3. **Length Bias Control (Контроль многословности):** DPO склонно вознаграждать просто более длинные ответы. Чтобы модель не начала лить "воду", разницу в длине между Chosen и Rejected жестко фильтруют (не более 100 токенов разницы). Это приводит к созданию лаконичных и четких ответов, что критически важно для Instruct-моделей.

### Reinforcement Learning (RLVR и GRPO)
Самая важная стадия, пробивающая "потолок" интеллекта. Здесь нет учителя (человека или GPT-4) — модель ищет решение сама и получает награду только по объективному результату (RL with Verifiable Rewards - RLVR).
*   **Алгоритм GRPO (Group Relative Policy Optimization):** Стал стандартом, вытеснив PPO. GRPO берет 1 промпт, генерирует группу ответов (например, 16), оценивает их награды и обновляет веса в пользу тех ответов, чья награда выше среднего по группе.
*   **Отсутствие штрафа KL:** В GRPO убирают KL-штраф, который в PPO накладывался по-токенно и заставлял модель отвечать как можно короче. Отсутствие штрафа позволяет модели "размышлять" тысячами токенов без наказания за многословность.
*   **Verifiers (Верификаторы наград):** Для математики — проверка совпадения числа через SymPy. Для кода — прогон юнит-тестов (Fail-to-Pass). Для инструкций — скрипты, проверяющие формат JSON или количество слов.

### Инфраструктура RL: Inflight Updates и Continuous Batching
RL-обучение упирается в скорость инференса (генерации ответов).
*   **Continuous Batching:** В отличие от Static Batching (где все ждут самого длинного ответа в батче), непрерывный батчинг мгновенно загружает в освободившийся "слот" GPU новый промпт. Это спасает до 50% вычислительной мощности.
*   **Inflight Updates (Асинхронные обновления):** Процесс генерации и процесс обновления весов разнесены. Обучающий сервер (Learner) обновляет веса, не останавливая сервер генерации (Actor). 
*   **TITO (Token-in-Token-out):** Генераторы отправляют напрямую ID токенов обучающему серверу, минуя стадию декодирования в текст и обратной ретокенизации. Это устраняет критические баги, при которых смещались отступы в генерируемом коде.
*   **Компенсация Off-Policy (IcePop):** Поскольку веса обновляются асинхронно, генераторы могут создавать текст с использованием чуть устаревшей политики. Применяются алгоритмы двойного усеченного сэмплирования по важности (Double-sided importance sampling / IcePop), которые маскируют токены, если разница между "старой" и "новой" политикой слишком велика.

### Cross-Stage Distillation (Дистилляция навыков)
Долгий RL на математике и коде приводит к **катастрофическому забыванию** (Catastrophic Forgetting) — модель становится умной, но теряет эмпатию, забывает языки и разучивается вести светскую беседу (alignment tax).
Чтобы этого избежать, в GLM-5 финальным этапом проводят **On-Policy Distillation**. Берутся чекпоинты с ранних стадий (например, сразу после SFT) и текущая RL-модель. Финальная модель штрафуется за отклонение своего распределения логитов от логитов SFT-учителя на общих диалоговых промптах. Модель вновь становится "человечной", сохраняя алгоритмический гений.

---

## 6. Агентная инженерия и масштабирование сред

Для обучения агентов автономному выполнению комплексных задач недостаточно датасетов с парами "вопрос-ответ". Модели GLM-5 и Intellect-3 тестируются и обучаются в сложнейших изолированных средах (sandboxes).

### Программная инженерия (SWE) и терминалы
*   **SWE Environments:** Десятки тысяч верифицируемых сред созданы на основе реальных баг-репортов с GitHub (SWE-bench Verified). Модель должна сама найти нужный файл, написать патч, запустить скрипт сборки и пройти юнит-тесты. Награда = 1 выдается только за успешный билд и пройденный тест.
*   **Терминальные среды:** Системы генерируют Docker-контейнеры, где модели выдается терминальный доступ. Агент должен установить зависимости, настроить базу данных или написать bash-скрипт. Для генерации таких сред сами модели (LLM) пишут Dockerfile'ы и тесты на основе веб-туториалов.
*   **Генерация UI / Слайдов:** Модель учится верстать HTML/CSS. Вместо оценки текста, RL-награда начисляется за *Runtime rendering properties* — код рендерится в безголовом браузере (Playwright), и оценивается перекрытие элементов, соблюдение пропорций 16:9 и визуальная эстетика (отсутствие пустых пятен).

### Управление контекстом для агентов (Keep-recent-k)
В процессе долгого поиска в интернете (Browsing Agents) или при дебаггинге логов, контекст быстро раздувается до 100K+ токенов, и модель начинает глупеть (забывать инструкции).
В GLM-5 применена иерархическая система **Keep-recent-k**:
Если история вызовов инструментов превышает $k$ шагов, старые логи и полные HTML-страницы "сворачиваются" (discard) или заменяются кратким саммари, оставляя в сыром виде только последние $k$ ходов. При превышении жесткого лимита в 32K токенов весь контекст тулзов сбрасывается с сохранением глобальной цели. Это позволяет агентам работать часами, не захлебываясь в мусорном контексте.

---

## 7. Инженерные трудности при обучении (Training Ops)

Масштабное обучение часто падает не из-за математики алгоритмов, а из-за узких мест инфраструктуры:

1. **Падение пропускной способности (Throughput vanishing):** 
   Кластеры хранения данных с трудом справляются с десятками терабайт мелких чтений. Если данные вытесняются на медленные диски (S3), GPU простаивают. Решение: локальное кэширование и использование Dataloader'ов с предзагрузкой (Tokenizedbytes).
2. **Шумный лосс и Dataloader:**
   Если Dataloader читает длинный репозиторий кода последовательно, батч заполняется только кодом, что вызывает скачок градиента. Необходимо жесткое перемешивание (shuffling) токенизированных последовательностей (техника **RSDB** — Random Sequential Document Buffer).
3. **Ошибки Tensor Parallelism:**
   Классическая и неочевидная ошибка: на разных GPU одной TP-группы задается один и тот же `random seed`. Модель начинает генерировать идентичные признаки и лосс перестает падать. Тщательный контроль сидов и инициализации матриц спасает месяцы обучения.

---
**Итог:** Создание Frontier-моделей сегодня — это невероятный синтез высокопроизводительной инфраструктуры, жесткой фильтрации триллионов токенов и передовых RL-алгоритмов. Простые генераторы текста эволюционировали в агентные системы, способные автономно писать код, рендерить страницы и размышлять перед каждым своим действием.

---

## Дополнительные детали из технических отчётов

Ниже собраны конкретные цифры, рецепты и практики из отчётов OLMo 3, блога Alex Wa (frontier methodologies), GLM-5 и других источников — чтобы статья была максимально полной.

### Минимальный плейсбук и общие принципы (по Alex Wa)
* **Закрепите эвалы заранее** по знаниям, математике, коду, длинному контексту и следованию инструкциям; реализацию эвалов лучше закончить до завершения базовой модели.
* **Базовый архитектурный выбор:** dense + GQA + RoPE/RNoPE по умолчанию; MoE — только если нужна инференс-эффективность и есть инфраструктура для балансировки.
* **Токенизатор** под целевую языковую и доменную смесь; словарь и спецтокены замораживают рано.
* **Пайплайн данных:** дедупликация, фильтрация, проверка на контаминацию; явные метрики качества данных.
* **Абляции:** по одной переменной за раз; быстрые и надёжные (с хорошей разделяющей способностью).
* **Многостадийный микс:** лучшие и «рассуждающие» данные сдвигают к концу обучения.
* **Стабильность:** logit softcapping (предпочтительно, по Gemma) или z-loss/QK-norm, gradient clipping, политика точности, алерты на спайки лосса.
* **Проверка пропускной способности** на длинных прогонах и поведения даталоадера (упаковка, перемешивание, случайный доступ).
* **Семена:** единообразие сидов, особенно при tensor parallelism.

### Токенизатор и эмбеддинги
* **Размер словаря:** для английского часто достаточно ~50K; для мультиязычных моделей — 100K+. Слишком большой словарь увеличивает матрицу эмбеддингов (в малых моделях до ~20% параметров).
* **Tied embeddings (разделение весов входа/выхода):** в экспериментах Hugging Face на 1.2B tied давал сопоставимое качество при 18% меньшем числе параметров; в малых моделях untied при том же бюджете параметров давал худший лосс и эвалы.
* **BPE** остаётся стандартом; метрики «fertility» и «proportion of continued words» помогают оценивать эффективность токенизатора.

### Pre-training: цифры и пайплайн (OLMo 3)
* **Объёмы:** претрейн на **Dolma3 mix** — ~5.93T токенов (пул ~9T); мидтрейн — **100B** токенов (**Dolmino-2**); long-context extension — **50B** (7B) или **100B** (32B).
* **Состав претрейна (6T mix):** Common Crawl ~76%, olmOCR PDF ~13.6%, Stack-Edu (код) ~6.9%, arXiv ~0.86%, FineMath 3+ ~2.56%, Wikipedia/Wikibooks ~0.04%.
* **Трёхстадийная дедупликация:** (1) точная по хэшу документа — убирает ~67% копий; (2) нечёткая MinHash (Jaccard) — ещё ~23%; (3) substring/suffix-array — удаление повторяющихся подстрок (500+ байт), убирает ~14% байт. Инструмент **Duplodocus** (Rust) для exact + MinHash; в итоге число документов падает на ~75%.
* **Topic & quality:** разбиение по 24 топикам (WebOrganizer) и по вижинтилям качества (5-процентильные интервалы); отдельные **upsampling-кривые** по топикам: нижние 40% по качеству отбрасывают, верхние 5% апсэмплируют до 7× (интеграл кривой задаёт целевой объём токенов).
* **Token-constrained mixing и conditional mixing:** роевой поиск по смесям (много маленьких прокси-моделей с разными весами из Dirichlet), регрессия BPB по задачам, условное обновление микса при появлении новых доменов без перезапуска всего роя.

### Mid-training: методология (OLMo 3)
* **Микроаннеали (microanneals):** 5B токенов целевого датасета + 5B веб-токенов, сравнение с 10B только веб — быстрая оценка влияния источника на базовые эвалы.
* **Интеграционные тесты:** полные 100B прогоны по кандидатским миксам; чекпоинты затем проходят SFT и эвалы посттрейна. Пять раундов интеграций; финальный раунд — с **деконтаминацией**.
* **Деконтаминация:** пакет **decon** — поиск n-грамм из бенчмарков в документах; при совпадении выше порога документ удаляют. Учитывают все сплиты бенчмарков (в т.ч. train), т.к. часть эвалов использует увеличенные выборки.
* **Мета-рассуждения (meta-reasoning):** семь когнитивных категорий (self-awareness, evaluation, goal management, hierarchical organization, backward chaining, backtracking, conceptual reasoning); задачи типа «от ответа назад к задаче» или «отладка кода»; генерация трасс с GPT-4.1 / o4-mini.
* **Program-verifiable data:** задачи с детерминированной проверкой ответа через Python; фильтрация по верификатору. ~250M таких токенов в 5B микроаннеале дают +1–2 пункта по GSM8K и MBPP.

### Базовая архитектура и обучение (OLMo 3)
* **Контекст:** 8192 токена в претрейне и мидтрейне (в OLMo 2 было 4096).
* **Sliding Window Attention (SWA):** в трёх из каждых четырёх слоёв — окно 4096; в последнем слое всегда полное внимание.
* **Learning rate:** косинусное расписание; 7B: пик 3e-4, warmup 2000 шагов; 32B: пик 6e-4, warmup 2000. Финальный LR ~10% от пика.
* **Пропускная способность:** 7B — 7700 TPS/GPU, 32B — 1960 TPS/GPU при длине 8192, bfloat16; MFU ~43% и ~41%. Стек **OLMo-core**, FlashAttention-2, асинхронный чекпоинтинг.
* **Токенизатор:** тот же, что в OLMo 2 — производная от **cl100k** (OpenAI).

### Гиперпараметры посттрейна (OLMo 3)
* **SFT:** батч 1M токенов (7B) или 4M (32B), 2 эпохи, упаковка, макс. длина 32K; два эпохи; иногда **model souping** — линейное слияние двух чекпоинтов с разным LR.
* **DPO:** length-normalized DPO loss; sweep по learning rate и размеру датасета (ранняя остановка важна — после оптимума качество падает).
* **RL (Thinking 7B/32B):** LR 1e-6 / 2e-6, constant schedule; 32B: 750 шагов, group size 8, max response 32K, clip 0.2/0.272; акторы и лёрнер на разных узлах (32B: 64 GPU лёрнер, 160 GPU акторы). **RL-Zero:** 13.3K математических промптов после дедупликации и офлайн-фильтрации; простой промпт без `<think>`; проверка на **spurious rewards** (случайные награды не должны улучшать эвалы — иначе контаминация).

### Эвалы и скейлинг
* **Бенчмарки из отчётов:** знание — MMLU, GPQA Diamond, SimpleQA; математика — MATH, AIME, Minerva; код — HumanEval, MBPP, LiveCodeBench, SWE-bench Verified; длинный контекст — RULER, HELMET, MRCR; следование инструкциям — IFEval, IFBench, MultiChallenge; выравнивание — AlpacaEval, LMArena.
* **Scaling laws:** Chinchilla-стиль C ≈ 6·N·D; многие команды «переобучают» по токенам относительно оптимума (например, Qwen 3 — 36T). Итоговый выбор часто диктуется стоимостью инференса и степенью спарсенности.
* **Критический размер батча** растёт по ходу обучения; batch size warmup — в начале батч меньше, к концу — больше.

### Альтернативы чистому RL
* **Online DPO:** онлайновые предпочтения (генерация кандидатов + LLM-sудья); стабильнее RL, но зависит от качества и покрытия меток.
* **On-policy distillation:** студент сэмплирует ответы, учитель даёт логиты; обучение по KL между студентом и учителем. Дешевле GRPO (один сэмпл на промпт), в отчётах (Qwen3, Thinking Machines) даёт сильный буст и снижает катастрофическое забывание при дообучении.

### Безопасность и «типичные причины сбоев»
* **gpt-oss-120b:** фильтрация претрейна по CBRN (хим/био/рад/ядер); Preparedness-эвалы; тесты на jailbreak (StrongReject), иерархия инструкций (system > developer > user > assistant > tool).
* **Usual suspects при падении обучения:** слишком высокий LR; «плохие» батчи данных; дисбаланс нагрузки в MoE; проблемы хранилища/сети; плохая инициализация (в OLMo 2 переход на N(0, 0.02) улучшил стабильность); fp16 без осторожности. При спайках — пропуск проблемных батчей или усиление gradient clipping.

### Инфраструктура GLM-5 (кратко)
* **Multi-Token Prediction (MTP):** разделение параметров трёх MTP-слоёв при обучении для согласованности с инференсом (приём 2 токенов за шаг); выше accept length, чем у DeepSeek-V3.2.
* **Фреймворк slime:** кастомные rollout’ы (многошаговые агентные сценарии), PD disaggregation (prefill и decode на разных ресурсах), FP8 для rollout’ов, heartbeat-based fault tolerance.
* **General RL:** три измерения — foundational correctness, emotional intelligence, task-specific quality; гибридные награды — rule-based + outcome reward models (ORM) + generative reward models (GRM); человеческие ответы как якоря стиля.
* **Память и параллелизм:** flexible MTP placement, Pipeline ZeRO2 для градиентов, zero-redundant communication для Muon, activation offloading, sequence-chunked output projection; INT4 QAT на этапе SFT.
* **GLM-5 масштаб и DSA:** объём обучения до **28.5T** токенов (744B параметров). **DSA (DeepSeek Sparse Attention):** замена плотного O(L²) внимания динамическим выбором важных токенов; continued pre-training из плотной базы: warmup 1000 шагов (только indexer), затем 20B токенов совместной адаптации; DSA снижает вычисления внимания примерно в 1.5–2× для длинных последовательностей, 128K контекст при существенно меньших затратах GPU.

### Thinking SFT: источники данных и фильтрация (OLMo 3)
* **Промпты и трассы:** математика — OpenThoughts3 (16× повтор, полные решения) и SYNTHETIC-2 (verified); неполные трассы дорегенерируют через QwQ-32B до 32K токенов. Код — AceCoder, The Algorithms (Python), Nemotron Post-training, OpenCodeReasoning; до 16 ответов на промпт от QwQ-32B, фильтр по синтетическим тестам от GPT-4.1. Чат и безопасность — WildChat (Tulu 3 и вне Tulu 3), OpenAssistant; трассы от DeepSeek R1. Instruction following — Tulu 3 + verifiable constraints (Pyatkin), Persona IF с Nemotron-Personas; верификация ответов. Наука и прочее — OpenThoughts3 science, TableGPT, Aya; регенерация неполных и генерация с DeepSeek R1.
* **Фильтрация SFT:** (1) лицензии (некоммерческие/неясные отбрасывают); (2) неполные цепочки рассуждений; (3) доменная корректность (проверка ограничений для IF, прогон тестов для кода); (4) упоминания других разработчиков моделей и даты; (5) избыточное повторение; (6) избыток китайских символов или политически окрашенных формулировок в трассах. **Топик-фильтрация:** классификация по OpenAI query taxonomy; отсев/даунсэмплинг нерелевантных топиков (генерация изображений, приветствия) улучшает поведение. Детали и ссылки на скрипты — в open-instruct и приложении отчёта.
* **Смешивание и деконтаминация:** методология как в мидтрейне — параллельный сбор, общие стандарты микса, несколько раундов интеграционных тестов; «базовый» микс 100K примеров из OpenThoughts3, затем добавление до 100K из каждой категории для абляций. Деконтаминация посттрейна: Tulu 3 procedure, 8-граммы, порог перекрытия 0.5; эвристики против ложных срабатываний (игнор общих фраз, в математике — игнор n-грамм из коротких токенов). **Model souping:** финальный чекпоинт Thinking SFT — линейная взвешенная слияние двух чекпоинтов с разным LR через mergekit.

### Delta Learning и DPO для рассуждений (OLMo 3)
* **Идея:** качество предпочтений определяется в первую очередь *дельтой* между выбранным и отвергнутым ответом, а не абсолютным качеством каждого. Парные данные (x, y_c, y_r) с явным контрастом по способностям дают выигрыш даже когда SFT по y_c уже не помогает или вредит.
* **Dolci-Think-DPO:** выбранные ответы — Qwen 3 32B (thinking), отвергнутые — Qwen 3 0.6B (thinking); один сильный и один слабый модель дают стабильный контраст. Пул промптов — из Dolci-Instruct SFT плюс DaringAnteater и UltraFeedback из набора предпочтений OLMo 2 7B. Для Instruct-DPO добавляют delta-aware GPT-judge пары и multi-turn предпочтения (self-talk и synthetic-context), при этом контролируют length bias (ограничение разницы по длине до 100 токенов для чата и multi-turn).

### Instruct: function-calling и предпочтения (OLMo 3)
* **Траектории с реальными инструментами:** Science QA (ASTA/ASC MCP, Semantic Scholar); Web search QA (DR Tulu, Serper API, HotpotQA, TaskCraft, WebWalkerQA, SearchArena, OpenScholar); фильтрация запросов через GPT-5 — остаются только те, что требуют поиска и допускают верифицируемый длинный ответ. **SimFC:** синтетические траектории с LLM-симулированной средой по пулу API (xLAM, ToolACE, публичные MCP); разнообразие сценариев — multi-turn, multi-step, отказы из-за недостатка информации.
* **Формат:** OpenAPI для описания инструментов, вызовы функций — в виде pythonic code blocks; спецификации в system prompt, выводы среды — в отдельной роли; расширение словаря спецтокенами под теги. Оценка: BFCLv3 (intrinsic function calling), LitQA2 (ASC), SimpleQA (поиск/браузинг); No-Tools режим для сравнения.
* **Старт от Thinking SFT:** обучение Instruct SFT с «тёплого старта» от Thinking SFT значительно улучшает метрики при сохранении коротких ответов без следов thinking. **Предпочтения:** комбинация delta-learning эвристики и delta-aware GPT-judge пар лучше любой одной сигнатуры; DPO по объёму данных даёт U-образную кривую — после оптимума качество падает (важна ранняя остановка и sweep по LR и размеру датасета). RL для Instruct: менее сложные math/code датасеты, без офлайн-фильтрации по сложности; max response 8K (7B) или 16K (32B); два кандидата DPO (лучший по среднему и лучший по «vibe test»), финал по среднему, длине и vibe test.

### Long-context extension (OLMo 3)
* **Объёмы и микс:** пул **longmino** 600B+ токенов; в обучении 34% long-context и 66% короткого из Dolmino-2. Для 7B — 50B токенов этапа расширения, для 32B — 100B. Сравнение: SmolLM3/GLM 4.5/DeepSeek V3 ~100–123B, Apertus 225B, Kimi K2 400B, Llama 3.1 800B; AFM и Nemotron Nano 2 — менее 20B токенов до 64K/128K.
* **Данные:** основа — olmOCR PDF; фильтрация по gzip-compressibility (отсекают 20% наиболее и наименее сжимаемых). Синтетическая аугментация в духе CLIPPER: разбиение документов 32K–65K на секции 8K–32K, tf-idf по существительным, по 8 сниппетов на фразу, генерация агрегационных задач (CWE — подсчёт вхождений, REX — переписывание в виньетках) через OLMo 2 Instruct 32B.
* **Рецепт:** YaRN применяют только к слоям с полным вниманием (не к sliding window). **Document packing:** best-fit упаковка документов. **Intra-document masking:** токены из одного документа не смешивают с другими в одной последовательности. **Инфраструктура:** 8-way context parallelism (по 8K на устройство), all-gather CP attention. **Model souping (32B):** слияние последних трёх чекпоинтов расширения (шаги 10K, 11K, 11 921). Эвалы: RULER — основная метрика разработки; HELMET — held-out.

### RL-Zero: данные, промпты, активный сэмплинг (OLMo 3)
* **Данные:** математика — агрессивная фильтрация DAPO Math, Klear-Reasoner Math, Open-Reasoner-Zero, Omega; дедупликация DAPO, только английский; семантическая кластеризация по Klear/Orz/Omega, один представитель на кластер + DAPO. Деконтаминация от претрейна и эвалов; офлайн-фильтрация — промпты, полностью решённые в 8 из 8 сэмплов финальной базой, удаляют. Итого **13.3K** математических промптов. Код, IF и общий чат — субсэмплирование из Dolci-Think-RL.
* **Промпт и эвалы:** простой шаблон без `<think>` сильно выигрывает у стандартных посттрейновых шаблонов при обучении от чистой базы (Dolmino не содержал специальной разметки). Эвалы очищают от спецформатирования (например \boxed{}) для сближения с тренировочными промптами. Длина ответа при обучении 16K, при эвале 32K и temperature 1.0 для pass@k.
* **Active sampling:** непрерывное подтягивание пар промпт–ответ из очереди после фильтра по ненулевому advantage сохраняет полный батч и стабилизирует обучение (меньше дисперсии лосса). **Spurious rewards:** обучение со случайными бинарными наградами не должно улучшать эвалы; если улучшает — признак контаминации. В OLMo 3 RL-Zero случайные награды не дают прироста — деконтаминация подтверждена.

### Формула quality-aware upsampling (OLMo 3)
* Семейство **усечённых степенно-экспоненциальных кривых** f(x): нуль ниже порога a, выше — C(x−a)^p·e^{λ(x−a)}. Ограничения: интеграл по [0,1] равен целевому отношению Z/X; максимальный средний апсэмплинг по любому бакету не выше M; монотонность. В практике: M=7, отбрасывают нижние 40% по качеству (a=0.4); по каждому топику WebOrganizer численно подбирают p, λ, C. Комбинация с token-constrained mixing: лучше всего показало семейство truncated power-exponential по сравнению с только mixing, только upsampling, арифметическим или геометрическим усреднением целей.

### Деконтаминация (decon): реализация
* Поиск n-грамм из эвалов в документах; по каждому шагу обхода получают step set совпадающих документов, пересечение с **active set**. Документ выбывает из active set после **11 промахов** подряд. Итог — отображение id документа в множество уникальных совпавших n-грамм. Детали — в репозитории [allenai/decon](https://github.com/allenai/decon).

### Эвалы посттрейна: дисперсия (OLMo 3)
* **Высокая дисперсия:** GPQA, AlpacaEval 3, IFEval. **Стабильные:** ZebraLogic, Omega, AIME 24 (Avg@32), HumanEvalPlus, AgiEval, BigBenchHard. **Очень стабильные:** LiveCodeBench (Avg@10), MBPPPlus, MATH, MMLU, PopQA. Стоимость эвалов на этапе разработки рецепта 7B — около 10–20% вычислительного бюджета.

---

## Использованные источники и ссылки на технические отчеты

В основу данной статьи легли материалы следующих технических отчетов и публикаций:

1. **GLM-5: from Vibe Coding to Agentic Engineering** (Zhipu AI & Tsinghua University, 2026) — [arXiv:2602.15763](https://arxiv.org/abs/2602.15763)
2. **Kimi K2.5: visual agentic intelligence** (Moonshot AI, 2026) — [arXiv:2602.02276](https://arxiv.org/abs/2602.02276)
3. **OLMo 3 Technical Report** (Allen Institute for AI, 2026) — Материалы по обучению моделей серии OLMo 3, включая подробные рецепты для стадий Base, Instruct, Thinking и RLZero.
4. **Frontier model training methodologies** (Alex Wa’s Blog, Jan 2026) — Подробный сводный анализ тренировки передовых моделей с открытыми весами. Включает разбор пайплайнов таких моделей, как Hugging Face **SmolLM3**, Prime Intellect **Intellect-3**, Nous Research **Hermes 4**, OpenAI **gpt-oss-120b** и Arcee **Trinity series**.
5. **DeepSeek-V3.2: pushing the frontier of open large language models** (DeepSeek-AI, 2025) — [arXiv:2512.02556](https://arxiv.org/abs/2512.02556)
6. **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning** (DeepSeek-AI, 2025) — [arXiv:2501.12948](https://arxiv.org/abs/2501.12948)