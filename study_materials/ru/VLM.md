# VLM: что нужно знать о строении и обучении

Подробный учебный материал о моделях «визуальный язык» (Vision-Language Models, VLM): из чего они состоят, как обрабатывают изображения и текст, как их тренируют и как ими пользоваться. Материал рассчитан на **полных новичков**: все термины поясняются при первом появлении, в начале есть расширенный словарик, по ходу — пошаговые примеры с числами и схемами. Если что-то останется непонятным — можно вернуться к словарику или к связанным разделам. Текстовую «половинку» VLM (как генерируется ответ) подробно разбирает [LLM.md](LLM.md); основы архитектуры внимания — [transformer.md](transformer.md).

---

## Что нужно знать заранее (минимум)

Чтобы материал был понятен с нуля, достаточно такой картины:

- **Языковая модель (LLM)** умеет по тексту (промпту) дописывать текст по одному «кусочку» — токену (подслову). Внутри она работает не с буквами, а с **векторами** — наборами чисел фиксированной длины. Подробнее — в [LLM.md](LLM.md).
- **Вектор** — это просто список чисел, например 768 или 4096 чисел подряд. Нейросети на входе и выходе работают с такими числами; «смысл» вектора модель выучивает сама при обучении.
- **Обучение нейросети** — это подбор параметров (весов), чтобы на примерах «вход → правильный выход» функция потерь (loss) становилась меньше. Чем меньше loss, тем лучше модель предсказывает то, что мы считаем правильным (например подпись к картинке).

Если вы никогда не сталкивались с LLM или нейросетями — не страшно: ниже все нужные понятия введены и разобраны по шагам. Словарик в начале можно перечитывать по ходу чтения.

---

## Краткий словарик терминов

Эти понятия часто встречаются в материале. Не обязательно учить их наизусть — можно возвращаться к таблице по ходу чтения.

| Термин | Простое объяснение |
|--------|---------------------|
| **VLM (Vision-Language Model)** | Модель, которая принимает **изображение и/или текст** и выдаёт **текст** (ответ, подпись, описание). Объединяет «зрение» и «язык» в одном пайплайне. |
| **Vision Encoder** | Нейросеть, которая превращает **изображение** в набор **векторов** (эмбеддингов). Обычно это ViT (Vision Transformer) или CNN; на выходе — последовательность «визуальных токенов». |
| **Вектор** | Набор чисел фиксированной длины (например 768 или 4096 чисел). В контексте VLM: представление одного патча картинки или одного текстового токена; модель везде работает с векторами. |
| **Патч (patch)** | Квадратный кусок изображения (например 14×14 или 16×16 пикселей), который считается одной «единицей» входа для ViT. Изображение разбивается на сетку патчей; каждый патч потом превращается в один вектор. |
| **Image token (визуальный токен)** | Один вектор после vision encoder (или после проекции). Для LLM внутри VLM это такой же «токен», как текстный, только он кодирует кусок картинки. Количество таких токенов зависит от разрешения и размера патча. |
| **Проекция (projection)** | Слой (линейный или MLP), который переводит **векторы от vision encoder** в **пространство эмбеддингов LLM**. Нужен, потому что энкодер и LLM обучены в разных пространствах (разная размерность и разный «смысл» координат). |
| **Caption (подпись)** | Текстовое описание изображения. В данных для обучения VLM часто используют пары (изображение, подпись); модель учится предсказывать подпись по картинке. |
| **Loss (функция потерь)** | Одно число, показывающее, насколько предсказание модели отличается от правильного ответа. Обучение = минимизация loss по множеству примеров. В VLM обычно используют cross-entropy по токенам (подробнее в разделе про обучение и в [LLM.md](LLM.md)). |
| **Заморозить (freeze)** | Не обновлять веса части модели при обучении. Например «заморозить энкодер» — оставить его веса фиксированными и обучать только проекцию и LLM. Экономит память и снижает риск «сломать» уже обученный блок. |
| **Multimodal** | Мультимодальный — работа с несколькими типами входа: изображение + текст, видео + текст и т.д. VLM — мультимодальная модель. |
| **Q-Former** | Модуль в архитектуре (например BLIP-2): небольшой Transformer с обучаемыми query-векторами, который «выжимает» из выхода энкодера изображения **фиксированное** число токенов для LLM. Уменьшает число визуальных токенов и стоимость вычислений. |
| **Interleaved** | Чередование: последовательность вида [image_tokens, text_tokens, image_tokens, text_tokens, ...]. Позволяет вставлять несколько картинок и текст в одном контексте. |
| **Image-text alignment** | Выравнивание изображения и текста в общем пространстве представлений. Достигается на этапе pretraining (обучение предсказывать подпись по картинке). |
| **LoRA** | Способ дообучения: к части слоёв добавляют маленькие «дополнительные» матрицы низкого ранга и обучают только их, основные веса замораживают. Позволяет подстраивать большую модель под свои данные при малом числе параметров и памяти. Подробнее в [LLM.md](LLM.md). |

---

## Что такое VLM

**VLM (Vision-Language Model)** — это модель, которая принимает на вход **изображение** (или несколько) и **текст** (вопрос, инструкция, контекст) и выдаёт **текст** (ответ, подпись, рассуждение). То есть это «LLM с глазами»: к языковой модели добавляется модуль, превращающий картинку в последовательность векторов, которые подаются в ту же модель, что и текстовые токены. Генерация ответа остаётся **авторегрессионной** — по одному токену за раз, как в обычной LLM.

```
  Общая схема VLM: изображение + текст → текст

  Изображение                    Текст (вопрос/промпт)
       │                              │
       ▼                              ▼
  ┌─────────────┐              Токенизатор
  │   Vision    │                    │
  │   Encoder   │                    ▼
  └──────┬──────┘              [ tok₁  tok₂  ...  tok_m ]
         │                              │
         ▼                              │
  [ img₁  img₂  ...  img_n ]            │
         │                              │
         ▼                              │
  ┌─────────────┐                       │
  │  Projection │                       │
  └──────┬──────┘                       │
         │                              │
         ▼                              ▼
  [ v₁   v₂   ...  v_n ]  +  [ t₁   t₂   ...  t_m ]
         │                              │
         └──────────────┬───────────────┘
                        ▼
              Объединённая последовательность
              [ v₁ ... v_n  t₁ ... t_m ]  (или interleaved)
                        │
                        ▼
              ┌─────────────────┐
              │   LLM (как в    │  ← те же блоки Transformer, что в LLM
              │   LLM.md)       │
              └────────┬────────┘
                       ▼
              Генерация ответа по одному токену (авторегрессия)
```

### Зачем нужны VLM

- **Вопросы по картинке:** «Что на фото?», «Сколько объектов?», «Опиши детали».
- **OCR и таблицы:** извлечение текста с изображений, ответы по графикам и схемам.
- **Рассуждение по визуалу:** по графику, диаграмме или скриншоту кода дать объяснение или решение.
- **Робототехника и авто:** понимание сцены по кадру с камеры.

VLM не заменяют классические LLM для чисто текстовых задач, но расширяют их на мир изображений (и при необходимости видео, если кадры обрабатывать как набор картинок).

### Почему нельзя просто подать картинку в LLM?

Возможный вопрос: раз LLM умеет обрабатывать последовательность токенов, почему бы не превратить каждый пиксель (или каждый кусок картинки) в «токен» и не подать в LLM как есть?

Причины такие:

1. **Объём.** Одна картинка 224×224 в RGB — это 224×224×3 = **150 528 чисел**. Обычная LLM рассчитана на последовательности в тысячи токенов (например 2k–32k), причём каждый токен — это один вектор (скажем, 4096 чисел). Если бы каждый пиксель был отдельной позицией, последовательность получилась бы огромной: LLM не обучена на такой длине, да и память/время взлетели бы.

2. **Другой «язык».**
   LLM обучена на **текстовых** эмбеддингах: числа в векторах кодируют смысл слов и подслов. Сырые пиксели (яркость красного, зелёного, синего в каждой точке) — это совсем другое пространство: там нет «словаря» и нет той структуры, к которой привыкла LLM. То есть даже если бы мы как-то впихнули пиксели в модель, она не умела бы с ними работать так же, как с текстом.

3. **Иерархия признаков.** Сначала картинку полезно «осмыслить»: выделить края, объекты, композицию. Этим как раз занимается **vision encoder** — он превращает пиксели в более высокоуровневые представления (векторы патчей). Уже эти векторы можно связать с языком через **проекцию** и подать в LLM как «токены изображения».

Итого: картинку сначала переводим в ограниченное число векторов (через энкодер и при необходимости сжатие), затем эти векторы приводим к «языку» LLM (проекция), и только потом подаём в LLM вместе с текстом.

---

## Архитектура: из чего состоит VLM

Три основных блока:

1. **Vision Encoder** — превращает изображение в последовательность векторов.
2. **Projection (проекция)** — переводит эти векторы в пространство эмбеддингов LLM.
3. **LLM** — та же архитектура, что у языковой модели (Transformer decoder): принимает объединённую последовательность «визуальных» + текстовых токенов и генерирует текст.

Ниже — по каждому блоку.

---

### Vision Encoder: от пикселей к векторам

**Задача:** по матрице пикселей изображения (например H×W×3) получить последовательность векторов фиксированной размерности (например 768 или 1024). Эти векторы потом станут «визуальными токенами» для LLM. Напоминание: **вектор** — это просто список чисел заданной длины; нейросеть на каждом шаге преобразует одни векторы в другие.

#### Зачем разбивать на патчи, а не подавать всё изображение целиком?

- Одна картинка 224×224×3 даёт **150 528** чисел. Обрабатывать такую длинную последовательность в Transformer дорого (память и время растут квадратично с длиной). Если разбить картинку на патчи 16×16, получится **196 кусочков** по 16×16×3 = 768 чисел каждый — то есть 196 «элементов» вместо 150 тысяч. Дальше каждый патч мы превращаем в один вектор (например длины 1024). В итоге у нас **196 векторов** — с таким числом позиций модель уже может работать.
- Патчи сохраняют **локальность**: один вектор описывает небольшой кусок изображения (уголок глаза, часть колеса и т.д.). Transformer потом через self-attention связывает эти кусочки между собой и собирает целостную картину.

#### Подход 1: Vision Transformer (ViT)

Самый распространённый вариант в современных VLM.

**Шаги по порядку:**

1. **Разбиение на патчи.** Изображение режется на квадратные патчи размера P×P (типично 14×14 или 16×16). Например при разрешении 224×224 и P=16 получается сетка 14×14 = **196 патчей**. Каждый патч — маленькая картинка 16×16×3 (768 чисел, если считать пиксели подряд).

2. **Линейная проекция патча (Patch Embedding).** Каждый патч разворачивается в вектор длины P×P×3 (например 16×16×3 = 768), затем проходит через линейный слой до размерности `encoder_hidden_size` (например 768 или 1024). Итог: каждый патч → один вектор.

3. **Позиционные эмбеддинги.** Как в Transformer, порядок важен. К векторам патчей добавляют обучаемые позиционные эмбеддинги (или 2D-позиции).

4. **Дополнительный [CLS]-токен (опционально).** В классическом ViT добавляют один специальный токен в начало; его итоговый вектор часто используют для классификации. В VLM иногда используют все токены патчей без [CLS].

5. **Transformer-блоки.** Последовательность векторов пропускается через несколько слоёв self-attention + FFN (как в [transformer.md](transformer.md)). Выход — последовательность той же длины.

```
  ViT: изображение → последовательность векторов

  Изображение 224×224×3
        │
        ▼  разбиение на патчи 16×16
  ┌────┬────┬────┬───┐
  │ p₁ │ p₂ │ p₃ │...│  14×14 = 196 патчей
  ├────┼────┼────┼───┤
  │    │    │    │   │
  └────┴────┴────┴───┘
        │
        ▼  Linear: каждый p_i (768 чисел) → вектор (1024)
  [ v₁   v₂   v₃  ...  v₁₉₆ ]  + позиционные эмбеддинги
        │
        ▼  Transformer blocks (attention + FFN)
  [ h₁   h₂   h₃  ...  h₁₉₆ ]   выход vision encoder
```

**Разрешение и число токенов:** при разрешении 336×336 и P=14 получается (336/14)² = 576 токенов. Чем выше разрешение, тем больше визуальных токенов и тем дороже последующий LLM (память и время растут с длиной последовательности). Поэтому часто используют 224–336 по короткой стороне или адаптивное разбиение.

**Пример с числами (для закрепления):** картинка 224×224, патчи 16×16 → 196 патчей. Каждый патч после линейного слоя → вектор длины 1024 (encoder_hidden_size). После Transformer-блоков на выходе энкодера мы имеем **196 векторов по 1024 числа** — это и есть «сырое» представление картинки. Эти 196 векторов потом нужно перевести в пространство LLM (проекция) и подставить в начало последовательности вместо «текстовых» токенов.

#### Подход 2: CNN (редко в новых VLM)

Ранние модели иногда использовали ResNet или другой CNN: карта признаков (feature map) либо глобально усреднялась (один вектор на картинку), либо разбивалась на регионы. Сейчас в большинстве открытых VLM стоит ViT (или его варианты: ViT-L, ViT-g, SigLIP и т.д.).

#### Подход 3: Энкодер из CLIP / SigLIP

Многие VLM не обучают энкодер с нуля, а берут **замороженный** vision encoder из CLIP или SigLIP — моделей, уже обученных на выравнивании изображение–текст. Плюсы: сильные визуальные представления, меньше параметров для обучения. Минусы: энкодер не заточен под конкретную задачу VLM; при необходимости его можно потом разморозить и дообучить.

**Итог:** на выходе vision encoder имеем последовательность из N векторов размерности `encoder_hidden_size` (например N=256, размерность 1024). Это «сырые» визуальные представления.

---

### Projection: визуальные векторы в пространство LLM

**Почему нельзя взять выход vision encoder и сразу подать его в LLM?**

- **Разная размерность.** Энкодер часто выдаёт векторы длины 768, 1024 или 1280 (своя `encoder_hidden_size`). LLM внутри работает с векторами длины, например, 4096 (свой `hidden_size`). У каждой позиции на входе LLM должен быть вектор именно этой длины — иначе матрицы внимания и линейных слоёв не сойдутся по размеру.
- **Разный «язык».** Даже если бы размерности совпадали, числа в векторах энкодера кодируют визуальные признаки (края, текстуры, объекты), а эмбеддинги в LLM кодируют смысл **слов**. Чтобы LLM могла «понять» картинку как часть одного контекста с текстом, нужно перевести визуальные векторы в то же пространство, где живут текстовые эмбеддинги. Этим и занимается **проекция**: один линейный слой (или MLP) переводит вектор из пространства энкодера в вектор длины `hidden_size` LLM. После проекции для LLM визуальные токены выглядят «как свои» — та же размерность и после обучения согласованный смысл.

**Проекция** — это слой (или небольшая сеть), который переводит каждый вектор от энкодера в вектор размерности **hidden_size** LLM. После этого визуальные токены можно подставить в начало последовательности вместе с текстовыми.

**Варианты реализации:**

1. **Один линейный слой:** `proj(x) = x W + b`, где W имеет размер `encoder_hidden_size × hidden_size`. Быстро и мало параметров.

2. **MLP (2–3 слоя):** несколько линейных слоёв с активацией (GELU, SiLU). Гибче, чуть больше параметров.

3. **Q-Former (BLIP-2):** не просто линейная проекция. Вводится набор **learnable query векторов** (например 32 штуки). Они проходят через Transformer, где делают cross-attention к выходам vision encoder. В итоге из N визуальных векторов получается **фиксированное** число векторов (по числу query), которые затем проецируются в пространство LLM. Плюс: сильное сжатие (много патчей → мало токенов для LLM), меньше стоимость. Минус: сложнее и больше параметров в проекции.

4. **Resampler (в части современных моделей):** по смыслу похож на Q-Former — сжатие множества визуальных токенов в фиксированное число через внимание с обучаемыми запросами.

```
  Проекция: два варианта

  Вариант A (линейная / MLP):
  [ h₁  h₂  ...  h_N ]  (encoder_hidden_size)
         │
         ▼  для каждого h_i: Linear или MLP
  [ v₁  v₂  ...  v_N ]  (hidden_size LLM)   ← N визуальных токенов

  Вариант B (Q-Former / Resampler):
  [ h₁  h₂  ...  h_N ]  (выход энкодера)
         │
         ▼  learnable queries + cross-attention к h₁..h_N
  [ q₁  q₂  ...  q_K ]  (K фиксировано, например 32)
         │
         ▼  Linear
  [ v₁  v₂  ...  v_K ]  (hidden_size LLM)   ← меньше токенов (K < N)
```

После проекции визуальные токены **ничем не отличаются для LLM** от текстовых с точки зрения формата: это последовательность векторов размерности hidden_size. Им присваиваются позиции (часто подряд в начале последовательности), и дальше они участвуют в self-attention и FFN как обычные токены.

---

### LLM: общая часть с языковой моделью

Текстовая часть VLM — это **та же архитектура**, что и у обычной LLM: токенизатор, token + positional embedding, стопка Transformer-блоков (causal self-attention + FFN), LM head. Подробно см. [LLM.md](LLM.md).

**Отличие только в том, откуда берётся входная последовательность:**

- В чистой LLM: только текстовые токены (эмбеддинги из словаря).
- В VLM: сначала идут **визуальные токены** (результат проекции), затем **текстовые** (вопрос, инструкция, системный промпт и т.д.). Модель «видит» и картинку, и текст в одном контексте и генерирует ответ авторегрессионно.

```
  Вход в LLM внутри VLM:

  Позиции:    1    2   ...   N    N+1   N+2   ...   N+M
  Содержимое: v₁   v₂  ...  v_N   t₁    t₂    ...  t_M
              ↑________________↑   ↑____________________↑
              визуальные токены   текстовые токены (вопрос)
              (проекция)          (embedding из словаря)

  Дальше — как в LLM: causal attention, FFN, LM head; генерация по одному токену.
```

**Спецтокены для изображений:** в чат-моделях часто вводят токены вида `<image>` или `<img>`, которые в последовательности заменяются на соответствующие визуальные токены (несколько позиций под одну «картинку»). Так формат диалога остаётся единым: пользователь может написать «Опиши это: <image>», и модель знает, куда подставить эмбеддинги изображения.

---

## Как изображение превращается в токены (итоговая схема)

1. **Ресайз и нормализация.** Изображение приводят к размеру, ожидаемому энкодером (например 224×224 или 336×336). **Нормализация** — приведение значений пикселей (обычно 0–255) к тому виду, на котором обучался энкодер: по каждому каналу (R, G, B) вычитают среднее и делят на стандартное отклонение (часто берут константы из датасета ImageNet). Так энкодер получает вход в привычном для него диапазоне.

2. **Патчи + ViT.** Разбиение на патчи → patch embedding → позиции → Transformer-блоки. Выход: N векторов (N = (H/P)×(W/P) при одном масштабе).

3. **Проекция.** N векторов → N (или K при Q-Former) векторов размерности hidden_size LLM.

4. **Объединение с текстом.** Текст токенизируется; эмбеддинги текста получают из таблицы token embedding. В последовательности сначала идут визуальные токены, затем текстовые (или в формате interleaved, если картинок несколько). Позиционные индексы назначаются подряд (визуальные занимают позиции 1..N, текст — N+1..N+M и т.д.).

5. **Позиционные эмбеддинги.** Для объединённой последовательности применяют RoPE (или аналог) по позициям — так же, как в LLM.

Дальше — стандартный forward LLM и генерация ответа.

```
  Полный пайплайн VLM (один проход)

  Изображение [H×W×3]
        │
        ▼  resize, normalize
  ┌─────────────┐
  │ Vision Enc │  ViT: patches → [h₁..h_N]
  └──────┬──────┘
         │
         ▼
  ┌─────────────┐
  │ Projection  │  [h₁..h_N] → [v₁..v_K]  (K=N или K фиксировано)
  └──────┬──────┘
         │
  Текст "Опиши изображение."
         │
         ▼  токенизатор → token embedding
         [ t₁  t₂  ...  t_m ]
         │
         ▼  конкатенация
  [ v₁  ...  v_K  t₁  ...  t_m ]
         │
         ▼  + positional embedding (RoPE и т.д.)
  ┌─────────────────────────────────┐
  │  Transformer blocks (LLM)       │
  └─────────────────────────────────┘
         │
         ▼  LM head на последней позиции
  Логиты → сэмплирование → следующий токен ответа → авторегрессия до EOS
```

---

## Разбор на одном примере (пошагово)

Чтобы всё сложилось в одну картину, проследим, что происходит от начала до конца на **одном** запросе: одна картинка и один вопрос.

**Вход:** изображение кота на диване (файл `cat.jpg`) и вопрос пользователя: «Что на фото?».

1. **Подготовка изображения.** Картинку загружают, ресайзят до 224×224 (или до того размера, который ожидает модель), нормализуют пиксели (вычитают среднее, делят на стандартное отклонение — часто по статистике ImageNet). На выходе — матрица 224×224×3.

2. **Vision encoder.** Изображение режут на патчи 16×16 → 196 патчей. Каждый патч превращают в вектор 1024 (patch embedding + позиции), пропускают через Transformer-блоки ViT. Получаем **196 векторов длины 1024** — это «визуальные» представления кусочков картинки.

3. **Проекция.** Каждый из 196 векторов пропускают через линейный слой (или MLP): 1024 → 4096 (если hidden_size LLM = 4096). Получаем **196 векторов длины 4096** — теперь они в том же пространстве, что и текстовые эмбеддинги LLM.

4. **Текст вопроса.** Строку «Что на фото?» токенизатор разбивает на токены (например «Что» « на» « фото» «?»). Каждому токену сопоставляется id из словаря. По id из таблицы token embedding достают векторы длины 4096. Допустим, получилось 4 токена → 4 вектора.

5. **Сборка последовательности.** Конкатенируем: сначала 196 визуальных токенов, потом 4 текстовых. Итого **200 позиций**, каждая — вектор 4096. Позициям присваиваются индексы 1…200; к ним применяются позиционные эмбеддинги (например RoPE), как в обычной LLM.

6. **Forward LLM.** Эти 200 векторов подаются в стопку Transformer-блоков (causal self-attention + FFN). На **последней** позиции (200-й — конец вопроса «Что на фото?») получаем контекстное представление. Через LM head из него получают логиты по всему словарю (десятки тысяч чисел).

7. **Генерация ответа.** По логитам считают распределение вероятностей по токенам (softmax), сэмплируют первый токен ответа — например «На». Этот токен добавляют к последовательности (теперь 201 позиция), снова считают forward (или используют KV-cache и считают только для новой позиции), получают следующий токен — « фото» — и так далее, пока модель не выдаст токен конца (EOS) или не достигнут лимит длины. Итоговый ответ: «На фото кот спит на диване.» (или в таком духе).

Важно: на шагах 6–7 модель «видит» и 196 визуальных токенов, и текст вопроса, и уже сгенерированные слова ответа — всё в одной последовательности. Attention позволяет каждой позиции «смотреть» на все предыдущие, в том числе на токены картинки, поэтому ответ может опираться на содержимое изображения.

```
  Один запрос: числа для ориентира

  Изображение 224×224  →  ViT  →  196 векторов × 1024
                                    →  проекция  →  196 векторов × 4096
  "Что на фото?"       →  токены  →  4 вектора × 4096

  Конкатенация: [ 196 визуальных | 4 текстовых ] = 200 позиций × 4096
       ↓
  Transformer (LLM)  →  на позиции 200: логиты по словарю
       ↓
  Сэмпл токена  →  "На"  →  добавляем к последовательности  →  повторяем
       ↓
  Ответ: "На фото кот спит на диване."
```

---

## Как тренируют VLM

Обучение VLM обычно идёт в несколько этапов, по идее похожих на LLM: сначала «базовое» выравнивание изображение–текст, затем инструкции/диалог, при необходимости — RL/выравнивание по предпочтениям.

---

### Этап 1: Pretraining (обучение связки изображение–текст)

**Цель:** научить модель связывать изображение с текстом. Обычно это **предсказание подписи (caption)** по картинке: на вход — изображение, на выход — подпись. Модель учится предсказывать следующий токен подписи по картинке и предыдущим токенам. **Loss** (функция потерь) — cross-entropy по токенам подписи: на каждой позиции сравниваем, какой токен модель предсказала (распределение по словарю), с реальным следующим токеном; чем ближе предсказание к правильному токену, тем меньше вклад в loss. Суммируем по всем позициям подписи и по всем примерам в батче, усредняем — получаем одно число. Обучение = подбор весов так, чтобы это число уменьшалось. Подробнее про cross-entropy и next-token prediction — в [LLM.md](LLM.md).

**Как выглядит один пример в данных:**

- Обычно это пара: **файл изображения** (например `train_001.jpg`) и **текстовая подпись** (например «Кот спит на красном диване у окна»). Датасет — это большой список таких пар. При загрузке батча берут, скажем, 32 пары: 32 картинки и 32 подписи. Каждую картинку пропускают через vision encoder и проекцию; каждую подпись токенизируют. В последовательность для LLM собирают: [визуальные токены этой картинки] + [токены подписи]. Модель учится по этим токенам подписи предсказывать следующий токен; loss считают только по позициям подписи (по визуальным токенам loss не считают — мы не «предсказываем» картинку по тексту в этой задаче).

**Данные:**

- Большие датасеты пар (изображение, текст): COCO, LAION, Conceptual Captions, внутренние подписи и т.д.
- Текст может быть короткой подписью или развёрнутым описанием. Качество и разнообразие подписей сильно влияют на результат.

**Что обучают (кто «заморожен», кто обновляется):**

- **Вариант A:** обучают только **проекцию**; vision encoder и LLM **заморожены** (их веса не обновляются, градиенты через них не идут). Быстро и дёшево по памяти; модель учится только «переводить» уже готовые визуальные векторы в пространство LLM. Часто используют, когда энкодер и LLM уже очень хорошие (например CLIP + LLaMA), и нужно лишь «соединить» их.
- **Вариант B:** обучают проекцию и **LLM**; vision encoder заморожен. LLM дообучается предсказывать подпись по визуальным токенам. Нужно больше данных и шагов, но качество обычно выше.
- **Вариант C:** обучают все три блока (энкодер + проекция + LLM). Максимальная гибкость, но больше риска переобучения, нужны большие ресурсы и аккуратный learning rate для энкодера (часто его обучают с меньшим LR).

**Формат входа:**

- Изображение → vision encoder → проекция → визуальные токены.
- Текст подписи токенизируется. В последовательности: [визуальные токены] + [токены подписи]. Loss считают **только по токенам подписи** (causal mask: при предсказании i-го токена подписи модель видит визуальные токены и предыдущие токены подписи). Маскирование — как в SFT для LLM: позиции до начала подписи не участвуют в loss.

```
  Pretraining VLM: один пример

  Изображение → [ v₁  v₂  ...  v_K ]
  Подпись:     "Кот спит на диване."
  Токены:        t₁   t₂   t₃   t₄   t₅   t₆   t₇

  Вход в LLM:  [ v₁ ... v_K  t₁  t₂  t₃  t₄  t₅  t₆  t₇ ]
  Цель (loss):              t₂  t₃  t₄  t₅  t₆  t₇  t₈(EOS?)
  (маска: loss только по позициям подписи, не по v_i)
```

После этого этапа модель умеет по картинке генерировать подпись, но не обязательно отвечает на вопросы в формате «Вопрос: … Ответ: …» или в чат-формате.

---

### Этап 2: SFT (Supervised Fine-Tuning) на инструкциях / диалоге

**Цель:** научить формат «вопрос по картинке → ответ» или многошаговый диалог с изображениями. После pretraining модель умеет подписывать картинку, но не обязательно отвечает на вопросы в формате «Пользователь: … Ассистент: …». SFT как раз этому и учит.

**Как выглядит один пример в данных:**

- Тройка: **изображение** + **вопрос/инструкция** (текст пользователя) + **правильный ответ** (текст ассистента). Например: фото улицы, вопрос «Есть ли на фото люди?», ответ «Да, на тротуаре видны два человека». В коде и в датасетах это часто одна строка в таблице/JSON с путём к файлу картинки и двумя текстовыми полями. При формировании батча картинку пропускают через энкодер и проекцию; текст собирают в одну последовательность по **чат-шаблону** модели (системный промпт, пользователь, ассистент, спецтокены). Там, где в шаблоне стоит плейсхолдер для картинки (например `<image>`), подставляют полученные визуальные токены.

**Данные:**

- Наборы троек (изображение, вопрос/инструкция, ответ) или целые диалоги с вложенными картинками. Примеры: LLaVA-style данные, VQA датасеты, инструкции по описанию, рассуждению, OCR и т.д.

**Формат последовательности и loss:**

- Последовательность собирается как в чат-модели: например `<image>` (заменяется на визуальные токены) + текст пользователя + ответ ассистента. **Loss считают только по токенам ответа ассистента** — по токенам системного промпта и вопроса пользователя loss не считают (маска 0), иначе модель училась бы «предсказывать» вопрос по картинке, а не отвечать на него. Так же устроен SFT для чисто текстовых чат-моделей — см. [LLM.md](LLM.md).

**Гиперпараметры:**

- Learning rate обычно меньше, чем при pretraining (например 1e-5 … 2e-5).
- Часто дообучают только проекцию + часть слоёв LLM или используют **LoRA** на LLM, чтобы не «сломать» базовые языковые способности и сэкономить память.

После SFT модель уже полезна как «чат с картинками»: отвечает на вопросы по изображениям, описывает, рассуждает.

---

### Этап 3: RL и выравнивание (по желанию)

Аналогично LLM: можно применить GRPO, DPO, PPO и т.д. на данных (изображение, вопрос, несколько ответов, предпочтение или награда). Цель — улучшить стиль, полноту, безопасность ответов. В деталях логика та же, что в [LLM.md](LLM.md) (этап 3); разница только в том, что на входе ещё и изображение.

---

## Инференс: как пользоваться VLM

**Кратко:** подготовить изображение(а) и текстовый промпт → получить визуальные токены → собрать последовательность [визуальные + текстовые токены] → запустить LLM и генерировать ответ по одному токену до EOS или лимита.

**Пошагово: что делает пользователь и что происходит внутри**

1. **Пользователь:** загружает картинку (файл или с камеры) и вводит текст, например «Опиши, что на изображении» или «Сколько здесь людей?».
2. **Система:** картинку ресайзит и нормализует, пропускает через vision encoder → получает N векторов (например 196). Пропускает их через проекцию → N векторов в пространстве LLM. Текст токенизируется, по словарю берутся эмбеддинги токенов. Формируется одна последовательность: сначала N визуальных токенов, потом токены промпта пользователя (и при необходимости системный промпт и спецтокены чата — зависит от формата модели).
3. **Первый forward:** эта последовательность подаётся в LLM. На **последней** позиции (конец промпта) модель выдаёт логиты по всему словарю. По ним считают распределение вероятностей (softmax), с учётом параметров (temperature, top-p и т.д.) **сэмплируют один токен** — первое слово (или подслово) ответа.
4. **Цикл генерации:** этот токен добавляют к последовательности. Снова вызывают LLM (часто только для новой позиции, переиспользуя KV-cache для всех предыдущих). Получают следующий токен. Повторяют, пока не выпадет токен конца ответа (EOS) или не наберётся максимальное число токенов. Собранную последовательность токенов ответа декодируют обратно в текст и показывают пользователю.

Сэмплирование (temperature, top-k, top-p), repetition penalty, stop sequences — работают так же, как в обычной LLM (подробнее в [LLM.md](LLM.md)). **KV-cache** хранит ключи и значения для всех уже обработанных позиций (включая визуальные и текст промпта), чтобы при генерации не пересчитывать их заново — это сильно ускоряет вывод.

**Несколько изображений:** в интерливированном формате в последовательности идут, например: [img1_tokens] [text1] [img2_tokens] [text2] … Модель различает картинки по позициям и контексту. Поддержка нескольких картинок зависит от обученного формата и чат-шаблона (см. документацию конкретной модели).

---

## Дообучение и тонкая настройка VLM

Если вы хотите **дообучить** готовую VLM под свою задачу (например свой домен изображений или свой формат ответов):

### Full fine-tuning

Обучаются все параметры (vision encoder + проекция + LLM). Максимальная гибкость, но нужны много GPU, много данных и риск катастрофического забывания. На практике редко делают без большого датасета.

### LoRA / QLoRA на LLM

Как в LLM: к слоям внимания (и опционально FFN) LLM добавляют низкоранговые матрицы LoRA; обучают только их, базу замораживают. Vision encoder и проекцию можно заморозить или дообучить с малым LR. Это стандартный способ «подкрутить» VLM под свои данные при ограниченной VRAM.

### Только проекция

Если базовая модель уже неплохо понимает картинки, иногда достаточно дообучить только слой проекции на своих примерах (изображение + желаемый ответ). Очень дёшево по параметрам и памяти.

### Разморозка vision encoder

При доменных данных (медицина, спутники, чертежи) бывает полезно разморозить vision encoder и дообучить его вместе с проекцией (и опционально LoRA на LLM) небольшим learning rate. Тогда энкодер подстраивается под вашу визуальную область.

**Практические советы:**

- Используйте **gradient checkpointing** при обучении, чтобы уместить батч в память.
- **Gradient accumulation** увеличивает эффективный размер батча без роста VRAM на один шаг.
- Сохраняйте чекпоинты и смотрите validation loss; при переобучении уменьшите эпохи или увеличьте регуляризацию.

---

## Память и скорость

- **Vision encoder:** один forward на изображение; при батче из нескольких картинок энкодер считают для каждой. Память растёт с разрешением (число патчей) и размером энкодера (ViT-L, ViT-g тяжелее).
- **Проекция:** мало параметров по сравнению с LLM; стоимость небольшая.
- **LLM:** основная стоимость — как у чистой LLM: длина последовательности = визуальные токены + текстовые. Чем больше визуальных токенов (высокое разрешение, много картинок), тем больше память и время. **KV-cache** при генерации хранит ключи/значения для всей последовательности, включая визуальные позиции.
- **Квантизация:** как и у LLM, веса можно квантизовать (INT8, INT4, QLoRA) для ускорения и экономии памяти при инференсе и при дообучении.

---

## Семейства и примеры VLM

- **LLaVA:** ViT (CLIP) + линейная проекция + LLaMA. Простая архитектура; много вариантов (LLaVA 1.5, LLaVA-NeXT) с разными разрешениями и размерами.
- **BLIP-2:** замороженный vision encoder + **Q-Former** + замороженный LLM (на первом этапе); затем размораживают LLM. Эффективное сжатие визуальных токенов.
- **Qwen-VL, Yi-VL:** коммерческие/открытые VLM с поддержкой нескольких изображений, разного разрешения и чат-формата.
- **InternVL, CogVLM:** более тяжёлые энкодеры и/или проекции, длинный контекст.
- **PaliGemma, SmolVLM:** небольшие VLM для дешёвого инференса и экспериментов.

Различия в основном в: выборе vision encoder (ViT, SigLIP, и т.д.), наличии/отсутствии Q-Former/Resampler, размере LLM и обученном формате (одна картинка vs несколько, разрешение, чат-шаблон).

---

## Типичные проблемы при обучении и использовании

- **Модель «игнорирует» картинку, отвечает только по тексту:** слабая проекция или недостаточное обучение связки изображение–текст. Решение: проверить, что визуальные токены действительно подаются в LLM (нет ли бага, где картинка не подставляется); дообучить проекцию на данных, где правильный ответ явно зависит от изображения; увеличить число визуальных токенов или качество pretraining.
- **OOM (Out of Memory):** не хватает видеопамяти. Решение: уменьшить batch size, разрешение входа, число визуальных токенов; включить gradient checkpointing; использовать LoRA и квантизацию (INT8/INT4, QLoRA при дообучении).
- **Плохое качество на своих данных:** модель обучена на общих картинках, а у вас специфика (медицина, спутники, чертежи). Решение: дообучить на доменных примерах (LoRA + проекция или разморозка энкодера); проверить качество подписей и формат данных (правильный чат-шаблон, корректная подстановка `<image>`).
- **Медленный инференс:** решение: уменьшить разрешение входа, использовать квантизованную модель, модель с меньшим числом визуальных токенов (Q-Former/Resampler), батчирование запросов при сервисе.

---

## Как понять, что модель действительно смотрит на картинку

Если вы только начинаете пользоваться VLM или дообучили свою, полезно убедиться, что ответы зависят от **содержимого изображения**, а не только от текста вопроса.

- **Тест с подменой картинки:** задайте один и тот же вопрос («Что на изображении?») для двух разных картинок (например кошка и машина). Ответы должны различаться. Если модель на обе картинки отвечает одинаково или очень общо — возможно, она слабо опирается на визуал.
- **Вопросы с явной опорой на детали:** «Какого цвета рубашка у человека на фото?», «Сколько окон на здании?» — если модель даёт правдоподобный ответ, значит она использует визуальные токены. Если всегда уклоняется или ошибается — стоит проверить подачу изображения и при необходимости дообучить связку.
- **Проверка при дообучении:** на validation-выборке смотрите не только loss, но и примеры ответов на одни и те же картинки от эпохи к эпохе — ответы должны становиться релевантнее изображению.

---

## Частые вопросы (FAQ)

**Нужно ли до чтения разбираться в LLM и Transformer?**  
Желательно иметь общее представление: LLM генерирует текст по токенам, внутри — векторы и блоки с вниманием. Если читаете VLM впервые, можно идти по порядку: словарик и раздел «Что такое VLM» дают нужный минимум; при необходимости можно параллельно заглядывать в [LLM.md](LLM.md) и [transformer.md](transformer.md) по ссылкам.

**Я не программирую / не знаю Python. Смогу ли я пользоваться VLM?**  
Пользоваться готовыми VLM (через веб-интерфейсы, API, приложения) можно без программирования. Чтобы **дообучать** модель или разбирать код, нужны базовые навыки: Python, работа с данными (изображения + текст), умение запускать скрипты и при необходимости править конфиг. Материал даёт понимание, что происходит внутри; реализация в коде у разных фреймворков (transformers, LLaMA-Factory и т.д.) различается — смотрите документацию выбранного инструмента.

**Сколько нужно данных для дообучения VLM?**  
Зависит от задачи. Только проекция на уже хорошей базе: иногда хватает тысяч пар (изображение, подпись/ответ). LoRA на LLM под свой формат ответов: часто от нескольких тысяч до десятков тысяч примеров. Full fine-tuning или дообучение энкодера под узкий домен — обычно больше данных и аккуратная настройка, чтобы не переобучиться.

**Чем VLM отличается от «просто распознавания изображений» (классификация, детекция)?**  
Классификация выдаёт метку из фиксированного набора (например «кошка», «собака»); детекция — боксы и классы объектов. VLM выдаёт **произвольный текст**: описание, ответ на вопрос, рассуждение. То есть один и тот же моделью можно и описать сцену, и ответить на вопрос по картинке, и извлечь текст (OCR) — в зависимости от промпта.

**Почему в части моделей картинка «сжимается» до 32 токенов (Q-Former), а не 196?**  
Чтобы уменьшить длину последовательности для LLM: меньше позиций — меньше память и время (внимание растёт квадратично с длиной). Q-Former учится «выжимать» из 196 (или больше) визуальных векторов 32 информативных. Для многих задач этого хватает; для задач, где важны мелкие детали (мелкий текст, много объектов), иногда предпочитают модели с большим числом визуальных токенов или большим разрешением.

---

## Что полезно помнить

- **VLM** = **Vision Encoder** (чаще ViT) + **Projection** + **LLM**. Изображение превращается в последовательность векторов (визуальные токены), которые конкатенируются с текстовыми и подаются в тот же Transformer, что и в языковой модели.
- **Обучение:** pretraining (подпись по картинке, cross-entropy по токенам подписи) → SFT (вопрос–ответ по изображению, loss по ответу) → при необходимости RL/выравнивание. Часто энкодер или LLM замораживают на первом этапе.
- **Использование:** изображение + текст → визуальные токены + текстовые токены → один контекст → авторегрессионная генерация ответа. Параметры генерации (temperature, top-p, stop) — как у LLM.
- **Дообучение:** на практике чаще всего LoRA на LLM + опционально проекция или разморозка энкодера; full fine-tuning — при больших ресурсах и датасетах.

Материал рассчитан на то, чтобы даже человек без предварительного опыта в VLM мог разобраться: что такое модель, из чего она состоит, как обучается и как её использовать и дообучать. Если после прочтения что-то осталось неясным — вернитесь к словарику или к соответствующему разделу (архитектура, обучение, инференс); при дообучении и использовании конкретных моделей ориентируйтесь на их документацию и примеры кода.
