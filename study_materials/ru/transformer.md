# Transformer: архитектура и как это устроено

Учебный материал об архитектуре **Transformer** («Attention Is All You Need», Vaswani et al., 2017): из чего состоит, как работает внимание, позиционные кодировки и варианты (encoder, decoder, encoder–decoder). Материал рассчитан на новичков: термины по мере появления поясняются, в начале есть краткий словарик.

---

## Для кого этот материал и как им пользоваться

- **Если вы только знакомитесь с нейросетями:** имеет смысл сначала пробежаться по **краткому словарику** ниже — так будет проще читать дальше. Не обязательно помнить всё наизусть: к сложным местам можно возвращаться по мере чтения.
- **Если вы уже знаете основы ML:** словарик можно использовать как напоминание; основная ценность материала — связное описание блоков Transformer и примеры кода на PyTorch.
- **Связь с другими файлами:** устройство LLM (токенизация, обучение, инференс) разобрано в [LLM.md](LLM.md). Здесь фокус на самой архитектуре Transformer.

---

## Краткий словарик терминов

Эти понятия будут часто встречаться в тексте. Краткие определения — чтобы не теряться.

| Термин | Простое объяснение |
|--------|---------------------|
| **Вектор** | Набор чисел фиксированной длины (например 768 чисел). В нейросетях так представляют «смысл» токена или позиции. |
| **Размерность (dimension)** | Сколько чисел в векторе. Пишут, например: «вектор размерности 768» или «768-мерный вектор». |
| **Последовательность** | Упорядоченный список элементов (токенов или векторов). Длина последовательности — сколько в ней элементов (например 100 токенов). |
| **Позиция** | Место элемента в последовательности: первый токен — позиция 1, второй — позиция 2 и т.д. Для модели порядок важен («кот ест» ≠ «ест кот»). |
| **Эмбеддинг (embedding)** | Представление объекта (токена, слова) в виде вектора. Обычно получают по таблице (lookup): id токена → строка матрицы = вектор. |
| **Слой (layer)** | Один шаг вычислений в сети: например «умножить на матрицу и применить функцию». Несколько слоёв подряд дают «глубину» модели. |
| **Блок (block)** | Несколько слоёв, объединённых в один повторяемый модуль. В Transformer один блок = внимание + feed-forward (и нормализация, остатки). |
| **Параметры модели** | Числа, которые модель обучает: веса матриц, сдвиги (bias). Количество параметров (например «7 миллиардов») — один из показателей размера модели. |
| **Линейное преобразование** | Умножение вектора на матрицу (и прибавление сдвига): $y = xW + b$. Меняет размерность и «смешивает» компоненты вектора. |
| **Softmax** | Превращает набор чисел в распределение вероятностей: все значения неотрицательны и сумма по строке = 1. Большое число становится ближе к 1, маленькие — к 0. В attention softmax применяют к скорам, чтобы получить веса «на кого смотреть». |
| **Остаточное соединение (residual)** | Когда выход слоя **прибавляют** к его входу: `выход = вход + слой(вход)`. Помогает градиенту проходить через много слоёв и стабилизирует обучение. |
| **Нормализация** | Приведение вектора к «нормальному» масштабу (например деление на величину разброса). Делает обучение стабильнее. |
| **Encoder / Decoder** | **Энкодер** — часть модели, которая превращает вход (например исходный текст) в внутреннее представление. **Декодер** — часть, которая по этому представлению (и по уже сгенерированному тексту) выдаёт следующий токен. В LLM часто используют только декодер. |

В тексте ниже эти термины будут встречаться снова с более подробным контекстом.

---

## Что такое Transformer

**Transformer** — это **архитектура** (то есть «чертёж») нейросети для работы с последовательностями: текст, речь, последовательности действий и т.д. Главная идея: вместо того чтобы обрабатывать последовательность **по шагам** (как в старых рекуррентных сетях RNN), Transformer обрабатывает **все позиции сразу** и связывает их друг с другом с помощью **механизма внимания (attention)** — каждая позиция «смотрит» на другие и обновляет своё представление. Порядок элементов задаётся не цепочкой скрытых состояний, а **позиционными кодировками** и (при генерации) маской «не смотреть в будущее».

**Простыми словами:** представьте предложение из 10 слов. RNN обрабатывает слово 1, потом 2, потом 3… — последовательно. Transformer берёт все 10 слов сразу, каждое превращает в вектор, а затем для каждого слова считает: «насколько оно связано с каждым из остальных?» По этим связям формируется новое представление. Такой «раунд» повторяется много раз (много блоков). В итоге каждое слово «знает» контекст всего предложения.

Технически: архитектура основана на **self-attention** (самовнимание) и **feed-forward** блоках; рекуррентных (RNN) и свёрточных слоёв для порядка нет. Все позиции обрабатываются **параллельно**, что удобно для GPU.

Изначально в статье описан **encoder–decoder** вариант (исходный текст → внутреннее представление → целевой текст), например для машинного перевода. Сейчас широко используются три варианта:

| Вариант | Роль | Примеры |
|--------|------|---------|
| **Encoder-only** | Только энкодер: текст превращается в представления; по ним делают классификацию, поиск, эмбеддинги. Не генерирует токены по одному. | BERT, RoBERTa |
| **Decoder-only** | Только декодер: по уже введённым токенам предсказывается следующий (авторегрессия). Так устроены все современные LLM. | GPT, LLaMA, Mistral |
| **Encoder–decoder** | Два стека: энкодер обрабатывает вход (например исходный язык), декодер генерирует выход (перевод, суммаризацию), заглядывая в «память» энкодера. | T5, оригинальный Transformer |

В LLM почти всегда используется **decoder-only** Transformer (см. [LLM.md](LLM.md)).

```
  Общая идея Transformer

  Вход: последовательность токенов (или их эмбеддинги)  [x₁, x₂, …, x_L]

  Каждый слой:
    1. Self-Attention: каждая позиция «смотрит» на другие и обновляет своё представление
    2. Feed-Forward: независимо по каждой позиции — нелинейное преобразование

  Порядок токенов задаётся не рекуррентными связями, а позиционными кодировками
  и (в decoder) маской «не смотреть в будущее».
```

---

## Оригинальная схема: Encoder–Decoder

В статье «Attention Is All You Need» модель состоит из **стека энкодера** и **стека декодера**.

### Encoder

- **Вход:** последовательность векторов (эмбеддинги токенов + позиционные кодировки).
- **Один слой энкодера:**  
  1. **Multi-Head Self-Attention** — каждая позиция учитывает все остальные (без маски).  
  2. **Add & Norm** (остаток + нормализация).  
  3. **Feed-Forward** (по позициям).  
  4. **Add & Norm**.
- Выход энкодера — набор векторов по позициям **память (memory)** $[m_1, \ldots, m_L]$, которые декодер использует через **cross-attention**.

### Decoder

- **Вход:** целевая последовательность (сдвинутая на один токен вправо; маска «не видеть будущее»).
- **Один слой декодера:**  
  1. **Masked Multi-Head Self-Attention** — только на целевой последовательности, с **causal mask** (позиция $i$ видит только $1..i$).  
  2. Add & Norm.  
  3. **Multi-Head Cross-Attention**: Query — из декодера, Key и Value — из выхода энкодера (memory). Так декодер «обращается» к исходному тексту.  
  4. Add & Norm.  
  5. **Feed-Forward**.  
  6. Add & Norm.
- После стека декодера — линейный слой в размер словаря (логиты для следующего токена при обучении/инференсе).

```
  Оригинальный Transformer (Encoder–Decoder)

  Исходная последовательность (source)     Целевая последовательность (target)
  [s₁, s₂, …, s_L]                         [t₁, t₂, …, t_M]
        │                                          │
        ▼                                          ▼
  ┌─────────────┐                           ┌─────────────┐
  │ Embedding + │                           │ Embedding + │
  │ Pos Enc     │                           │ Pos Enc     │
  └──────┬──────┘                           └──────┬──────┘
         │                                         │
         ▼                                         ▼
  ┌─────────────┐                           ┌─────────────┐
  │ Enc Layer 1 │  Self-Attn (все на все)   │ Dec Layer 1 │  Masked Self-Attn
  │   + FFN     │                           │ + Cross-Attn (Q from dec, K,V from enc)
  └──────┬──────┘                           │   + FFN     │
         │                                  └──────┬──────┘
         ▼                                         │
        ...                                        ...
         │                                         │
         ▼                                         ▼
  ┌─────────────┐                           ┌─────────────┐
  │ Enc Layer N │  ──────────────────────►  │ Dec Layer N │  ──► Linear ──► логиты
  └─────────────┘   memory [m₁…m_L]         └─────────────┘
```

---

## Self-Attention (самовнимание)

**Внимание (attention)** в общем смысле — это механизм «посмотреть на другие элементы и смешать их информацию с весами». **Self-attention** («самовнимание») значит: мы смотрим на **ту же самую** последовательность (на другие позиции в ней). То есть каждый элемент последовательности «спрашивает» остальные: «что вы несёте?» — и по ответам формирует своё обновлённое представление.

Основа Transformer — **Scaled Dot-Product Attention** (масштабированное скалярно-точечное внимание). Для каждой позиции по **всем** релевантным позициям (с учётом маски: в декодере нельзя смотреть «в будущее») считаются **веса** — насколько сильно на неё смотреть — и выход получается как **взвешенная сумма** векторов **Value** этих позиций.

### Шаги по формулам

1. **Query, Key, Value (Q, K, V)** — три **линейных** преобразования одного и того же входа $X$ (три разные матрицы весов). То есть из каждого вектора позиции делают три вектора:
   - **Query** — «запрос»: с чем я хочу сравнить другие позиции;
   - **Key** — «ключ»: как меня можно сравнить с запросами других;
   - **Value** — «значение»: что я отдаю в общую сумму, если на меня «посмотрели».
   Формально: $Q = X W_Q$, $K = X W_K$, $V = X W_V$.  
   Размерности: $Q, K \in \mathbb{R}^{L \times d_k}$, $V \in \mathbb{R}^{L \times d_v}$ (часто $d_k = d_v$).

2. **Скоры (scores)** — для каждой пары позиций $(i, j)$ считаем «схожесть» запроса $i$ и ключа $j$ (скалярное произведение), и делим на $\sqrt{d_k}$ для стабильности:
   $$\text{scores} = \frac{Q K^\top}{\sqrt{d_k}}$$
   Матрица размера $L \times L$: элемент $(i,j)$ — «насколько позиция $i$ должна смотреть на позицию $j$».

3. **Масштабирование на $\sqrt{d_k}$** нужно, чтобы при большой $d_k$ скалярные произведения не росли по величине: иначе после softmax веса становятся почти один горячий, градиенты маленькие. Делитель стабилизирует масштаб и обучение.

4. **Маска** (в decoder): при генерации текста модель не должна «подглядывать» в будущие токены. Поэтому для пар «позиция $i$ смотрит на позицию $j$» при $j > i$ скор делают равным $-\infty$. После softmax такие элементы превращаются в 0 — то есть на будущее мы «не смотрим». Такую маску называют **causal** (причинная): учитывается только прошлое.

5. **Веса внимания:** по **строкам** матрицы скоров применяется **softmax**. В результате каждая строка даёт набор неотрицательных чисел с суммой 1 — это и есть веса «сколько взять от каждой позиции»:
   $$A = \mathrm{softmax}(\text{scores}) \quad \Rightarrow \quad \sum_j A_{ij} = 1.$$

6. **Выход:** для каждой позиции $i$ берём **взвешенную сумму** векторов Value по всем позициям $j$, с весами $A_{ij}$. Итог — новый вектор на каждой позиции (её «обновлённое» представление с учётом контекста):
   $$\mathrm{Attention}(Q,K,V) = A V.$$
   Форма: $L \times d_v$ — для каждой из $L$ позиций свой вектор размерности $d_v$.

```
  Self-Attention (одна голова)

  X [L × H] ──► Q = X W_Q   K = X W_K   V = X W_V
                     │           │           │
                     ▼           ▼           ▼
  scores = Q K^T / √d_k    [L × L], для decoder нижний треугольник, остальное -∞
                     │
                     ▼  softmax по строкам
  A [L × L], каждая строка — распределение весов по позициям
                     │
                     ▼  A · V
  выход [L × d_v] — новая последовательность (контекстное представление)
```

### Смысл Q, K, V

- **Query** — «запрос» текущей позиции: «что мне нужно из контекста?»
- **Key** — «ключ» каждой позиции: «по чему меня можно найти?»
- **Value** — «содержимое» позиции: «что я отдаю в взвешенную сумму».

Скор $Q_i \cdot K_j$ — насколько запрос позиции $i$ совпадает с ключом позиции $j$; по этим скорам усредняются значения $V_j$.

---

## Multi-Head Attention

**Multi-Head** — это когда механизм внимания запускают **не один раз**, а несколько раз **параллельно**, с разными наборами весов. Каждый такой запуск называют **головой (head)**. У каждой головы свои матрицы $W_Q, W_K, W_V$, поэтому каждая голова учится смотреть на связи своего типа: одна может сильнее реагировать на соседние слова, другая — на грамматические связи, третья — на повторяющиеся темы и т.д. Выходы всех голов **склеивают** (конкатенируют) по размерности и пропускают через ещё один линейный слой $W_O$, чтобы смешать информацию от разных голов.

Технически: размерность на одну голову обычно $d_k = d_v = H / h$ (где $h$ — число голов), чтобы суммарный объём вычислений оставался разумным. Выходы голов конкатенируют и получают снова вектор длины $H$, затем применяют $W_O$:

$$\mathrm{MultiHead}(X) = \mathrm{Concat}(\mathrm{head}_1, \ldots, \mathrm{head}_h) W_O,$$
$$\mathrm{head}_i = \mathrm{Attention}(X W_Q^{(i)}, X W_K^{(i)}, X W_V^{(i)}).$$

Так модель может одновременно учитывать разные типы связей (соседние слова, дальние зависимости, синтаксис и т.д.).

```
  Multi-Head Attention (h голов)

  X [L × H] ──┬──► head_1 (Q₁,K₁,V₁) ──► [L × H/h]
              ├──► head_2 (Q₂,K₂,V₂) ──► [L × H/h]
              ├──► ...
              └──► head_h (Qₕ,Kₕ,Vₕ) ──► [L × H/h]
                              │
                              ▼  Concat
                        [L × H]
                              │
                              ▼  Linear W_O
                        выход [L × H]
```

---

## Cross-Attention (в encoder–decoder)

В декодере **cross-attention** связывает целевой текст с энкодером:

- **Query** — из выхода декодера (текущее состояние целевой последовательности).
- **Key** и **Value** — из выхода энкодера (memory).

Формула та же: $\mathrm{Attention}(Q_{\mathrm{dec}}, K_{\mathrm{enc}}, V_{\mathrm{enc}})$. Маска по позициям энкодера не нужна (декодер может смотреть на все позиции источника). Causal mask остаётся только в self-attention декодера.

```
  Cross-Attention (один слой декодера)

  Декодер: [d₁, d₂, …, d_M]   Энкодер (memory): [m₁, m₂, …, m_L]
       │                              │
       ▼                              ▼
  Q = Dec · W_Q                 K = Enc · W_K    V = Enc · W_V
       │                              │
       └──────────► scores = Q K^T / √d_k  [M × L]  (без causal по L)
                              │
                              ▼  softmax, затем A · V
  выход [M × d_v] — для каждой позиции декодера — взвешенная сумма по memory
```

---

## Feed-Forward Network (FFN)

После блока внимания в каждом слое Transformer идёт **Feed-Forward Network (FFN)** — небольшой кусок обычной нейросети. Важно: она применяется **к каждой позиции отдельно и одинаково** — то есть между позициями здесь нет обмена информацией, одна и та же пара (или тройка) линейных слоёв с нелинейностью используется для всех позиций. Сначала размерность увеличивают (например в 4 раза), применяют нелинейную функцию (ReLU или SiLU), затем снова сжимают до исходной размерности. Смысл: внимание «собрало» контекст, а FFN даёт модели «место» для нелинейных преобразований и хранения паттернов.

**Классический вариант (оригинальный Transformer, GPT-2):**
$$\mathrm{FFN}(x) = \mathrm{ReLU}(x W_1 + b_1) W_2 + b_2.$$

**SwiGLU (многие современные LLM, например LLaMA):**
$$\mathrm{FFN}(x) = \big(\sigma(x W_1) \odot (x W_3)\big) W_2,$$
где $\sigma$ — SiLU (Swish), $\odot$ — поэлементное умножение. Размерность внутреннего слоя (столбцов $W_1$, $W_3$) обычно в **3–4 раза больше** hidden_size (например 11008 при 4096).

FFN даёт «место» для хранения паттернов и фактов; внимание — для доступа к нужным позициям.

```
  FFN (применяется к каждой позиции отдельно, веса общие)

  x [H] ──► W₁ [H → 4H] ──► ReLU/SiLU ──► W₂ [4H → H] ──► выход

  SwiGLU:  x ──► W₁ ──► σ ──┐
            x ──► W₃ ──►────┴── ⊙ ──► W₂ ──► выход
```

---

## Нормализация и остаточные связи

Чтобы сеть из многих блоков хорошо обучалась, используют два приёма.

- **Остаточное соединение (residual):** выход подуровня **прибавляют** к его входу: $x_{\mathrm{out}} = x + \mathrm{Sublayer}(x)$. То есть подуровень только «добавляет поправку» к уже имеющемуся представлению. Так градиент при обратном проходе может идти напрямую по этим сложениям и не затухать в глубине — обучение стабильнее.

- **Нормализация:** перед (или после) подуровнем вектор приводят к «нормальному» масштабу — например вычитают среднее и делят на стандартное отклонение по последней оси, или делят на RMS. Чаще всего в Transformer делают **Pre-LN** — нормализуют **до** подуровня: $x_{\mathrm{out}} = x + \mathrm{Sublayer}(\mathrm{Norm}(x))$. Тогда подуровень всегда получает данные в предсказуемом масштабе.

**LayerNorm:** по последней оси (по компонентам вектора) считают среднее $\mu$ и дисперсию, затем $y = \gamma \cdot (x - \mu) / \sqrt{\sigma^2 + \varepsilon} + \beta$. Параметры $\gamma$ и $\beta$ обучаются.

**RMSNorm** (во многих LLM): среднее не вычитают — только масштабирование по **RMS** (корень из среднего квадрата) по последней оси плюс один обучаемый масштаб. Проще и дешевле по вычислениям; в практиках LLaMA и аналогов этого достаточно.

Типичный порядок в **одном блоке** (decoder-only, Pre-LN):

1. $x_1 = x + \mathrm{Attention}(\mathrm{Norm}(x))$
2. $x_2 = x_1 + \mathrm{FFN}(\mathrm{Norm}(x_1))$

```
  Один Transformer-блок (decoder, Pre-LN)

  x ──► Norm ──► Attention ──► (+) ──► x₁
    │                ▲          │
    └────────────────┘          │
                                 ▼
  x₁ ──► Norm ──► FFN ──► (+) ──► x₂  (выход блока)
    │              ▲       │
    └──────────────┘       ┘
```

---

## Стек блоков: глубина и количество слоёв

Transformer — это **однотипные блоки**, поставленные друг на друга. **Стек** здесь значит просто «несколько блоков подряд». **Глубина** модели — это как раз число таких блоков (слоёв). Один блок не меняет размерность (вход и выход имеют форму `[L × H]`); меняется только **содержимое** представлений: каждый следующий блок уточняет контекстную информацию. Число блоков задаётся до обучения и не меняется — это **гиперпараметр** (параметр «устройства» модели, в отличие от весов, которые обучаются).

### Что даёт один блок

- **Attention** в блоке: позиции обмениваются информацией; каждая позиция получает взвешенную сумму по всем доступным (с учётом causal mask в decoder).
- **FFN**: по каждой позиции независимо — нелинейное преобразование; здесь часто «хранятся» паттерны и факты, к которым внимание потом обращается из других слоёв.

Один блок = один «раунд» взаимодействия позиций + один «раунд» локальной обработки. Этого мало для сложного языка; поэтому блоки повторяют.

### Сколько блоков бывает

Число блоков (**num_layers**, глубина модели) — ключевой гиперпараметр. Типичные значения для decoder-only LLM:

| Масштаб модели | num_layers | hidden_size (пример) | Примеры |
|----------------|------------|----------------------|---------|
| Малая (100M–500M) | 12–24 | 768–1024 | GPT-2 small, TinyLlama |
| Средняя (1B–7B) | 24–32 | 2048–4096 | LLaMA-7B, Mistral-7B |
| Большая (13B–70B) | 32–80 | 5120–8192 | LLaMA-70B, Qwen-72B |

Чем больше блоков, тем больше «шагов» для передачи информации между далёкими токенами и тем больше параметров. Рост глубины обычно даёт лучшую способность к рассуждениям и сложным зависимостям, но обучение дороже и требует аккуратной инициализации и нормализации (Pre-LN, остатки).

### Как информация течёт по слоям

- **Нижние слои (1–⅓ глубины):** часто сильнее локальные паттерны — соседние слова, морфология, простой синтаксис.
- **Средние слои:** более глобальные связи — согласование, ссылки на предыдущие части предложения.
- **Верхние слои:** абстрактные представления, «решение» задачи (что предсказать дальше); ближе к логитам и выходу.

Это не жёсткое правило: внимание в каждой голове и слое обучается, и распределение «ролей» может отличаться в разных моделях.

### Схема стека

```
  Стек decoder-only (num_layers = N)

  Эмбеддинги + позиции  [L × H]
            │
            ▼
  ┌─────────────────┐
  │  Block 1        │  →  [L × H]
  └────────┬────────┘
           ▼
  ┌─────────────────┐
  │  Block 2        │  →  [L × H]
  └────────┬────────┘
           │
           ⋮   (все блоки одинаковой формы)
           │
           ▼
  ┌─────────────────┐
  │  Block N        │  →  [L × H]
  └────────┬────────┘
           ▼
  Final norm → LM head (Linear H → vocab_size) → логиты
```

Порядок внутри одного блока везде один и тот же: Norm → Attention → residual → Norm → FFN → residual. Меняются только веса от блока к блоку.

### Параметры по глубине

При фиксированных `hidden_size`, `num_heads`, `intermediate_size` **число параметров растёт линейно с num_layers**: каждый блок добавляет свои матрицы Q, K, V, O и FFN. Поэтому основная доля параметров большой модели — в повторяющихся блоках, а не в одном эмбеддинге или одном слое.

---

## Позиционные кодировки (Positional Encoding)

У self-attention есть важное свойство: он обрабатывает позиции через суммы и взвешенные комбинации, и **не заложен** никакой информации «кто первый, кто второй». Если мы переставим токены в другом порядке, но оставим те же векторы, скоры внимания и выходы будут те же. То есть **порядок сам по себе модель не видит**. Чтобы модель понимала «первое слово», «второе слово» и т.д., в представление каждой позиции добавляют **позиционную кодировку** — вектор, зависящий от номера позиции. Варианты таких кодировок описаны ниже.

### 1. Синусоидальные (Sinusoidal) — оригинальная статья

Для позиции $pos$ и размерности $i$:
$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d}), \quad PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d}).$$

Плюсы: фиксированные, можно экстраполировать на длины больше обучающих. Минусы: модель не всегда хорошо использует длинный контекст; эмбеддинги и позиции просто складываются.

### 2. Обучаемые (Learned)

Отдельный вектор для каждой позиции $1, \ldots, N_{\max}$; обучаются вместе с моделью. Просто и эффективно на длинах до $N_{\max}$, но за пределами $N_{\max}$ модель не обучена.

### 3. RoPE (Rotary Position Embedding)

Позиция вносится не сложением, а **поворотом** векторов в пространстве. Для пар измерений $(2k, 2k+1)$ векторы Query и Key поворачиваются на угол $\theta_k \cdot pos$, где $\theta_k = 10000^{-2k/d}$. В результате скалярное произведение $Q_i \cdot K_j$ зависит от **разности позиций** $i - j$. Используется в LLaMA, Mistral, Qwen и др. Хорошая экстраполяция на длинные контексты.

### 4. ALiBi (Attention with Linear Biases)

Позиция не в эмбеддинге, а в виде **линейного штрафа** к логам внимания: к $\mathrm{scores}_{ij}$ добавляется $-m \cdot |i - j|$ (коэффициент $m$ может зависеть от головы). Простая схема, часто хорошо ведёт себя на длинных последовательностях.

```
  Где вносятся позиции

  Sinusoidal / Learned:   token_emb(x) + pos_emb(pos)  →  вход в блоки

  RoPE:   применяется к Q и K внутри attention (перед Q K^T)

  ALiBi:   scores = Q K^T / √d_k + bias(i, j),  bias = -m·|i-j|
```

---

## Почему Transformer работает

1. **Параллелизм:** все позиции обрабатываются одновременно; нет последовательной зависимости как в RNN. Обучение на GPU очень эффективно.

2. **Длинные зависимости:** путь между любыми двумя позициями в одном слое — один шаг внимания; при нескольких слоях информация может проходить через разные «маршруты». В RNN путь между далёкими токенами длинный, градиенты затухают.

3. **Гибкость внимания:** веса $A_{ij}$ обучаются; модель сама решает, на какие позиции смотреть (локально или глобально).

4. **Масштабирование:** увеличение глубины (слоёв), ширины (hidden_size, число голов) и данных даёт предсказуемый прирост качества (scaling laws).

---

## Примеры кода (PyTorch)

Ниже — минимальные, но рабочие куски на PyTorch, соответствующие формулам выше. Размерности в комментариях: `B` = batch, `L` = seq_len, `H` = hidden_size, `d` = head dimension.

### Scaled Dot-Product Attention (одна голова, с causal mask)

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(
    q: torch.Tensor,  # [B, L, d]
    k: torch.Tensor,  # [B, L, d]
    v: torch.Tensor,  # [B, L, d]
    causal: bool = True,
) -> torch.Tensor:
    d_k = q.size(-1)
    scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)  # [B, L, L]

    if causal:
        L = scores.size(-1)
        mask = torch.triu(
            torch.ones(L, L, device=scores.device, dtype=torch.bool), diagonal=1
        )
        scores = scores.masked_fill(mask, float("-inf"))

    attn_weights = F.softmax(scores, dim=-1)  # [B, L, L]
    out = torch.matmul(attn_weights, v)        # [B, L, d]
    return out
```

### Multi-Head Attention (decoder, causal)

```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size: int, num_heads: int, causal: bool = True):
        super().__init__()
        assert hidden_size % num_heads == 0
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.causal = causal

        self.w_q = nn.Linear(hidden_size, hidden_size)
        self.w_k = nn.Linear(hidden_size, hidden_size)
        self.w_v = nn.Linear(hidden_size, hidden_size)
        self.w_o = nn.Linear(hidden_size, hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [B, L, H]
        B, L, H = x.shape
        q = self.w_q(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)  # [B, h, L, d]
        k = self.w_k(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.w_v(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        # объединяем batch и головы для вызова attention: [B*h, L, d]
        q = q.reshape(B * self.num_heads, L, self.head_dim)
        k = k.reshape(B * self.num_heads, L, self.head_dim)
        v = v.reshape(B * self.num_heads, L, self.head_dim)
        out = scaled_dot_product_attention(q, k, v, causal=self.causal)  # [B*h, L, d]
        out = out.view(B, self.num_heads, L, self.head_dim).transpose(1, 2).contiguous().view(B, L, H)
        return self.w_o(out)
```

### RMSNorm

```python
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        rms = (x.pow(2).mean(-1, keepdim=True) + self.eps).sqrt()
        return x / rms * self.weight
```

### Feed-Forward: классический ReLU и SwiGLU

```python
class FeedForwardReLU(nn.Module):
    def __init__(self, hidden_size: int, intermediate_size: int):
        super().__init__()
        self.fc1 = nn.Linear(hidden_size, intermediate_size)
        self.fc2 = nn.Linear(intermediate_size, hidden_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.fc2(F.relu(self.fc1(x)))


class FeedForwardSwiGLU(nn.Module):
    def __init__(self, hidden_size: int, intermediate_size: int):
        super().__init__()
        self.w1 = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.w2 = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.w3 = nn.Linear(hidden_size, intermediate_size, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

### Один Transformer-блок (decoder-only, Pre-LN)

```python
class TransformerBlock(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        intermediate_size: int,
        use_swiglu: bool = True,
    ):
        super().__init__()
        self.ln1 = RMSNorm(hidden_size)
        self.attn = MultiHeadAttention(hidden_size, num_heads, causal=True)
        self.ln2 = RMSNorm(hidden_size)
        self.ffn = (
            FeedForwardSwiGLU(hidden_size, intermediate_size)
            if use_swiglu
            else FeedForwardReLU(hidden_size, intermediate_size)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.attn(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x
```

### Стек блоков + эмбеддинг и LM head (минимальный decoder-only)

```python
class TransformerDecoder(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        hidden_size: int,
        num_layers: int,
        num_heads: int,
        intermediate_size: int,
        max_seq_len: int = 2048,
    ):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, hidden_size)
        self.pos_embed = nn.Embedding(max_seq_len, hidden_size)
        self.layers = nn.ModuleList([
            TransformerBlock(hidden_size, num_heads, intermediate_size)
            for _ in range(num_layers)
        ])
        self.ln_f = RMSNorm(hidden_size)
        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        # token_ids: [B, L]
        B, L = token_ids.shape
        x = self.embed(token_ids) + self.pos_embed(torch.arange(L, device=token_ids.device))
        for layer in self.layers:
            x = layer(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)  # [B, L, vocab_size]
        return logits
```

Вызов: `logits = model(token_ids)`. Для предсказания следующего токена берут `logits[:, -1, :]` и применяют softmax/сэмплирование. Для обучения считают cross-entropy по `logits[:, :-1]` и целевым токенам `token_ids[:, 1:]`.

---

## Сводка: размерности и гиперпараметры

| Параметр | Обозначение | Смысл | Типичные значения |
|----------|-------------|--------|-------------------|
| Длина последовательности | $L$ | Число токенов на входе | до 2k–128k |
| Размер скрытого слоя | $H$ (hidden_size) | Размер эмбеддингов и выхода блоков | 768, 2048, 4096, 8192 |
| Число голов | $h$ (num_heads) | В multi-head attention | 12, 32, 64; $H$ кратно $h$ |
| Размер на голову | $d_k$, $d_v$ | Обычно $H / h$ | 64, 128 |
| Внутренний размер FFN | intermediate_size | Обычно 3–4 × $H$ | 3072, 11008 |
| **Число блоков (слоёв)** | $N$ (num_layers) | Сколько раз повторяется блок | 12–24 (малые), 32–40 (7B), 80 (70B+) |

**Типичные соотношения:** $d_k = d_v = H/h$; один блок содержит матрицы внимания (Q, K, V, O) и два (или три при SwiGLU) матрицы в FFN. Большая часть параметров — в **повторяющихся блоках** (attention + FFN), а не в эмбеддингах (при умеренном словаре). Полный пайплайн от токенов до логитов см. в разделе «Примеры кода (PyTorch)».

---

## Связь с другими материалами

- **Decoder-only** Transformer как основа LLM, токенизация, обучение и инференс — в [LLM.md](LLM.md).
- Здесь разобрана сама архитектура: внимание, FFN, позиции, варианты encoder/decoder, **количество и роль блоков**, а также **примеры на PyTorch** (attention, FFN, блок, стек до логитов).
