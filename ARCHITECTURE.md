# HomeLLM â€” ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

> ĞŸĞ°ĞºĞµÑ‚ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…

---

## ğŸ“ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

```
models-at-home/
â”œâ”€â”€ homellm/                    # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°ĞºĞµÑ‚
â”‚   â”œâ”€â”€ __init__.py             # Ğ’ĞµÑ€ÑĞ¸Ñ: 0.1.0
â”‚   â”œâ”€â”€ models/                 # ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ home_model.py       # HomeConfig, HomeModel, HomeForCausalLM
â”‚   â”œâ”€â”€ training/               # Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ pretrain.py         # ĞŸÑ€ĞµÑ‚Ñ€ĞµĞ¹Ğ½ Ñ Ğ½ÑƒĞ»Ñ
â”‚   â””â”€â”€ cli/                    # Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ chat.py             # Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚
â”œâ”€â”€ configs/                    # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ distributed training
â”‚   â”œâ”€â”€ accelerate_default.yaml
â”‚   â”œâ”€â”€ accelerate_multi_gpu.yaml
â”‚   â”œâ”€â”€ accelerate_fsdp.yaml
â”‚   â”œâ”€â”€ accelerate_deepspeed_zero2.yaml
â”‚   â”œâ”€â”€ accelerate_deepspeed_zero3.yaml
â”‚   â””â”€â”€ accelerate_deepspeed_zero3_offload.yaml
â”œâ”€â”€ datasets/                    # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
â”‚   â””â”€â”€ fineweb_ru_1GB.jsonl    # Ğ ÑƒÑÑĞºĞ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ (1 Ğ“Ğ‘)
â”œâ”€â”€ scripts/                    # Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
â”‚   â”œâ”€â”€ run_pretrain.sh         # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚ Ğ¿Ñ€ĞµÑ‚Ñ€ĞµĞ¹Ğ½Ğ°
â”‚   â””â”€â”€ run_chat.sh             # Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ‡Ğ°Ñ‚Ğ°
â”œâ”€â”€ out/                        # Ğ§ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
â”‚   â””â”€â”€ home_pretrain/          # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ°
â”œâ”€â”€ requirements.txt            # Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
â””â”€â”€ ARCHITECTURE.md             # Ğ­Ñ‚Ğ¾Ñ‚ Ñ„Ğ°Ğ¹Ğ»
```

---

## ğŸ§  ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (HomeForCausalLM)

### ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (HomeConfig)

| ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ | ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|----------|--------------|----------|
| `vocab_size` | 50257 | Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ (GPT-2 ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹) |
| `hidden_size` | 512 | Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ |
| `num_hidden_layers` | 8 | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»Ğ¾Ñ‘Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° |
| `num_attention_heads` | 8 | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ |
| `intermediate_size` | hidden_size Ã— 4 | Ğ Ğ°Ğ·Ğ¼ĞµÑ€ FFN |
| `dropout` | 0.0 | Dropout (0 Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°) |
| `max_position_embeddings` | 4096 | ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° |
| `rope_theta` | 10000.0 | ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ RoPE |

### ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      HomeForCausalLM                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ Embedding     â”‚  vocab_size â†’ hidden_size                â”‚
â”‚  â”‚ (embed_tokens)â”‚  + Weight Tying Ñ lm_head               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚          â”‚                                                  â”‚
â”‚          â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚              HomeBlock Ã— N layers             â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚
â”‚  â”‚  â”‚ RMSNorm â†’ Attention (+ RoPE) â†’ Residual â”‚  â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚
â”‚  â”‚  â”‚ RMSNorm â†’ FeedForward (SwiGLU) â†’ Resid  â”‚  â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚          â”‚                                                  â”‚
â”‚          â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ RMSNorm       â”‚  Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚          â”‚                                                  â”‚
â”‚          â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ LM Head       â”‚  hidden_size â†’ vocab_size                â”‚
â”‚  â”‚ (lm_head)     â”‚  (Ğ²ĞµÑĞ° ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ embed_tokens)          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ

### 1. RMSNorm (Root Mean Square Normalization)

Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ LayerNorm Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ RMSNorm â€” Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°:

```python
class RMSNorm(nn.Module):
    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    
    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ LayerNorm (Ğ½ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾)
- Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² LLaMA, Mistral, Qwen

### 2. RoPE (Rotary Position Embedding)

ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ¸:

```python
def precompute_freqs_cis(dim, end, theta):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
    t = torch.arange(end)
    freqs = torch.outer(t, freqs)
    return torch.cos(freqs), torch.sin(freqs)

def apply_rope(q, k, cos, sin, position_ids):
    q_embed = q * cos + rotate_half(q) * sin
    k_embed = k * cos + rotate_half(k) * sin
    return q_embed, k_embed
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- ĞÑ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ "Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¾Ğ±ĞºĞ¸"
- Ğ­ĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹
- Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ÑÑ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ¸ ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ÑÑ

### 3. SwiGLU (Swish-Gated Linear Unit)

Feed-Forward ÑĞ»Ğ¾Ğ¹ Ñ Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼:

```python
class FeedForward(nn.Module):
    def forward(self, x):
        # SwiGLU: SiLU(W1Â·x) âŠ™ W3Â·x â†’ W2
        return self.w2(self.act(self.w1(x)) * self.w3(x))
```

**Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°:**
- `w1`: gate projection (hidden â†’ intermediate)
- `w3`: up projection (hidden â†’ intermediate)
- `w2`: down projection (intermediate â†’ hidden)
- ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ: SiLU (Swish)

### 4. Flash Attention

ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ PyTorch SDPA Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸:

```python
self.flash = hasattr(F, "scaled_dot_product_attention")

if self.flash and seq_len > 1 and past_key_value is None:
    output = F.scaled_dot_product_attention(q, k, v, is_causal=True)
else:
    # Fallback Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ°ÑĞºĞ¾Ğ¹
    ...
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ VRAM (O(N) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ O(NÂ²))
- Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ fallback

### 5. KV-Cache Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸

ĞŸÑ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑÑˆĞ¸Ñ€ÑƒÑÑ‚ÑÑ ĞºĞ»ÑÑ‡Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ:

```python
def forward(self, ..., past_key_value=None, use_cache=False):
    # ... compute q, k, v ...
    
    if past_key_value is not None:
        pk, pv = past_key_value
        k = torch.cat([pk, k], dim=2)  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğº ÑÑ‚Ğ°Ñ€Ñ‹Ğ¼
        v = torch.cat([pv, v], dim=2)
    
    present = (k, v) if use_cache else None
    return output, present
```

### 6. Weight Tying

Ğ’ĞµÑĞ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ LM head ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹:

```python
self.lm_head.weight = self.home_model.embed_tokens.weight
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ~30% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
- Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

---

## ğŸ“Š Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¿Ñ€ĞµÑ‚Ñ€ĞµĞ¹Ğ½Ğ° (pretrain.py)

### ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸

| Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ | Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ |
|---------|------------|
| **ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ** | HuggingFace Accelerate (CPU/GPU/Multi-GPU) |
| **Mixed Precision** | fp16 / bf16 Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ»Ğ°Ğ³Ğ¸ |
| **Gradient Checkpointing** | ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ VRAM |
| **Streaming Dataset** | ĞĞµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ²ĞµÑÑŒ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ² RAM |
| **LR Schedule** | Cosine Ñ warmup |
| **Ğ§ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ñ‹** | Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹ Ñ HF Hub |

### Streaming Dataset

```python
class StreamingTextDataset(IterableDataset):
    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        
        for idx, text in enumerate(self._read_lines()):
            # Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ°Ğ¼Ğ¸
            if worker_info is not None:
                if idx % worker_info.num_workers != worker_info.id:
                    continue
            
            tokens = self.tokenizer(text, ...)
            yield torch.tensor(tokens)
```

### ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸

```bash
python -m homellm.training.pretrain \
    --data_path datasets/fineweb_ru_1GB.jsonl \
    --output_dir out/home_pretrain \
    --arch home \
    --hidden_size 512 \
    --num_layers 8 \
    --n_heads 8 \
    --seq_len 512 \
    --batch_size 16 \
    --gradient_accumulation 4 \
    --learning_rate 5e-4 \
    --warmup_steps 1000 \
    --fp16
```

---

## ğŸ’¬ CLI Chat (chat.py)

### Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸

- **Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼**: Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³
- **ĞĞ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ**: Ñ‡ĞµÑ€ĞµĞ· `--prompt`
- **Streaming Ğ²Ñ‹Ğ²Ğ¾Ğ´**: Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
- **Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ**: Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ `home` Ğ¸ `gpt2`

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

```bash
# Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
python -m homellm.cli.chat --model_dir out/home_pretrain/final_model

# ĞĞ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
python -m homellm.cli.chat \
    --model_dir out/home_pretrain/final_model \
    --prompt "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°?" \
    --max_new_tokens 100 \
    --temperature 0.8
```

---

## ğŸš€ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚

### 1. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹

```bash
pip install -r requirements.txt
```

### 2. Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ!)

```bash
./scripts/run_studio.sh
```

ĞÑ‚ĞºÑ€Ğ¾ĞµÑ‚ÑÑ Ğ² Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğµ: http://localhost:8501

Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸:
- ğŸ¨ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ÑĞ»Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹, Ğ¿Ñ€ĞµÑĞµÑ‚Ñ‹)
- ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ loss, lr)
- ğŸ“ Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ°
- ğŸ’¾ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ²
- â¹ï¸ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

### 3. Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ñ€ĞµÑ‚Ñ€ĞµĞ¹Ğ½Ğ° (CLI)

```bash
# Ğ¡ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
./scripts/run_pretrain.sh

# Ğ¡ fp16 Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
./scripts/run_pretrain.sh --fp16

# ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
HIDDEN_SIZE=768 NUM_LAYERS=12 ./scripts/run_pretrain.sh --bf16
```

### 3. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

```bash
./scripts/run_chat.sh --prompt "Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸ Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸"
```

---

## âš¡ Distributed Training (FSDP / DeepSpeed)

HomeLLM Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· HuggingFace Accelerate Ñ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸.

### Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹

| Ğ ĞµĞ¶Ğ¸Ğ¼ | ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ | ĞšĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ |
|-------|--------|----------|-------------------|
| **Multi-GPU (DDP)** | `accelerate_multi_gpu.yaml` | Ğ ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ GPU | ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ² 1 GPU |
| **FSDP** | `accelerate_fsdp.yaml` | PyTorch Fully Sharded Data Parallel | ĞœĞ¾Ğ´ĞµĞ»ÑŒ >1B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ GPU |
| **DeepSpeed ZeRO-2** | `accelerate_deepspeed_zero2.yaml` | Ğ¨Ğ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² | 100M-1B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² |
| **DeepSpeed ZeRO-3** | `accelerate_deepspeed_zero3.yaml` | ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ | >1B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² |
| **ZeRO-3 + Offload** | `accelerate_deepspeed_zero3_offload.yaml` | Ğ¨Ğ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ + offload Ğ½Ğ° CPU | ĞÑ‡ĞµĞ½ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, 1 GPU |

### Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº

```bash
# Multi-GPU (DDP)
DISTRIBUTED=multi_gpu ./scripts/run_pretrain.sh --bf16

# FSDP (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹)
DISTRIBUTED=fsdp ./scripts/run_pretrain.sh --bf16

# DeepSpeed ZeRO-2
DISTRIBUTED=deepspeed_zero2 ./scripts/run_pretrain.sh

# DeepSpeed ZeRO-3
DISTRIBUTED=deepspeed_zero3 ./scripts/run_pretrain.sh

# ZeRO-3 + CPU Offload (Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ VRAM)
DISTRIBUTED=deepspeed_zero3_offload ./scripts/run_pretrain.sh
```

### Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VRAM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  DDP:        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] Model + Optimizer + Gradients   â”‚
â”‚                                                                     â”‚
â”‚  ZeRO-2:     [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] Model  [ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: Opt + Grad]        â”‚
â”‚                                                                     â”‚
â”‚  ZeRO-3:     [â–ˆâ–ˆâ–ˆâ–ˆ] [Ğ²ÑÑ‘ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ GPU]                     â”‚
â”‚                                                                     â”‚
â”‚  ZeRO-3+Off: [â–ˆâ–ˆ] [ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ + offload Ğ½Ğ° CPU]                    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### FSDP vs DeepSpeed

| ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ | FSDP | DeepSpeed |
|----------|------|-----------|
| **Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸** | Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ PyTorch | deepspeed Ğ¿Ğ°ĞºĞµÑ‚ |
| **Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ** | PyTorch native | Ğ¡Ğ²Ğ¾Ğ¹ backend |
| **CPU Offload** | Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ | ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ (ZeRO-Infinity) |
| **NVMe Offload** | ĞĞµÑ‚ | Ğ”Ğ° (ZeRO-Infinity) |
| **Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾** | PyTorch | Microsoft |

### ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° GPU

```bash
# ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ÑĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ GPU
DISTRIBUTED=fsdp ./scripts/run_pretrain.sh

# Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° GPU
NUM_GPUS=4 DISTRIBUTED=fsdp ./scripts/run_pretrain.sh

# Ğ’Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… GPU
CUDA_VISIBLE_DEVICES=0,1 DISTRIBUTED=fsdp ./scripts/run_pretrain.sh
```

### Ğ ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº Ñ‡ĞµÑ€ĞµĞ· accelerate

```bash
# Ğ¡ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¾Ğ¼
accelerate launch --config_file configs/accelerate_fsdp.yaml \
    -m homellm.training.pretrain \
    --data_path datasets/fineweb_ru_1GB.jsonl \
    --output_dir out/home_pretrain \
    --arch home

# Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° (ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³)
accelerate config
accelerate launch -m homellm.training.pretrain --data_path ...
```

---

## ğŸ“ˆ Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹

| ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ | ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ | VRAM (fp16) | Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ |
|--------------|-----------|-------------|--------------|
| 512-8-8 | ~25M | ~2 GB | GTX 1060+ |
| 768-12-12 | ~80M | ~4 GB | RTX 2060+ |
| 1024-16-16 | ~200M | ~8 GB | RTX 3080+ |

Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: `hidden-layers-heads`

---

## ğŸ”— Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ HuggingFace

ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ HuggingFace:

```python
from homellm.models import HomeConfig, HomeForCausalLM
from transformers import AutoTokenizer

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = HomeForCausalLM.from_pretrained("out/home_pretrain/final_model")

# Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ
inputs = tokenizer("ĞŸÑ€Ğ¸Ğ²ĞµÑ‚", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸ“ Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°

ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°:

### JSONL (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)
```json
{"text": "ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚..."}
{"text": "Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚..."}
```

### Plain text
```
ĞŸĞµÑ€Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° â€” Ğ¾Ğ´Ğ¸Ğ½ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
Ğ’Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° â€” Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
```

---

## ğŸ›  Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸

### ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ

| ĞŸĞ°ĞºĞµÑ‚ | Ğ’ĞµÑ€ÑĞ¸Ñ | ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ |
|-------|--------|------------|
| torch | â‰¥2.0.0 | PyTorch Ñ Flash Attention |
| transformers | â‰¥4.40.0 | Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ |
| accelerate | â‰¥0.28.0 | Distributed training |
| tqdm | â‰¥4.66.0 | Progress bars |

### Ğ”Ğ»Ñ DeepSpeed (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)

| ĞŸĞ°ĞºĞµÑ‚ | Ğ’ĞµÑ€ÑĞ¸Ñ | ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ |
|-------|--------|------------|
| deepspeed | â‰¥0.14.0 | ZeRO optimizer, offloading |
| ninja | latest | Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ JIT-ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ DeepSpeed ops |

---

## ğŸ“„ Ğ›Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ

Ğ¡Ğ¼. Ñ„Ğ°Ğ¹Ğ» `LICENSE` Ğ² ĞºĞ¾Ñ€Ğ½Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°.

