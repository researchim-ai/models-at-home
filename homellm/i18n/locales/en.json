{
  "app.title": "Models at Home Training Studio",
  "app.subtitle": "Visual interface for training language models at home",
  
  "sidebar.architecture": "Architecture and Mode",
  "sidebar.training_stage": "Training Stage",
  "sidebar.experiment_name": "Experiment Name",
  "sidebar.base_model": "Base Model",
  "sidebar.model_params": "Model Parameters",
  "sidebar.hyperparams": "Hyperparameters",
  "sidebar.data": "Data",
  "sidebar.save": "Save",
  "sidebar.gpu_memory": "GPU and Memory",
  "sidebar.precision_memory": "Precision & Memory",
  "sidebar.tuning_method": "Tuning Method",
  
  "stage.pretrain": "Pretraining (from scratch)",
  "stage.continual_pretrain": "Continual Pretraining (continue)",
  "stage.sft": "SFT (Fine-Tuning)",
  "stage.grpo": "GRPO (RL for Reasoning)",
  
  "mode.pretrain": "Pretraining Mode (From Scratch)",
  "mode.sft": "SFT Mode (Fine-Tuning)",
  "mode.grpo": "GRPO Mode (RL for Reasoning)",
  "mode.continual_pretrain": "Continual Pretraining Mode (Continue)",
  
  "model.select": "Select model",
  "model.path": "Path to model",
  "model.path_manual": "Manual model path",
  "model.type": "Model type",
  "model.architecture": "Model architecture",
  "model.params": "Parameters",
  "model.downloaded": "Downloaded Models",
  "model.architecture": "Model Architecture",
  "model.context": "Context",
  "model.params_approx": "Parameters (â‰ˆ)",
  "model.hidden_size": "Hidden Size",
  "model.num_layers": "Num Layers",
  "model.attention_heads": "Attention Heads",
  "model.seq_length": "Seq Length",
  "model.preset": "Preset",
  "model.vocab_size": "Vocabulary Size",
  "model.max_context": "Max Context",
  
  "training.batch_size": "Batch Size",
  "training.gradient_accumulation": "Gradient Accumulation",
  "training.effective_batch": "Effective batch",
  "training.learning_rate": "Learning Rate",
  "training.lr_scheduler": "LR scheduler",
  "training.warmup_steps": "Warmup Steps",
  "training.epochs": "Epochs",
  "training.max_steps": "Max Steps",
  "training.logs": "Process Logs",
  "training.max_grad_norm": "Max Gradient Norm",
  "training.mode": "Training mode",
  "training.mode_epochs": "By epochs",
  "training.mode_steps": "By steps",
  "training.min_lr_ratio": "Min LR Ratio (Cosine floor)",
  
  "data.select_dataset": "Select dataset",
  "data.dataset_path": "Dataset path",
  "data.no_datasets": "No datasets found in datasets/",
  "data.download_hf": "Download from HuggingFace",
  "data.available_datasets": "Available Datasets",
  "data.preview": "Preview data from dataset",
  "data.upload_local": "Upload local files",
  "data.filters_limits": "Filters and Limits",
  "data.for_pretrain": "for Pretrain / Continual Pretrain",
  "data.popular_datasets": "Popular Datasets",
  "data.limit_type": "Limit type",
  "data.limit_gb": "GB (Size)",
  "data.limit_rows": "Rows (Count)",
  "data.row_count": "Row count",
  "data.size_gb": "Size (GB)",
  "data.column_filters": "Column filter settings",
  "data.filename_save": "Filename to save",
  "data.validation": "Validation",
  "data.val_fraction": "Validation fraction",
  "data.eval_every": "Eval Every N Steps",
  "data.eval_batches": "Eval Batches",
  "data.sharding": "Sharding",
  "data.sharding_mode": "Sharding mode",
  "data.max_samples": "Max samples (0 = all)",
  
  "save.output_dir": "Output Directory (Experiment Root)",
  "save.save_every": "Save Checkpoint Every N Steps",
  "save.log_every": "Log Every N Steps",
  "save.export_on_checkpoint": "Export final_model on each checkpoint save",
  "save.merge_lora": "Merge LoRA into final_model",
  "save.checkpoints_found": "Checkpoints found",
  "save.final_model_saved": "Final model saved",
  
  "gpu.found": "Found GPU",
  "gpu.not_found": "GPU not found, will use CPU",
  "gpu.select": "Select GPU",
  "gpu.single_gpu": "Single GPU used",
  "gpu.vram": "VRAM",
  "gpu.utilization": "Utilization",
  
  "parallel.type": "Type",
  "parallel.mode": "Mode",
  "parallel.title": "Parallelism",
  "parallel.device": "Device",
  "parallel.single_gpu": "Single GPU / CPU",
  "parallel.description.default": "Training on single device without parallelism",
  "parallel.description.multi_gpu": "Distributed Data Parallel â€” each GPU gets model copy and batch portion",
  "parallel.description.fsdp": "Fully Sharded Data Parallel (PyTorch native). Liger fused CE works!",
  "parallel.description.fsdp_offload": "FSDP + CPU offload. Saves VRAM, but Liger fused CE disabled.",
  "parallel.description.fsdp2": "FSDP v2 with DTensor + CPU. Saves VRAM, but Liger fused CE disabled.",
  "parallel.description.deepspeed_zero2": "Sharding optimizer and gradients between GPUs",
  "parallel.description.deepspeed_zero3": "Full sharding: model + optimizer + gradients",
  "parallel.description.deepspeed_zero3_offload": "Full sharding + CPU offload for VRAM savings",
  
  "precision.mixed": "Mixed Precision",
  "precision.fp16_pure": "FP16 Pure (fp16 weights, no GradScaler)",
  "precision.grad_checkpoint": "Gradient Checkpointing",
  "precision.flash_attention": "FlashAttention (attention speedup)",
  "precision.liger_kernel": "Liger Kernel optimizations",
  "precision.fused_loss": "Fused Loss (memory savings)",
  
  "lora.method": "Method",
  "lora.params": "LoRA parameters",
  "lora.rank": "LoRA r (rank)",
  "lora.alpha": "LoRA alpha",
  "lora.dropout": "LoRA dropout",
  "lora.target_modules": "Target modules",
  
  "grpo.params": "GRPO Parameters",
  "grpo.algorithm": "Algorithm",
  "grpo.group_size": "Group size (G)",
  "grpo.prompt_batch_size": "Prompt batch size (prompts/step)",
  "grpo.max_new_tokens": "Max new tokens",
  "grpo.temperature": "Temperature",
  "grpo.train_batch_size": "Train Batch Size",
  "grpo.max_prompts": "Max prompts (by dataset)",
  "grpo.epochs_per_step": "Epochs per step",
  "grpo.kl_weight": "KL weight",
  "grpo.advanced": "Advanced parameters",
  "grpo.clip_eps_low": "Clip Îµ (low)",
  "grpo.clip_eps_high": "Clip Îµ (high)",
  "grpo.rollout_engine": "Rollout engine (separate model for generation)",
  "grpo.use_rollout_engine": "Use separate model for generation",
  "grpo.rollout_backend": "Rollout backend",
  "grpo.sync_interval": "Weight sync (every N rollout-step)",
  "grpo.trainable_only": "Sync only trainable parameters (LoRA)",
  "grpo.offload_cpu": "Offload rollout model to CPU between generations",
  "grpo.vllm_gpu": "vLLM GPU",
  "grpo.vllm_gpu_memory": "vLLM GPU Memory (%)",
  "grpo.log_completions": "Show generated responses",
  "grpo.log_interval": "Logging interval (steps)",
  "grpo.dataset_reasoning": "Dataset for Reasoning",
  "grpo.reward_designer": "Reward Designer",
  "grpo.sandbox": "Design Sandbox",
  "grpo.rule_constructor": "Reward Rule Constructor",
  "grpo.reasoning_format": "Reasoning Format",
  "grpo.monitoring": "GRPO Monitoring",
  "grpo.samples": "Generation Examples",
  "grpo.reward_designer_desc": "Create flexible reward rules with conditions, patterns and formulas",
  "grpo.field_mapping_desc": "Specify which fields to use for prompt and answer",
  "grpo.prompt_template_desc": "Configure how the prompt will be formatted for the model",
  "grpo.data_loaded_from_dataset": "ðŸ’¡ Data automatically loaded from selected dataset",
  "grpo.extractors_desc": "Extract values from text into variables `{{extracted.name}}`",
  "grpo.test_data": "Test Data",
  "grpo.help_vars": "Help: variables and syntax",
  "grpo.response_format_example": "Response format example",
  
  "sft.data_config": "Data Configuration for SFT",
  "sft.example_record": "Example record",
  "sft.detected_chat": "Autodetect: Chat format found in field",
  "sft.detected_instruct": "Autodetect: Instruct format (separate fields)",
  "sft.data_format": "Data format",
  "sft.chat_format": "Chat (message list)",
  "sft.instruct_format": "Instruct (separate fields)",
  "sft.chat_config": "Chat Format Configuration",
  "sft.instruct_config": "Instruct Format Configuration",
  "sft.messages_field": "Messages field",
  "sft.role_field": "Role field",
  "sft.content_field": "Text field",
  "sft.role_mapping": "Role mapping",
  "sft.tags_system": "Tags and system prompt",
  "sft.default_system": "Default system prompt",
  "sft.preview": "Preview",
  "sft.preview_generated": "Preview generated via",
  "sft.input_instruction": "question/instruction",
  "sft.output_answer": "answer",
  "sft.select_role_fields": "Select fields for each role:",
  "sft.chat_template": "Model Chat Template",
  "sft.default_system": "System prompt (default)",
  "sft.tags_system": "Tags and system prompt",
  
  "tabs.launch": "Launch",
  
  "launch.config_title": "Training Configuration",
  "launch.model": "Model",
  "launch.data": "Data",
  "launch.training_mode": "Training Mode",
  "tabs.monitoring": "Monitoring",
  "tabs.chat": "Chat",
  "tabs.data": "Data",
  "tabs.models": "Models",
  "tabs.docs": "Docs",
  
  "history.title": "Run History",
  "tabs.history": "History",
  
  "status.running": "Process running",
  "status.completed": "Training completed",
  "status.error": "Error",
  "status.stopped": "Training stopped",
  "status.initializing": "Initializing",
  "status.training": "Training",
  "status.progress": "Progress",
  "status.step": "Step",
  "status.time": "Time",
  "status.remaining": "Remaining",
  "status.select_run": "Select run to view metrics",
  "status.no_metrics": "No metrics found",
  "status.files_deleted": "Files deleted",
  "status.viewing_metrics": "Viewing metrics",
  
  "metrics.train_loss": "Train Loss",
  "metrics.val_loss": "Val Loss",
  "metrics.reward": "Reward",
  "metrics.kl_divergence": "KL Divergence",
  "metrics.grad_norm": "Grad Norm",
  "metrics.buffer_size": "Buffer Size",
  "metrics.gpu_load": "GPU Load",
  
  "button.start_training": "Start Training",
  "button.stop_training": "Stop Training",
  "button.add_rule": "Add Rule",
  "button.add_template": "Add Template",
  "button.empty_rule": "Empty Rule",
  "button.save_files": "Save Files",
  "button.download": "Download",
  "button.delete": "Delete",
  "button.send": "Send",
  "button.clear_history": "Clear History",
  "button.export_model": "Export Model",
  "button.add_extractor": "Add extractor",
  "button.add_condition": "Add condition",
  "button.check_repo": "Check repository",
  "button.download_process": "Download and process",
  "button.download_model": "Download model",
  "button.use": "Use",
  "button.load_model": "Load model",
  "button.continue_training": "Continue",
  "button.from_model": "From model",
  "button.clear": "Clear",
  "button.start_grpo": "Start GRPO training",
  
  "error.file_read": "Error reading file: {error}",
  "error.file_empty": "Could not read file or it is empty",
  "error.no_model": "No models available",
  "error.no_dataset": "No dataset selected",
  "error.training": "Error occurred during training",
  "error.download": "Download error",
  "error.config_read": "Config read error: {error}",
  "error.generic": "Error",
  "error.fetch_info": "Could not fetch info",
  "error.no_config": "config.json not found in downloaded model",
  "error.missing_repo_id": "Specify repo_id and name!",
  "error.export": "Export error",
  "error.traceback": "Error details (Traceback)",
  
  "success.ready": "Ready!",
  "success.dataset_loaded": "Dataset loaded",
  "success.model_loaded": "Model loaded",
  "success.training_complete": "Training completed in {duration}",
  "success.saved": "Done! Saved",
  "success.found_configs": "Found configs:",
  "success.downloaded": "Downloaded {count} lines ({size} MB) to {path}",
  
  "warning.gpu_not_found": "GPU not found, will use CPU",
  "warning.unsloth_not_installed": "Unsloth not installed!",
  "warning.select_fields": "Select **User** and **Assistant** fields",
  "warning.no_messages_field": "No message list fields found!",
  "warning.checkpoint_only": "Only checkpoints available. For continual pretraining better use final_model.",
  "warning.stop_failed": "Could not stop (may already be finished)",
  "warning.select_dataset": "Select a dataset or download on **Data** tab",
  "warning.select_template": "Select a template",
  "warning.field_empty": "Answer field is empty in dataset! Select another field or enter manually.",
  "warning.preview_load_failed": "Could not load data for preview",
  "warning.model_exists": "Model `{name}` already exists!",
  "warning.high_memory": "Activations use a lot of memory! Enable Gradient Checkpointing or reduce Batch Size.",
  "warning.tokenizer_load_failed": "Could not load tokenizer",
  "warning.select_model": "Select model",
  "warning.select_data": "Select dataset",
  "warning.select_mode": "Select mode",
  "warning.config_not_found": "Config not found, enter parameters manually",
  "warning.check_params_match": "Make sure parameters match the base model!",
  
  "info.params_loaded": "Parameters loaded from",
  "info.architecture_fixed": "Architecture fixed (from base model)",
  "info.training_context": "Training on context {context}, model supports up to {max}",
  "info.hf_model": "HuggingFace model",
  "info.checkpoint": "Checkpoint selected. Resume will be performed.",
  "info.select_dataset_above": "Select dataset above to load examples",
  "info.waiting_loss": "Waiting for loss data...",
  "info.waiting_generations": "Waiting for generations...",
  "info.samples_will_appear": "Samples will appear here after generations start.",
  "info.no_downloaded_models": "No downloaded models. Download a model for Continual Pretraining or SFT.",
  "info.no_datasets": "No datasets loaded. Upload files on the left.",
  "info.dataset_structure_not_loaded": "âš ï¸ Dataset structure not loaded. Only size limits available.",
  "info.no_chat_template": "â„¹ï¸ Base model has no chat_template. Will be generated automatically on save.",
  
  "help.batch_size": "Batch size per GPU",
  "help.grad_accum": "Gradient accumulation steps",
  "help.learning_rate": "Initial learning rate",
  "help.warmup": "Warmup steps before full LR",
  "help.max_grad_norm": "Gradient clipping for stability (0 = disable)",
  "help.grad_checkpoint": "Saves VRAM but slower. For GRPO often must-have.",
  "help.flash_attention": "Enables fast attention where possible.",
  "help.liger_kernel": "Optimized Triton kernels for speedup and memory savings.",
  "help.fused_loss": "Does NOT materialize full logits tensor â€” up to 80% memory savings!",
  "help.experiment_folder": "Name of folder for saving",
  "help.training_stage": "Select stage: training from scratch, continue pretrain, fine-tuning (SFT) or RL training (GRPO)",
  "help.training_mode": "Choose how to define training duration",
  "help.qwen_template": "Generate Qwen-style chat template",
  "help.continue_training": "Continue training from last checkpoint",
  "help.checkpoint_not_found": "Checkpoint not found on disk",
  "help.chat_mode": "Chat: uses model's chat_template for dialog formatting",
  "help.no_chat_template": "Model has no chat_template - only Completion mode available",
  
  "chat.title": "Chat with Model",
  "chat.model_select": "Select model for chat",
  "chat.no_models": "No models available for chat",
  "chat.system_prompt": "System Prompt",
  "chat.title": "Chat with Model",
  "chat.select_model": "Select model or checkpoint",
  "chat.final_model": "Final model",
  "chat.final_model_saved": "Final model saved",
  "chat.load_model_prompt": "Load model...",
  "chat.max_tokens": "Max New Tokens",
  "chat.faster": "faster",
  "chat.inference_backend": "Inference Backend",
  "chat.training_type": "Training",
  "chat.mode": "Mode",
  "chat.no_template": "no chat_template",
  "chat.message_placeholder": "Enter message...",
  "chat.thinking": "Model is thinking...",
  "chat.generation_params": "Generation Parameters",
  
  "data.manager": "Data Management",
  "data.upload_local": "Upload local files",
  "data.drag_files": "Drag files here",
  "data.hf_download": "Download from HuggingFace",
  "data.dataset_repo": "Dataset repository",
  "data.subset": "Subset (subset/config)",
  "data.split": "Split",
  "data.download_limit": "Download limit",
  "data.rows_count": "Rows (Count)",
  "data.gb_size": "GB (Size)",
  "data.save_as": "Save as",
  "data.local_datasets": "Local Datasets",
  "data.reasoning_datasets": "Reasoning Datasets",
  "data.sft_datasets": "SFT Datasets",
  "data.pretrain_datasets": "Pretraining Datasets",
  
  "models.manager": "Model Management",
  "models.download_hf": "Download from HuggingFace",
  "models.model_id": "Model ID (e.g. Qwen/Qwen2.5-0.5B)",
  "models.local_models": "Local Models",
  "models.trained_models": "Trained Models",
  "models.lora_adapters": "LoRA Adapters",
  "models.popular": "Popular Models",
  "models.repo_id": "Repository (ID)",
  "models.save_name": "Save name",
  "models.estimated_size": "Estimated size",
  "models.recommendations": "Recommendations",
  "models.rec_smollm2": "modern compact models from HuggingFace",
  "models.rec_pythia": "great for experiments, various sizes",
  "models.rec_tinyllama": "popular, well-trained on 3T tokens",
  "models.rec_qwen": "strong models from Alibaba",
  "models.deleted": "Model deleted",
  "models.usage_tip": "ðŸ’¡ **How to use downloaded model:**\\n1. Click **ðŸš€ Use** on desired model\\n2. Go to **ðŸš€ Launch** tab\\n3. In sidebar select **Continual Pretrain** or **SFT** mode\\n4. Model will be automatically set as base",
  
  "docs.title": "Tutorial and Reference",
  "docs.quick_start": "Step-by-step Guide",
  "docs.scaling_laws": "Scaling Laws",
  "docs.how_it_works": "How it works?",
  "docs.terminology": "Terminology",
  "docs.data_preparation": "Data Preparation",
  "docs.welcome": "Welcome to **Models at Home**! Here you will learn to create your own neural networks from scratch.",
  
  "docs.guide_content": "### How to Create Your Own Model: Full Cycle\n\nCreating your own LLM consists of two main stages: **Pretraining** (Training from scratch) and **SFT** (Instruction Fine-tuning).\n\n---\n\n#### ðŸŸ¢ Step 1: Pretraining (Training from scratch)\nAt this stage, the model learns to understand language, grammar, and facts about the world by reading gigabytes of text.\n\n**1. Downloading data**\n*   Go to the **ðŸ’¾ Data** tab.\n*   By default, `HuggingFaceFW/fineweb-2` is already selected.\n*   Click **ðŸ” Check repository**.\n*   **Important:** Set limits to avoid downloading the entire internet!\n    *   In the **ðŸ› ï¸ Filters and Limits** block, select **\"Limit type: Rows\"**.\n    *   Set `100,000` rows for the first try (this is ~100MB of text).\n    *   Or select **\"GB (Size)\"** and specify `1.0` GB.\n*   Click **ðŸš€ Download and process**. The `dataset.jsonl` file will appear in the list on the right.\n\n**2. Model setup**\n*   Go to the **ðŸš€ Launch** tab.\n*   In the sidebar (left), find the **ðŸ§  Architecture and Mode** section.\n*   **Training stage:** Select `Pretraining`.\n*   **Experiment name:** Come up with a name, e.g., `my_first_model`.\n*   **Preset:** Select `Tiny (25M)` or `Small (80M)`. This is optimal for a home PC.\n\n**3. Starting training**\n*   In the **âš™ï¸ Training parameters** section:\n    *   **Batch Size:** 4 or 8.\n    *   **Max Steps:** 1000 (for testing) or 10,000 (for results).\n*   Make sure your `dataset.jsonl` is selected in the **ðŸ“ Dataset** field.\n*   Click the big **â–¶ï¸ Start Training** button.\n*   Watch the **Loss** graph on the **ðŸ“Š Monitoring** tab. It should go down ðŸ“‰.\n\n---\n\n#### ðŸ”µ Step 2: SFT (Supervised Fine-Tuning)\nNow let's turn the \"reader\" into a \"conversationalist\". We'll teach the model to answer questions.\n\n**1. Preparing data for SFT**\n*   Go to the **ðŸ’¾ Data** tab again.\n*   In **\"Popular datasets\"**, select an SFT dataset (e.g., `OpenOrca`).\n*   Download it the same way as in Step 1. Name the file, e.g., `sft_data.jsonl`.\n\n**2. Selecting base model**\n*   **ðŸš€ Launch** tab.\n*   **Training stage:** Change to `SFT (Fine-Tuning)`.\n*   A **ðŸ“¦ Base model** field will appear. Select the model you trained in Step 1 (e.g., `home_pretrain/final_model`).\n*   âœ¨ **Magic:** Architecture parameters (layers, sizes) will be loaded automatically! You don't need to change them.\n\n**3. Format setup (Chat Template)**\n*   A **ðŸ› ï¸ SFT Data Configuration** block will appear in the center of the screen.\n*   The system will automatically detect the data format (Chat or Instruct).\n*   Assign fields for System, User, Assistant.\n*   Look at the **ðŸ‘ï¸ Preview** to make sure the dialog looks correct.\n\n**4. Starting SFT**\n*   Click **â–¶ï¸ Start Training**.\n*   SFT usually requires fewer steps (e.g., 500â€“1000).\n\n---\n\n#### ðŸ’¬ Step 3: Testing\n*   When SFT finishes, go to the **ðŸ’¬ Chat** tab.\n*   Select your new SFT model.\n*   Type \"Hello!\".",
  
  "docs.scaling_intro": "## ðŸ“ˆ Scaling Laws\n\n**Scaling Laws** are empirical patterns showing how model quality depends on:\n- **N** â€” number of model parameters\n- **D** â€” amount of data (tokens)\n- **C** â€” compute (FLOPs)\n\n> ðŸ’¡ **Key discovery:** Quality improves according to a **power law**.\n> Each doubling of resources gives a predictable but diminishing return.\n\n### ðŸ¤” Why is this important?\n\nScaling Laws allow you to **predict** results BEFORE you spend resources:\n\n1. **Planning:** How much data to collect? What model size to choose?\n2. **Budgeting:** How many GPU-hours are needed for desired quality?\n3. **Optimization:** How to best distribute compute between model size and data volume?\n\n---\n\n### ðŸ“ Key Formulas\n\n#### 1. Loss Function (Chinchilla, 2022)\n\nValidation loss can be approximated by:\n\n$$L(N, D) = E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}$$\n\n#### 2. Compute (FLOPs)\n\n$$C â‰ˆ 6 \\cdot N \\cdot D$$\n\n#### 3. Compute-Optimal ratio (\"20 tokens rule\")\n\n$$D_{opt} â‰ˆ 20 \\cdot N$$\n\n---",
  
  "docs.calc_title": "Interactive Scaling Laws Calculator",
  "docs.calc_question": "What do you want to calculate?",
  "docs.calc_data_for_model": "How much data for a model?",
  "docs.calc_model_for_data": "What model can I train on my data?",
  "docs.calc_compute": "How much compute will it take?",
  "docs.calc_model_size": "Model size (parameters)",
  "docs.calc_tokens_per_param": "Tokens per parameter",
  "docs.calc_tokens_help": "Chinchilla-optimal â‰ˆ 20. For over-training (cheaper inference) use 50-100.",
  "docs.calc_tokens_needed": "Tokens needed",
  "docs.calc_compute_flops": "Compute (FLOPs)",
  "docs.calc_data_size": "~Data size",
  "docs.calc_time_estimate": "Estimated training time",
  "docs.calc_select_gpu": "Select GPU",
  "docs.calc_mfu_help": "Real GPU utilization efficiency. Usually 20-40%.",
  "docs.calc_approx": "Approximately",
  "docs.calc_days": "days",
  "docs.calc_hours": "hours",
  "docs.calc_on": "on",
  "docs.calc_how_many_tokens": "How many tokens do you have?",
  "docs.calc_optimal_size": "Optimal model size",
  "docs.calc_recommend": "Recommended",
  "docs.calc_tokens_for_training": "Tokens for training",
  "docs.calc_gpu_days": "A100 GPU-days",
  
  "docs.model_tiny_desc": "Toy LM",
  "docs.model_small_desc": "Quick experiments",
  "docs.model_base_desc": "Minimal LLM",
  "docs.model_medium_desc": "SFT sandbox",
  "docs.model_large_desc": "Domain assistant",
  "docs.model_xl_desc": "Noticeable quality",
  "docs.model_xxl_desc": "Small general LLM",
  "docs.model_1b_desc": "Good base for tuning",
  
  "docs.scaling_tables": "### ðŸ“‹ Model Size Table\n\n| Size N | D optimal (tokens) | Compute (FLOPs) | Use Case |\n|:------:|:-----------------:|:---------------:|:---------|\n| **25M** | 0.5B | 7.5Ã—10Â¹â¶ | ðŸ§ª Pipeline testing |\n| **100M** | 2.0B | 1.2Ã—10Â¹â¸ | ðŸ“Š First \"real\" LLM |\n| **500M** | 10.0B | 3.0Ã—10Â¹â¹ | âœ¨ Good generation |\n| **1B** | 20.0B | 1.2Ã—10Â²â° | ðŸš€ Production base |\n\n---\n\n### ðŸŽ“ Key Takeaways\n\n1. **More data > Bigger model** â€” With limited compute, add data not parameters.\n2. **20 tokens rule** â€” Chinchilla-optimal: ~20 tokens per parameter.\n3. **Data quality > quantity** â€” Well-cleaned data beats the power law.\n\n---\n\n### ðŸ“š Recommended Sources\n\n- **Kaplan et al. (2020)** â€” [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n- **Hoffmann et al. (2022)** â€” [Training Compute-Optimal LLMs (Chinchilla)](https://arxiv.org/abs/2203.15556)",
  
  "docs.how_it_works_content": "### What Happens Inside?\n\n**1. Neural network is a next-word predictor**\nImagine you're reading a book, and I cover the last word:\n> *\"The cat sat on the... [?]\"*\n\nYou'll probably guess *\"mat\"*. A neural network (LLM) does the same.\n\n**2. Training**\n* We give the model text.\n* It makes a prediction.\n* If wrong, we \"punish\" it (**Loss**).\n* We adjust its weights so it guesses better.\n* Repeat thousands of times.\n\n**3. Tokenization**\nComputers understand numbers, not words.\n*\"Hello world\"* â†’ `[1054, 2033]`. These are **tokens**.\n\n---\n\n### ðŸ—ï¸ Transformer Architecture\n\n```\nInput Tokens â†’ Embedding â†’ [Transformer Blocks Ã— N] â†’ Output\n```\n\n**Key parameters:**\n- `hidden_size` â€” internal representation size\n- `num_layers` â€” model depth\n- `num_heads` â€” parallel attention heads\n- `vocab_size` â€” vocabulary size",
  
  "docs.terminology_content": "### ðŸ“– Parameter Cheat Sheet\n\n| Term | Explanation |\n|---|---|\n| **Loss** | The LOWER, the smarter. Starts ~10, ends ~2-3. |\n| **Perplexity** | `PPL = e^Loss`. If PPL=100, model chooses from 100 words. |\n| **Batch Size** | Examples at once. More = faster, but more VRAM. |\n| **Gradient Accumulation** | Virtual batch. Accumulate N steps, then update. |\n| **Epoch** | One pass through dataset. |\n| **Learning Rate** | Training speed. 3e-4 is a good start. |\n| **VRAM** | GPU memory. Most precious resource! |\n| **Checkpoint** | Saved model state. |\n\n---\n\n### ðŸŽ›ï¸ What to Tune?\n\n| If... | Try |\n|-------|-----|\n| Loss doesn't decrease | â†‘ LR or â†“ Batch Size |\n| Out of Memory | â†“ Batch Size, âœ… Gradient Checkpointing |\n| Training too slow | â†‘ Batch Size, â†“ Max Steps |",
  
  "docs.data_prep_content": "### What Data is Needed?\n\nModels train on text files. We use **JSONL** format.\n\n---\n\n#### ðŸŸ¢ For Pretraining\n\nJust text in `text` field:\n\n```json\n{\"text\": \"The capital of France is Paris.\"}\n```\n\n---\n\n#### ðŸ”µ For SFT (two formats)\n\n**Instruct format:**\n```json\n{\"instruction\": \"Translate\", \"input\": \"Hello\", \"output\": \"Bonjour\"}\n```\n\n**Chat format:**\n```json\n{\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}, {\"role\": \"assistant\", \"content\": \"Hello!\"}]}\n```\n\n---\n\n### ðŸ’¡ Data Tips\n\n1. **Quality > Quantity** â€” 100K good examples > 1M garbage.\n2. **Check your data** â€” Open JSONL, see what's inside.\n3. **Diversity** â€” Mix sources for robust model.\n4. **Scaling Law** â€” For N params, need ~20N tokens.",
  
  "backend.models_at_home": "models-at-home",
  "backend.unsloth": "Unsloth",
  "backend.recommended": "recommended",
  "backend.faster": "faster",
  
  "algorithm.grpo": "GRPO",
  "algorithm.grpo_desc": "Standard Group Relative Policy Optimization",
  "algorithm.dapo": "DAPO (Dynamic Advantage)",
  "algorithm.dr_grpo": "Dr.GRPO (improved)",
  
  "common.enabled": "Enabled",
  "common.disabled": "Disabled",
  "common.yes": "Yes",
  "common.no": "No",
  "common.none": "None",
  "common.select": "Select",
  "common.cancel": "Cancel",
  "common.confirm": "Confirm",
  "common.loading": "Loading...",
  "common.processing": "Processing...",
  "common.weight": "Weight",
  "common.name": "Name",
  "common.type": "Type",
  "common.path": "Path",
  "common.size": "Size",
  "common.date": "Date",
  "common.actions": "Actions",
  "common.optional": "optional",
  "common.compare_with": "Compare with",
  "common.select_column": "Select column",
  "common.status": "Status",
  "common.name": "Name",
  "common.weight": "Weight",
  "common.value": "Value",
  "common.number": "Number",
  "common.min_length": "Min length",
  "common.max_length": "Max length",
  "common.unknown": "Unknown",
  "common.none": "none",
  "common.configuration": "Configuration",
  "common.control": "Control",
  
  "visual_builder.subtitle": "Model Architecture Editor",
  "visual_builder.footer": "ðŸš€ Model Builder supports standard PyTorch blocks and custom operations."
}
