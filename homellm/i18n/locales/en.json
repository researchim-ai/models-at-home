{
  "app.title": "Models at Home Training Studio",
  "app.subtitle": "Visual interface for training language models at home",
  
  "sidebar.architecture": "Architecture and Mode",
  "sidebar.training_stage": "Training Stage",
  "sidebar.experiment_name": "Experiment Name",
  "sidebar.base_model": "Base Model",
  "sidebar.model_params": "Model Parameters",
  "sidebar.hyperparams": "Hyperparameters",
  "sidebar.data": "Data",
  "sidebar.save": "Save",
  "sidebar.gpu_memory": "GPU and Memory",
  "sidebar.precision_memory": "Precision & Memory",
  "sidebar.tuning_method": "Tuning Method",
  
  "stage.pretrain": "Pretraining (from scratch)",
  "stage.continual_pretrain": "Continual Pretraining (continue)",
  "stage.sft": "SFT (Fine-Tuning)",
  "stage.grpo": "GRPO (RL for Reasoning)",
  
  "mode.pretrain": "Pretraining Mode (From Scratch)",
  "mode.sft": "SFT Mode (Fine-Tuning)",
  "mode.grpo": "GRPO Mode (RL for Reasoning)",
  "mode.continual_pretrain": "Continual Pretraining Mode (Continue)",
  
  "model.select": "Select model",
  "model.path": "Path to model",
  "model.path_manual": "Manual model path",
  "model.type": "Model type",
  "model.architecture": "Model architecture",
  "model.params": "Parameters",
  "model.downloaded": "Downloaded Models",
  "model.architecture": "Model Architecture",
  "model.context": "Context",
  "model.params_approx": "Parameters (‚âà)",
  "model.hidden_size": "Hidden Size",
  "model.num_layers": "Num Layers",
  "model.attention_heads": "Attention Heads",
  "model.seq_length": "Seq Length",
  "model.preset": "Preset",
  "model.vocab_size": "Vocabulary Size",
  "model.max_context": "Max Context",
  
  "training.batch_size": "Batch Size",
  "training.gradient_accumulation": "Gradient Accumulation",
  "training.effective_batch": "Effective batch",
  "training.learning_rate": "Learning Rate",
  "training.lr_scheduler": "LR scheduler",
  "training.warmup_steps": "Warmup Steps",
  "training.epochs": "Epochs",
  "training.max_steps": "Max Steps",
  "training.logs": "Process Logs",
  "training.max_grad_norm": "Max Gradient Norm",
  "training.mode": "Training mode",
  "training.mode_epochs": "By epochs",
  "training.mode_steps": "By steps",
  "training.min_lr_ratio": "Min LR Ratio (Cosine floor)",
  
  "data.select_dataset": "Select dataset",
  "data.dataset_path": "Dataset path",
  "data.no_datasets": "No datasets found in datasets/",
  "data.download_hf": "Download from HuggingFace",
  "data.available_datasets": "Available Datasets",
  "data.preview": "Preview data from dataset",
  "data.upload_local": "Upload local files",
  "data.filters_limits": "Filters and Limits",
  "data.for_pretrain": "for Pretrain / Continual Pretrain",
  "data.popular_datasets": "Popular Datasets",
  "data.limit_type": "Limit type",
  "data.limit_gb": "GB (Size)",
  "data.limit_rows": "Rows (Count)",
  "data.row_count": "Row count",
  "data.size_gb": "Size (GB)",
  "data.column_filters": "Column filter settings",
  "data.filename_save": "Filename to save",
  "data.validation": "Validation",
  "data.val_fraction": "Validation fraction",
  "data.eval_every": "Eval Every N Steps",
  "data.eval_batches": "Eval Batches",
  "data.sharding": "Sharding",
  "data.sharding_mode": "Sharding mode",
  "data.max_samples": "Max samples (0 = all)",
  
  "save.output_dir": "Output Directory (Experiment Root)",
  "save.save_every": "Save Checkpoint Every N Steps",
  "save.log_every": "Log Every N Steps",
  "save.export_on_checkpoint": "Export final_model on each checkpoint save",
  "save.merge_lora": "Merge LoRA into final_model",
  "save.checkpoints_found": "Checkpoints found",
  "save.final_model_saved": "Final model saved",
  
  "gpu.found": "Found GPU",
  "gpu.not_found": "GPU not found, will use CPU",
  "gpu.select": "Select GPU",
  "gpu.single_gpu": "Single GPU used",
  "gpu.vram": "VRAM",
  "gpu.utilization": "Utilization",
  
  "parallel.type": "Type",
  "parallel.mode": "Mode",
  "parallel.title": "Parallelism",
  "parallel.device": "Device",
  "parallel.single_gpu": "Single GPU / CPU",
  "parallel.description.default": "Training on single device without parallelism",
  "parallel.description.multi_gpu": "Distributed Data Parallel ‚Äî each GPU gets model copy and batch portion",
  "parallel.description.fsdp": "Fully Sharded Data Parallel (PyTorch native). Liger fused CE works!",
  "parallel.description.fsdp_offload": "FSDP + CPU offload. Saves VRAM, but Liger fused CE disabled.",
  "parallel.description.fsdp2": "FSDP v2 with DTensor + CPU. Saves VRAM, but Liger fused CE disabled.",
  "parallel.description.deepspeed_zero2": "Sharding optimizer and gradients between GPUs",
  "parallel.description.deepspeed_zero3": "Full sharding: model + optimizer + gradients",
  "parallel.description.deepspeed_zero3_offload": "Full sharding + CPU offload for VRAM savings",
  
  "precision.mixed": "Mixed Precision",
  "precision.fp16_pure": "FP16 Pure (fp16 weights, no GradScaler)",
  "precision.grad_checkpoint": "Gradient Checkpointing",
  "precision.flash_attention": "FlashAttention (attention speedup)",
  "precision.liger_kernel": "Liger Kernel optimizations",
  "precision.fused_loss": "Fused Loss (memory savings)",
  
  "lora.method": "Method",
  "lora.params": "LoRA parameters",
  "lora.rank": "LoRA r (rank)",
  "lora.alpha": "LoRA alpha",
  "lora.dropout": "LoRA dropout",
  "lora.target_modules": "Target modules",
  
  "grpo.params": "GRPO Parameters",
  "grpo.algorithm": "Algorithm",
  "grpo.group_size": "Group size (G)",
  "grpo.prompt_batch_size": "Prompt batch size (prompts/step)",
  "grpo.max_new_tokens": "Max new tokens",
  "grpo.temperature": "Temperature",
  "grpo.train_batch_size": "Train Batch Size",
  "grpo.max_prompts": "Max prompts (by dataset)",
  "grpo.epochs_per_step": "Epochs per step",
  "grpo.kl_weight": "KL weight",
  "grpo.advanced": "Advanced parameters",
  "grpo.clip_eps_low": "Clip Œµ (low)",
  "grpo.clip_eps_high": "Clip Œµ (high)",
  "grpo.rollout_engine": "Rollout engine (separate model for generation)",
  "grpo.use_rollout_engine": "Use separate model for generation",
  "grpo.rollout_backend": "Rollout backend",
  "grpo.sync_interval": "Weight sync (every N rollout-step)",
  "grpo.trainable_only": "Sync only trainable parameters (LoRA)",
  "grpo.offload_cpu": "Offload rollout model to CPU between generations",
  "grpo.vllm_gpu": "vLLM GPU",
  "grpo.vllm_gpu_memory": "vLLM GPU Memory (%)",
  "grpo.log_completions": "Show generated responses",
  "grpo.log_interval": "Logging interval (steps)",
  "grpo.dataset_reasoning": "Dataset for Reasoning",
  "grpo.reward_designer": "Reward Designer",
  "grpo.sandbox": "Design Sandbox",
  "grpo.rule_constructor": "Reward Rule Constructor",
  "grpo.reasoning_format": "Reasoning Format",
  "grpo.monitoring": "GRPO Monitoring",
  "grpo.samples": "Generation Examples",
  "grpo.reward_designer_desc": "Create flexible reward rules with conditions, patterns and formulas",
  "grpo.field_mapping_desc": "Specify which fields to use for prompt and answer",
  "grpo.prompt_template_desc": "Configure how the prompt will be formatted for the model",
  "grpo.data_loaded_from_dataset": "üí° Data automatically loaded from selected dataset",
  "grpo.extractors_desc": "Extract values from text into variables `{{extracted.name}}`",
  "grpo.test_data": "Test Data",
  "grpo.help_vars": "Help: variables and syntax",
  "grpo.response_format_example": "Response format example",
  
  "sft.data_config": "Data Configuration for SFT",
  "sft.example_record": "Example record",
  "sft.detected_chat": "Autodetect: Chat format found in field",
  "sft.detected_instruct": "Autodetect: Instruct format (separate fields)",
  "sft.data_format": "Data format",
  "sft.chat_format": "Chat (message list)",
  "sft.instruct_format": "Instruct (separate fields)",
  "sft.chat_config": "Chat Format Configuration",
  "sft.instruct_config": "Instruct Format Configuration",
  "sft.messages_field": "Messages field",
  "sft.role_field": "Role field",
  "sft.content_field": "Text field",
  "sft.role_mapping": "Role mapping",
  "sft.tags_system": "Tags and system prompt",
  "sft.default_system": "Default system prompt",
  "sft.preview": "Preview",
  "sft.preview_generated": "Preview generated via",
  "sft.input_instruction": "question/instruction",
  "sft.output_answer": "answer",
  "sft.select_role_fields": "Select fields for each role:",
  "sft.chat_template": "Model Chat Template",
  "sft.default_system": "System prompt (default)",
  "sft.tags_system": "Tags and system prompt",
  
  "tabs.launch": "Launch",
  
  "launch.config_title": "Training Configuration",
  "launch.model": "Model",
  "launch.data": "Data",
  "launch.training_mode": "Training Mode",
  "tabs.monitoring": "Monitoring",
  "tabs.chat": "Chat",
  "tabs.data": "Data",
  "tabs.models": "Models",
  "tabs.docs": "Docs",
  
  "history.title": "Run History",
  "tabs.history": "History",
  
  "status.running": "Process running",
  "status.completed": "Training completed",
  "status.error": "Error",
  "status.stopped": "Training stopped",
  "status.initializing": "Initializing",
  "status.training": "Training",
  "status.progress": "Progress",
  "status.step": "Step",
  "status.time": "Time",
  "status.remaining": "Remaining",
  "status.select_run": "Select run to view metrics",
  "status.no_metrics": "No metrics found",
  "status.files_deleted": "Files deleted",
  "status.viewing_metrics": "Viewing metrics",
  
  "metrics.train_loss": "Train Loss",
  "metrics.val_loss": "Val Loss",
  "metrics.reward": "Reward",
  "metrics.kl_divergence": "KL Divergence",
  "metrics.grad_norm": "Grad Norm",
  "metrics.buffer_size": "Buffer Size",
  "metrics.gpu_load": "GPU Load",
  
  "button.start_training": "Start Training",
  "button.stop_training": "Stop Training",
  "button.add_rule": "Add Rule",
  "button.add_template": "Add Template",
  "button.empty_rule": "Empty Rule",
  "button.save_files": "Save Files",
  "button.download": "Download",
  "button.delete": "Delete",
  "button.send": "Send",
  "button.clear_history": "Clear History",
  "button.export_model": "Export Model",
  "button.add_extractor": "Add extractor",
  "button.add_condition": "Add condition",
  "button.check_repo": "Check repository",
  "button.download_process": "Download and process",
  "button.download_model": "Download model",
  "button.use": "Use",
  "button.load_model": "Load model",
  "button.continue_training": "Continue",
  "button.from_model": "From model",
  "button.clear": "Clear",
  "button.start_grpo": "Start GRPO training",
  
  "error.file_read": "Error reading file: {error}",
  "error.file_empty": "Could not read file or it is empty",
  "error.no_model": "No models available",
  "error.no_dataset": "No dataset selected",
  "error.training": "Error occurred during training",
  "error.download": "Download error",
  "error.config_read": "Config read error: {error}",
  "error.generic": "Error",
  "error.fetch_info": "Could not fetch info",
  "error.no_config": "config.json not found in downloaded model",
  "error.missing_repo_id": "Specify repo_id and name!",
  "error.export": "Export error",
  "error.traceback": "Error details (Traceback)",
  
  "success.ready": "Ready!",
  "success.dataset_loaded": "Dataset loaded",
  "success.model_loaded": "Model loaded",
  "success.training_complete": "Training completed in {duration}",
  "success.saved": "Done! Saved",
  "success.found_configs": "Found configs:",
  "success.downloaded": "Downloaded {count} lines ({size} MB) to {path}",
  
  "warning.gpu_not_found": "GPU not found, will use CPU",
  "warning.unsloth_not_installed": "Unsloth not installed!",
  "warning.select_fields": "Select **User** and **Assistant** fields",
  "warning.no_messages_field": "No message list fields found!",
  "warning.checkpoint_only": "Only checkpoints available. For continual pretraining better use final_model.",
  "warning.stop_failed": "Could not stop (may already be finished)",
  "warning.select_dataset": "Select a dataset or download on **Data** tab",
  "warning.select_template": "Select a template",
  "warning.field_empty": "Answer field is empty in dataset! Select another field or enter manually.",
  "warning.preview_load_failed": "Could not load data for preview",
  "warning.model_exists": "Model `{name}` already exists!",
  "warning.high_memory": "Activations use a lot of memory! Enable Gradient Checkpointing or reduce Batch Size.",
  "warning.tokenizer_load_failed": "Could not load tokenizer",
  "warning.select_model": "Select model",
  "warning.select_data": "Select dataset",
  "warning.select_mode": "Select mode",
  "warning.config_not_found": "Config not found, enter parameters manually",
  "warning.check_params_match": "Make sure parameters match the base model!",
  
  "info.params_loaded": "Parameters loaded from",
  "info.architecture_fixed": "Architecture fixed (from base model)",
  "info.training_context": "Training on context {context}, model supports up to {max}",
  "info.hf_model": "HuggingFace model",
  "info.checkpoint": "Checkpoint selected. Resume will be performed.",
  "info.select_dataset_above": "Select dataset above to load examples",
  "info.waiting_loss": "Waiting for loss data...",
  "info.waiting_generations": "Waiting for generations...",
  "info.samples_will_appear": "Samples will appear here after generations start.",
  "info.no_downloaded_models": "No downloaded models. Download a model for Continual Pretraining or SFT.",
  "info.no_datasets": "No datasets loaded. Upload files on the left.",
  "info.dataset_structure_not_loaded": "‚ö†Ô∏è Dataset structure not loaded. Only size limits available.",
  "info.no_chat_template": "‚ÑπÔ∏è Base model has no chat_template. Will be generated automatically on save.",
  
  "help.batch_size": "Batch size per GPU",
  "help.grad_accum": "Gradient accumulation steps",
  "help.learning_rate": "Initial learning rate",
  "help.warmup": "Warmup steps before full LR",
  "help.max_grad_norm": "Gradient clipping for stability (0 = disable)",
  "help.grad_checkpoint": "Saves VRAM but slower. For GRPO often must-have.",
  "help.flash_attention": "Enables fast attention where possible.",
  "help.liger_kernel": "Optimized Triton kernels for speedup and memory savings.",
  "help.fused_loss": "Does NOT materialize full logits tensor ‚Äî up to 80% memory savings!",
  "help.experiment_folder": "Name of folder for saving",
  "help.training_stage": "Select stage: training from scratch, continue pretrain, fine-tuning (SFT) or RL training (GRPO)",
  "help.training_mode": "Choose how to define training duration",
  "help.qwen_template": "Generate Qwen-style chat template",
  "help.continue_training": "Continue training from last checkpoint",
  "help.checkpoint_not_found": "Checkpoint not found on disk",
  "help.chat_mode": "Chat: uses model's chat_template for dialog formatting",
  "help.no_chat_template": "Model has no chat_template - only Completion mode available",
  
  "chat.title": "Chat with Model",
  "chat.model_select": "Select model for chat",
  "chat.no_models": "No models available for chat",
  "chat.system_prompt": "System Prompt",
  "chat.title": "Chat with Model",
  "chat.select_model": "Select model or checkpoint",
  "chat.final_model": "Final model",
  "chat.final_model_saved": "Final model saved",
  "chat.load_model_prompt": "Load model...",
  "chat.max_tokens": "Max New Tokens",
  "chat.faster": "faster",
  "chat.inference_backend": "Inference Backend",
  "chat.training_type": "Training",
  "chat.mode": "Mode",
  "chat.no_template": "no chat_template",
  "chat.message_placeholder": "Enter message...",
  "chat.thinking": "Model is thinking...",
  "chat.generation_params": "Generation Parameters",
  
  "data.manager": "Data Management",
  "data.upload_local": "Upload local files",
  "data.drag_files": "Drag files here",
  "data.hf_download": "Download from HuggingFace",
  "data.dataset_repo": "Dataset repository",
  "data.subset": "Subset (subset/config)",
  "data.split": "Split",
  "data.download_limit": "Download limit",
  "data.rows_count": "Rows (Count)",
  "data.gb_size": "GB (Size)",
  "data.save_as": "Save as",
  "data.local_datasets": "Local Datasets",
  "data.reasoning_datasets": "Reasoning Datasets",
  "data.sft_datasets": "SFT Datasets",
  "data.pretrain_datasets": "Pretraining Datasets",
  
  "models.manager": "Model Management",
  "models.download_hf": "Download from HuggingFace",
  "models.model_id": "Model ID (e.g. Qwen/Qwen2.5-0.5B)",
  "models.local_models": "Local Models",
  "models.trained_models": "Trained Models",
  "models.lora_adapters": "LoRA Adapters",
  "models.popular": "Popular Models",
  "models.repo_id": "Repository (ID)",
  "models.save_name": "Save name",
  "models.estimated_size": "Estimated size",
  "models.recommendations": "Recommendations",
  "models.rec_smollm2": "modern compact models from HuggingFace",
  "models.rec_pythia": "great for experiments, various sizes",
  "models.rec_tinyllama": "popular, well-trained on 3T tokens",
  "models.rec_qwen": "strong models from Alibaba",
  "models.deleted": "Model deleted",
  "models.usage_tip": "üí° **How to use downloaded model:**\\n1. Click **üöÄ Use** on desired model\\n2. Go to **üöÄ Launch** tab\\n3. In sidebar select **Continual Pretrain** or **SFT** mode\\n4. Model will be automatically set as base",
  
  "docs.title": "Tutorial and Reference",
  "docs.quick_start": "Step-by-step Guide",
  "docs.scaling_laws": "Scaling Laws",
  "docs.how_it_works": "How it works?",
  "docs.terminology": "Terminology",
  "docs.data_preparation": "Data Preparation",
  "docs.welcome": "Welcome to **Models at Home**! Here you will learn to create your own neural networks from scratch.",
  
  "docs.guide_content": "### How to Create Your Own Model: Full Cycle\n\nCreating your own LLM consists of two main stages: **Pretraining** (Training from scratch) and **SFT** (Instruction Fine-tuning).\n\n---\n\n#### üü¢ Step 1: Pretraining (Training from scratch)\nAt this stage, the model learns to understand language, grammar, and facts about the world by reading gigabytes of text.\n\n**1. Downloading data**\n*   Go to the **üíæ Data** tab.\n*   By default, `HuggingFaceFW/fineweb-2` is already selected.\n*   Click **üîç Check repository**.\n*   **Important:** Set limits to avoid downloading the entire internet!\n    *   In the **üõ†Ô∏è Filters and Limits** block, select **\"Limit type: Rows\"**.\n    *   Set `100,000` rows for the first try (this is ~100MB of text).\n    *   Or select **\"GB (Size)\"** and specify `1.0` GB.\n*   Click **üöÄ Download and process**. The `dataset.jsonl` file will appear in the list on the right.\n\n**2. Model setup**\n*   Go to the **üöÄ Launch** tab.\n*   In the sidebar (left), find the **üß† Architecture and Mode** section.\n*   **Training stage:** Select `Pretraining`.\n*   **Experiment name:** Come up with a name, e.g., `my_first_model`.\n*   **Preset:** Select `Tiny (25M)` or `Small (80M)`. This is optimal for a home PC.\n\n**3. Starting training**\n*   In the **‚öôÔ∏è Training parameters** section:\n    *   **Batch Size:** 4 or 8.\n    *   **Max Steps:** 1000 (for testing) or 10,000 (for results).\n*   Make sure your `dataset.jsonl` is selected in the **üìÅ Dataset** field.\n*   Click the big **‚ñ∂Ô∏è Start Training** button.\n*   Watch the **Loss** graph on the **üìä Monitoring** tab. It should go down üìâ.\n\n---\n\n#### üîµ Step 2: SFT (Supervised Fine-Tuning)\nNow let's turn the \"reader\" into a \"conversationalist\". We'll teach the model to answer questions.\n\n**1. Preparing data for SFT**\n*   Go to the **üíæ Data** tab again.\n*   In **\"Popular datasets\"**, select an SFT dataset (e.g., `OpenOrca`).\n*   Download it the same way as in Step 1. Name the file, e.g., `sft_data.jsonl`.\n\n**2. Selecting base model**\n*   **üöÄ Launch** tab.\n*   **Training stage:** Change to `SFT (Fine-Tuning)`.\n*   A **üì¶ Base model** field will appear. Select the model you trained in Step 1 (e.g., `home_pretrain/final_model`).\n*  Architecture parameters (layers, sizes) will be loaded automatically! You don't need to change them.\n\n**3. Format setup (Chat Template)**\n*   A **üõ†Ô∏è SFT Data Configuration** block will appear in the center of the screen.\n*   The system will automatically detect the data format (Chat or Instruct).\n*   Assign fields for System, User, Assistant.\n*   Look at the **üëÅÔ∏è Preview** to make sure the dialog looks correct.\n\n**4. Starting SFT**\n*   Click **‚ñ∂Ô∏è Start Training**.\n*   SFT usually requires fewer steps (e.g., 500‚Äì1000).\n\n---\n\n#### üí¨ Step 3: Testing\n*   When SFT finishes, go to the **üí¨ Chat** tab.\n*   Select your new SFT model.\n*   Type \"Hello!\".",
  
  "docs.scaling_intro": "## üìà Scaling Laws\n\n**Scaling Laws** are empirical patterns showing how model quality depends on:\n- **N** ‚Äî number of model parameters\n- **D** ‚Äî amount of data (tokens)\n- **C** ‚Äî compute (FLOPs)\n\n> üí° **Key discovery:** Quality improves according to a **power law**.\n> Each doubling of resources gives a predictable but diminishing return.\n\n### ü§î Why is this important?\n\nScaling Laws allow you to **predict** results BEFORE you spend resources:\n\n1. **Planning:** How much data to collect? What model size to choose?\n2. **Budgeting:** How many GPU-hours are needed for desired quality?\n3. **Optimization:** How to best distribute compute between model size and data volume?\n\n---\n\n### üìê Key Formulas\n\n#### 1. Loss Function (Chinchilla, 2022)\n\nValidation loss can be approximated by:\n\n$$L(N, D) = E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}$$\n\n#### 2. Compute (FLOPs)\n\n$$C ‚âà 6 \\cdot N \\cdot D$$\n\n#### 3. Compute-Optimal ratio (\"20 tokens rule\")\n\n$$D_{opt} ‚âà 20 \\cdot N$$\n\n---",
  
  "docs.calc_title": "Interactive Scaling Laws Calculator",
  "docs.calc_question": "What do you want to calculate?",
  "docs.calc_data_for_model": "How much data for a model?",
  "docs.calc_model_for_data": "What model can I train on my data?",
  "docs.calc_compute": "How much compute will it take?",
  "docs.calc_model_size": "Model size (parameters)",
  "docs.calc_tokens_per_param": "Tokens per parameter",
  "docs.calc_tokens_help": "Chinchilla-optimal ‚âà 20. For over-training (cheaper inference) use 50-100.",
  "docs.calc_tokens_needed": "Tokens needed",
  "docs.calc_compute_flops": "Compute (FLOPs)",
  "docs.calc_data_size": "~Data size",
  "docs.calc_time_estimate": "Estimated training time",
  "docs.calc_select_gpu": "Select GPU",
  "docs.calc_mfu_help": "Real GPU utilization efficiency. Usually 20-40%.",
  "docs.calc_approx": "Approximately",
  "docs.calc_days": "days",
  "docs.calc_hours": "hours",
  "docs.calc_on": "on",
  "docs.calc_how_many_tokens": "How many tokens do you have?",
  "docs.calc_optimal_size": "Optimal model size",
  "docs.calc_recommend": "Recommended",
  "docs.calc_tokens_for_training": "Tokens for training",
  "docs.calc_gpu_days": "A100 GPU-days",
  
  "docs.model_tiny_desc": "Toy LM",
  "docs.model_small_desc": "Quick experiments",
  "docs.model_base_desc": "Minimal LLM",
  "docs.model_medium_desc": "SFT sandbox",
  "docs.model_large_desc": "Domain assistant",
  "docs.model_xl_desc": "Noticeable quality",
  "docs.model_xxl_desc": "Small general LLM",
  "docs.model_1b_desc": "Good base for tuning",
  
  "docs.scaling_tables": "### üìã Model Size Table\n\n| Size N | D optimal (tokens) | Compute (FLOPs) | Use Case |\n|:------:|:-----------------:|:---------------:|:---------|\n| **25M** | 0.5B | 7.5√ó10¬π‚Å∂ | üß™ Pipeline testing |\n| **100M** | 2.0B | 1.2√ó10¬π‚Å∏ | üìä First \"real\" LLM |\n| **500M** | 10.0B | 3.0√ó10¬π‚Åπ | ‚ú® Good generation |\n| **1B** | 20.0B | 1.2√ó10¬≤‚Å∞ | üöÄ Production base |\n\n---\n\n### üéì Key Takeaways\n\n1. **More data > Bigger model** ‚Äî With limited compute, add data not parameters.\n2. **20 tokens rule** ‚Äî Chinchilla-optimal: ~20 tokens per parameter.\n3. **Data quality > quantity** ‚Äî Well-cleaned data beats the power law.\n\n---\n\n### üìö Recommended Sources\n\n- **Kaplan et al. (2020)** ‚Äî [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n- **Hoffmann et al. (2022)** ‚Äî [Training Compute-Optimal LLMs (Chinchilla)](https://arxiv.org/abs/2203.15556)",
  
  "docs.how_it_works_content": "### What Happens Inside?\n\n**1. Neural network is a next-word predictor**\nImagine you're reading a book, and I cover the last word:\n> *\"The cat sat on the... [?]\"*\n\nYou'll probably guess *\"mat\"*. A neural network (LLM) does the same.\n\n**2. Training**\n* We give the model text.\n* It makes a prediction.\n* If wrong, we \"punish\" it (**Loss**).\n* We adjust its weights so it guesses better.\n* Repeat thousands of times.\n\n**3. Tokenization**\nComputers understand numbers, not words.\n*\"Hello world\"* ‚Üí `[1054, 2033]`. These are **tokens**.\n\n---\n\n### üèóÔ∏è Transformer Architecture\n\n```\nInput Tokens ‚Üí Embedding ‚Üí [Transformer Blocks √ó N] ‚Üí Output\n```\n\n**Key parameters:**\n- `hidden_size` ‚Äî internal representation size\n- `num_layers` ‚Äî model depth\n- `num_heads` ‚Äî parallel attention heads\n- `vocab_size` ‚Äî vocabulary size",
  
  "docs.terminology_content": "### üìñ Parameter Cheat Sheet\n\n| Term | Explanation |\n|---|---|\n| **Loss** | The LOWER, the smarter. Starts ~10, ends ~2-3. |\n| **Perplexity** | `PPL = e^Loss`. If PPL=100, model chooses from 100 words. |\n| **Batch Size** | Examples at once. More = faster, but more VRAM. |\n| **Gradient Accumulation** | Virtual batch. Accumulate N steps, then update. |\n| **Epoch** | One pass through dataset. |\n| **Learning Rate** | Training speed. 3e-4 is a good start. |\n| **VRAM** | GPU memory. Most precious resource! |\n| **Checkpoint** | Saved model state. |\n\n---\n\n### üéõÔ∏è What to Tune?\n\n| If... | Try |\n|-------|-----|\n| Loss doesn't decrease | ‚Üë LR or ‚Üì Batch Size |\n| Out of Memory | ‚Üì Batch Size, ‚úÖ Gradient Checkpointing |\n| Training too slow | ‚Üë Batch Size, ‚Üì Max Steps |",
  
  "docs.data_prep_content": "### What Data is Needed?\n\nModels train on text files. We use **JSONL** format.\n\n---\n\n#### üü¢ For Pretraining\n\nJust text in `text` field:\n\n```json\n{\"text\": \"The capital of France is Paris.\"}\n```\n\n---\n\n#### üîµ For SFT (two formats)\n\n**Instruct format:**\n```json\n{\"instruction\": \"Translate\", \"input\": \"Hello\", \"output\": \"Bonjour\"}\n```\n\n**Chat format:**\n```json\n{\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}, {\"role\": \"assistant\", \"content\": \"Hello!\"}]}\n```\n\n---\n\n### üí° Data Tips\n\n1. **Quality > Quantity** ‚Äî 100K good examples > 1M garbage.\n2. **Check your data** ‚Äî Open JSONL, see what's inside.\n3. **Diversity** ‚Äî Mix sources for robust model.\n4. **Scaling Law** ‚Äî For N params, need ~20N tokens.",
  
  "docs.grpo_tutorial": "RL Algorithms",
  "docs.grpo_intro": "## üéì Reinforcement Learning for LLMs\n\n**Why RL?** SFT teaches the model to imitate data. RL teaches the model to **improve results** ‚Äî finding better solutions, even if they're not in the training data.\n\n### Core Idea\n\n1. Model generates multiple answers to one question\n2. Each answer gets a **reward** score\n3. Model learns to generate high-reward answers\n\n### Policy Gradient Formula\n\nAll our algorithms are based on **Policy Gradient**:",
  
  "docs.grpo_algorithms": "Algorithms",
  "docs.grpo_algo_standard": "Original from DeepSeek-R1",
  "docs.grpo_algo_simple": "Base algorithm",
  "docs.grpo_algo_length_bias": "Has bias issues",
  "docs.drgrpo_algo_improved": "Bias fixes",
  "docs.drgrpo_algo_no_bias": "No length bias",
  "docs.drgrpo_algo_stable": "More stable",
  "docs.dapo_algo_advanced": "4 improvements from ByteDance",
  "docs.dapo_algo_dynamic": "Dynamic sampling",
  "docs.dapo_algo_best": "Clip higher + token-level",
  
  "docs.grpo_content": "### üìê Algorithm Mathematics\n\n---\n\n## 1Ô∏è‚É£ GRPO (Group Relative Policy Optimization)\n\nüìÑ **Paper:** [DeepSeekMath: Pushing the Limits of Mathematical Reasoning](https://arxiv.org/abs/2402.03300)\n\n**Idea:** Compare answers within a group. Good answers get positive advantage, bad ones get negative.\n\n**Advantage Formula:**\n$$A_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}$$\n\n**Loss Formula:**\n$$L = -\\mathbb{E}\\left[\\min\\left(\\frac{\\pi_\\theta}{\\pi_{old}} A, \\text{clip}\\left(\\frac{\\pi_\\theta}{\\pi_{old}}, 1-\\varepsilon, 1+\\varepsilon\\right) A\\right)\\right]$$\n\n**GRPO Problems:**\n- Division by std creates **reward hacking** (model can \"cheat\" the metric)\n- **Length bias** ‚Äî shorter answers get an advantage\n\n---\n\n## 2Ô∏è‚É£ Dr.GRPO (GRPO Done Right)\n\nüìÑ **Paper:** [Understanding R1-Zero-Like Training: A Critical Perspective](https://arxiv.org/abs/2503.20783)\n\n**Idea:** Remove division by std and use fixed length normalizer.\n\n**Advantage Formula:**\n$$A_i = r_i - \\text{mean}(r)$$\n\n**Changes:**\n- ‚ùå Removed division by std ‚Üí no reward hacking\n- ‚úÖ Fixed length normalizer ‚Üí no length bias\n\n**When to use:** When GRPO is unstable or model generates too short answers.\n\n---\n\n## 3Ô∏è‚É£ DAPO (Dynamic sAmpling Policy Optimization)\n\nüìÑ **Paper:** [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)\n\n**4 Improvements:**\n\n| Improvement | Formula/Description |\n|-------------|---------------------|\n| **Clip Higher** | $\\varepsilon_{high} = 0.28$ instead of $0.2$ ‚Äî more freedom to improve |\n| **Dynamic Sampling** | Filter groups where all rewards are equal (no gradient) |\n| **Token-level Loss** | Each token weighted equally, not each sample |\n| **Overlong Penalty** | Penalty for answers exceeding limit |\n\n**When to use:** For long reasoning (math, code).\n\n---\n\n## 4Ô∏è‚É£ SDPO (Self-Distilled Policy Optimization)\n\nüìÑ **Paper:** [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)\n\n**Idea:** Successful trajectories become the \"teacher\". Model learns not just to maximize reward, but to imitate successful solutions.\n\n**Loss Formula:**\n$$L = L_{GRPO} + \\lambda \\cdot D_{KL}(\\pi_{student} || \\pi_{teacher})$$\n\n**How it works:**\n1. Generate a group of answers\n2. Answers with reward ‚â• threshold ‚Üí teacher\n3. Add KL-loss between student and teacher\n\n**Advantages:**\n- Denser learning signal (not just scalar)\n- Environment feedback (compilation/runtime errors)\n- Natural regularization",
  
  "docs.grpo_rewards": "Rewards",
  "docs.grpo_rewards_content": "### How Rewards Work\n\nReward is a number showing how good an answer is.\n\n**Example for math:**\n```python\nreward = 0.0\n\nif has_think_tags:        # Has reasoning?\n    reward += 0.2\n    \nif answer == correct:     # Answer correct?\n    reward += 0.8\n```\n\n### Built-in rewards\n\n| Type | What it checks |\n|------|----------------|\n| **Format** | Required tags present |\n| **Exact Match** | Answer = reference |\n| **Regex** | Pattern in text |\n| **Length** | Length penalty |",
  
  "docs.grpo_tips": "Comparison",
  "docs.grpo_tips_content": "### Algorithm Comparison Table\n\n| Algorithm | Advantage | Clipping | Features |\n|-----------|-----------|----------|----------|\n| **GRPO** | $(r - \\mu) / \\sigma$ | $[0.8, 1.2]$ | Baseline |\n| **Dr.GRPO** | $r - \\mu$ | $[0.8, 1.2]$ | No std, fixed length |\n| **DAPO** | $r - \\mu$ | $[0.8, 1.28]$ | Dynamic sampling, token-level |\n| **SDPO** | $r - \\mu$ | $[0.8, 1.2]$ | + Self-distillation |\n\n### Which to Choose?\n\n```\n‚îå‚îÄ Simple task, quick start?\n‚îÇ  ‚îî‚îÄ‚Üí GRPO\n‚îÇ\n‚îú‚îÄ Model generates too short answers?\n‚îÇ  ‚îî‚îÄ‚Üí Dr.GRPO\n‚îÇ\n‚îú‚îÄ Long reasoning (math, code)?\n‚îÇ  ‚îî‚îÄ‚Üí DAPO\n‚îÇ\n‚îî‚îÄ Verifiable tasks + want maximum quality?\n   ‚îî‚îÄ‚Üí SDPO\n```",

  "docs.lora_tutorial": "LoRA Fine-Tuning",
  "docs.lora_intro": "## üîß LoRA: Efficient Fine-Tuning\n\n**LoRA (Low-Rank Adaptation)** lets you fine-tune large models by training only a small number of additional parameters, while keeping the original model frozen.\n\n### Why LoRA?\n\n| Full Fine-Tuning | LoRA |\n|-----------------|------|\n| Updates ALL parameters | Updates ~0.1-1% of parameters |\n| Needs 2-3x model VRAM | Needs ~1.2x model VRAM |\n| Creates full model copy | Creates small adapter (~10-100MB) |\n| Risk of catastrophic forgetting | Preserves base model knowledge |",
  
  "docs.lora_math": "How LoRA Works",
  "docs.lora_math_explanation": "Instead of updating the full weight matrix **W**, we add a low-rank decomposition:\n\n- **W_frozen** ‚Äî Original model weights (not trained)\n- **B** ‚Äî Down-projection matrix (d √ó r)\n- **A** ‚Äî Up-projection matrix (r √ó d)  \n- **Œ±** ‚Äî Scaling factor\n- **r** ‚Äî Rank (typically 8-64)\n\nThis means instead of training d√ód parameters, we only train 2√ód√ór parameters!",
  
  "docs.lora_params": "Key Parameters",
  "docs.lora_params_content": "### Parameter Guide\n\n| Parameter | Range | Effect |\n|-----------|-------|--------|\n| **r (rank)** | 8-128 | Higher = more capacity, more VRAM |\n| **alpha** | r to 2√ór | Scaling. alpha=r is common default |\n| **dropout** | 0.0-0.1 | Regularization. 0.05 is safe default |\n| **target_modules** | varies | Which layers to adapt |\n\n### Target Modules\n\n```python\n# Minimal (fastest, least capacity)\ntarget_modules = [\"q_proj\", \"v_proj\"]\n\n# Standard (good balance)\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n# Full (maximum adaptation)\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n```",
  
  "docs.lora_recommendation": "Recommended Settings",
  "docs.lora_task_type": "What's your task?",
  "docs.lora_task_sft": "SFT / Instruction Tuning",
  "docs.lora_task_grpo": "GRPO / RL Training",
  "docs.lora_task_domain": "Domain Adaptation",
  "docs.lora_rec_sft": "**For SFT:** r=16, alpha=32, target=[q,k,v,o]_proj\n\nThis gives good instruction-following with minimal overhead.",
  "docs.lora_rec_grpo": "**For GRPO:** r=32-64, alpha=64-128, target=all linear layers\n\nRL needs more capacity to learn new reasoning patterns.",
  "docs.lora_rec_domain": "**For Domain:** r=64-128, alpha=128-256, target=all linear layers\n\nDomain shift requires significant adaptation capacity.",
  
  "docs.lora_content": "### üìã Using LoRA in Models at Home\n\n#### 1. Enable LoRA\n\nIn the sidebar under **üîß Tuning Method**, select **LoRA**.\n\n#### 2. Configure Parameters\n\n- **r (rank):** Start with 16, increase if underfitting\n- **alpha:** Set equal to r, or 2√ór for stronger adaptation\n- **target_modules:** Use preset or customize\n\n#### 3. Training\n\nLoRA training is the same as full fine-tuning, but:\n- Uses less VRAM (can train larger models)\n- Trains faster (fewer parameters to update)\n- Saves small adapter files\n\n#### 4. Merging\n\nAfter training, you can:\n- **Keep separate:** Load base + adapter (flexible, can swap adapters)\n- **Merge:** Combine into single model (simpler deployment)\n\nUse **\"Merge LoRA\"** checkbox in save settings to auto-merge.\n\n---\n\n### üéì QLoRA: Quantized LoRA\n\n**QLoRA** combines LoRA with 4-bit quantization:\n\n- Base model in 4-bit (NF4 format)\n- LoRA adapters in fp16/bf16\n- Computation in bf16\n\nThis allows fine-tuning 7B models on 8GB VRAM!",

  "docs.distributed_training": "Multi-GPU Training",
  "docs.distributed_intro": "## üñ•Ô∏è Distributed Training\n\nWhen one GPU isn't enough, you have several options for scaling across multiple GPUs or machines.\n\n### Quick Decision Guide\n\n```\n                    ‚îå‚îÄ Model fits in 1 GPU?\n                    ‚îÇ\n            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n            ‚îÇ YES           ‚îÇ NO\n            ‚ñº               ‚ñº\n        Use DDP         Use FSDP/DeepSpeed\n        (fastest)       (memory efficient)\n```",
  
  "docs.distributed_decision": "Find Your Setup",
  "docs.distributed_num_gpus": "How many GPUs?",
  "docs.distributed_model_fits": "Model fits in single GPU memory?",
  "docs.distributed_rec_single": "üí° **Single GPU:** No parallelism needed. Enable optimizations like Flash Attention and Gradient Checkpointing for best performance.",
  "docs.distributed_rec_ddp": "üí° **DDP (Distributed Data Parallel):** Best choice! Each GPU holds full model copy. Select `Multi-GPU (DDP)` in parallelism settings.",
  "docs.distributed_rec_fsdp": "üí° **FSDP or DeepSpeed ZeRO-3:** Model is sharded across GPUs. More overhead but enables training larger models. Select `FSDP` or `DeepSpeed ZeRO-3`.",
  
  "docs.distributed_content": "### üìã Setup Guide\n\n#### DDP (Distributed Data Parallel)\n\n**When:** Model fits in single GPU, you have 2+ GPUs\n\n**How it works:**\n1. Each GPU has full model copy\n2. Data is split across GPUs\n3. Gradients are synchronized after backward pass\n\n**In Models at Home:**\n1. Go to **üñ•Ô∏è GPU and Memory** in sidebar\n2. Select **Multi-GPU (DDP)**\n3. Choose which GPUs to use\n\n---\n\n#### FSDP (Fully Sharded Data Parallel)\n\n**When:** Model doesn't fit in single GPU\n\n**How it works:**\n1. Model weights are sharded across GPUs\n2. Each GPU holds 1/N of the model\n3. Weights are gathered when needed for computation\n\n**In Models at Home:**\n1. Select **FSDP** in parallelism settings\n2. Optionally enable **CPU Offload** for even larger models\n\n---\n\n#### DeepSpeed ZeRO\n\n**Stages:**\n- **ZeRO-1:** Optimizer states sharded\n- **ZeRO-2:** + Gradients sharded\n- **ZeRO-3:** + Model weights sharded\n\n**In Models at Home:**\n1. Select **DeepSpeed ZeRO-2** or **ZeRO-3**\n2. ZeRO-3 + Offload for maximum memory savings",
  
  "docs.distributed_comparison": "Comparison Table",
  "docs.distributed_table": "| Method | Memory per GPU | Speed | Complexity | Best For |\n|--------|---------------|-------|------------|----------|\n| **Single GPU** | Full model | ‚ö°‚ö°‚ö° | Simple | Small models |\n| **DDP** | Full model | ‚ö°‚ö°‚ö° | Simple | Model fits in GPU |\n| **FSDP** | Model/N | ‚ö°‚ö° | Medium | Large models |\n| **ZeRO-2** | Model + Opt/N | ‚ö°‚ö° | Medium | Medium models |\n| **ZeRO-3** | Everything/N | ‚ö° | Complex | Very large models |\n| **ZeRO-3 + Offload** | Minimal | üêå | Complex | Huge models, limited VRAM |",

  "docs.optimizations": "Memory & Speed",
  "docs.optimizations_intro": "## ‚ö° Optimizations\n\nGet the most out of your hardware with these optimization techniques. Some save memory, some improve speed ‚Äî many do both!\n\n### Priority Order\n\n1. **Flash Attention** ‚Äî Always enable (free speedup)\n2. **Mixed Precision** ‚Äî Always use bf16 if GPU supports it\n3. **Gradient Checkpointing** ‚Äî When memory-limited\n4. **Liger Kernels** ‚Äî For fused operations\n5. **Fused Loss** ‚Äî Major memory savings for large vocab",
  
  "docs.optimizations_checklist": "Optimization Checklist",
  "docs.opt_memory": "Memory Savers",
  "docs.opt_speed": "Speed Boosters",
  
  "docs.optimizations_content": "### üìã Detailed Guide\n\n#### Flash Attention\n\n**What:** Optimized attention implementation that's both faster and uses less memory.\n\n**Memory:** O(N) instead of O(N¬≤)\n**Speed:** 2-4x faster attention\n\n**Enable:** Check **\"FlashAttention\"** in Precision settings\n\n---\n\n#### Mixed Precision (bf16/fp16)\n\n**What:** Use 16-bit floats instead of 32-bit.\n\n**Memory:** ~50% reduction\n**Speed:** 2x on tensor cores\n\n**Enable:** Check **\"Mixed Precision\"** ‚Äî bf16 preferred if available\n\n---\n\n#### Gradient Checkpointing\n\n**What:** Recompute activations during backward pass instead of storing them.\n\n**Memory:** ~60-70% activation memory reduction\n**Speed:** ~20-30% slower (recomputation cost)\n\n**Enable:** Check **\"Gradient Checkpointing\"** ‚Äî essential for GRPO!\n\n---\n\n#### Liger Kernels\n\n**What:** Triton-optimized kernels for common operations.\n\n**Includes:**\n- RMSNorm\n- RoPE\n- SwiGLU\n- CrossEntropy\n\n**Memory:** 10-20% reduction\n**Speed:** 10-20% faster\n\n**Enable:** Check **\"Liger Kernel\"** in Precision settings\n\n---\n\n#### Fused Loss\n\n**What:** Compute cross-entropy loss without materializing full logits tensor.\n\n**Memory:** Up to 80% reduction in loss computation!\n**Why:** Logits tensor is `[batch, seq_len, vocab_size]` ‚Äî huge for large vocabularies\n\n**Enable:** Check **\"Fused Loss\"** ‚Äî requires Liger",
  
  "docs.opt_vram_calc": "VRAM Estimation",
  "docs.optimizations_vram": "### Rough VRAM Formula\n\n```\nVRAM ‚âà Model + Optimizer + Gradients + Activations\n\nModel (fp16):        2 bytes √ó params\nOptimizer (Adam):    8 bytes √ó params (states)\nGradients (fp16):    2 bytes √ó params  \nActivations:         ~batch √ó seq √ó hidden √ó layers √ó 2 bytes\n```\n\n### Examples\n\n| Model | Base VRAM | With Optimizations |\n|-------|-----------|-------------------|\n| 125M | ~2 GB | ~1 GB |\n| 350M | ~4 GB | ~2 GB |\n| 1B | ~10 GB | ~5 GB |\n| 3B | ~24 GB | ~12 GB |\n| 7B | ~56 GB | ~28 GB |\n\n*\"With Optimizations\" = bf16 + gradient checkpointing + fused loss*",

  "docs.troubleshooting": "Troubleshooting",
  "docs.troubleshooting_intro": "## üõ†Ô∏è Troubleshooting Guide\n\nCommon problems and their solutions. Click on an issue to expand.",
  
  "docs.trouble_common": "Common Issues",
  
  "docs.trouble_oom": "CUDA Out of Memory",
  "docs.trouble_oom_content": "### Solutions (try in order)\n\n1. **Reduce Batch Size** ‚Äî Most direct solution\n2. **Enable Gradient Checkpointing** ‚Äî Trades speed for memory\n3. **Enable Fused Loss** ‚Äî Big savings for large vocabularies\n4. **Use LoRA** ‚Äî Train fewer parameters\n5. **Reduce Sequence Length** ‚Äî If your data allows\n6. **Use DeepSpeed ZeRO** ‚Äî Shard optimizer states\n\n### Quick Fixes\n\n```python\n# Clear cache between runs\nimport torch\ntorch.cuda.empty_cache()\n```\n\n### Memory Debugging\n\nCheck what's using memory:\n```python\nprint(torch.cuda.memory_summary())\n```",
  
  "docs.trouble_loss_spike": "Loss Suddenly Spikes",
  "docs.trouble_loss_spike_content": "### Causes\n\n1. **Learning rate too high**\n   - Fix: Reduce LR by 2-10x\n   \n2. **Bad data batch**\n   - Fix: Check data for outliers, corrupted examples\n   \n3. **Gradient explosion**\n   - Fix: Enable gradient clipping (max_grad_norm=1.0)\n   \n4. **Numerical instability**\n   - Fix: Use bf16 instead of fp16, or add epsilon to norms\n\n### Recovery\n\nIf training was going well before spike:\n1. Load last good checkpoint\n2. Reduce learning rate\n3. Resume training",
  
  "docs.trouble_loss_plateau": "Loss Stops Decreasing",
  "docs.trouble_loss_plateau_content": "### Causes & Solutions\n\n1. **Learning rate too low**\n   - Fix: Increase LR, or restart with warmup\n   \n2. **Hit local minimum**\n   - Fix: Try different LR scheduler (cosine with restarts)\n   \n3. **Model capacity reached**\n   - Fix: Larger model, or accept current loss\n   \n4. **Data exhausted**\n   - Fix: More data, or stop training (you've converged!)\n\n### Diagnostic\n\nCheck validation loss:\n- Val loss still decreasing ‚Üí Train more\n- Val loss also plateau ‚Üí Model has converged\n- Val loss increasing ‚Üí Overfitting, stop training",
  
  "docs.trouble_gibberish": "Model Outputs Gibberish",
  "docs.trouble_gibberish_content": "### After Pretraining\n\n1. **Not enough training**\n   - Fix: Train longer (loss should be < 3.0)\n   \n2. **Bad tokenizer**\n   - Fix: Ensure tokenizer matches model architecture\n\n### After SFT\n\n1. **Chat template wrong**\n   - Fix: Check preview, ensure format matches training data\n   \n2. **Catastrophic forgetting**\n   - Fix: Use LoRA, lower LR, fewer steps\n   \n3. **Wrong model loaded**\n   - Fix: Verify you're loading the SFT checkpoint, not base\n\n### After GRPO\n\n1. **Over-optimization**\n   - Fix: Load earlier checkpoint, add KL penalty\n   \n2. **Reward hacking**\n   - Fix: Improve reward function, add format requirements",
  
  "docs.trouble_slow": "Training is Very Slow",
  "docs.trouble_slow_content": "### Checklist\n\n1. **Flash Attention enabled?**\n   - 2-4x speedup for attention\n   \n2. **Using GPU?**\n   - Check: `torch.cuda.is_available()`\n   \n3. **Batch size too small?**\n   - GPU underutilized. Increase batch or use gradient accumulation\n   \n4. **Data loading bottleneck?**\n   - Check CPU usage. If high, data loading is slow\n   - Fix: Preprocess data, use faster storage\n   \n5. **Logging too often?**\n   - Logging every step is slow. Use every 10-100 steps\n\n### Expected Speeds\n\n| Model | GPU | Tokens/sec (approx) |\n|-------|-----|--------------------|\n| 125M | RTX 3090 | 50-100K |\n| 350M | RTX 3090 | 20-50K |\n| 1B | RTX 3090 | 10-20K |\n| 7B | A100 80GB | 5-15K |",
  
  "docs.trouble_grpo_no_improve": "GRPO Reward Not Improving",
  "docs.trouble_grpo_no_improve_content": "### Diagnostic Questions\n\n1. **Is reward variance > 0?**\n   - If all rewards are same, no learning signal\n   - Fix: Make reward function more discriminative\n   \n2. **Is group size large enough?**\n   - Need variety to estimate advantages\n   - Fix: Increase to 8-16\n   \n3. **Is temperature too low?**\n   - All generations identical = no exploration\n   - Fix: Increase to 0.7-1.0\n   \n4. **Is base model good enough?**\n   - GRPO refines, doesn't teach\n   - Fix: Better SFT first\n\n### Common Fixes\n\n| Problem | Solution |\n|---------|----------|\n| Reward always 0 | Easier problems, lower bar |\n| Reward always 1 | Harder problems, stricter grading |\n| Reward oscillates | Lower LR, more steps |\n| Model gets worse | Add KL penalty, early stop |",
  
  "docs.trouble_diagnostics": "Diagnostics",
  "docs.troubleshooting_diagnostics": "### Health Checks\n\n#### 1. GPU Check\n```bash\nnvidia-smi\n```\nShould show your GPU with CUDA support.\n\n#### 2. PyTorch Check\n```python\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n```\n\n#### 3. Memory Check\n```python\nprint(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\nprint(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n```\n\n#### 4. Common Environment Issues\n\n| Issue | Check | Fix |\n|-------|-------|-----|\n| Wrong CUDA | `nvcc --version` | Install matching CUDA toolkit |\n| Old PyTorch | `torch.__version__` | Update PyTorch |\n| Missing Flash | `pip show flash-attn` | `pip install flash-attn` |",
  
  "backend.models_at_home": "models-at-home",
  "backend.unsloth": "Unsloth",
  "backend.recommended": "recommended",
  "backend.faster": "faster",
  
  "algorithm.grpo": "GRPO",
  "algorithm.grpo_desc": "Standard Group Relative Policy Optimization",
  "algorithm.dapo": "DAPO (Dynamic sAmpling)",
  "algorithm.dr_grpo": "Dr.GRPO (improved)",
  
  "common.enabled": "Enabled",
  "common.disabled": "Disabled",
  "common.yes": "Yes",
  "common.no": "No",
  "common.none": "None",
  "common.select": "Select",
  "common.cancel": "Cancel",
  "common.confirm": "Confirm",
  "common.loading": "Loading...",
  "common.processing": "Processing...",
  "common.weight": "Weight",
  "common.name": "Name",
  "common.type": "Type",
  "common.path": "Path",
  "common.size": "Size",
  "common.date": "Date",
  "common.actions": "Actions",
  "common.optional": "optional",
  "common.compare_with": "Compare with",
  "common.select_column": "Select column",
  "common.status": "Status",
  "common.name": "Name",
  "common.weight": "Weight",
  "common.value": "Value",
  "common.number": "Number",
  "common.min_length": "Min length",
  "common.max_length": "Max length",
  "common.unknown": "Unknown",
  "common.none": "none",
  "common.configuration": "Configuration",
  "common.control": "Control",
  
  "visual_builder.subtitle": "Model Architecture Editor",
  "visual_builder.footer": "üöÄ Model Builder supports standard PyTorch blocks and custom operations."
}
