"""
Unsloth backend –¥–ª—è SFT —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Unsloth –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- 2x –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ
- –î–æ 70% –º–µ–Ω—å—à–µ VRAM
- Triton —è–¥—Ä–∞ (RMSNorm, RoPE, SwiGLU)
- Smart Gradient Checkpointing
"""
from __future__ import annotations

import json
import logging
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

import torch

logger = logging.getLogger(__name__)


def is_unsloth_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Unsloth."""
    try:
        import unsloth
        return True
    except ImportError:
        return False


def run_unsloth_sft(
    config: Dict[str, Any],
    metrics_logger: Any,
) -> None:
    """
    –ó–∞–ø—É—Å–∫ SFT —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Unsloth backend.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        metrics_logger: –õ–æ–≥–≥–µ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è UI
    """
    # –í–ê–ñ–ù–û: Unsloth –¥–æ–ª–∂–µ–Ω –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –ü–ï–†–í–´–ú, –¥–æ transformers/trl
    try:
        from unsloth import FastLanguageModel
        from unsloth import is_bfloat16_supported
    except ImportError as e:
        raise ImportError(
            "Unsloth –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–µ—Ä–µ–∑: pip install unsloth\n"
            f"–û—à–∏–±–∫–∞: {e}"
        )
    
    from trl import SFTTrainer
    try:
        from trl import SFTConfig as TrainingArguments
    except ImportError:
        from transformers import TrainingArguments
    
    from datasets import load_dataset, Dataset
    from transformers import DataCollatorForSeq2Seq
    
    metrics_logger.update(status="loading_model", backend="unsloth")
    
    # === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
    base_model_path = config.get("base_model_path")
    if not base_model_path:
        raise ValueError("base_model_path required for SFT")
    
    max_seq_length = config.get("seq_len", 2048)
    dtype = None  # Auto-detect
    
    # Precision
    mixed_precision = config.get("mixed_precision", "bf16")
    if mixed_precision == "bf16" and is_bfloat16_supported():
        dtype = torch.bfloat16
    elif mixed_precision == "fp16":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    # LoRA config
    use_lora = config.get("use_lora", True)
    lora_r = config.get("lora_r", 16)
    lora_alpha = config.get("lora_alpha", 32)
    lora_dropout = config.get("lora_dropout", 0.0)
    
    # Quantization
    load_in_4bit = config.get("load_in_4bit", True)
    
    # === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Unsloth ===
    logger.info(f"ü¶• Unsloth: Loading model from {base_model_path}")
    logger.info(f"   max_seq_length={max_seq_length}, dtype={dtype}, load_in_4bit={load_in_4bit}")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=base_model_path,
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
        trust_remote_code=True,
    )
    
    # === –î–æ–±–∞–≤–ª—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã ===
    if use_lora:
        logger.info(f"ü¶• Unsloth: Adding LoRA adapters (r={lora_r}, alpha={lora_alpha})")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º target modules –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        model = FastLanguageModel.get_peft_model(
            model,
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
            ],
            bias="none",
            use_gradient_checkpointing="unsloth",  # Unsloth smart checkpointing
            random_state=42,
            max_seq_length=max_seq_length,
        )
    
    # Pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    metrics_logger.update(status="loading_dataset")
    
    # === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===
    data_path = config.get("data_path")
    if not data_path:
        raise ValueError("data_path required for SFT")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
    sft_columns = config.get("sft_columns", {})
    sft_template = config.get("sft_template", {})
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    if data_path.endswith((".json", ".jsonl")):
        dataset = load_dataset("json", data_files=data_path, split="train")
    else:
        # HuggingFace dataset
        dataset = load_dataset(data_path, split="train")
    
    # === –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SFT ===
    def format_instruction(example):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä –≤ instruction-response —Ñ–æ—Ä–º–∞—Ç."""
        fmt = sft_columns.get("format", "instruct")
        
        if fmt == "chat":
            # Chat format —Å messages
            messages_col = sft_columns.get("messages", "messages")
            messages = example.get(messages_col, [])
            if not messages:
                return {"text": ""}
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template –µ—Å–ª–∏ –µ—Å—Ç—å
            if hasattr(tokenizer, "apply_chat_template"):
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False,
                )
            else:
                # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é
                text = ""
                for msg in messages:
                    role = msg.get("role", "")
                    content = msg.get("content", "")
                    if role == "user":
                        text += f"### User:\n{content}\n\n"
                    elif role == "assistant":
                        text += f"### Assistant:\n{content}\n\n"
                    elif role == "system":
                        text += f"{content}\n\n"
            
            return {"text": text}
        else:
            # Instruct format
            instr_col = sft_columns.get("instruction", "instruction")
            input_col = sft_columns.get("input", "input")
            output_col = sft_columns.get("output", "output")
            
            instruction = example.get(instr_col, "")
            inp = example.get(input_col, "")
            output = example.get(output_col, "")
            
            # –®–∞–±–ª–æ–Ω
            system = sft_template.get("system", "You are a helpful assistant.")
            user_tag = sft_template.get("user_tag", "### User:")
            bot_tag = sft_template.get("bot_tag", "### Assistant:")
            separator = sft_template.get("separator", "\n\n")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            text = f"{system}{separator}"
            if inp:
                text += f"{user_tag}{separator}{instruction}\n\nInput: {inp}{separator}"
            else:
                text += f"{user_tag}{separator}{instruction}{separator}"
            text += f"{bot_tag}{separator}{output}"
            
            return {"text": text}
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
    dataset = dataset.filter(lambda x: len(x.get("text", "")) > 0)
    
    logger.info(f"ü¶• Dataset prepared: {len(dataset)} examples")
    
    metrics_logger.update(
        status="training",
        num_train_examples=len(dataset),
        backend="unsloth",
    )
    
    # === Training Arguments ===
    output_dir = Path(config.get("output_dir", "out/unsloth_sft"))
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –í—ã—á–∏—Å–ª—è–µ–º —à–∞–≥–∏
    batch_size = config.get("batch_size", 4)
    gradient_accumulation = config.get("gradient_accumulation", 4)
    epochs = config.get("epochs", 1)
    max_steps = config.get("max_steps", -1)  # -1 = –≤—Å–µ —ç–ø–æ—Ö–∏
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=epochs,
        max_steps=max_steps if max_steps > 0 else -1,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation,
        learning_rate=config.get("learning_rate", 2e-4),
        weight_decay=config.get("weight_decay", 0.01),
        warmup_steps=config.get("warmup_steps", 100),
        lr_scheduler_type=config.get("lr_schedule", "cosine"),
        logging_steps=config.get("log_every", 10),
        save_steps=config.get("save_every", 500),
        save_total_limit=3,
        bf16=mixed_precision == "bf16" and is_bfloat16_supported(),
        fp16=mixed_precision == "fp16",
        optim="adamw_8bit",  # Unsloth —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç 8bit optimizer
        seed=42,
        max_grad_norm=config.get("max_grad_norm", 1.0),
        report_to="none",  # –ú—ã —Å–∞–º–∏ –ª–æ–≥–∏—Ä—É–µ–º
    )
    
    # === –°–æ–∑–¥–∞—ë–º Trainer ===
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        args=training_args,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        packing=False,  # –ë–µ–∑ packing –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
    )
    
    # === Callback –¥–ª—è –º–µ—Ç—Ä–∏–∫ ===
    from transformers import TrainerCallback
    
    class MetricsCallback(TrainerCallback):
        def __init__(self, metrics_logger, start_time):
            self.metrics_logger = metrics_logger
            self.start_time = start_time
        
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs is None:
                return
            
            step = state.global_step
            loss = logs.get("loss", 0.0)
            lr = logs.get("learning_rate", 0.0)
            
            # –í—Ä–µ–º—è —à–∞–≥–∞
            elapsed = time.time() - self.start_time
            samples_per_sec = (step * batch_size * gradient_accumulation) / max(1, elapsed)
            
            self.metrics_logger.log_step(
                step=step,
                loss=loss,
                lr=lr,
                samples_per_sec=samples_per_sec,
            )
        
        def on_save(self, args, state, control, **kwargs):
            ckpt_path = str(output_dir / f"checkpoint-{state.global_step}")
            self.metrics_logger.log_checkpoint(ckpt_path)
    
    trainer.add_callback(MetricsCallback(metrics_logger, time.time()))
    
    # === –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ ===
    logger.info("ü¶• Unsloth: Starting training...")
    start_time = time.time()
    
    # Unsloth train
    try:
        from unsloth import unsloth_train
        unsloth_train(trainer)
    except ImportError:
        # Fallback –µ—Å–ª–∏ unsloth_train –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        trainer.train()
    
    total_time = time.time() - start_time
    
    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ ===
    metrics_logger.update(status="saving_model")
    
    final_dir = output_dir / "final_model"
    final_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (–µ—Å–ª–∏ use_lora=True) –∏–ª–∏ –≤—Å—é –º–æ–¥–µ–ª—å
    if use_lora:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
        model.save_pretrained(final_dir)
        tokenizer.save_pretrained(final_dir)
        logger.info(f"ü¶• Saved LoRA adapters to {final_dir}")
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: merge –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏
        if config.get("merge_lora", False):
            merged_dir = output_dir / "merged_model"
            merged_dir.mkdir(parents=True, exist_ok=True)
            
            # Merge LoRA –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
            model = model.merge_and_unload()
            model.save_pretrained(merged_dir)
            tokenizer.save_pretrained(merged_dir)
            logger.info(f"ü¶• Saved merged model to {merged_dir}")
    else:
        # Full fine-tuning - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë
        model.save_pretrained(final_dir)
        tokenizer.save_pretrained(final_dir)
        logger.info(f"ü¶• Saved full model to {final_dir}")
    
    # === –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===
    hours, rem = divmod(total_time, 3600)
    minutes, seconds = divmod(rem, 60)
    duration_str = f"{int(hours):02}:{int(minutes):02}:{seconds:05.2f}"
    
    metrics_logger.update(
        status="completed",
        total_time_seconds=total_time,
        training_duration=duration_str,
        final_model_path=str(final_dir),
        backend="unsloth",
    )
    
    logger.info(f"ü¶• Unsloth SFT completed in {duration_str}")
