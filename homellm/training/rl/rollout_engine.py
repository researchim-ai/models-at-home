"""
Rollout engine (–æ—Ç–¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏):

- training engine (DDP/ZeRO/FSDP) –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ teacher-forcing logprobs + backprop
- rollout engine –æ—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞ autoregressive generate()

–í–∞–∂–Ω–æ–µ –¥–ª—è ZeRO-3/FSDP:
autoregressive generation –≤–Ω—É—Ç—Ä–∏ sharded training engine –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ –ø–æ—Ä—è–¥–∫–∏ –º–µ–¥–ª–µ–Ω–Ω–µ–µ.

–î–≤–∞ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã:
1. LoRA fine-tuning: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (–±—ã—Å—Ç—Ä–æ, ~MB)
2. Full fine-tuning: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –≤–µ—Å–∞ (–¥–æ—Ä–æ–≥–æ, ~GB, –¥–µ–ª–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏)
"""

from __future__ import annotations

import json
import logging
import os
import select
import subprocess
import sys
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch
from transformers import AutoModelForCausalLM, PreTrainedModel

logger = logging.getLogger(__name__)




def _is_zero3_model(model: Any, accelerator: Any) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—ë—Ä–Ω—É—Ç–∞ –ª–∏ –º–æ–¥–µ–ª—å –≤ DeepSpeed ZeRO-3."""
    if accelerator is None:
        return False
    ds_plugin = getattr(accelerator.state, "deepspeed_plugin", None)
    if ds_plugin is None:
        return False
    return getattr(ds_plugin, "zero_stage", 0) == 3


def _gather_full_state_dict_zero3(
    model: PreTrainedModel,
    accelerator: Any,
) -> Optional[Dict[str, torch.Tensor]]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –ø–æ–ª–Ω—ã–π state_dict –¥–ª—è ZeRO-3 sharded –º–æ–¥–µ–ª–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç state_dict —Ç–æ–ª—å–∫–æ –Ω–∞ rank 0, –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö ‚Äî None.
    """
    try:
        import deepspeed
        from deepspeed.runtime.zero.partition_parameters import GatheredParameters
    except ImportError:
        logger.warning("DeepSpeed –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ –º–æ–≥—É —Å–æ–±—Ä–∞—Ç—å ZeRO-3 –≤–µ—Å–∞")
        return None

    # –î–ª—è ZeRO-3: —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ rank 0
    params = list(model.parameters())
    
    with GatheredParameters(params, modifier_rank=0):
        if accelerator.is_main_process:
            # –ù–∞ rank 0 —Ç–µ–ø–µ—Ä—å –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é
            state_dict = model.state_dict()
            # –ö–æ–ø–∏—Ä—É–µ–º –Ω–∞ CPU —á—Ç–æ–±—ã –æ—Å–≤–æ–±–æ–¥–∏—Ç—å GPU
            state_dict = {k: v.cpu().clone() for k, v in state_dict.items()}
            return state_dict
    
    return None


def _gather_lora_state_dict(
    model: PreTrainedModel,
    accelerator: Any,
) -> Tuple[Dict[str, torch.Tensor], bool]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –º–æ–¥–µ–ª–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (state_dict, is_lora_model).
    –î–ª—è ZeRO-3 ‚Äî —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
    """
    is_peft = getattr(model, "peft_type", None) is not None or hasattr(model, "get_base_model")
    
    if not is_peft:
        return {}, False
    
    # –ü–æ–ª—É—á–∞–µ–º trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—ç—Ç–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã)
    trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}
    
    if _is_zero3_model(model, accelerator):
        try:
            from deepspeed.runtime.zero.partition_parameters import GatheredParameters
            
            params_to_gather = list(trainable_params.values())
            with GatheredParameters(params_to_gather, modifier_rank=0):
                if accelerator.is_main_process:
                    # –ö–æ–ø–∏—Ä—É–µ–º trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    lora_state = {k: v.cpu().clone() for k, v in trainable_params.items()}
                    return lora_state, True
            return {}, True  # –ù–µ main process
        except ImportError:
            pass
    
    # –î–ª—è –Ω–µ-ZeRO-3: –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º
    lora_state = {k: v.cpu().clone() for k, v in trainable_params.items()}
    return lora_state, True


@dataclass
class RolloutSyncStats:
    synced_keys: int
    synced_tensors: int
    total_numel: int


class HFRolloutEngine:
    """
    –û—Ç–¥–µ–ª—å–Ω–∞—è HF-–º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏:
    1. LoRA-only (trainable_only=True): —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã ‚Äî –±—ã—Å—Ç—Ä–æ
    2. Full weights (trainable_only=False): —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –≤–µ—Å–∞ ‚Äî –º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è full fine-tuning
    
    –ü—Ä–∏ ZeRO-3/FSDP –≤–µ—Å–∞ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ GatheredParameters –Ω–∞ rank 0, 
    –∑–∞—Ç–µ–º broadcast –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–Ω–∫–∏.
    """

    def __init__(
        self,
        base_model_path: str,
        device: torch.device,
        torch_dtype: torch.dtype,
        use_flash_attention: bool = True,
        trust_remote_code: bool = True,
        offload_to_cpu: bool = False,
    ) -> None:
        self.base_model_path = str(base_model_path)
        self.device = device
        self.torch_dtype = torch_dtype
        self.use_flash_attention = bool(use_flash_attention)
        self.trust_remote_code = bool(trust_remote_code)
        self.offload_to_cpu = bool(offload_to_cpu)

        self.model: Optional[PreTrainedModel] = None
        self._sync_count = 0

    def ensure_loaded(self) -> None:
        if self.model is not None:
            return

        model_kwargs = {
            "torch_dtype": self.torch_dtype,
            "trust_remote_code": self.trust_remote_code,
        }
        if self.use_flash_attention:
            # HF >= 4.36: attn_implementation supports flash_attention_2 for compatible models
            model_kwargs["attn_implementation"] = "flash_attention_2"

        logger.info(f"üß© RolloutEngine(HF): loading model {self.base_model_path}...")
        model = AutoModelForCausalLM.from_pretrained(self.base_model_path, **model_kwargs)
        model.eval()
        model.requires_grad_(False)

        if self.offload_to_cpu:
            model.to(torch.device("cpu"))
        else:
            model.to(self.device)

        self.model = model
        logger.info("üß© RolloutEngine(HF): model loaded")

    def ensure_on_device(self) -> None:
        self.ensure_loaded()
        assert self.model is not None
        if self.offload_to_cpu:
            # move to target device just-in-time for generation
            self.model.to(self.device)

    def maybe_offload(self) -> None:
        if not self.offload_to_cpu:
            return
        if self.model is None:
            return
        self.model.to(torch.device("cpu"))
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def apply_state_dict(
        self,
        state_dict: Dict[str, torch.Tensor],
        *,
        strict: bool,
    ) -> RolloutSyncStats:
        self.ensure_loaded()
        assert self.model is not None

        missing, unexpected = self.model.load_state_dict(state_dict, strict=strict)
        if strict and (missing or unexpected):
            raise RuntimeError(f"RolloutEngine state_dict mismatch: missing={len(missing)}, unexpected={len(unexpected)}")

        total_numel = 0
        for v in state_dict.values():
            try:
                total_numel += int(v.numel())
            except Exception:
                pass

        return RolloutSyncStats(
            synced_keys=len(state_dict),
            synced_tensors=len(state_dict),
            total_numel=total_numel,
        )

    def sync_weights(
        self,
        training_model: PreTrainedModel,
        accelerator: Any,
        trainable_only: bool = True,
    ) -> Optional[RolloutSyncStats]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –∏–∑ training model –≤ rollout engine.
        
        Args:
            training_model: –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ ZeRO-3/DDP)
            accelerator: Accelerator –æ–±—ä–µ–∫—Ç
            trainable_only: True = —Ç–æ–ª—å–∫–æ LoRA (–±—ã—Å—Ç—Ä–æ), False = –≤—Å–µ –≤–µ—Å–∞ (–º–µ–¥–ª–µ–Ω–Ω–æ)
        
        Returns:
            RolloutSyncStats –∏–ª–∏ None –µ—Å–ª–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞ –Ω–∞ —ç—Ç–æ–º —Ä–∞–Ω–∫–µ.
        """
        self.ensure_loaded()
        is_zero3 = _is_zero3_model(training_model, accelerator)
        
        if trainable_only:
            # LoRA-only —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
            lora_state, is_lora = _gather_lora_state_dict(training_model, accelerator)
            
            if not is_lora:
                logger.warning(
                    "‚ö†Ô∏è trainable_only=True, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LoRA. "
                    "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞. –î–ª—è full fine-tuning —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ trainable_only=False."
                )
                return None
            
            if accelerator.is_main_process and lora_state:
                # Broadcast LoRA state dict –Ω–∞ –≤—Å–µ —Ä–∞–Ω–∫–∏
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ shared storage –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
                stats = self.apply_state_dict(lora_state, strict=False)
                self._sync_count += 1
                logger.info(f"üîÑ RolloutEngine: LoRA sync #{self._sync_count}, {stats.total_numel:,} params")
                return stats
            elif not accelerator.is_main_process:
                # –ù–∞ –Ω–µ-main –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö –∂–¥—ë–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
                accelerator.wait_for_everyone()
                return None
        else:
            # Full weights —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
            if is_zero3:
                state_dict = _gather_full_state_dict_zero3(training_model, accelerator)
            else:
                # DDP –∏–ª–∏ single GPU ‚Äî –ø—Ä–æ—Å—Ç–æ –±–µ—Ä—ë–º state_dict
                unwrapped = accelerator.unwrap_model(training_model) if accelerator else training_model
                state_dict = {k: v.cpu().clone() for k, v in unwrapped.state_dict().items()}
            
            if accelerator.is_main_process and state_dict:
                stats = self.apply_state_dict(state_dict, strict=True)
                self._sync_count += 1
                logger.info(f"üîÑ RolloutEngine: Full sync #{self._sync_count}, {stats.total_numel:,} params")
                return stats
            elif not accelerator.is_main_process:
                accelerator.wait_for_everyone()
                return None
        
        return None
    
    @torch.inference_mode()
    def generate(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        **generate_kwargs,
    ) -> torch.Tensor:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ HF model.generate().
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç offload –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω.
        """
        self.ensure_on_device()
        assert self.model is not None
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Ö–æ–¥—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        device = next(self.model.parameters()).device
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        
        outputs = self.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **generate_kwargs,
        )
        
        self.maybe_offload()
        return outputs


class VLLMSubprocessEngine:
    """
    vLLM —á–µ—Ä–µ–∑ subprocess.Popen ‚Äî –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å –Ω–∞ –û–¢–î–ï–õ–¨–ù–û–ô GPU!
    
    –ó–∞–ø—É—Å–∫–∞–µ—Ç vLLM –≤ –ù–ê–°–¢–û–Ø–©–ï–ú –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ —Å CUDA_VISIBLE_DEVICES 
    —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –î–û –∑–∞–ø—É—Å–∫–∞ Python –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä–∞.
    
    –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ stdin/stdout —Å JSON lines.
    """
    
    def __init__(
        self,
        base_model_path: str,
        torch_dtype: torch.dtype,
        gpu_id: int = 0,
        max_model_len: int = 4096,
        gpu_memory_utilization: float = 0.85,
        enable_lora: bool = True,
        max_lora_rank: int = 64,  # vLLM max_lora_rank - –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= lora_r
        output_dir: Optional[str] = None,
    ) -> None:
        self.base_model_path = str(base_model_path)
        self.torch_dtype = torch_dtype
        self.gpu_id = int(gpu_id)
        self.max_model_len = int(max_model_len)
        self.gpu_memory_utilization = float(gpu_memory_utilization)
        self.enable_lora = bool(enable_lora)
        self.max_lora_rank = int(max_lora_rank)
        self.output_dir = output_dir
        
        self._process = None
        self._lora_adapter_path: Optional[str] = None
        self._sync_count = 0
    
    def ensure_loaded(self) -> None:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç subprocess —Å vLLM –µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–ø—É—â–µ–Ω."""
        if self._process is not None and self._process.poll() is None:
            return
        
        logger.info(f"üß© VLLMSubprocessEngine: –∑–∞–ø—É—Å–∫ –Ω–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π GPU {self.gpu_id}")
        logger.info(f"üß© VLLMSubprocessEngine: model={self.base_model_path}, memory={self.gpu_memory_utilization:.0%}")
        
        # –ü—É—Ç—å –∫ worker —Å–∫—Ä–∏–ø—Ç—É
        worker_script = Path(__file__).parent / "vllm_worker.py"
        if not worker_script.exists():
            raise RuntimeError(f"vLLM worker script not found: {worker_script}")
        
        # –°–æ–∑–¥–∞—ë–º environment —Å CUDA_VISIBLE_DEVICES
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = str(self.gpu_id)
        # –û—á–∏—â–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –º–µ—à–∞—Ç—å
        env.pop("CUDA_DEVICE_ORDER", None)
        
        logger.info(f"üß© VLLMSubprocessEngine: –∑–∞–ø—É—Å–∫ —Å CUDA_VISIBLE_DEVICES={self.gpu_id}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º subprocess
        self._process = subprocess.Popen(
            [sys.executable, str(worker_script)],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=None,  # stderr –∏–¥—ë—Ç –≤ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            env=env,
            bufsize=1,  # line buffered
            text=True,
        )
        logger.info(f"üß© VLLMSubprocessEngine: subprocess started (PID={self._process.pid})")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        dtype_str = "bfloat16" if self.torch_dtype == torch.bfloat16 else (
            "float16" if self.torch_dtype == torch.float16 else "float32"
        )
        config = {
            "model_path": self.base_model_path,
            "dtype": dtype_str,
            "max_model_len": self.max_model_len,
            "gpu_memory_utilization": self.gpu_memory_utilization,
            "enable_lora": self.enable_lora,
            "max_lora_rank": self.max_lora_rank,
        }
        self._send(config)
        
        # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        logger.info(f"üß© VLLMSubprocessEngine: –æ–∂–∏–¥–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏...")
        try:
            response = self._recv(timeout=300)  # 5 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É
            if response.get("status") == "error":
                raise RuntimeError(f"vLLM worker failed: {response.get('error')}")
            logger.info(f"üß© VLLMSubprocessEngine: ‚úÖ ready on physical GPU {self.gpu_id}")
        except Exception as e:
            logger.error(f"üß© VLLMSubprocessEngine: failed to start: {e}")
            self.shutdown()
            raise RuntimeError(f"vLLM subprocess failed to start: {e}")
    
    def _send(self, data: dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç JSON –≤ subprocess."""
        line = json.dumps(data) + "\n"
        self._process.stdin.write(line)
        self._process.stdin.flush()
    
    def _recv(self, timeout: float = 60) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç JSON –∏–∑ subprocess."""
        # –ñ–¥—ë–º –¥–∞–Ω–Ω—ã–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        ready, _, _ = select.select([self._process.stdout], [], [], timeout)
        if not ready:
            raise TimeoutError(f"vLLM worker timeout after {timeout}s")
        
        line = self._process.stdout.readline()
        if not line:
            raise RuntimeError("vLLM worker closed connection")
        
        return json.loads(line.strip())
    
    def shutdown(self) -> None:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç subprocess."""
        if self._process is not None:
            try:
                self._send({"cmd": "shutdown"})
            except:
                pass
            self._process.terminate()
            try:
                self._process.wait(timeout=5)
            except:
                self._process.kill()
            self._process = None
        logger.info("üß© VLLMSubprocessEngine: shutdown")
    
    def __del__(self):
        self.shutdown()
    
    def set_lora_adapter(self, *, lora_path: Optional[str], lora_name: Optional[str] = None, lora_int_id: int = 1) -> None:
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä."""
        self.ensure_loaded()
        # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º lora_int_id —á—Ç–æ–±—ã vLLM –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏–ª –∞–¥–∞–ø—Ç–µ—Ä
        self._send({
            "cmd": "set_lora", 
            "lora_path": lora_path,
            "lora_name": lora_name or "rollout_lora",
            "lora_int_id": int(lora_int_id),
        })
        response = self._recv(timeout=60)
        if response.get("status") == "error":
            raise RuntimeError(f"set_lora failed: {response.get('error')}")
        self._lora_adapter_path = lora_path
        logger.info(f"üß© VLLMSubprocessEngine: LoRA set to {lora_path} (id={lora_int_id})")
    
    def make_sampling_params(
        self,
        *,
        n: int,
        temperature: float,
        top_p: float,
        max_tokens: int,
        stop_token_ids: Optional[List[int]] = None,
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–∞–∫ dict –¥–ª—è subprocess)."""
        params = {
            "n": int(n),
            "temperature": float(temperature),
            "top_p": float(top_p),
            "max_tokens": int(max_tokens),
        }
        if stop_token_ids is not None:
            params["stop_token_ids"] = list(stop_token_ids)
        return params
    
    def generate(
        self,
        prompts: List[str],
        sampling_params: Any,
    ) -> List[Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç completions —á–µ—Ä–µ–∑ subprocess."""
        self.ensure_loaded()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º SamplingParams –≤ dict
        if isinstance(sampling_params, dict):
            params_dict = sampling_params
        elif hasattr(sampling_params, "__dict__"):
            params_dict = {
                "n": getattr(sampling_params, "n", 1),
                "max_tokens": getattr(sampling_params, "max_tokens", 1024),
                "temperature": getattr(sampling_params, "temperature", 0.7),
                "top_p": getattr(sampling_params, "top_p", 0.9),
                "stop_token_ids": getattr(sampling_params, "stop_token_ids", None),
            }
        else:
            params_dict = {}
        
        self._send({
            "cmd": "generate",
            "prompts": prompts,
            "sampling_params": params_dict,
        })
        
        response = self._recv(timeout=600)  # 10 –º–∏–Ω—É—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        if response.get("status") == "error":
            raise RuntimeError(f"generate failed: {response.get('error')}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–±—ä–µ–∫—Ç—ã –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ vLLM outputs
        outputs = response.get("outputs", [])
        return [_VLLMOutput(o) for o in outputs]
    
    def sync_weights(
        self,
        training_model: PreTrainedModel,
        accelerator: Any,
        trainable_only: bool = True,
    ) -> Optional[RolloutSyncStats]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –≤ subprocess."""
        is_peft = getattr(training_model, "peft_type", None) is not None or hasattr(training_model, "get_base_model")
        
        if not trainable_only or not is_peft:
            logger.warning("‚ö†Ô∏è VLLMSubprocessEngine –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ LoRA sync")
            return None
        
        self.ensure_loaded()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
        if self.output_dir:
            adapter_dir = Path(self.output_dir) / "rollout_engine" / "vllm_adapters" / f"step_{self._sync_count}"
        else:
            adapter_dir = Path(tempfile.mkdtemp()) / f"vllm_lora_{self._sync_count}"
        
        adapter_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            training_model.save_pretrained(str(adapter_dir), safe_serialization=True)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è LoRA: {e}")
            return None
        
        self.set_lora_adapter(lora_path=str(adapter_dir))
        self._sync_count += 1
        
        return RolloutSyncStats(
            time_sync=0.0,
            time_save=0.0,
            params_synced=0,
            bytes_synced=0,
        )


class _VLLMOutput:
    """–ü—Ä–æ—Å—Ç–æ–π wrapper –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ subprocess."""
    def __init__(self, data: dict):
        self.prompt = data.get("prompt", "")
        self.outputs = [_VLLMCompletionOutput(o) for o in data.get("outputs", [])]


class _VLLMCompletionOutput:
    """Wrapper –¥–ª—è –æ–¥–Ω–æ–≥–æ completion."""
    def __init__(self, data: dict):
        self.text = data.get("text", "")
        self.token_ids = data.get("token_ids", [])
        self.finish_reason = data.get("finish_reason", None)


class VLLMRolloutEngine:
    """
    vLLM rollout engine ‚Äî –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–ù–ê –¢–û–ô –ñ–ï GPU).
    
    –í–ê–ñ–ù–û: –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–æ–π –∂–µ GPU —á—Ç–æ training!
    –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è vLLM –Ω–∞ –û–¢–î–ï–õ–¨–ù–û–ô GPU –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ VLLMSubprocessEngine.

    vLLM –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è inference throughput (continuous batching, PagedAttention).
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏:
    
    1. **LoRA fine-tuning** (trainable_only=True):
       - –ë—ã—Å—Ç—Ä–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ LoRARequest
       - –ê–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –¥–∏—Å–∫, vLLM –ø–æ–¥–≥—Ä—É–∂–∞–µ—Ç –µ–≥–æ "–Ω–∞ –ª–µ—Ç—É"
       - ~—Å–µ–∫—É–Ω–¥—ã –Ω–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é
    
    2. **Full fine-tuning** (trainable_only=False):
       - vLLM –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–æ—Ä—è—á—É—é –∑–∞–º–µ–Ω—É –≤—Å–µ—Ö –≤–µ—Å–æ–≤
       - –ü–æ—ç—Ç–æ–º—É: —Å–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint ‚Üí –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º vLLM
       - ~5-15 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é (–¥–ª—è 1.5B –º–æ–¥–µ–ª–∏)
       - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å rollout_sync_interval –¥–æ 10-50
       - –í—Å—ë —Ä–∞–≤–Ω–æ –±—ã—Å—Ç—Ä–µ–µ —á–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ ZeRO-3!
    """

    def __init__(
        self,
        base_model_path: str,
        torch_dtype: torch.dtype,
        trust_remote_code: bool = True,
        tensor_parallel_size: int = 1,
        max_model_len: int = 4096,
        gpu_memory_utilization: float = 0.90,
        enable_lora: bool = True,
        max_lora_rank: int = 64,  # vLLM max_lora_rank - –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= lora_r
        output_dir: Optional[str] = None,
    ) -> None:
        self.base_model_path = str(base_model_path)
        self.torch_dtype = torch_dtype
        self.trust_remote_code = bool(trust_remote_code)
        self.tensor_parallel_size = int(tensor_parallel_size)
        self.max_model_len = int(max_model_len)
        self.gpu_memory_utilization = float(gpu_memory_utilization)
        self.enable_lora = bool(enable_lora)
        self.max_lora_rank = int(max_lora_rank)
        self.output_dir = output_dir

        self.llm = None
        self._sampling_params_cls = None
        self._lora_request_cls = None
        self._current_lora_request = None
        self._lora_adapter_path: Optional[str] = None
        self._sync_count = 0

    def ensure_loaded(self) -> None:
        if self.llm is not None:
            return
        try:
            from vllm import LLM, SamplingParams
        except Exception as e:
            raise RuntimeError(
                "vLLM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `pip install vllm` (—Ç—Ä–µ–±—É–µ—Ç—Å—è CUDA) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ backend='hf'."
            ) from e

        # LoRARequest API –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –ø—É—Ç—è—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏ vLLM
        lora_request_cls = None
        try:
            from vllm.lora.request import LoRARequest  # type: ignore
            lora_request_cls = LoRARequest
        except Exception:
            try:
                from vllm.lora import LoRARequest  # type: ignore
                lora_request_cls = LoRARequest
            except Exception:
                lora_request_cls = None

        self._sampling_params_cls = SamplingParams
        self._lora_request_cls = lora_request_cls

        dtype_str = "bfloat16" if self.torch_dtype == torch.bfloat16 else ("float16" if self.torch_dtype == torch.float16 else "float32")
        logger.info(
            f"üß© RolloutEngine(vLLM): loading model {self.base_model_path} "
            f"(tp={self.tensor_parallel_size}, dtype={dtype_str}, max_model_len={self.max_model_len}, enable_lora={self.enable_lora})"
        )

        # –í vLLM –≤—ã–±–æ—Ä GPU –¥–µ–ª–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ CUDA_VISIBLE_DEVICES –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å.
        llm_kwargs = {
            "model": self.base_model_path,
            "trust_remote_code": self.trust_remote_code,
            "dtype": dtype_str,
            "tensor_parallel_size": self.tensor_parallel_size,
            "max_model_len": self.max_model_len,
            "gpu_memory_utilization": self.gpu_memory_utilization,
        }
        if self.enable_lora:
            llm_kwargs["enable_lora"] = True
            # vLLM —Ç—Ä–µ–±—É–µ—Ç max_loras –∏ max_lora_rank –µ—Å–ª–∏ enable_lora=True
            llm_kwargs["max_loras"] = 1
            llm_kwargs["max_lora_rank"] = self.max_lora_rank
        
        self.llm = LLM(**llm_kwargs)
        logger.info("üß© RolloutEngine(vLLM): model loaded")

    def set_lora_adapter(self, *, lora_path: Optional[str], lora_name: Optional[str] = None, lora_int_id: int = 1) -> None:
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        –ï—Å–ª–∏ lora_path=None ‚Äî –æ—Ç–∫–ª—é—á–∞–µ—Ç LoRA.
        """
        self.ensure_loaded()
        if self._lora_request_cls is None:
            if lora_path is None:
                self._current_lora_request = None
                return
            raise RuntimeError("vLLM LoRARequest –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ vLLM.")

        if lora_path is None:
            self._current_lora_request = None
            self._lora_adapter_path = None
            return

        name = lora_name or "rollout_lora"
        self._current_lora_request = self._lora_request_cls(name, int(lora_int_id), str(lora_path))
        self._lora_adapter_path = lora_path
        logger.info(f"üß© RolloutEngine(vLLM): LoRA adapter set: {lora_path}")

    def sync_weights(
        self,
        training_model: PreTrainedModel,
        accelerator: Any,
        trainable_only: bool = True,
    ) -> Optional[RolloutSyncStats]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –∏–∑ training model –≤ vLLM.
        
        –î–≤–∞ —Ä–µ–∂–∏–º–∞:
        1. trainable_only=True (LoRA): –±—ã—Å—Ç—Ä–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ LoRARequest
        2. trainable_only=False (full): –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ–π –º–æ–¥–µ–ª–∏ (–¥–æ—Ä–æ–≥–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)
        
        Args:
            training_model: –ú–æ–¥–µ–ª—å (PEFT –∏–ª–∏ –æ–±—ã—á–Ω–∞—è)
            accelerator: Accelerator –æ–±—ä–µ–∫—Ç
            trainable_only: True = —Ç–æ–ª—å–∫–æ LoRA, False = –≤—Å–µ –≤–µ—Å–∞ (–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ vLLM)
        """
        is_peft = getattr(training_model, "peft_type", None) is not None or hasattr(training_model, "get_base_model")
        
        if trainable_only:
            # LoRA sync ‚Äî –±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å —á–µ—Ä–µ–∑ LoRARequest
            if not is_peft:
                logger.warning(
                    "‚ö†Ô∏è trainable_only=True, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ PEFT. "
                    "–î–ª—è full fine-tuning —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ trainable_only=False."
                )
                return None
            
            self.ensure_loaded()
            lora_state, _ = _gather_lora_state_dict(training_model, accelerator)
            
            if accelerator.is_main_process and lora_state:
                self._sync_count += 1
                adapter_save_path = self._get_adapter_save_path()
                
                try:
                    training_model.save_pretrained(adapter_save_path)
                    logger.info(f"üîÑ RolloutEngine(vLLM): LoRA adapter saved to {adapter_save_path}")
                except Exception as e:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä: {e}")
                    return None
                
                self.set_lora_adapter(lora_path=adapter_save_path, lora_name=f"step_{self._sync_count}")
                
                total_numel = sum(v.numel() for v in lora_state.values())
                logger.info(f"üîÑ RolloutEngine(vLLM): LoRA sync #{self._sync_count}, {total_numel:,} params")
                
                return RolloutSyncStats(
                    synced_keys=len(lora_state),
                    synced_tensors=len(lora_state),
                    total_numel=total_numel,
                )
            
            if not accelerator.is_main_process:
                accelerator.wait_for_everyone()
            return None
        
        else:
            # Full weights sync ‚Äî –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º vLLM —Å –Ω–æ–≤—ã–º–∏ –≤–µ—Å–∞–º–∏
            # –≠—Ç–æ –¥–æ—Ä–æ–≥–æ (~5-15 —Å–µ–∫), –Ω–æ –≤—Å—ë —Ä–∞–≤–Ω–æ –±—ã—Å—Ç—Ä–µ–µ —á–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ ZeRO-3
            return self._sync_full_weights(training_model, accelerator)
    
    def _sync_full_weights(
        self,
        training_model: PreTrainedModel,
        accelerator: Any,
    ) -> Optional[RolloutSyncStats]:
        """
        –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤: —Å–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint, –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º vLLM.
        """
        import time
        
        is_zero3 = _is_zero3_model(training_model, accelerator)
        self._sync_count += 1
        
        # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint'–∞
        checkpoint_path = self._get_full_checkpoint_path()
        
        # 1. –°–æ–±–∏—Ä–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –Ω–∞ rank 0
        if accelerator.is_main_process:
            logger.info(f"üîÑ RolloutEngine(vLLM): Full sync #{self._sync_count} ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint...")
            start_time = time.time()
            
            if is_zero3:
                # ZeRO-3: —Å–æ–±–∏—Ä–∞–µ–º sharded –≤–µ—Å–∞
                state_dict = _gather_full_state_dict_zero3(training_model, accelerator)
                if state_dict:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ HF
                    unwrapped = accelerator.unwrap_model(training_model)
                    unwrapped.save_pretrained(
                        checkpoint_path,
                        state_dict=state_dict,
                        safe_serialization=True,
                    )
            else:
                # DDP –∏–ª–∏ single GPU
                unwrapped = accelerator.unwrap_model(training_model)
                unwrapped.save_pretrained(checkpoint_path, safe_serialization=True)
            
            save_time = time.time() - start_time
            logger.info(f"üîÑ RolloutEngine(vLLM): checkpoint saved in {save_time:.1f}s")
        
        # 2. –ë–∞—Ä—å–µ—Ä ‚Äî –≤—Å–µ —Ä–∞–Ω–∫–∏ –∂–¥—É—Ç –ø–æ–∫–∞ checkpoint —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è
        accelerator.wait_for_everyone()
        
        # 3. –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º vLLM –Ω–∞ –≤—Å–µ—Ö —Ä–∞–Ω–∫–∞—Ö —Å –Ω–æ–≤—ã–º checkpoint'–æ–º
        if accelerator.is_main_process:
            logger.info(f"üîÑ RolloutEngine(vLLM): –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º vLLM —Å {checkpoint_path}...")
            start_time = time.time()
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π vLLM
        old_llm = self.llm
        self.llm = None
        del old_llm
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # –ú–µ–Ω—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –Ω–∞ checkpoint
        self.base_model_path = str(checkpoint_path)
        # –î–ª—è full fine-tuning –æ—Ç–∫–ª—é—á–∞–µ–º LoRA –≤ vLLM
        self.enable_lora = False
        self._current_lora_request = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–∞–Ω–æ–≤–æ
        self.ensure_loaded()
        
        if accelerator.is_main_process:
            reload_time = time.time() - start_time
            logger.info(f"üîÑ RolloutEngine(vLLM): vLLM reloaded in {reload_time:.1f}s")
        
        # –ë–∞—Ä—å–µ—Ä –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏
        accelerator.wait_for_everyone()
        
        # –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        try:
            from transformers import AutoConfig
            cfg = AutoConfig.from_pretrained(checkpoint_path)
            estimated_params = getattr(cfg, "num_parameters", None)
            if estimated_params is None:
                # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞
                estimated_params = cfg.hidden_size * cfg.num_hidden_layers * 12
        except Exception:
            estimated_params = 0
        
        return RolloutSyncStats(
            synced_keys=1,  # –æ–¥–∏–Ω checkpoint
            synced_tensors=1,
            total_numel=estimated_params,
        )
    
    def _get_full_checkpoint_path(self) -> str:
        """–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ checkpoint'–∞."""
        if self.output_dir:
            ckpt_dir = Path(self.output_dir) / "rollout_engine" / "vllm_checkpoints" / f"step_{self._sync_count}"
        else:
            ckpt_dir = Path(tempfile.gettempdir()) / "vllm_full_checkpoints" / f"step_{self._sync_count}"
        
        ckpt_dir.mkdir(parents=True, exist_ok=True)
        return str(ckpt_dir)
    
    def _get_adapter_save_path(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞."""
        if self.output_dir:
            adapter_dir = Path(self.output_dir) / "rollout_engine" / "vllm_adapters" / f"step_{self._sync_count}"
        else:
            adapter_dir = Path(tempfile.gettempdir()) / "vllm_lora_adapters" / f"step_{self._sync_count}"
        
        adapter_dir.mkdir(parents=True, exist_ok=True)
        return str(adapter_dir)

    def make_sampling_params(
        self,
        *,
        n: int,
        temperature: float,
        top_p: float,
        max_tokens: int,
        stop_token_ids: Optional[List[int]] = None,
    ):
        self.ensure_loaded()
        assert self._sampling_params_cls is not None
        kwargs = {
            "n": int(n),
            "temperature": float(temperature),
            "top_p": float(top_p),
            "max_tokens": int(max_tokens),
        }
        if stop_token_ids is not None:
            kwargs["stop_token_ids"] = list(stop_token_ids)
        return self._sampling_params_cls(**kwargs)

    def generate(self, prompts: List[str], sampling_params) -> List:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ vLLM.
        
        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ (—Å—Ç—Ä–æ–∫–∏)
            sampling_params: vLLM SamplingParams
            
        Returns:
            list[RequestOutput] (vLLM API)
        """
        self.ensure_loaded()
        assert self.llm is not None
        # lora_request –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º –±–∞—Ç—á–∞
        return self.llm.generate(prompts, sampling_params, lora_request=self._current_lora_request)

