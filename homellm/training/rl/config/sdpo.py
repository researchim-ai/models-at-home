"""
SDPO (Self-Distilled Policy Optimization) –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è.

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è SDPO —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç–∞—Ç—å–µ "Reinforcement Learning via Self-Distillation":
- Self-distillation: –º–æ–¥–µ–ª—å —Å feedback –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è teacher
- Reprompting: –º–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∏/–∏–ª–∏ feedback –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
- Environment feedback: –æ—à–∏–±–∫–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏/runtime, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤
- Top-K Distillation —Å tail bucket –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

Paper: https://arxiv.org/abs/2601.20802
"""
from dataclasses import dataclass, field
from typing import Optional, Dict, Any

from .base import RLAlgorithm
from .drgrpo import DrGRPOConfig


@dataclass
class SDPOConfig(DrGRPOConfig):
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è SDPO –∞–ª–≥–æ—Ä–∏—Ç–º–∞.
    
    SDPO —Ä–∞—Å—à–∏—Ä—è–µ—Ç GRPO —Å self-distillation:
    - –ú–æ–¥–µ–ª—å —Å feedback/solution –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è "teacher"
    - Teacher –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    - KL divergence –º–µ–∂–¥—É student –∏ teacher —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏
    
    –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è: pl–æ—Ç–Ω—ã–π credit assignment –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤
    –≤–º–µ—Å—Ç–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ reward –Ω–∞ –≤–µ—Å—å –æ—Ç–≤–µ—Ç.
    
    Attributes:
        # Core SDPO
        success_threshold: –ü–æ—Ä–æ–≥ reward –¥–ª—è "—É—Å–ø–µ—à–Ω–æ–π" —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        alpha: –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è KL divergence (0=forward, 1=reverse, 0.5=JSD)
        
        # Distillation
        full_logit_distillation: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        distillation_topk: Top-k –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è distillation (None = –≤—Å—ë)
        distillation_add_tail: –î–æ–±–∞–≤–ª—è—Ç—å tail bucket –¥–ª—è top-k
        is_clip: Importance Sampling clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        # Reprompting
        include_environment_feedback: –í–∫–ª—é—á–∞—Ç—å feedback –æ—Ç —Å—Ä–µ–¥—ã
        environment_feedback_only_without_solution: Feedback —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç solution
        dont_reprompt_on_self_success: –ù–µ reprompt —É—Å–ø–µ—à–Ω—É—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é —Å–∞–º—É –Ω–∞ —Å–µ–±—è
        remove_thinking_from_demonstration: –£–±–∏—Ä–∞—Ç—å <think> –∏–∑ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π
        max_reprompt_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ reprompt –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        reprompt_truncation: –ú–µ—Ç–æ–¥ –æ–±—Ä–µ–∑–∫–∏ ("left", "right", "error")
        
        # Teacher
        ema_rate: EMA rate –¥–ª—è teacher –º–æ–¥–µ–ª–∏ (0 = –±–µ–∑ EMA)
        loss_weight: –í–µ—Å SDPO loss –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ GRPO loss
    """
    
    # ============================================================
    # SDPO CORE PARAMETERS
    # ============================================================
    
    # –ü–æ—Ä–æ–≥ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
    # –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Å reward >= threshold –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    success_threshold: float = 1.0
    
    # Alpha –¥–ª—è KL divergence:
    # - 0.0: Forward KL (student ‚Üí teacher) ‚Äî mode-seeking, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
    # - 1.0: Reverse KL (teacher ‚Üí student) ‚Äî mode-covering  
    # - 0.5: Jensen-Shannon Divergence
    alpha: float = 0.0
    
    # ============================================================
    # DISTILLATION PARAMETERS
    # ============================================================
    
    # Full-logit vs selected-token distillation
    # True: KL –ø–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é [vocab_size] –∏–ª–∏ [top-k] ‚Äî —Ç–æ—á–Ω–µ–µ
    # False: KL —Ç–æ–ª—å–∫–æ –ø–æ selected tokens ‚Äî —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
    full_logit_distillation: bool = True
    
    # Top-k –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è distillation
    # None = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –ª–æ–≥–∏—Ç—ã (–¥–æ—Ä–æ–≥–æ –ø–æ –ø–∞–º—è—Ç–∏!)
    # 50-100 = —Ç–æ–ª—å–∫–æ top-k (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
    distillation_topk: Optional[int] = 100
    
    # üî• –î–æ–±–∞–≤–ª—è—Ç—å "tail" bucket –¥–ª—è top-k distillation
    # Tail = log(1 - sum(top_k_probs)) ‚Äî —É—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Å—Ç–∞–≤—à—É—é—Å—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
    # True: –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
    # False: —Ç–æ–ª—å–∫–æ top-k –±–µ–∑ tail
    distillation_add_tail: bool = True
    
    # Importance Sampling clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—â–∏—Ö—Å—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
    # None = –±–µ–∑ IS weighting
    is_clip: Optional[float] = 2.0
    
    # ============================================================
    # REPROMPTING PARAMETERS
    # ============================================================
    
    # üî• –í–∫–ª—é—á–∞—Ç—å environment feedback –≤ reprompt
    # True: –æ—à–∏–±–∫–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏, runtime errors, failed tests –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è
    include_environment_feedback: bool = True
    
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å feedback —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ solution
    # True: feedback OR solution, –Ω–æ –Ω–µ –æ–±–∞ –≤–º–µ—Å—Ç–µ
    # False: feedback –ò solution –º–æ–≥—É—Ç –±—ã—Ç—å –≤–º–µ—Å—Ç–µ
    environment_feedback_only_without_solution: bool = True
    
    # üî• –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É—Å–ø–µ—à–Ω—É—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –∫–∞–∫ teacher –¥–ª—è —Å–∞–º–æ–π —Å–µ–±—è
    # True: –≤—ã–±–∏—Ä–∞–µ–º –¥—Ä—É–≥—É—é —É—Å–ø–µ—à–Ω—É—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –∏–∑ –≥—Ä—É–ø–ø—ã
    # False: —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤–æ–∏–º –∂–µ teacher
    dont_reprompt_on_self_success: bool = True
    
    # üî• –£–±–∏—Ä–∞—Ç—å <think>...</think> –∏–∑ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π
    # True: —É–±–∏—Ä–∞–µ–º chain-of-thought, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç
    # False: –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π reasoning
    remove_thinking_from_demonstration: bool = True
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ reprompted –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
    max_reprompt_len: int = 10240
    
    # –ú–µ—Ç–æ–¥ –æ–±—Ä–µ–∑–∫–∏ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ max_reprompt_len
    # "right": –æ–±—Ä–µ–∑–∞–µ–º —Å–ø—Ä–∞–≤–∞ (–Ω–∞—á–∞–ª–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)
    # "left": –æ–±—Ä–µ–∑–∞–µ–º —Å–ª–µ–≤–∞ (–∫–æ–Ω–µ—Ü —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)
    # "error": –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
    reprompt_truncation: str = "right"
    
    # ============================================================
    # TEACHER MODEL PARAMETERS
    # ============================================================
    
    # EMA rate –¥–ª—è teacher –º–æ–¥–µ–ª–∏
    # 0.0: –±–µ–∑ EMA, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ teacher
    # 0.01-0.1: –º–µ–¥–ª–µ–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º—ã–π teacher (–±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ)
    ema_rate: float = 0.05
    
    # ============================================================
    # LOSS COMBINATION
    # ============================================================
    
    # –í–µ—Å SDPO loss (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ loss_mode="sdpo")
    # –ü—Ä–∏ loss_mode="sdpo": —Ç–æ–ª—å–∫–æ distillation loss (loss_weight –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)
    # –ü—Ä–∏ loss_mode="grpo": —Ç–æ–ª—å–∫–æ GRPO loss (loss_weight –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)
    loss_weight: float = 1.0
    
    # –†–µ–∂–∏–º loss: "sdpo" (—Ç–æ–ª—å–∫–æ distillation) –∏–ª–∏ "grpo" (—Ç–æ–ª—å–∫–æ GRPO)
    loss_mode: str = "sdpo"
    
    # ============================================================
    # TEMPLATES
    # ============================================================
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —à–∞–±–ª–æ–Ω –¥–ª—è reprompting
    # Placeholders: {prompt}, {solution}, {feedback}
    reprompt_template: str = """{prompt}{solution}{feedback}

Correctly solve the original question."""
    
    # –®–∞–±–ª–æ–Ω –¥–ª—è solution —Å–µ–∫—Ü–∏–∏
    # Placeholder: {successful_previous_attempt}
    solution_template: str = """

Correct solution:

{successful_previous_attempt}

"""
    
    # –®–∞–±–ª–æ–Ω –¥–ª—è feedback —Å–µ–∫—Ü–∏–∏
    # Placeholder: {feedback_raw}
    feedback_template: str = """

The following is feedback from your unsuccessful earlier attempt:

{feedback_raw}

"""
    
    # ============================================================
    # INHERITED OVERRIDES
    # ============================================================
    
    # SDPO —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ —Å dynamic sampling
    dynamic_sampling: bool = True
    
    # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º Liger Fused Loss –¥–ª—è SDPO (–Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∫ logits)
    liger_fused_grpo: bool = False
    
    @property
    def algorithm(self) -> RLAlgorithm:
        return RLAlgorithm.SDPO
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥ –≤ —Å–ª–æ–≤–∞—Ä—å."""
        result = super().to_dict()
        result.update({
            # Core
            "success_threshold": self.success_threshold,
            "alpha": self.alpha,
            # Distillation
            "full_logit_distillation": self.full_logit_distillation,
            "distillation_topk": self.distillation_topk,
            "distillation_add_tail": self.distillation_add_tail,
            "is_clip": self.is_clip,
            # Reprompting
            "include_environment_feedback": self.include_environment_feedback,
            "environment_feedback_only_without_solution": self.environment_feedback_only_without_solution,
            "dont_reprompt_on_self_success": self.dont_reprompt_on_self_success,
            "remove_thinking_from_demonstration": self.remove_thinking_from_demonstration,
            "max_reprompt_len": self.max_reprompt_len,
            "reprompt_truncation": self.reprompt_truncation,
            # Teacher
            "ema_rate": self.ema_rate,
            "loss_weight": self.loss_weight,
            "loss_mode": self.loss_mode,
        })
        return result
