"""
Loss —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è GRPO/RL.

–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã:
- GRPO (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)
- Dr.GRPO (–±–µ–∑ std –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–ª–∏—Ç–µ–ª—å)
- DAPO (token-level loss, clip higher)
- ü¶Å LigerFusedLinearGRPOLoss ‚Äî –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits (–¥–æ 80% —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏!)
"""
import logging
from typing import Optional, Tuple, TYPE_CHECKING
import torch
import torch.nn as nn
import torch.nn.functional as F

from .experience import Experience
from .legacy_config import GRPOConfig, RLAlgorithm

if TYPE_CHECKING:
    from transformers import PreTrainedModel

logger = logging.getLogger(__name__)


def approx_kl_divergence(
    log_probs: torch.Tensor,
    log_probs_ref: torch.Tensor,
    action_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Monte-Carlo –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ (k3 estimator).
    
    –°–º.: http://joschu.net/blog/kl-approx.html
    
    KL ‚âà exp(log_ref - log) - (log_ref - log) - 1
    
    Args:
        log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–µ [batch, seq_len]
        log_probs_ref: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ [batch, seq_len]
        action_mask: –ú–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—á—ë—Ç–∞ [batch, seq_len]
        
    Returns:
        KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è [batch, seq_len]
    """
    log_ratio = log_probs_ref.float() - log_probs.float()
    if action_mask is not None:
        log_ratio = log_ratio * action_mask
    
    return log_ratio.exp() - log_ratio - 1


def masked_mean(
    tensor: torch.Tensor,
    mask: Optional[torch.Tensor] = None,
    dim: Optional[int] = None,
) -> torch.Tensor:
    """
    –°—Ä–µ–¥–Ω–µ–µ —Å —É—á—ë—Ç–æ–º –º–∞—Å–∫–∏.
    
    Args:
        tensor: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä
        mask: –ú–∞—Å–∫–∞ (1 = —É—á–∏—Ç—ã–≤–∞—Ç—å, 0 = –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å)
        dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —Ä–µ–¥—É–∫—Ü–∏–∏
        
    Returns:
        –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    """
    if mask is None:
        return tensor.mean(dim=dim)
    
    masked = tensor * mask
    return masked.sum(dim=dim) / mask.sum(dim=dim).clamp(min=1e-8)


def masked_sum(
    tensor: torch.Tensor,
    mask: Optional[torch.Tensor] = None,
    dim: Optional[int] = None,
    constant_normalizer: Optional[float] = None,
) -> torch.Tensor:
    """
    –°—É–º–º–∞ —Å –º–∞—Å–∫–æ–π –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Dr.GRPO –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è length bias:
    –≤–º–µ—Å—Ç–æ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é –¥–ª–∏–Ω—É –¥–µ–ª–∏–º –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É.
    
    Args:
        tensor: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä
        mask: –ú–∞—Å–∫–∞
        dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —Ä–µ–¥—É–∫—Ü–∏–∏
        constant_normalizer: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–ª–∏—Ç–µ–ª—å (–¥–ª—è Dr.GRPO)
        
    Returns:
        –°—É–º–º–∞ / –¥–µ–ª–∏—Ç–µ–ª—å
    """
    if mask is None:
        summed = tensor.sum(dim=dim)
    else:
        summed = (tensor * mask).sum(dim=dim)
    
    if constant_normalizer is not None and constant_normalizer > 0:
        return summed / constant_normalizer
    return summed


def compute_advantages(
    returns: torch.Tensor,
    use_std_normalization: bool = True,
    eps: float = 1e-6,
) -> torch.Tensor:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç advantages (–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞) –¥–ª—è –≥—Ä—É–ø–ø—ã rollout'–æ–≤.
    
    –§–æ—Ä–º—É–ª–∞ GRPO:
        A_i = (r_i - mean(r)) / std(r)
    
    –§–æ—Ä–º—É–ª–∞ Dr.GRPO (–±–µ–∑ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ std):
        A_i = r_i - mean(r)
    
    üî• –í–∞–∂–Ω–æ (–∏–∑ verl): –¥–ª—è –≥—Ä—É–ø–ø—ã –∏–∑ 1 —ç–ª–µ–º–µ–Ω—Ç–∞:
        mean = 0, std = 1 (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0)
    
    Args:
        returns: Rewards –¥–ª—è –≥—Ä—É–ø–ø—ã [group_size] –∏–ª–∏ [batch, group_size]
        use_std_normalization: –î–µ–ª–∏—Ç—å –ª–∏ –Ω–∞ std (True –¥–ª—è GRPO, False –¥–ª—è DrGRPO)
        eps: –≠–ø—Å–∏–ª–æ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (verl –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 1e-6)
        
    Returns:
        Advantages —Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    """
    # üî• –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã –∏–∑ 1 —ç–ª–µ–º–µ–Ω—Ç–∞ (–∏–∑ verl)
    # –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ mean = 0, std = 1 —á—Ç–æ–±—ã advantage = reward
    if returns.numel() == 1:
        return returns.clone()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é
    group_size = returns.size(-1) if returns.dim() > 0 else 1
    if group_size == 1:
        # –î–ª—è –≥—Ä—É–ø–ø—ã –∏–∑ 1: advantage = reward (mean=0, std=1)
        return returns.clone()
    
    mean_return = returns.mean(dim=-1, keepdim=True)
    advantages = returns - mean_return
    
    if use_std_normalization:
        std_return = returns.std(dim=-1, keepdim=True)
        # üî• –ï—Å–ª–∏ std –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π (–≤—Å–µ rewards –æ–¥–∏–Ω–∞–∫–æ–≤—ã), –Ω–µ –¥–µ–ª–∏–º
        # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ advantages
        std_return = torch.where(
            std_return < eps,
            torch.ones_like(std_return),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 1 –≤–º–µ—Å—Ç–æ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ eps
            std_return
        )
        advantages = advantages / std_return
    
    return advantages


class GRPOLoss(nn.Module):
    """
    GRPO Loss —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞.
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç:
    - PPO-style surrogate loss —Å –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º
    - KL —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - Token-level vs sample-level –∞–≥—Ä–µ–≥–∞—Ü–∏—é
    - –ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ (clip higher –¥–ª—è DAPO)
    
    –§–æ—Ä–º—É–ª–∞:
        L = -min(ratio * A, clip(ratio) * A) + Œ≤ * KL
    
    –≥–¥–µ ratio = exp(log_œÄ - log_œÄ_old)
    """
    
    def __init__(
        self,
        config: Optional[GRPOConfig] = None,
        clip_eps_low: float = 0.2,
        clip_eps_high: float = 0.2,
        kl_weight: float = 0.0,
        token_level_loss: bool = False,
        fixed_length_normalizer: Optional[int] = None,
    ) -> None:
        """
        Args:
            config: GRPOConfig (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è)
            clip_eps_low: –ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –∫–ª–∏–ø–ø–∏–Ω–≥–∞
            clip_eps_high: –í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –∫–ª–∏–ø–ø–∏–Ω–≥–∞ (–¥–ª—è DAPO: 0.28)
            kl_weight: –í–µ—Å KL —à—Ç—Ä–∞—Ñ–∞ (0 –¥–ª—è reasoning-RL)
            token_level_loss: Token-level –∞–≥—Ä–µ–≥–∞—Ü–∏—è (vs sample-level)
            fixed_length_normalizer: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–ª–∏—Ç–µ–ª—å –¥–ª—è Dr.GRPO
        """
        super().__init__()
        
        if config is not None:
            self.clip_eps_low = config.clip_eps_low
            self.clip_eps_high = config.clip_eps_high
            self.kl_weight = config.kl_weight
            self.token_level_loss = config.token_level_loss
            self.fixed_length_normalizer = config.fixed_length_normalizer
            self.algorithm = config.algorithm
        else:
            self.clip_eps_low = clip_eps_low
            self.clip_eps_high = clip_eps_high
            self.kl_weight = kl_weight
            self.token_level_loss = token_level_loss
            self.fixed_length_normalizer = fixed_length_normalizer
            self.algorithm = RLAlgorithm.GRPO
        
        # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ loss
        self.last_components: dict = {}
    
    def forward(
        self,
        log_probs: torch.Tensor,
        experience: Experience,
    ) -> Tuple[torch.Tensor, dict]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç GRPO loss.
        
        Args:
            log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–µ [batch, seq_len]
            experience: Experience —Å action_log_probs, advantages, masks
            
        Returns:
            Tuple[loss, metrics_dict]
        """
        old_log_probs = experience.action_log_probs
        log_probs_ref = experience.log_probs_ref
        action_mask = experience.action_mask
        advantages = experience.advantages
        
        # KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        if log_probs_ref is not None and self.kl_weight > 0:
            kl = approx_kl_divergence(
                log_probs=log_probs,
                log_probs_ref=log_probs_ref,
                action_mask=action_mask,
            )
        else:
            kl = torch.zeros_like(log_probs)
        
        # Policy ratio: r(Œ∏) = œÄ(a|s) / œÄ_old(a|s) = exp(log_œÄ - log_œÄ_old)
        # üî• Clamp –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–∏–∑ verl)
        log_ratio = log_probs - old_log_probs
        log_ratio = torch.clamp(log_ratio, min=-20.0, max=20.0)
        ratio = log_ratio.exp()
        
        # PPO-style clipped surrogate loss
        # –î–ª—è DAPO –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ [1-eps_low, 1+eps_high]
        ratio_clipped = ratio.clamp(
            1.0 - self.clip_eps_low, 
            1.0 + self.clip_eps_high
        )
        
        # Surrogate objectives
        # verl –∏—Å–ø–æ–ª—å–∑—É–µ—Ç max(-r*A, -clip(r)*A), —á—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ -min(r*A, clip(r)*A)
        pg_losses1 = -advantages * ratio
        pg_losses2 = -advantages * ratio_clipped
        clip_pg_losses = torch.maximum(pg_losses1, pg_losses2)
        
        # üî• Dual-clip PPO –¥–ª—è negative advantages (–∏–∑ verl)
        # –ù–∏–∂–Ω–∏–π –ø–æ—Ä–æ–≥ c=3.0 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ A < 0
        clip_ratio_c = 3.0
        pg_losses_lower = -advantages * clip_ratio_c
        dual_clip_losses = torch.minimum(pg_losses_lower, clip_pg_losses)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º dual-clip —Ç–æ–ª—å–∫–æ –¥–ª—è negative advantages
        policy_loss = torch.where(advantages < 0, dual_clip_losses, clip_pg_losses)
        loss_per_token = policy_loss + self.kl_weight * kl
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è loss
        if self.token_level_loss:
            # Token-level: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –∏–º–µ–µ—Ç —Ä–∞–≤–Ω—ã–π –≤–µ—Å (DAPO)
            if action_mask is not None:
                total_tokens = action_mask.sum()
                loss = (loss_per_token * action_mask).sum() / total_tokens.clamp(min=1)
            else:
                loss = loss_per_token.mean()
        else:
            # Sample-level —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (Dr.GRPO)
            if self.fixed_length_normalizer is not None:
                # Dr.GRPO: –¥–µ–ª–∏–º –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
                loss = masked_sum(
                    loss_per_token, 
                    action_mask, 
                    dim=-1,
                    constant_normalizer=self.fixed_length_normalizer
                ).mean()
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –≤ –∫–∞–∂–¥–æ–º —Å—ç–º–ø–ª–µ
                loss = masked_mean(loss_per_token, action_mask, dim=-1).mean()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        metrics = {
            "loss": loss.item(),
            "kl_mean": masked_mean(kl, action_mask).mean().item() if self.kl_weight > 0 else 0,
            "ratio_mean": ratio.mean().item(),
            "ratio_std": ratio.std().item(),
            "ratio_max": ratio.max().item(),
            "ratio_min": ratio.min().item(),
            "advantages_mean": advantages.mean().item(),
            "advantages_std": advantages.std().item(),
            "clip_fraction": ((ratio < 1 - self.clip_eps_low) | (ratio > 1 + self.clip_eps_high)).float().mean().item(),
            # üî• Dual-clip –º–µ—Ç—Ä–∏–∫–∏ (–∏–∑ verl)
            "dual_clip_fraction": ((advantages < 0) & (clip_pg_losses > pg_losses_lower)).float().mean().item(),
        }
        self.last_components = metrics
        
        return loss, metrics


class PolicyGradientLoss(nn.Module):
    """
    –ü—Ä–æ—Å—Ç–æ–π Policy Gradient loss (REINFORCE).
    
    –ú–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å GRPO.
    
    L = -log_œÄ(a|s) * A
    """
    
    def __init__(
        self,
        baseline_subtract: bool = True,
        normalize_advantages: bool = True,
    ) -> None:
        super().__init__()
        self.baseline_subtract = baseline_subtract
        self.normalize_advantages = normalize_advantages
    
    def forward(
        self,
        log_probs: torch.Tensor,
        returns: torch.Tensor,
        action_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, dict]:
        """
        Args:
            log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π [batch, seq_len]
            returns: Rewards [batch, 1] –∏–ª–∏ [batch]
            action_mask: –ú–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ [batch, seq_len]
            
        Returns:
            loss, metrics
        """
        if returns.dim() == 1:
            returns = returns.unsqueeze(-1)
        
        # Baseline: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –±–∞—Ç—á—É
        if self.baseline_subtract:
            advantages = returns - returns.mean()
        else:
            advantages = returns
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if self.normalize_advantages and advantages.std() > 1e-8:
            advantages = advantages / (advantages.std() + 1e-8)
        
        # Policy gradient: L = -log_œÄ * A
        # –†–∞—Å—à–∏—Ä—è–µ–º advantages –¥–ª—è —É–º–Ω–æ–∂–µ–Ω–∏—è —Å log_probs
        if advantages.size(-1) == 1 and log_probs.dim() > 1:
            advantages = advantages.expand_as(log_probs)
        
        loss_per_token = -log_probs * advantages
        
        if action_mask is not None:
            loss = masked_mean(loss_per_token, action_mask, dim=-1).mean()
        else:
            loss = loss_per_token.mean()
        
        metrics = {
            "loss": loss.item(),
            "advantages_mean": advantages.mean().item(),
            "advantages_std": advantages.std().item(),
        }
        
        return loss, metrics


def compute_entropy(log_probs: torch.Tensor, action_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏.
    
    H = -Œ£ p * log(p) = -Œ£ exp(log_p) * log_p
    
    –î–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–æ–ª–∂–Ω–∞ –º–µ–¥–ª–µ–Ω–Ω–æ —Ä–∞—Å—Ç–∏ –ø—Ä–∏ –∑–¥–æ—Ä–æ–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏.
    –†–µ–∑–∫–æ–µ –ø–∞–¥–µ–Ω–∏–µ = entropy collapse.
    
    Args:
        log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [batch, seq_len]
        action_mask: –ú–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
        
    Returns:
        –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è
    """
    probs = log_probs.exp()
    entropy = -probs * log_probs
    
    if action_mask is not None:
        return masked_mean(entropy, action_mask)
    return entropy.mean()


def compute_overlong_penalty(
    completion_lengths: torch.Tensor,
    max_length: int,
    buffer_length: int = 0,
    penalty_value: float = -1.0,
) -> torch.Tensor:
    """
    –ú—è–≥–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (–∏–∑ DAPO).
    
    - –î–ª–∏–Ω–∞ < (max_length - buffer): —à—Ç—Ä–∞—Ñ = 0
    - –î–ª–∏–Ω–∞ > max_length: —à—Ç—Ä–∞—Ñ = penalty_value
    - –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –¥–ª–∏–Ω–∞: –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
    
    Args:
        completion_lengths: –î–ª–∏–Ω—ã completions [batch]
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ø—É—Å—Ç–∏–º–∞—è –¥–ª–∏–Ω–∞
        buffer_length: –ë—É—Ñ–µ—Ä–Ω–∞—è –∑–æ–Ω–∞
        penalty_value: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ (–æ–±—ã—á–Ω–æ -1)
        
    Returns:
        –®—Ç—Ä–∞—Ñ—ã [batch]
    """
    threshold_start = max_length - buffer_length
    
    penalties = torch.zeros_like(completion_lengths, dtype=torch.float)
    
    # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –∑–æ–Ω–∞: –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
    in_buffer = (completion_lengths >= threshold_start) & (completion_lengths <= max_length)
    if in_buffer.any():
        progress = (completion_lengths[in_buffer] - threshold_start).float() / max(buffer_length, 1)
        penalties[in_buffer] = progress * penalty_value
    
    # –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º–∞
    over_max = completion_lengths > max_length
    penalties[over_max] = penalty_value
    
    return penalties


# ============================================================
# ü¶Å LIGER FUSED GRPO LOSS
# ============================================================

class LigerFusedGRPOLoss(nn.Module):
    """
    üî• Fused GRPO Loss —Å Liger Kernel ‚Äî –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits!
    
    –≠—Ç–æ —Å–∞–º–∞—è –º–æ—â–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è GRPO:
    - –í–º–µ—Å—Ç–æ model() -> logits [B, T, V] -> log_probs -> loss
    - –î–µ–ª–∞–µ—Ç: model(output_hidden_states=True) -> hidden [B, T, H] -> fused_loss
    
    –î–ª—è vocab=150k –∏ seq_len=512 —ç–∫–æ–Ω–æ–º–∏—Ç ~3GB –Ω–∞ batch!
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        loss_fn = LigerFusedGRPOLoss(model, config)
        
        # –í training loop:
        outputs = model(input_ids, attention_mask, output_hidden_states=True)
        hidden_states = outputs.hidden_states[-1]  # –ø–æ—Å–ª–µ–¥–Ω–∏–π layer
        loss, metrics = loss_fn(
            hidden_states=hidden_states,
            selected_token_ids=completion_ids,  # target tokens
            attention_mask=action_mask,
            advantages=advantages,
            old_per_token_logps=old_log_probs,
            ref_per_token_logps=ref_log_probs,
        )
    """
    
    def __init__(
        self,
        model: "PreTrainedModel",
        config: Optional[GRPOConfig] = None,
        beta: float = 0.0,
        loss_type: str = "grpo",
        epsilon_low: float = 0.2,
        epsilon_high: float = 0.2,
        max_completion_length: Optional[int] = None,
        chunk_size: int = 1,
    ):
        super().__init__()
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Liger
        from .liger_utils import get_liger_fused_linear_grpo, is_liger_available
        
        if not is_liger_available():
            raise RuntimeError("LigerFusedGRPOLoss —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π liger-kernel!")
        
        LigerFusedLinearGRPOLoss = get_liger_fused_linear_grpo()
        if LigerFusedLinearGRPOLoss is None:
            raise RuntimeError("LigerFusedLinearGRPOLoss –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ config –∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        if config is not None:
            beta = config.kl_weight
            loss_type = getattr(config, 'liger_grpo_loss_type', 'dapo')
            epsilon_low = config.clip_eps_low
            epsilon_high = config.clip_eps_high
            max_completion_length = config.max_new_tokens
            chunk_size = getattr(config, 'liger_chunk_size', 1)
        
        self.liger_loss = LigerFusedLinearGRPOLoss(
            beta=beta,
            loss_type=loss_type,
            epsilon_low=epsilon_low,
            epsilon_high=epsilon_high,
            max_completion_length=max_completion_length,
            chunk_size=chunk_size,
            use_ref_model=beta > 0,
            compiled=False,  # torch.compile –ø–∞–¥–∞–µ—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –≤ GRPO
            importance_sampling_level="token",
            temperature=1.0,
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ lm_head
        self.lm_head_weight = model.lm_head.weight
        self.lm_head_bias = getattr(model.lm_head, 'bias', None)
        
        self.beta = beta
        self.loss_type = loss_type
        self.last_components: dict = {}
        
        logger.info(f"ü¶Å LigerFusedGRPOLoss –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"   - loss_type: {loss_type}")
        logger.info(f"   - beta (KL weight): {beta}")
        logger.info(f"   - epsilon: [{epsilon_low}, {epsilon_high}]")
        logger.info(f"   - chunk_size: {chunk_size}")
        logger.info(f"   ‚ö° Logits –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É—é—Ç—Å—è ‚Äî —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏!")
    
    def forward(
        self,
        hidden_states: torch.Tensor,  # [batch, seq, hidden] ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state
        selected_token_ids: torch.Tensor,  # [batch, seq] ‚Äî target token IDs
        attention_mask: torch.Tensor,  # [batch, seq] ‚Äî action mask
        advantages: torch.Tensor,  # [batch] ‚Äî advantages
        old_per_token_logps: Optional[torch.Tensor] = None,  # [batch, seq]
        ref_per_token_logps: Optional[torch.Tensor] = None,  # [batch, seq]
    ) -> Tuple[torch.Tensor, dict]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç Fused GRPO Loss.
        
        Args:
            hidden_states: –ü–æ—Å–ª–µ–¥–Ω–∏–π hidden state –º–æ–¥–µ–ª–∏ [batch, seq, hidden]
            selected_token_ids: Target token IDs (completion tokens) [batch, seq]
            attention_mask: –ú–∞—Å–∫–∞ completion —Ç–æ–∫–µ–Ω–æ–≤ [batch, seq]
            advantages: Advantages –¥–ª—è –∫–∞–∂–¥–æ–≥–æ sample [batch]
            old_per_token_logps: Log probs –∏–∑ rollout policy [batch, seq]
            ref_per_token_logps: Log probs –∏–∑ reference model [batch, seq]
        
        Returns:
            (loss, metrics_dict)
        """
        # Liger –æ–∂–∏–¥–∞–µ—Ç _input –≤ —Ñ–æ—Ä–º–∞—Ç–µ [batch, seq, hidden] ‚Äî –ø–µ—Ä–µ–¥–∞—ë–º –Ω–∞–ø—Ä—è–º—É—é –ë–ï–ó reshape
        # chunk_forward –¥–µ–ª–∞–µ—Ç: logits = torch.matmul(input_chunk, weight.t())  # [B, T, H] @ [H, V]
        
        # –í—ã–∑—ã–≤–∞–µ–º Liger Fused Loss
        result = self.liger_loss(
            hidden_states,         # [batch, seq, hidden] ‚Äî –ù–ï –¥–µ–ª–∞–µ–º flatten!
            self.lm_head_weight,   # [vocab, hidden]
            selected_token_ids,    # [batch, seq]
            attention_mask,        # [batch, seq]
            advantages,            # [batch]
            bias=self.lm_head_bias,
            ref_per_token_logps=ref_per_token_logps,
            old_per_token_logps=old_per_token_logps,
        )
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if isinstance(result, tuple):
            loss = result[0]
            metrics_list = result[1] if len(result) > 1 else []
        else:
            loss = result
            metrics_list = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "loss": loss.item() if hasattr(loss, 'item') else float(loss),
            "advantages_mean": advantages.mean().item(),
            "advantages_std": advantages.std().item(),
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ Liger
        if len(metrics_list) >= 1 and self.beta > 0:
            kl_val = metrics_list[0]
            metrics["kl_mean"] = kl_val.item() if hasattr(kl_val, 'item') else float(kl_val)
        
        if len(metrics_list) >= 2:
            clip_idx = 1 if self.beta > 0 else 0
            if clip_idx < len(metrics_list):
                clip_val = metrics_list[clip_idx]
                metrics["clip_fraction"] = clip_val.item() if hasattr(clip_val, 'item') else float(clip_val)
        
        self.last_components = metrics
        return loss, metrics
    
    def forward_with_experience(
        self,
        hidden_states: torch.Tensor,
        experience: Experience,
    ) -> Tuple[torch.Tensor, dict]:
        """
        –£–¥–æ–±–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å Experience –æ–±—ä–µ–∫—Ç–æ–º.
        
        Args:
            hidden_states: –ü–æ—Å–ª–µ–¥–Ω–∏–π hidden state [batch, seq, hidden]
            experience: Experience –æ–±—ä–µ–∫—Ç —Å action_mask, advantages –∏ —Ç.–¥.
        
        Returns:
            (loss, metrics_dict)
        """
        # CAUSAL SHIFT –¥–ª—è next-token prediction:
        # - hidden_states[i] –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç tokens[i+1]
        # - action_log_probs —É–∂–µ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä [batch, T] (shifted –ø—Ä–∏ rollout)
        # - action_mask —É–∂–µ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä [batch, T] (shifted –ø—Ä–∏ rollout)
        #
        # –í–ê–ñ–ù–û: sequences –∏ action_log_probs –±–∞—Ç—á–∞—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ,
        # –ø–æ—ç—Ç–æ–º—É –∏—Ö max_len –º–æ–∂–µ—Ç –Ω–µ —Å–æ–≤–ø–∞–¥–∞—Ç—å –Ω–∞ 1!
        # –ù—É–∂–Ω–æ –≤—ã—Ä–æ–≤–Ω—è—Ç—å –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –¥–æ –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
        seq_len = hidden_states.size(1)  # –æ—Ç –º–æ–¥–µ–ª–∏
        target_len = experience.action_log_probs.size(1)  # shifted logprobs
        
        # –°–¥–≤–∏–≥–∞–µ–º hidden states (—É–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π ‚Äî –æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω)
        # –†–µ–∑—É–ª—å—Ç–∞—Ç: [batch, seq_len-1, hidden]
        shifted_hidden = hidden_states[:, :-1, :].contiguous()
        
        # –°–¥–≤–∏–≥–∞–µ–º sequences (–ø–æ–ª—É—á–∞–µ–º labels)
        # –†–µ–∑—É–ª—å—Ç–∞—Ç: [batch, seq_len-1]
        shifted_labels = experience.sequences[:, 1:].contiguous()
        
        # –¢–µ–ø–µ—Ä—å shifted_hidden –∏ shifted_labels –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä seq_len-1
        # –ê action_log_probs –∏ action_mask –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä target_len
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–æ –º–µ–Ω—å—à–µ–≥–æ –∏–∑ –¥–≤—É—Ö
        final_len = min(shifted_hidden.size(1), target_len)
        
        # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –¥–æ final_len (—Å–ø—Ä–∞–≤–∞, —Ç.–∫. left-padding)
        shifted_hidden = shifted_hidden[:, -final_len:, :].contiguous()
        shifted_labels = shifted_labels[:, -final_len:].contiguous()
        action_log_probs = experience.action_log_probs[:, -final_len:].contiguous()
        action_mask = experience.action_mask[:, -final_len:]
        ref_log_probs = experience.log_probs_ref
        if ref_log_probs is not None:
            ref_log_probs = ref_log_probs[:, -final_len:].contiguous()
        
        # Advantages –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å [batch], –Ω–µ [batch, 1]
        advantages = experience.advantages
        if advantages.dim() > 1:
            advantages = advantages.squeeze(-1)
        
        # Action mask: Liger –æ–∂–∏–¥–∞–µ—Ç float/int, –Ω–µ bool
        if action_mask.dtype == torch.bool:
            action_mask = action_mask.to(shifted_hidden.dtype)
        
        return self.forward(
            hidden_states=shifted_hidden,             # [batch, final_len, hidden]
            selected_token_ids=shifted_labels,        # [batch, final_len]
            attention_mask=action_mask,               # [batch, final_len] ‚Äî float
            advantages=advantages,                    # [batch]
            old_per_token_logps=action_log_probs,     # [batch, final_len]
            ref_per_token_logps=ref_log_probs,        # [batch, final_len] –∏–ª–∏ None
        )


# ============================================================
# üéì SDPO LOSS (Self-Distilled Policy Optimization)
# ============================================================

class SDPOLoss(nn.Module):
    """
    üéì SDPO Loss ‚Äî Self-Distilled Policy Optimization.
    
    SDPO –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç GRPO loss —Å self-distillation:
    - GRPO loss –Ω–∞ –æ–±—ã—á–Ω—ã—Ö rollouts (–∫–∞–∫ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º GRPO)
    - Self-distillation loss: KL –º–µ–∂–¥—É student –∏ teacher (–Ω–∞ reprompted –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
    
    –ò–¥–µ—è: —É—Å–ø–µ—à–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è "—É—á–∏—Ç–µ–ª–µ–º" –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏,
    —á—Ç–æ –¥–∞—ë—Ç –±–æ–ª–µ–µ –ø–ª–æ—Ç–Ω—ã–π learning signal —á–µ–º —Ç–æ–ª—å–∫–æ —Å–∫–∞–ª—è—Ä–Ω—ã–π reward.
    
    total_loss = grpo_loss + sdpo_loss_weight * distillation_loss
    
    Attributes:
        alpha: –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è KL (0=forward, 1=reverse, 0.5=JSD)
        success_threshold: –ü–æ—Ä–æ–≥ reward –¥–ª—è "—É—Å–ø–µ—à–Ω–æ–π" —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        is_clip: Importance sampling clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        loss_weight: –í–µ—Å distillation loss –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ GRPO loss
    """
    
    def __init__(
        self,
        config: Optional[GRPOConfig] = None,
        # GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        clip_eps_low: float = 0.2,
        clip_eps_high: float = 0.2,
        kl_weight: float = 0.0,
        token_level_loss: bool = False,
        fixed_length_normalizer: Optional[int] = None,
        # SDPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        alpha: float = 0.5,
        success_threshold: float = 0.5,
        full_logit_distillation: bool = False,
        distillation_topk: Optional[int] = None,
        is_clip: float = 2.0,
        loss_weight: float = 1.0,
    ) -> None:
        """
        Args:
            config: GRPOConfig —Å SDPO –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            alpha: 0=forward KL, 1=reverse KL, 0.5=JSD (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
            success_threshold: –ü–æ—Ä–æ–≥ reward –¥–ª—è —É—á–∏—Ç–µ–ª—è
            full_logit_distillation: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            distillation_topk: Top-k –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è distillation
            is_clip: IS clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            loss_weight: –í–µ—Å SDPO loss
        """
        super().__init__()
        
        # GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if config is not None:
            self.clip_eps_low = config.clip_eps_low
            self.clip_eps_high = config.clip_eps_high
            self.kl_weight = config.kl_weight
            self.token_level_loss = getattr(config, 'token_level_loss', False)
            self.fixed_length_normalizer = getattr(config, 'fixed_length_normalizer', None)
            # SDPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self.alpha = getattr(config, 'sdpo_alpha', alpha)
            self.success_threshold = getattr(config, 'sdpo_success_threshold', success_threshold)
            self.full_logit_distillation = getattr(config, 'sdpo_full_logit_distillation', full_logit_distillation)
            self.distillation_topk = getattr(config, 'sdpo_distillation_topk', distillation_topk)
            self.is_clip = getattr(config, 'sdpo_is_clip', is_clip)
            self.loss_weight = getattr(config, 'sdpo_loss_weight', loss_weight)
        else:
            self.clip_eps_low = clip_eps_low
            self.clip_eps_high = clip_eps_high
            self.kl_weight = kl_weight
            self.token_level_loss = token_level_loss
            self.fixed_length_normalizer = fixed_length_normalizer
            self.alpha = alpha
            self.success_threshold = success_threshold
            self.full_logit_distillation = full_logit_distillation
            self.distillation_topk = distillation_topk
            self.is_clip = is_clip
            self.loss_weight = loss_weight
        
        # –ë–∞–∑–æ–≤—ã–π GRPO loss
        self.grpo_loss = GRPOLoss(
            clip_eps_low=self.clip_eps_low,
            clip_eps_high=self.clip_eps_high,
            kl_weight=self.kl_weight,
            token_level_loss=self.token_level_loss,
            fixed_length_normalizer=self.fixed_length_normalizer,
        )
        
        self.last_components: dict = {}
        
        logger.info(f"üéì SDPOLoss –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"   - alpha (KL type): {self.alpha} ({'JSD' if self.alpha == 0.5 else 'forward' if self.alpha == 0 else 'reverse'})")
        logger.info(f"   - success_threshold: {self.success_threshold}")
        logger.info(f"   - loss_weight: {self.loss_weight}")
        logger.info(f"   - full_logit_distillation: {self.full_logit_distillation}")
        logger.info(f"   - is_clip: {self.is_clip}")
    
    def compute_distillation_loss(
        self,
        student_log_probs: torch.Tensor,  # [batch, seq] –∏–ª–∏ [batch, seq, vocab/topk]
        teacher_log_probs: torch.Tensor,  # [batch, seq] –∏–ª–∏ [batch, seq, vocab/topk]
        action_mask: torch.Tensor,  # [batch, seq]
        old_log_probs: Optional[torch.Tensor] = None,  # [batch, seq]
        distillation_mask: Optional[torch.Tensor] = None,  # [batch] ‚Äî –∫–∞–∫–∏–µ —Å—ç–º–ø–ª—ã –∏–º–µ—é—Ç teacher
        student_topk_log_probs: Optional[torch.Tensor] = None,  # [batch, seq, topk] ‚Äî Top-K –ª–æ–≥–∏—Ç—ã
        teacher_topk_log_probs: Optional[torch.Tensor] = None,  # [batch, seq, topk] ‚Äî Top-K –ª–æ–≥–∏—Ç—ã
    ) -> Tuple[torch.Tensor, dict]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç self-distillation loss –º–µ–∂–¥—É student –∏ teacher.
        
        üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Top-K Distillation –∏–∑ verl!
        –í–º–µ—Å—Ç–æ KL –ø–æ –≤—Å–µ–º—É vocab (152k) –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ top-k —Ç–æ–∫–µ–Ω–æ–≤.
        –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏: 99.97% –ø—Ä–∏ k=50 vs vocab=152k
        
        Args:
            student_log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ [batch, seq]
            teacher_log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ teacher [batch, seq]
            action_mask: –ú–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ [batch, seq]
            old_log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏–∑ rollout (–¥–ª—è IS)
            distillation_mask: –ú–∞—Å–∫–∞ —Å—ç–º–ø–ª–æ–≤ —Å teacher [batch]
            student_topk_log_probs: Top-K log_probs student [batch, seq, k] (–¥–ª—è full_logit_distillation)
            teacher_topk_log_probs: Top-K log_probs teacher [batch, seq, k] (–¥–ª—è full_logit_distillation)
            
        Returns:
            (loss, metrics)
        """
        metrics = {}
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –º–∞—Å–∫–∏
        loss_mask = action_mask.float()
        if distillation_mask is not None:
            # distillation_mask [batch] -> [batch, 1] –¥–ª—è broadcasting
            loss_mask = loss_mask * distillation_mask.unsqueeze(1).float()
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å—ç–º–ø–ª–æ–≤ –¥–ª—è distillation ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0
        if loss_mask.sum() < 1:
            metrics["sdpo_empty_batch"] = True
            return torch.tensor(0.0, device=student_log_probs.device), metrics
        
        # ============================================================
        # üî• FULL-LOGIT / TOP-K DISTILLATION (–∏–∑ verl)
        # ============================================================
        if self.full_logit_distillation and student_topk_log_probs is not None and teacher_topk_log_probs is not None:
            # Top-K distillation: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ top-k —Ç–æ–∫–µ–Ω–æ–≤
            # –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç ~99.97% –ø–∞–º—è—Ç–∏ –ø—Ä–∏ k=50 vs vocab=152k
            
            # –†–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è top-k log probs (—á—Ç–æ–±—ã —Å—É–º–º–∞ prob = 1)
            def renorm_topk_log_probs(logp: torch.Tensor) -> torch.Tensor:
                """–†–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç top-k log_probs —á—Ç–æ–±—ã —Å—É–º–º–∞ prob = 1."""
                logZ = torch.logsumexp(logp, dim=-1, keepdim=True)
                return logp - logZ
            
            student_distill = renorm_topk_log_probs(student_topk_log_probs)  # [batch, seq, k]
            teacher_distill = renorm_topk_log_probs(teacher_topk_log_probs)  # [batch, seq, k]
            
            # KL divergence –ø–æ top-k —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é
            if self.alpha == 0.0:
                # Forward KL: KL(teacher || student)
                kl_loss = F.kl_div(
                    student_distill, teacher_distill, reduction="none", log_target=True
                )
            elif self.alpha == 1.0:
                # Reverse KL: KL(student || teacher)
                kl_loss = F.kl_div(
                    teacher_distill, student_distill, reduction="none", log_target=True
                )
            else:
                # Jensen-Shannon Divergence
                alpha_t = torch.tensor(
                    self.alpha, dtype=student_distill.dtype, device=student_distill.device
                )
                mixture_log_probs = torch.logsumexp(
                    torch.stack([
                        student_distill + torch.log(1 - alpha_t),
                        teacher_distill + torch.log(alpha_t)
                    ]),
                    dim=0,
                )
                kl_teacher = F.kl_div(mixture_log_probs, teacher_distill, reduction="none", log_target=True)
                kl_student = F.kl_div(mixture_log_probs, student_distill, reduction="none", log_target=True)
                kl_loss = torch.lerp(kl_student, kl_teacher, alpha_t)
            
            # –°—É–º–º–∏—Ä—É–µ–º –ø–æ k-dimension -> [batch, seq]
            per_token_loss = kl_loss.sum(dim=-1)
            metrics["sdpo_topk_distill"] = True
            metrics["sdpo_topk_k"] = student_topk_log_probs.shape[-1]
        else:
            # ============================================================
            # SIMPLE PER-TOKEN DISTILLATION (fallback)
            # ============================================================
            # –í—ã—á–∏—Å–ª—è–µ–º KL divergence –ø–æ per-token log_probs
            if self.alpha == 0.0:
                # Forward KL: student -> teacher (mode-seeking)
                per_token_loss = (teacher_log_probs.exp() * (teacher_log_probs - student_log_probs))
            elif self.alpha == 1.0:
                # Reverse KL: teacher -> student (mode-covering)
                log_ratio = student_log_probs - teacher_log_probs
                per_token_loss = log_ratio.detach() * student_log_probs
            else:
                # Jensen-Shannon Divergence (alpha = 0.5)
                alpha_t = torch.tensor(self.alpha, dtype=student_log_probs.dtype, device=student_log_probs.device)
                
                mixture_log_probs = torch.logsumexp(
                    torch.stack([
                        student_log_probs + torch.log(1 - alpha_t),
                        teacher_log_probs + torch.log(alpha_t)
                    ]),
                    dim=0,
                )
                
                kl_student = student_log_probs - mixture_log_probs
                kl_teacher = teacher_log_probs - mixture_log_probs
                
                per_token_loss = (1 - alpha_t) * (student_log_probs.exp() * kl_student) + \
                                 alpha_t * (teacher_log_probs.exp() * kl_teacher)
            
            metrics["sdpo_topk_distill"] = False
        
        # Importance Sampling clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        if self.is_clip is not None and old_log_probs is not None:
            negative_approx_kl = (student_log_probs - old_log_probs).detach()
            negative_approx_kl = torch.clamp(negative_approx_kl, min=-20.0, max=20.0)
            ratio = torch.exp(negative_approx_kl).clamp(max=self.is_clip)
            per_token_loss = per_token_loss * ratio
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è loss
        if self.token_level_loss:
            total_tokens = loss_mask.sum().clamp(min=1)
            loss = (per_token_loss * loss_mask).sum() / total_tokens
        else:
            if self.fixed_length_normalizer is not None:
                loss = masked_sum(
                    per_token_loss,
                    loss_mask,
                    dim=-1,
                    constant_normalizer=self.fixed_length_normalizer
                ).mean()
            else:
                loss = masked_mean(per_token_loss, loss_mask, dim=-1).mean()
        
        metrics["sdpo_distill_loss"] = loss.item()
        metrics["sdpo_empty_batch"] = False
        
        return loss, metrics
    
    def forward(
        self,
        log_probs: torch.Tensor,
        experience: Experience,
        teacher_log_probs: Optional[torch.Tensor] = None,
        distillation_mask: Optional[torch.Tensor] = None,
        student_topk_log_probs: Optional[torch.Tensor] = None,  # üî• Top-K optimization
        teacher_topk_log_probs: Optional[torch.Tensor] = None,  # üî• Top-K optimization
    ) -> Tuple[torch.Tensor, dict]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SDPO loss.
        
        üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Top-K Distillation!
        –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã student_topk_log_probs –∏ teacher_topk_log_probs,
        –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è KL –ø–æ top-k —Ç–æ–∫–µ–Ω–∞–º –≤–º–µ—Å—Ç–æ –≤—Å–µ–≥–æ vocab.
        
        Args:
            log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ [batch, seq]
            experience: Experience —Å rollout –¥–∞–Ω–Ω—ã–º–∏
            teacher_log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ teacher [batch, seq] (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            distillation_mask: –ú–∞—Å–∫–∞ —Å—ç–º–ø–ª–æ–≤ —Å teacher [batch]
            student_topk_log_probs: Top-K log_probs student [batch, seq, k] (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            teacher_topk_log_probs: Top-K log_probs teacher [batch, seq, k] (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            (total_loss, metrics)
        """
        # 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO loss
        grpo_loss, grpo_metrics = self.grpo_loss(log_probs, experience)
        
        # 2. Self-distillation loss (–µ—Å–ª–∏ –µ—Å—Ç—å teacher –¥–∞–Ω–Ω—ã–µ)
        if teacher_log_probs is not None:
            distill_loss, distill_metrics = self.compute_distillation_loss(
                student_log_probs=log_probs,
                teacher_log_probs=teacher_log_probs,
                action_mask=experience.action_mask,
                old_log_probs=experience.action_log_probs,
                distillation_mask=distillation_mask,
                student_topk_log_probs=student_topk_log_probs,
                teacher_topk_log_probs=teacher_topk_log_probs,
            )
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º losses
            total_loss = grpo_loss + self.loss_weight * distill_loss
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = {**grpo_metrics, **distill_metrics}
            metrics["grpo_loss"] = grpo_loss.item()
            metrics["sdpo_weight"] = self.loss_weight
            metrics["total_loss"] = total_loss.item()
        else:
            # –ù–µ—Ç teacher –¥–∞–Ω–Ω—ã—Ö ‚Äî —Ç–æ–ª—å–∫–æ GRPO loss
            total_loss = grpo_loss
            metrics = grpo_metrics
            metrics["sdpo_distill_loss"] = 0.0
            metrics["sdpo_empty_batch"] = True
        
        self.last_components = metrics
        return total_loss, metrics


def create_loss_function(
    model: "PreTrainedModel",
    config: GRPOConfig,
) -> nn.Module:
    """
    –°–æ–∑–¥–∞—ë—Ç loss —Ñ—É–Ω–∫—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    
    –í—ã–±–æ—Ä loss —Ñ—É–Ω–∫—Ü–∏–∏:
    - SDPO: SDPOLoss (GRPO + self-distillation)
    - Liger GRPO: LigerFusedGRPOLoss (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏)
    - Default: GRPOLoss
    
    Args:
        model: HuggingFace –º–æ–¥–µ–ª—å
        config: GRPOConfig
    
    Returns:
        Loss module (SDPOLoss, LigerFusedGRPOLoss –∏–ª–∏ GRPOLoss)
    """
    # SDPO: –∏—Å–ø–æ–ª—å–∑—É–µ–º SDPOLoss
    is_sdpo = (
        getattr(config, 'algorithm', None) == RLAlgorithm.SDPO or
        getattr(config, 'use_self_distillation', False)
    )
    
    if is_sdpo:
        logger.info("üéì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SDPOLoss (GRPO + Self-Distillation)")
        return SDPOLoss(config=config)
    
    # Liger Fused GRPO: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
    use_liger_fused = getattr(config, 'liger_fused_grpo', False) and getattr(config, 'use_liger', False)
    
    if use_liger_fused:
        try:
            from .liger_utils import is_liger_available
            
            if is_liger_available():
                loss_fn = LigerFusedGRPOLoss(model=model, config=config)
                logger.info("ü¶Å –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LigerFusedGRPOLoss (–ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!)")
                return loss_fn
            else:
                logger.warning("‚ö†Ô∏è Liger –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, fallback –Ω–∞ GRPOLoss")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LigerFusedGRPOLoss: {e}")
            logger.warning("   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss")
    
    logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss")
    return GRPOLoss(config=config)
