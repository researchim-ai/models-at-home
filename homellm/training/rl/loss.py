"""
Loss —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è GRPO/RL.

–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã:
- GRPO (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)
- Dr.GRPO (–±–µ–∑ std –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–ª–∏—Ç–µ–ª—å)
- DAPO (token-level loss, clip higher)
- ü¶Å LigerFusedLinearGRPOLoss ‚Äî –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits (–¥–æ 80% —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏!)
"""
import logging
from typing import Optional, Tuple, TYPE_CHECKING
import torch
import torch.nn as nn
import torch.nn.functional as F

from .experience import Experience
from .config import GRPOConfig, RLAlgorithm

if TYPE_CHECKING:
    from transformers import PreTrainedModel

logger = logging.getLogger(__name__)


def approx_kl_divergence(
    log_probs: torch.Tensor,
    log_probs_ref: torch.Tensor,
    action_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Monte-Carlo –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ (k3 estimator).
    
    –°–º.: http://joschu.net/blog/kl-approx.html
    
    KL ‚âà exp(log_ref - log) - (log_ref - log) - 1
    
    Args:
        log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–µ [batch, seq_len]
        log_probs_ref: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ [batch, seq_len]
        action_mask: –ú–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—á—ë—Ç–∞ [batch, seq_len]
        
    Returns:
        KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è [batch, seq_len]
    """
    log_ratio = log_probs_ref.float() - log_probs.float()
    if action_mask is not None:
        log_ratio = log_ratio * action_mask
    
    return log_ratio.exp() - log_ratio - 1


def masked_mean(
    tensor: torch.Tensor,
    mask: Optional[torch.Tensor] = None,
    dim: Optional[int] = None,
) -> torch.Tensor:
    """
    –°—Ä–µ–¥–Ω–µ–µ —Å —É—á—ë—Ç–æ–º –º–∞—Å–∫–∏.
    
    Args:
        tensor: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä
        mask: –ú–∞—Å–∫–∞ (1 = —É—á–∏—Ç—ã–≤–∞—Ç—å, 0 = –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å)
        dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —Ä–µ–¥—É–∫—Ü–∏–∏
        
    Returns:
        –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    """
    if mask is None:
        return tensor.mean(dim=dim)
    
    masked = tensor * mask
    return masked.sum(dim=dim) / mask.sum(dim=dim).clamp(min=1e-8)


def masked_sum(
    tensor: torch.Tensor,
    mask: Optional[torch.Tensor] = None,
    dim: Optional[int] = None,
    constant_normalizer: Optional[float] = None,
) -> torch.Tensor:
    """
    –°—É–º–º–∞ —Å –º–∞—Å–∫–æ–π –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Dr.GRPO –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è length bias:
    –≤–º–µ—Å—Ç–æ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é –¥–ª–∏–Ω—É –¥–µ–ª–∏–º –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É.
    
    Args:
        tensor: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä
        mask: –ú–∞—Å–∫–∞
        dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è —Ä–µ–¥—É–∫—Ü–∏–∏
        constant_normalizer: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–ª–∏—Ç–µ–ª—å (–¥–ª—è Dr.GRPO)
        
    Returns:
        –°—É–º–º–∞ / –¥–µ–ª–∏—Ç–µ–ª—å
    """
    if mask is None:
        summed = tensor.sum(dim=dim)
    else:
        summed = (tensor * mask).sum(dim=dim)
    
    if constant_normalizer is not None and constant_normalizer > 0:
        return summed / constant_normalizer
    return summed


def compute_advantages(
    returns: torch.Tensor,
    use_std_normalization: bool = True,
    eps: float = 1e-8,
) -> torch.Tensor:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç advantages (–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞) –¥–ª—è –≥—Ä—É–ø–ø—ã rollout'–æ–≤.
    
    –§–æ—Ä–º—É–ª–∞ GRPO:
        A_i = (r_i - mean(r)) / std(r)
    
    –§–æ—Ä–º—É–ª–∞ Dr.GRPO (–±–µ–∑ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ std):
        A_i = r_i - mean(r)
    
    Args:
        returns: Rewards –¥–ª—è –≥—Ä—É–ø–ø—ã [group_size] –∏–ª–∏ [batch, group_size]
        use_std_normalization: –î–µ–ª–∏—Ç—å –ª–∏ –Ω–∞ std (True –¥–ª—è GRPO, False –¥–ª—è DrGRPO)
        eps: –≠–ø—Å–∏–ª–æ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
    Returns:
        Advantages —Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    """
    mean_return = returns.mean(dim=-1, keepdim=True)
    advantages = returns - mean_return
    
    if use_std_normalization:
        std_return = returns.std(dim=-1, keepdim=True)
        advantages = advantages / (std_return + eps)
    
    return advantages


class GRPOLoss(nn.Module):
    """
    GRPO Loss —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞.
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç:
    - PPO-style surrogate loss —Å –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º
    - KL —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - Token-level vs sample-level –∞–≥—Ä–µ–≥–∞—Ü–∏—é
    - –ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ (clip higher –¥–ª—è DAPO)
    
    –§–æ—Ä–º—É–ª–∞:
        L = -min(ratio * A, clip(ratio) * A) + Œ≤ * KL
    
    –≥–¥–µ ratio = exp(log_œÄ - log_œÄ_old)
    """
    
    def __init__(
        self,
        config: Optional[GRPOConfig] = None,
        clip_eps_low: float = 0.2,
        clip_eps_high: float = 0.2,
        kl_weight: float = 0.0,
        token_level_loss: bool = False,
        fixed_length_normalizer: Optional[int] = None,
    ) -> None:
        """
        Args:
            config: GRPOConfig (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è)
            clip_eps_low: –ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –∫–ª–∏–ø–ø–∏–Ω–≥–∞
            clip_eps_high: –í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –∫–ª–∏–ø–ø–∏–Ω–≥–∞ (–¥–ª—è DAPO: 0.28)
            kl_weight: –í–µ—Å KL —à—Ç—Ä–∞—Ñ–∞ (0 –¥–ª—è reasoning-RL)
            token_level_loss: Token-level –∞–≥—Ä–µ–≥–∞—Ü–∏—è (vs sample-level)
            fixed_length_normalizer: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–ª–∏—Ç–µ–ª—å –¥–ª—è Dr.GRPO
        """
        super().__init__()
        
        if config is not None:
            self.clip_eps_low = config.clip_eps_low
            self.clip_eps_high = config.clip_eps_high
            self.kl_weight = config.kl_weight
            self.token_level_loss = config.token_level_loss
            self.fixed_length_normalizer = config.fixed_length_normalizer
            self.algorithm = config.algorithm
        else:
            self.clip_eps_low = clip_eps_low
            self.clip_eps_high = clip_eps_high
            self.kl_weight = kl_weight
            self.token_level_loss = token_level_loss
            self.fixed_length_normalizer = fixed_length_normalizer
            self.algorithm = RLAlgorithm.GRPO
        
        # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ loss
        self.last_components: dict = {}
    
    def forward(
        self,
        log_probs: torch.Tensor,
        experience: Experience,
    ) -> Tuple[torch.Tensor, dict]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç GRPO loss.
        
        Args:
            log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–µ [batch, seq_len]
            experience: Experience —Å action_log_probs, advantages, masks
            
        Returns:
            Tuple[loss, metrics_dict]
        """
        old_log_probs = experience.action_log_probs
        log_probs_ref = experience.log_probs_ref
        action_mask = experience.action_mask
        advantages = experience.advantages
        
        # KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        if log_probs_ref is not None and self.kl_weight > 0:
            kl = approx_kl_divergence(
                log_probs=log_probs,
                log_probs_ref=log_probs_ref,
                action_mask=action_mask,
            )
        else:
            kl = torch.zeros_like(log_probs)
        
        # Policy ratio: r(Œ∏) = œÄ(a|s) / œÄ_old(a|s) = exp(log_œÄ - log_œÄ_old)
        ratio = (log_probs - old_log_probs).exp()
        
        # PPO-style clipped surrogate loss
        # –î–ª—è DAPO –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ [1-eps_low, 1+eps_high]
        ratio_clipped = ratio.clamp(
            1.0 - self.clip_eps_low, 
            1.0 + self.clip_eps_high
        )
        
        # Surrogate objectives
        surr1 = ratio * advantages
        surr2 = ratio_clipped * advantages
        
        # Loss = -min(surr1, surr2) + kl_weight * kl
        policy_loss = -torch.min(surr1, surr2)
        loss_per_token = policy_loss + self.kl_weight * kl
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è loss
        if self.token_level_loss:
            # Token-level: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –∏–º–µ–µ—Ç —Ä–∞–≤–Ω—ã–π –≤–µ—Å (DAPO)
            if action_mask is not None:
                total_tokens = action_mask.sum()
                loss = (loss_per_token * action_mask).sum() / total_tokens.clamp(min=1)
            else:
                loss = loss_per_token.mean()
        else:
            # Sample-level —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (Dr.GRPO)
            if self.fixed_length_normalizer is not None:
                # Dr.GRPO: –¥–µ–ª–∏–º –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
                loss = masked_sum(
                    loss_per_token, 
                    action_mask, 
                    dim=-1,
                    constant_normalizer=self.fixed_length_normalizer
                ).mean()
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –≤ –∫–∞–∂–¥–æ–º —Å—ç–º–ø–ª–µ
                loss = masked_mean(loss_per_token, action_mask, dim=-1).mean()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        metrics = {
            "loss": loss.item(),
            "kl_mean": masked_mean(kl, action_mask).mean().item() if self.kl_weight > 0 else 0,
            "ratio_mean": ratio.mean().item(),
            "ratio_std": ratio.std().item(),
            "ratio_max": ratio.max().item(),
            "ratio_min": ratio.min().item(),
            "advantages_mean": advantages.mean().item(),
            "advantages_std": advantages.std().item(),
            "clip_fraction": ((ratio < 1 - self.clip_eps_low) | (ratio > 1 + self.clip_eps_high)).float().mean().item(),
        }
        self.last_components = metrics
        
        return loss, metrics


class PolicyGradientLoss(nn.Module):
    """
    –ü—Ä–æ—Å—Ç–æ–π Policy Gradient loss (REINFORCE).
    
    –ú–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å GRPO.
    
    L = -log_œÄ(a|s) * A
    """
    
    def __init__(
        self,
        baseline_subtract: bool = True,
        normalize_advantages: bool = True,
    ) -> None:
        super().__init__()
        self.baseline_subtract = baseline_subtract
        self.normalize_advantages = normalize_advantages
    
    def forward(
        self,
        log_probs: torch.Tensor,
        returns: torch.Tensor,
        action_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, dict]:
        """
        Args:
            log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π [batch, seq_len]
            returns: Rewards [batch, 1] –∏–ª–∏ [batch]
            action_mask: –ú–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ [batch, seq_len]
            
        Returns:
            loss, metrics
        """
        if returns.dim() == 1:
            returns = returns.unsqueeze(-1)
        
        # Baseline: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –±–∞—Ç—á—É
        if self.baseline_subtract:
            advantages = returns - returns.mean()
        else:
            advantages = returns
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if self.normalize_advantages and advantages.std() > 1e-8:
            advantages = advantages / (advantages.std() + 1e-8)
        
        # Policy gradient: L = -log_œÄ * A
        # –†–∞—Å—à–∏—Ä—è–µ–º advantages –¥–ª—è —É–º–Ω–æ–∂–µ–Ω–∏—è —Å log_probs
        if advantages.size(-1) == 1 and log_probs.dim() > 1:
            advantages = advantages.expand_as(log_probs)
        
        loss_per_token = -log_probs * advantages
        
        if action_mask is not None:
            loss = masked_mean(loss_per_token, action_mask, dim=-1).mean()
        else:
            loss = loss_per_token.mean()
        
        metrics = {
            "loss": loss.item(),
            "advantages_mean": advantages.mean().item(),
            "advantages_std": advantages.std().item(),
        }
        
        return loss, metrics


def compute_entropy(log_probs: torch.Tensor, action_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏.
    
    H = -Œ£ p * log(p) = -Œ£ exp(log_p) * log_p
    
    –î–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–æ–ª–∂–Ω–∞ –º–µ–¥–ª–µ–Ω–Ω–æ —Ä–∞—Å—Ç–∏ –ø—Ä–∏ –∑–¥–æ—Ä–æ–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏.
    –†–µ–∑–∫–æ–µ –ø–∞–¥–µ–Ω–∏–µ = entropy collapse.
    
    Args:
        log_probs: Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [batch, seq_len]
        action_mask: –ú–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
        
    Returns:
        –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è
    """
    probs = log_probs.exp()
    entropy = -probs * log_probs
    
    if action_mask is not None:
        return masked_mean(entropy, action_mask)
    return entropy.mean()


def compute_overlong_penalty(
    completion_lengths: torch.Tensor,
    max_length: int,
    buffer_length: int = 0,
    penalty_value: float = -1.0,
) -> torch.Tensor:
    """
    –ú—è–≥–∫–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (–∏–∑ DAPO).
    
    - –î–ª–∏–Ω–∞ < (max_length - buffer): —à—Ç—Ä–∞—Ñ = 0
    - –î–ª–∏–Ω–∞ > max_length: —à—Ç—Ä–∞—Ñ = penalty_value
    - –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –¥–ª–∏–Ω–∞: –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
    
    Args:
        completion_lengths: –î–ª–∏–Ω—ã completions [batch]
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ø—É—Å—Ç–∏–º–∞—è –¥–ª–∏–Ω–∞
        buffer_length: –ë—É—Ñ–µ—Ä–Ω–∞—è –∑–æ–Ω–∞
        penalty_value: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ (–æ–±—ã—á–Ω–æ -1)
        
    Returns:
        –®—Ç—Ä–∞—Ñ—ã [batch]
    """
    threshold_start = max_length - buffer_length
    
    penalties = torch.zeros_like(completion_lengths, dtype=torch.float)
    
    # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –∑–æ–Ω–∞: –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
    in_buffer = (completion_lengths >= threshold_start) & (completion_lengths <= max_length)
    if in_buffer.any():
        progress = (completion_lengths[in_buffer] - threshold_start).float() / max(buffer_length, 1)
        penalties[in_buffer] = progress * penalty_value
    
    # –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º–∞
    over_max = completion_lengths > max_length
    penalties[over_max] = penalty_value
    
    return penalties


# ============================================================
# ü¶Å LIGER FUSED GRPO LOSS
# ============================================================

class LigerFusedGRPOLoss(nn.Module):
    """
    üî• Fused GRPO Loss —Å Liger Kernel ‚Äî –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits!
    
    –≠—Ç–æ —Å–∞–º–∞—è –º–æ—â–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è GRPO:
    - –í–º–µ—Å—Ç–æ model() -> logits [B, T, V] -> log_probs -> loss
    - –î–µ–ª–∞–µ—Ç: model(output_hidden_states=True) -> hidden [B, T, H] -> fused_loss
    
    –î–ª—è vocab=150k –∏ seq_len=512 —ç–∫–æ–Ω–æ–º–∏—Ç ~3GB –Ω–∞ batch!
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        loss_fn = LigerFusedGRPOLoss(model, config)
        
        # –í training loop:
        outputs = model(input_ids, attention_mask, output_hidden_states=True)
        hidden_states = outputs.hidden_states[-1]  # –ø–æ—Å–ª–µ–¥–Ω–∏–π layer
        loss, metrics = loss_fn(
            hidden_states=hidden_states,
            selected_token_ids=completion_ids,  # target tokens
            attention_mask=action_mask,
            advantages=advantages,
            old_per_token_logps=old_log_probs,
            ref_per_token_logps=ref_log_probs,
        )
    """
    
    def __init__(
        self,
        model: "PreTrainedModel",
        config: Optional[GRPOConfig] = None,
        beta: float = 0.0,
        loss_type: str = "grpo",
        epsilon_low: float = 0.2,
        epsilon_high: float = 0.2,
        max_completion_length: Optional[int] = None,
        chunk_size: int = 1,
    ):
        super().__init__()
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Liger
        from .liger_utils import get_liger_fused_linear_grpo, is_liger_available
        
        if not is_liger_available():
            raise RuntimeError("LigerFusedGRPOLoss —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π liger-kernel!")
        
        LigerFusedLinearGRPOLoss = get_liger_fused_linear_grpo()
        if LigerFusedLinearGRPOLoss is None:
            raise RuntimeError("LigerFusedLinearGRPOLoss –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ config –∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        if config is not None:
            beta = config.kl_weight
            loss_type = getattr(config, 'liger_grpo_loss_type', 'dapo')
            epsilon_low = config.clip_eps_low
            epsilon_high = config.clip_eps_high
            max_completion_length = config.max_new_tokens
            chunk_size = getattr(config, 'liger_chunk_size', 1)
        
        self.liger_loss = LigerFusedLinearGRPOLoss(
            beta=beta,
            loss_type=loss_type,
            epsilon_low=epsilon_low,
            epsilon_high=epsilon_high,
            max_completion_length=max_completion_length,
            chunk_size=chunk_size,
            use_ref_model=beta > 0,
            compiled=False,  # torch.compile –ø–∞–¥–∞–µ—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –≤ GRPO
            importance_sampling_level="token",
            temperature=1.0,
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ lm_head
        self.lm_head_weight = model.lm_head.weight
        self.lm_head_bias = getattr(model.lm_head, 'bias', None)
        
        self.beta = beta
        self.loss_type = loss_type
        self.last_components: dict = {}
        
        logger.info(f"ü¶Å LigerFusedGRPOLoss –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"   - loss_type: {loss_type}")
        logger.info(f"   - beta (KL weight): {beta}")
        logger.info(f"   - epsilon: [{epsilon_low}, {epsilon_high}]")
        logger.info(f"   - chunk_size: {chunk_size}")
        logger.info(f"   ‚ö° Logits –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É—é—Ç—Å—è ‚Äî —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏!")
    
    def forward(
        self,
        hidden_states: torch.Tensor,  # [batch, seq, hidden] ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state
        selected_token_ids: torch.Tensor,  # [batch, seq] ‚Äî target token IDs
        attention_mask: torch.Tensor,  # [batch, seq] ‚Äî action mask
        advantages: torch.Tensor,  # [batch] ‚Äî advantages
        old_per_token_logps: Optional[torch.Tensor] = None,  # [batch, seq]
        ref_per_token_logps: Optional[torch.Tensor] = None,  # [batch, seq]
    ) -> Tuple[torch.Tensor, dict]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç Fused GRPO Loss.
        
        Args:
            hidden_states: –ü–æ—Å–ª–µ–¥–Ω–∏–π hidden state –º–æ–¥–µ–ª–∏ [batch, seq, hidden]
            selected_token_ids: Target token IDs (completion tokens) [batch, seq]
            attention_mask: –ú–∞—Å–∫–∞ completion —Ç–æ–∫–µ–Ω–æ–≤ [batch, seq]
            advantages: Advantages –¥–ª—è –∫–∞–∂–¥–æ–≥–æ sample [batch]
            old_per_token_logps: Log probs –∏–∑ rollout policy [batch, seq]
            ref_per_token_logps: Log probs –∏–∑ reference model [batch, seq]
        
        Returns:
            (loss, metrics_dict)
        """
        # Liger –æ–∂–∏–¥–∞–µ—Ç _input –≤ —Ñ–æ—Ä–º–∞—Ç–µ [batch, seq, hidden] ‚Äî –ø–µ—Ä–µ–¥–∞—ë–º –Ω–∞–ø—Ä—è–º—É—é –ë–ï–ó reshape
        # chunk_forward –¥–µ–ª–∞–µ—Ç: logits = torch.matmul(input_chunk, weight.t())  # [B, T, H] @ [H, V]
        
        # –í—ã–∑—ã–≤–∞–µ–º Liger Fused Loss
        result = self.liger_loss(
            hidden_states,         # [batch, seq, hidden] ‚Äî –ù–ï –¥–µ–ª–∞–µ–º flatten!
            self.lm_head_weight,   # [vocab, hidden]
            selected_token_ids,    # [batch, seq]
            attention_mask,        # [batch, seq]
            advantages,            # [batch]
            bias=self.lm_head_bias,
            ref_per_token_logps=ref_per_token_logps,
            old_per_token_logps=old_per_token_logps,
        )
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if isinstance(result, tuple):
            loss = result[0]
            metrics_list = result[1] if len(result) > 1 else []
        else:
            loss = result
            metrics_list = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "loss": loss.item() if hasattr(loss, 'item') else float(loss),
            "advantages_mean": advantages.mean().item(),
            "advantages_std": advantages.std().item(),
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ Liger
        if len(metrics_list) >= 1 and self.beta > 0:
            kl_val = metrics_list[0]
            metrics["kl_mean"] = kl_val.item() if hasattr(kl_val, 'item') else float(kl_val)
        
        if len(metrics_list) >= 2:
            clip_idx = 1 if self.beta > 0 else 0
            if clip_idx < len(metrics_list):
                clip_val = metrics_list[clip_idx]
                metrics["clip_fraction"] = clip_val.item() if hasattr(clip_val, 'item') else float(clip_val)
        
        self.last_components = metrics
        return loss, metrics
    
    def forward_with_experience(
        self,
        hidden_states: torch.Tensor,
        experience: Experience,
    ) -> Tuple[torch.Tensor, dict]:
        """
        –£–¥–æ–±–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å Experience –æ–±—ä–µ–∫—Ç–æ–º.
        
        Args:
            hidden_states: –ü–æ—Å–ª–µ–¥–Ω–∏–π hidden state [batch, seq, hidden]
            experience: Experience –æ–±—ä–µ–∫—Ç —Å action_mask, advantages –∏ —Ç.–¥.
        
        Returns:
            (loss, metrics_dict)
        """
        # CAUSAL SHIFT –¥–ª—è next-token prediction:
        # - hidden_states[i] –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç tokens[i+1]
        # - action_log_probs —É–∂–µ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä [batch, T] (shifted –ø—Ä–∏ rollout)
        # - action_mask —É–∂–µ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä [batch, T] (shifted –ø—Ä–∏ rollout)
        #
        # –í–ê–ñ–ù–û: sequences –∏ action_log_probs –±–∞—Ç—á–∞—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ,
        # –ø–æ—ç—Ç–æ–º—É –∏—Ö max_len –º–æ–∂–µ—Ç –Ω–µ —Å–æ–≤–ø–∞–¥–∞—Ç—å –Ω–∞ 1!
        # –ù—É–∂–Ω–æ –≤—ã—Ä–æ–≤–Ω—è—Ç—å –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –¥–æ –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
        seq_len = hidden_states.size(1)  # –æ—Ç –º–æ–¥–µ–ª–∏
        target_len = experience.action_log_probs.size(1)  # shifted logprobs
        
        # –°–¥–≤–∏–≥–∞–µ–º hidden states (—É–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π ‚Äî –æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω)
        # –†–µ–∑—É–ª—å—Ç–∞—Ç: [batch, seq_len-1, hidden]
        shifted_hidden = hidden_states[:, :-1, :].contiguous()
        
        # –°–¥–≤–∏–≥–∞–µ–º sequences (–ø–æ–ª—É—á–∞–µ–º labels)
        # –†–µ–∑—É–ª—å—Ç–∞—Ç: [batch, seq_len-1]
        shifted_labels = experience.sequences[:, 1:].contiguous()
        
        # –¢–µ–ø–µ—Ä—å shifted_hidden –∏ shifted_labels –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä seq_len-1
        # –ê action_log_probs –∏ action_mask –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä target_len
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–æ –º–µ–Ω—å—à–µ–≥–æ –∏–∑ –¥–≤—É—Ö
        final_len = min(shifted_hidden.size(1), target_len)
        
        # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –¥–æ final_len (—Å–ø—Ä–∞–≤–∞, —Ç.–∫. left-padding)
        shifted_hidden = shifted_hidden[:, -final_len:, :].contiguous()
        shifted_labels = shifted_labels[:, -final_len:].contiguous()
        action_log_probs = experience.action_log_probs[:, -final_len:].contiguous()
        action_mask = experience.action_mask[:, -final_len:]
        ref_log_probs = experience.log_probs_ref
        if ref_log_probs is not None:
            ref_log_probs = ref_log_probs[:, -final_len:].contiguous()
        
        # Advantages –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å [batch], –Ω–µ [batch, 1]
        advantages = experience.advantages
        if advantages.dim() > 1:
            advantages = advantages.squeeze(-1)
        
        # Action mask: Liger –æ–∂–∏–¥–∞–µ—Ç float/int, –Ω–µ bool
        if action_mask.dtype == torch.bool:
            action_mask = action_mask.to(shifted_hidden.dtype)
        
        return self.forward(
            hidden_states=shifted_hidden,             # [batch, final_len, hidden]
            selected_token_ids=shifted_labels,        # [batch, final_len]
            attention_mask=action_mask,               # [batch, final_len] ‚Äî float
            advantages=advantages,                    # [batch]
            old_per_token_logps=action_log_probs,     # [batch, final_len]
            ref_per_token_logps=ref_log_probs,        # [batch, final_len] –∏–ª–∏ None
        )


def create_loss_function(
    model: "PreTrainedModel",
    config: GRPOConfig,
) -> nn.Module:
    """
    –°–æ–∑–¥–∞—ë—Ç loss —Ñ—É–Ω–∫—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    
    –ï—Å–ª–∏ liger_fused_grpo=True –∏ Liger –¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî —Å–æ–∑–¥–∞—ë—Ç LigerFusedGRPOLoss.
    –ò–Ω–∞—á–µ ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss.
    
    Args:
        model: HuggingFace –º–æ–¥–µ–ª—å
        config: GRPOConfig
    
    Returns:
        Loss module (GRPOLoss –∏–ª–∏ LigerFusedGRPOLoss)
    """
    use_liger_fused = getattr(config, 'liger_fused_grpo', False) and getattr(config, 'use_liger', False)
    
    if use_liger_fused:
        try:
            from .liger_utils import is_liger_available
            
            if is_liger_available():
                loss_fn = LigerFusedGRPOLoss(model=model, config=config)
                logger.info("ü¶Å –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LigerFusedGRPOLoss (–ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!)")
                return loss_fn
            else:
                logger.warning("‚ö†Ô∏è Liger –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, fallback –Ω–∞ GRPOLoss")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LigerFusedGRPOLoss: {e}")
            logger.warning("   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss")
    
    logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss")
    return GRPOLoss(config=config)
