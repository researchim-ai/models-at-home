"""
Experience Ð±ÑƒÑ„ÐµÑ€ Ð´Ð»Ñ GRPO.

Ð¥Ñ€Ð°Ð½Ð¸Ñ‚ Ð¾Ð¿Ñ‹Ñ‚ Ð¾Ñ‚ rollout'Ð¾Ð² Ð´Ð»Ñ Ð¿Ð¾ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ.
"""
from __future__ import annotations
from dataclasses import dataclass, field, fields
from typing import Optional, List
import torch
import torch.nn.functional as F


def zero_pad_sequences(
    sequences: List[torch.Tensor], 
    side: str = "left",
    pad_value: int = 0,
) -> torch.Tensor:
    """
    ÐŸÐ°Ð´Ð´Ð¸Ð½Ð³ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹ Ð´Ð¾ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ð¾Ð¹ Ð´Ð»Ð¸Ð½Ñ‹.
    
    Args:
        sequences: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ð¾Ð² Ñ€Ð°Ð·Ð½Ð¾Ð¹ Ð´Ð»Ð¸Ð½Ñ‹
        side: "left" Ð¸Ð»Ð¸ "right" Ð¿Ð°Ð´Ð´Ð¸Ð½Ð³
        pad_value: Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ð°Ð´Ð´Ð¸Ð½Ð³Ð°
        
    Returns:
        Ð‘Ð°Ñ‚Ñ‡ Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ð¾Ð² Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ð¾Ð¹ Ð´Ð»Ð¸Ð½Ñ‹
    """
    assert side in ("left", "right")
    max_len = max(seq.size(-1) for seq in sequences)
    padded_sequences = []
    
    for seq in sequences:
        pad_len = max_len - seq.size(-1)
        if pad_len > 0:
            if side == "left":
                padding = (pad_len, 0)
            else:
                padding = (0, pad_len)
            padded = F.pad(seq, padding, value=pad_value)
        else:
            padded = seq
        padded_sequences.append(padded)
        
    return torch.stack(padded_sequences, dim=0)


@dataclass
class Experience:
    """
    ÐžÐ´Ð¸Ð½ Ð¾Ð¿Ñ‹Ñ‚ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ (completion Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°).
    
    Attributes:
        sequences: ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ (prompt + completion) [seq_len]
        prompt_length: Ð”Ð»Ð¸Ð½Ð° Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° (Ð´Ð»Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ)
        action_log_probs: Log-Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² completion Ð¿Ð¾ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐµ [seq_len-1]
        log_probs_ref: Log-Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾ Ñ€ÐµÑ„ÐµÑ€ÐµÐ½ÑÐ½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð´Ð»Ñ KL) [seq_len-1]
        returns: ÐÐ°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð½Ð°Ñ Ð½Ð°Ð³Ñ€Ð°Ð´Ð° Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ completion [1]
        advantages: Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð¾ (advantage) [1] Ð¸Ð»Ð¸ [seq_len-1]
        attention_mask: ÐœÐ°ÑÐºÐ° Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ [seq_len]
        action_mask: ÐœÐ°ÑÐºÐ° Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ completion) [seq_len-1]
        kl: KL Ð´Ð¸Ð²ÐµÑ€Ð³ÐµÐ½Ñ†Ð¸Ñ [seq_len-1]
        prompt_id: ID Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° (Ð´Ð»Ñ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸)
    """
    sequences: torch.Tensor
    prompt_length: int
    action_log_probs: torch.Tensor
    log_probs_ref: Optional[torch.Tensor] = None
    returns: Optional[torch.Tensor] = None
    advantages: Optional[torch.Tensor] = None
    attention_mask: Optional[torch.Tensor] = None
    action_mask: Optional[torch.Tensor] = None
    kl: Optional[torch.Tensor] = None
    prompt_id: Optional[int] = None
    prompt_ids: Optional[List[int]] = None  # Ð¡Ð¿Ð¸ÑÐ¾Ðº prompt_ids Ð´Ð»Ñ batch (Ð´Ð»Ñ SDPO)
    completion_text: Optional[str] = None  # Ð”Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸
    
    def to(self, device: torch.device) -> "ReplayBuffer":
        """ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰Ð°ÐµÑ‚ Ð²ÑÐµ Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ñ‹ Ð½Ð° ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ðµ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾."""
        members = {}
        for f in fields(self):
            v = getattr(self, f.name)
            if isinstance(v, torch.Tensor):
                v = v.to(device=device)
            members[f.name] = v
        return Experience(**members)
    
    @property
    def completion_length(self) -> int:
        """Ð”Ð»Ð¸Ð½Ð° completion (Ð¾Ñ‚Ð²ÐµÑ‚Ð°)."""
        return self.sequences.size(-1) - self.prompt_length
    
    @property
    def total_length(self) -> int:
        """ÐŸÐ¾Ð»Ð½Ð°Ñ Ð´Ð»Ð¸Ð½Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸."""
        return self.sequences.size(-1)


def split_experience_batch(experience: Experience) -> List[Experience]:
    """Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÑ‚ Ð±Ð°Ñ‚Ñ‡ Experience Ð½Ð° ÑÐ¿Ð¸ÑÐ¾Ðº Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ñ… Experience."""
    batch_size = experience.sequences.size(0)
    batch_data = [{} for _ in range(batch_size)]
    
    tensor_keys = (
        "sequences", "action_log_probs", "log_probs_ref",
        "returns", "advantages", "attention_mask", "action_mask", "kl"
    )
    
    for key in tensor_keys:
        value = getattr(experience, key)
        if value is None:
            vals = [None] * batch_size
        else:
            vals = torch.unbind(value)
        for i, v in enumerate(vals):
            batch_data[i][key] = v
    
    # Ð¡ÐºÐ°Ð»ÑÑ€Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ (ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ)
    for i in range(batch_size):
        batch_data[i]["prompt_length"] = experience.prompt_length
        batch_data[i]["prompt_id"] = experience.prompt_id
        batch_data[i]["completion_text"] = None
    
    return [Experience(**data) for data in batch_data]


def join_experience_batch(items: List[Experience]) -> Experience:
    """ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Experience Ð² Ð¾Ð´Ð¸Ð½ Ð±Ð°Ñ‚Ñ‡ Ñ Ð¿Ð°Ð´Ð´Ð¸Ð½Ð³Ð¾Ð¼."""
    batch_data = {}
    
    tensor_keys = (
        "sequences", "action_log_probs", "log_probs_ref",
        "returns", "advantages", "attention_mask", "action_mask", "kl"
    )
    
    for key in tensor_keys:
        vals = [getattr(item, key) for item in items]
        if all(v is not None for v in vals):
            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð´Ð´Ð¸Ð½Ð³Ð°
            pad_value = 0
            if key == "action_mask" or key == "attention_mask":
                pad_value = 0  # ÐœÐ°ÑÐºÐ¸ Ð¿Ð°Ð´Ð´Ð¸Ð¼ Ð½ÑƒÐ»ÑÐ¼Ð¸
            
            data = zero_pad_sequences(vals, side="left", pad_value=pad_value)
        else:
            data = None
        batch_data[key] = data
    
    # Ð”Ð»Ñ prompt_length Ð±ÐµÑ€Ñ‘Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ (Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ðµ Ð² Ð³Ñ€ÑƒÐ¿Ð¿Ðµ)
    batch_data["prompt_length"] = items[0].prompt_length if items else 0
    batch_data["prompt_id"] = items[0].prompt_id if items else None
    # ðŸŽ“ SDPO: ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÑÐµ prompt_ids Ð´Ð»Ñ batch
    batch_data["prompt_ids"] = [item.prompt_id for item in items] if items else None
    batch_data["completion_text"] = None
    
    return Experience(**batch_data)


class ReplayBuffer:
    """
    Ð‘ÑƒÑ„ÐµÑ€ Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¾Ð¿Ñ‹Ñ‚Ð°.
    
    ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚:
    - Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð¿Ñ‹Ñ‚Ð° (Ð±Ð°Ñ‚Ñ‡ Ð¸Ð»Ð¸ Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ)
    - Ð˜Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼
    - Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸ÑŽ zero-gradient Ð³Ñ€ÑƒÐ¿Ð¿ (Ð´Ð»Ñ dynamic sampling)
    """
    
    def __init__(self, limit: int = 0) -> None:
        """
        Args:
            limit: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð±ÑƒÑ„ÐµÑ€Ð° (0 = Ð±ÐµÐ· Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ)
        """
        self.limit = limit
        self.items: List[Experience] = []
        
        # Ð”Ð»Ñ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¿Ð¾ prompt_id
        self._prompt_groups: dict = {}
    
    def append(self, experience: Experience) -> None:
        """Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ Ð¾Ð¿Ñ‹Ñ‚ (Ð±Ð°Ñ‚Ñ‡ Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹)."""
        if experience.sequences.dim() > 1:
            # Ð‘Ð°Ñ‚Ñ‡ -> Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹
            items = split_experience_batch(experience)
            self.items.extend(items)
        else:
            self.items.append(experience)
        
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð»Ð¸Ð¼Ð¸Ñ‚
        if self.limit > 0 and len(self.items) > self.limit:
            samples_to_remove = len(self.items) - self.limit
            self.items = self.items[samples_to_remove:]
    
    def append_group(
        self, 
        experiences: List[Experience], 
        prompt_id: int,
        filter_zero_gradient: bool = False,
    ) -> bool:
        """
        Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ Ð³Ñ€ÑƒÐ¿Ð¿Ñƒ Ð¾Ð¿Ñ‹Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°.
        
        Args:
            experiences: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¾Ð¿Ñ‹Ñ‚Ð¾Ð² (G completions Ð´Ð»Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°)
            prompt_id: ID Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
            filter_zero_gradient: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹ Ñ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ð¼ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð¼
            
        Returns:
            True ÐµÑÐ»Ð¸ Ð³Ñ€ÑƒÐ¿Ð¿Ð° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð°, False ÐµÑÐ»Ð¸ Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð°
        """
        if filter_zero_gradient:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð²ÑÐµ rewards Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ðµ
            returns = torch.stack([exp.returns for exp in experiences])
            if returns.max() == returns.min():
                return False  # Zero-gradient Ð³Ñ€ÑƒÐ¿Ð¿Ð°
        
        for exp in experiences:
            exp.prompt_id = prompt_id
            self.items.append(exp)
        
        self._prompt_groups[prompt_id] = len(experiences)
        return True
    
    def clear(self) -> None:
        """ÐžÑ‡Ð¸Ñ‰Ð°ÐµÑ‚ Ð±ÑƒÑ„ÐµÑ€."""
        self.items.clear()
        self._prompt_groups.clear()
    
    def __len__(self) -> int:
        return len(self.items)
    
    def __getitem__(self, idx: int) -> Experience:
        return self.items[idx]
    
    def get_stats(self) -> dict:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð±ÑƒÑ„ÐµÑ€Ð°."""
        if not self.items:
            return {"size": 0, "num_groups": 0}
        
        returns = torch.stack([item.returns for item in self.items if item.returns is not None])
        
        return {
            "size": len(self.items),
            "num_groups": len(self._prompt_groups),
            "returns_mean": returns.mean().item() if len(returns) > 0 else 0,
            "returns_std": returns.std().item() if len(returns) > 0 else 0,
            "returns_max": returns.max().item() if len(returns) > 0 else 0,
            "returns_min": returns.min().item() if len(returns) > 0 else 0,
        }
