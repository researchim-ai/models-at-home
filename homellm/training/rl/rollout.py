"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è rollout'–æ–≤ (completions) –¥–ª—è GRPO.

Rollout = –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º rewards.
"""
import logging
import re
from contextlib import contextmanager, nullcontext
from dataclasses import dataclass
from typing import List, Optional, Tuple, Callable, Any, Dict
import torch
import torch.nn.functional as F

from transformers import (
    PreTrainedModel,
    PreTrainedTokenizer,
    GenerationConfig,
)

# Liger Kernel –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ cross-entropy
from homellm.training.rl.liger_utils import (
    liger_cross_entropy,
    chunked_cross_entropy,
    is_liger_available,
)

logger = logging.getLogger(__name__)


@contextmanager
def ds3_gather_for_generation(model, accelerator):
    """
    Context manager –¥–ª—è —Å–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ZeRO-3 –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π.
    
    –ö–†–ò–¢–ò–ß–ù–û –î–õ–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:
    –ü—Ä–∏ ZeRO-3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã sharded –º–µ–∂–¥—É GPU. –ë–µ–∑ GatheredParameters 
    –∫–∞–∂–¥—ã–π forward (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞!) –¥–µ–ª–∞–µ—Ç all-gather = –û–ß–ï–ù–¨ –º–µ–¥–ª–µ–Ω–Ω–æ.
    
    GatheredParameters —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –û–î–ò–ù —Ä–∞–∑ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π.
    –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è ZeRO-3 —Å offload, –∏ –±–µ–∑ offload.
    
    –ò—Å—Ç–æ—á–Ω–∏–∫: grpo_optimizations.md, TRL docs (ds3_gather_for_generation)
    """
    if accelerator is None:
        yield
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ZeRO-3
    ds_plugin = getattr(accelerator.state, 'deepspeed_plugin', None)
    if ds_plugin is None:
        yield
        return
    
    zero_stage = getattr(ds_plugin, 'zero_stage', 0)
    if zero_stage != 3:
        yield
        return
    
    # ZeRO-3: –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º GatheredParameters –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    # –ë–µ–∑ —ç—Ç–æ–≥–æ –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω = all-gather = –∑–∞–≤–∏—Å–∞–Ω–∏–µ
    try:
        from deepspeed.runtime.zero.partition_parameters import GatheredParameters
        
        # model —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å unwrapped (–ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∏–∑ generate_rollouts)
        params_to_gather = list(model.parameters())
        if not params_to_gather:
            logger.warning("  ‚ö†Ô∏è ds3_gather: –Ω–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞")
            yield
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ CPU offload (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
        has_cpu_offload = False
        try:
            ds_config = ds_plugin.deepspeed_config
            offload_param = ds_config.get('zero_optimization', {}).get('offload_param', {})
            param_device = offload_param.get('device', 'none') if isinstance(offload_param, dict) else 'none'
            has_cpu_offload = param_device == 'cpu'
        except Exception:
            pass
        
        offload_str = " (—Å CPU offload)" if has_cpu_offload else ""
        logger.info(f"  üîÑ ds3_gather: —Å–æ–±–∏—Ä–∞–µ–º {len(params_to_gather)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤{offload_str}...")
        
        # modifier_rank=None –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ –≤—Å–µ —Ä–∞–Ω–∫–∏ –º–æ–≥—É—Ç —á–∏—Ç–∞—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        with GatheredParameters(params_to_gather, modifier_rank=None):
            logger.info("  ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–±—Ä–∞–Ω—ã, –Ω–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é")
            yield
            logger.info("  ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    
    except ImportError:
        logger.warning("  ‚ö†Ô∏è DeepSpeed –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ ds3_gather (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ)")
        yield
    except Exception as e:
        logger.warning(f"  ‚ö†Ô∏è ds3_gather –æ—à–∏–±–∫–∞: {e}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ)")
        yield

from .experience import Experience
from .legacy_config import GRPOConfig
from .rewards.base import RewardResult


@dataclass
class Rollout:
    """
    –†–µ–∑—É–ª—å—Ç–∞—Ç rollout'–∞ (–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏).
    
    Attributes:
        prompt: –ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt_ids: Token IDs –ø—Ä–æ–º–ø—Ç–∞
        completions: –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        completion_ids: Token IDs –æ—Ç–≤–µ—Ç–æ–≤
        rewards: Rewards –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        is_truncated: –§–ª–∞–≥–∏ –æ–±—Ä–µ–∑–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç max_length)
        feedbacks: üî• SDPO: feedback –æ—Ç reward —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ completion
    """
    prompt: str
    prompt_ids: torch.Tensor
    completions: List[str]
    completion_ids: List[torch.Tensor]
    rewards: torch.Tensor
    is_truncated: List[bool]
    
    # üî• SDPO: feedback –æ—Ç reward —Ñ—É–Ω–∫—Ü–∏–π (–æ—à–∏–±–∫–∏, –ø–æ—è—Å–Ω–µ–Ω–∏—è)
    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è rich environment feedback –≤ self-distillation
    feedbacks: Optional[List[Optional[str]]] = None
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è reward
    metadata: Optional[Dict[str, Any]] = None


def build_reasoning_prompt(
    question: str,
    tokenizer: PreTrainedTokenizer,
    reasoning_format: str = "deepseek",
    system_prompt: Optional[str] = None,
) -> str:
    """
    –°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è reasoning –∑–∞–¥–∞—á–∏.
    
    Args:
        question: –í–æ–ø—Ä–æ—Å/–∑–∞–¥–∞—á–∞
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è chat template
        reasoning_format: "deepseek" (<think>) –∏–ª–∏ "simple" (<reasoning>)
        system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    """
    if reasoning_format == "deepseek":
        default_system = """A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>
<answer> answer here </answer>"""
    else:  # simple
        default_system = """–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
<reasoning>
(–®–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è)
</reasoning>
<answer>
(–ö–æ—Ä–æ—Ç–∫–∏–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç)
</answer>"""
    
    system = system_prompt or default_system
    
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": question},
    ]
    
    if hasattr(tokenizer, "apply_chat_template"):
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    else:
        # Fallback –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –±–µ–∑ chat template
        prompt = f"{system}\n\nUser: {question}\n\nAssistant:"
    
    return prompt


def extract_answer_from_completion(
    completion: str,
    reasoning_format: str = "deepseek",
) -> Tuple[Optional[str], Optional[str]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç reasoning –∏ –æ—Ç–≤–µ—Ç –∏–∑ completion.
    
    Args:
        completion: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        reasoning_format: "deepseek" –∏–ª–∏ "simple"
        
    Returns:
        Tuple[reasoning, answer]
    """
    if reasoning_format == "deepseek":
        reasoning_pat = re.compile(r"<think>(.*?)</think>", re.DOTALL)
        answer_pat = re.compile(r"<answer>(.*?)</answer>", re.DOTALL)
    else:
        reasoning_pat = re.compile(r"<reasoning>(.*?)</reasoning>", re.DOTALL)
        answer_pat = re.compile(r"<answer>(.*?)</answer>", re.DOTALL)
    
    reasoning_match = reasoning_pat.search(completion)
    answer_match = answer_pat.search(completion)
    
    reasoning = reasoning_match.group(1).strip() if reasoning_match else None
    answer = answer_match.group(1).strip() if answer_match else None
    
    return reasoning, answer


def sequence_log_probs_from_logits(
    logits: torch.Tensor,
    output_ids: torch.Tensor,
) -> torch.Tensor:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ logits.
    
    Args:
        logits: –í—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ [batch, seq_len, vocab_size]
        output_ids: ID —Ç–æ–∫–µ–Ω–æ–≤ [batch, seq_len]
        
    Returns:
        Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [batch, seq_len]
    """
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–ú–Ø–¢–ò: –ò—Å–ø–æ–ª—å–∑—É–µ–º Liger CrossEntropy –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    # Liger CE –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –ø–æ –ø–∞–º—è—Ç–∏ —á–µ–º F.cross_entropy
    # –î–ª—è –±–æ–ª—å—à–∏—Ö vocab (Qwen ~152k) —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ
    
    batch_size, seq_len, vocab_size = logits.shape
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunked cross-entropy –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    # –≠—Ç–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ –µ—Å–ª–∏ batch*seq —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
    # chunk_size=4096 —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ GPU
    nll = chunked_cross_entropy(
        logits,
        output_ids,
        chunk_size=4096,
        ignore_index=-100,
    )
    
    # log_prob = -nll
    log_probs = -nll
    
    return log_probs


def compute_log_probs(
    model: PreTrainedModel,
    sequence_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    accelerator=None,
    chunk_size: Optional[int] = None,  # üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: auto-detect
) -> torch.Tensor:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    
    üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–ú–Ø–¢–ò: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç sequences –ø–æ —á–∞—Å—Ç—è–º (chunked forward pass)
    —á—Ç–æ–±—ã –Ω–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—Å–µ logits [batch, seq, vocab] –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
    
    –î–ª—è batch=8, seq=1200, vocab=152k —ç—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç ~2.9 GB –Ω–∞ logits!
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∂–∏–º:
    - no_grad context: chunk_size=1 (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è rollout)
    - with grad: chunk_size=batch_size (–Ω—É–∂–Ω—ã –≤—Å–µ activations –¥–ª—è backprop)
    
    –í–ê–ñ–ù–û: –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç @torch.no_grad(), —á—Ç–æ–±—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –º–æ–≥–ª–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å
    –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ torch.no_grad() –≤—Ä—É—á–Ω—É—é —Ç–∞–º –≥–¥–µ –Ω—É–∂–Ω–æ.
    
    Args:
        model: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DDP)
        sequence_ids: Token IDs [batch, seq_len]
        attention_mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è [batch, seq_len]
        accelerator: Accelerator –æ–±—ä–µ–∫—Ç –¥–ª—è unwrap –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        chunk_size: –°–∫–æ–ª—å–∫–æ sequences –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∑–∞ —Ä–∞–∑ (None=auto-detect)
        
    Returns:
        Log-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [batch, seq_len-1]
    """
    forward_model = model
    batch_size, seq_len = sequence_ids.shape
    device = sequence_ids.device
    
    # üî• AUTO-DETECT: –µ—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã, –Ω–µ –¥–µ–ª–∞–µ–º chunking (–∏–Ω–∞—á–µ backprop —Å–ª–æ–º–∞–µ—Ç—Å—è)
    # –ï—Å–ª–∏ no_grad context ‚Äî chunk –ø–æ 1 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    if chunk_size is None:
        if torch.is_grad_enabled():
            # Training mode: –Ω—É–∂–Ω—ã –≤—Å–µ activations –¥–ª—è backprop
            chunk_size = batch_size
        else:
            # Inference mode (rollout): chunk –ø–æ 1 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            chunk_size = 1
    
    # Position IDs
    position_ids = attention_mask.long().cumsum(dim=-1) - 1
    position_ids.masked_fill_(mask=(attention_mask == 0), value=1)
    
    # üî• CHUNKED FORWARD: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ chunk_size sequences –∑–∞ —Ä–∞–∑
    if batch_size <= chunk_size:
        # Batch –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–π ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ä–∞–∑—É
        output = forward_model(
            input_ids=sequence_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            use_cache=False,
        )
        logits = output.logits[:, :-1]
        target_ids = sequence_ids[:, 1:]
        log_probs = sequence_log_probs_from_logits(logits, target_ids)
        del output, logits  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
        return log_probs
    
    # Chunked processing (—Ç–æ–ª—å–∫–æ –¥–ª—è no_grad mode)
    all_log_probs = []
    for start_idx in range(0, batch_size, chunk_size):
        end_idx = min(start_idx + chunk_size, batch_size)
        
        # Forward pass –¥–ª—è chunk
        chunk_output = forward_model(
            input_ids=sequence_ids[start_idx:end_idx],
            attention_mask=attention_mask[start_idx:end_idx],
            position_ids=position_ids[start_idx:end_idx],
            use_cache=False,
        )
        
        chunk_logits = chunk_output.logits[:, :-1]
        chunk_targets = sequence_ids[start_idx:end_idx, 1:]
        
        chunk_log_probs = sequence_log_probs_from_logits(chunk_logits, chunk_targets)
        all_log_probs.append(chunk_log_probs)
        
        # üî• –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ chunk
        del chunk_output, chunk_logits
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    return torch.cat(all_log_probs, dim=0)


def _batch_generate_multi_prompt(
    generate_model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    prompt_batch: List[str],
    config: GRPOConfig,
    generation_config: GenerationConfig,
    device: torch.device,
    autocast_ctx,
) -> Tuple[List[torch.Tensor], List[int]]:
    """
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë–∞—Ç—á–µ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
    
    –í–º–µ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–æ–º–ø—Ç—É, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ –±–∞—Ç—á,
    —á—Ç–æ –¥–∞—ë—Ç –ª—É—á—à—É—é —É—Ç–∏–ª–∏–∑–∞—Ü–∏—é GPU (–æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–∞—Ö).
    
    Args:
        generate_model: –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt_batch: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–∞—Ç—á–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO
        generation_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        autocast_ctx: –ö–æ–Ω—Ç–µ–∫—Å—Ç mixed precision
        
    Returns:
        Tuple[List[generated_ids], List[prompt_lengths]]
    """
    batch_size = len(prompt_batch)
    group_size = config.group_size
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –ø—Ä–æ–º–ø—Ç—ã –≤ –±–∞—Ç—á —Å padding
    prompt_inputs = tokenizer(
        prompt_batch,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=config.max_prompt_length,
    ).to(device)
    
    prompt_lengths = [
        (prompt_inputs["attention_mask"][i] == 1).sum().item()
        for i in range(batch_size)
    ]
    
    # –†–∞—Å—à–∏—Ä—è–µ–º –¥–ª—è group_size: –∫–∞–∂–¥—ã–π –ø—Ä–æ–º–ø—Ç –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è G —Ä–∞–∑
    # [prompt0, prompt0, ..., prompt1, prompt1, ...]
    expanded_input_ids = prompt_inputs["input_ids"].repeat_interleave(group_size, dim=0)
    expanded_attention_mask = prompt_inputs["attention_mask"].repeat_interleave(group_size, dim=0)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö completions –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
    with autocast_ctx:
        outputs = generate_model.generate(
            input_ids=expanded_input_ids,
            attention_mask=expanded_attention_mask,
            generation_config=generation_config,
            return_dict_in_generate=True,
            output_scores=False,
        )
    
    # –†–∞–∑–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –ø–æ –ø—Ä–æ–º–ø—Ç–∞–º
    all_generated = outputs.sequences  # [batch_size * group_size, seq_len]
    generated_per_prompt = []
    for i in range(batch_size):
        start_idx = i * group_size
        end_idx = start_idx + group_size
        generated_per_prompt.append(all_generated[start_idx:end_idx])
    
    return generated_per_prompt, prompt_lengths


@torch.no_grad()
def generate_rollouts(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    prompts: List[str],
    reference_answers: List[str],
    reward_fn: Callable,
    config: GRPOConfig,
    reference_model: Optional[PreTrainedModel] = None,
    device: Optional[torch.device] = None,
    accelerator=None,
    prompt_ids: Optional[List[int]] = None,
    metadata_list: Optional[List[Dict[str, Any]]] = None,
) -> List[Rollout]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç rollout'—ã –¥–ª—è —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤.
    
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:
    - Prefix Grouper: shared KV-cache –¥–ª—è G completions (2-3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
    - ds3_gather_for_generation: —Å–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ZeRO-3 –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (10-100x)
    - Multi-prompt batching: –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º (1.5-2x)
    
    Args:
        model: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (–ø–æ–ª–∏—Ç–∏–∫–∞) - –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DDP
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ (–≤–æ–ø—Ä–æ—Å–æ–≤)
        reference_answers: –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è reward
        reward_fn: –§—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è reward(completion, reference) -> float
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO
        reference_model: –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è KL (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        accelerator: Accelerator –æ–±—ä–µ–∫—Ç –¥–ª—è unwrap –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        metadata_list: –°–ø–∏—Å–æ–∫ metadata –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–π)
        
    Returns:
        –°–ø–∏—Å–æ–∫ Rollout –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
    """
    # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DDP, –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å unwrapped –º–æ–¥–µ–ª—å –¥–ª—è generate()
    # DDP –Ω–µ –ø–µ—Ä–µ–¥–∞–µ—Ç –º–µ—Ç–æ–¥—ã —Ç–∏–ø–∞ generate() –Ω–∞–ø—Ä—è–º—É—é
    if accelerator is not None:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º unwrapped –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        unwrapped_model = accelerator.unwrap_model(model)
    elif hasattr(model, 'module'):
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DDP –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ accelerator)
        unwrapped_model = model.module
    else:
        # –ú–æ–¥–µ–ª—å –Ω–µ –æ–±–µ—Ä–Ω—É—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
        unwrapped_model = model
    
    unwrapped_model.eval()
    if device is None:
        # –í–ê–ñ–ù–û: –ü—Ä–∏ ZeRO-3 —Å CPU offload –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–∞ CPU
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerator.device –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if accelerator is not None:
            device = accelerator.device
        else:
            try:
                device = next(unwrapped_model.parameters()).device
            except StopIteration:
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    logger.info(f"üé≤ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é {len(prompts)} rollouts –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {device}")
    
    rollouts = []
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    generation_config = GenerationConfig(
        do_sample=True,
        temperature=config.temperature,
        top_p=config.top_p,
        max_new_tokens=config.max_new_tokens,
        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ ZeRO-3 (–¥–ª—è DeepSpeed inference)
    is_zero3 = False
    if accelerator is not None:
        ds_plugin = getattr(accelerator.state, 'deepspeed_plugin', None)
        if ds_plugin is not None:
            zero_stage = getattr(ds_plugin, 'zero_stage', 0)
            is_zero3 = zero_stage == 3
    
    # Mixed precision –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
    mp = (getattr(config, "mixed_precision", None) or "bf16").lower()
    use_autocast = torch.cuda.is_available() and mp in ("bf16", "fp16")
    if use_autocast:
        amp_dtype = torch.bfloat16 if mp == "bf16" else torch.float16
        autocast_ctx = torch.amp.autocast("cuda", enabled=True, dtype=amp_dtype)
    else:
        autocast_ctx = nullcontext()
    
    # –í–ê–ñ–ù–û: –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º unwrapped –º–æ–¥–µ–ª—å
    # - DDP: generate() –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ DDP wrapper
    # - ZeRO-3 + GatheredParameters: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–±—Ä–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–ø—Ä—è–º—É—é
    generate_model = unwrapped_model
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Prefix Grouper (shared KV-cache)
    # –í–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ-ZeRO-3 —Ä–µ–∂–∏–º–æ–≤ (ZeRO-3 –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å KV-cache)
    use_prefix_grouper = getattr(config, 'use_prefix_grouper', True) and not is_zero3
    
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Multi-prompt batching
    # –°–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º (1 = –æ—Ç–∫–ª—é—á–µ–Ω–æ)
    # –ù–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å Prefix Grouper (–æ–Ω–∏ –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ)
    rollout_batch_size = getattr(config, 'rollout_batch_size', 1)
    use_multi_prompt_batch = rollout_batch_size > 1 and not use_prefix_grouper
    
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: ds3_gather_for_generation
    # –ü—Ä–∏ ZeRO-3 —Å–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–¥–∏–Ω —Ä–∞–∑ –ø–µ—Ä–µ–¥ –≤—Å–µ–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è–º–∏
    # –ü–µ—Ä–µ–¥–∞—ë–º unwrapped –º–æ–¥–µ–ª—å ‚Äî —Ç–∞–º –±–µ—Ä—É—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è gather
    use_ds3_gather = getattr(config, 'ds3_gather_for_generation', True) and is_zero3
    ds3_gather_ctx = ds3_gather_for_generation(unwrapped_model, accelerator) if use_ds3_gather else nullcontext()
    
    with ds3_gather_ctx:
        # ============================================================
        # MULTI-PROMPT BATCHING: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –∑–∞ —Ä–∞–∑
        # ============================================================
        if use_multi_prompt_batch:
            logger.info(f"  üöÄ Multi-prompt batching: rollout_batch_size={rollout_batch_size}")
            
            for batch_start in range(0, len(prompts), rollout_batch_size):
                batch_end = min(batch_start + rollout_batch_size, len(prompts))
                prompt_batch = prompts[batch_start:batch_end]
                ref_batch = reference_answers[batch_start:batch_end]
                
                if batch_start == 0:
                    logger.info(f"  üìä First batch: {len(prompt_batch)} prompts, group_size={config.group_size}")
                    logger.info(f"  üìä Total generations per batch: {len(prompt_batch) * config.group_size}")
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞—Ç—á
                generated_per_prompt, prompt_lengths = _batch_generate_multi_prompt(
                    generate_model=generate_model,
                    tokenizer=tokenizer,
                    prompt_batch=prompt_batch,
                    config=config,
                    generation_config=generation_config,
                    device=device,
                    autocast_ctx=autocast_ctx,
                )
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –≤ –±–∞—Ç—á–µ
                for i, (prompt, ref_answer, generated_ids, prompt_length) in enumerate(
                    zip(prompt_batch, ref_batch, generated_per_prompt, prompt_lengths)
                ):
                    prompt_idx = batch_start + i
                    
                    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ completions
                    completions = tokenizer.batch_decode(
                        generated_ids[:, prompt_length:],
                        skip_special_tokens=True,
                    )
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º truncated –æ—Ç–≤–µ—Ç—ã
                    is_truncated = []
                    for j in range(config.group_size):
                        completion_length = (generated_ids[j, prompt_length:] != tokenizer.pad_token_id).sum().item()
                        is_truncated.append(completion_length >= config.max_new_tokens)
                    
                    # üî• SDPO: –í—ã—á–∏—Å–ª—è–µ–º rewards –∏ —Å–æ–±–∏—Ä–∞–µ–º feedback
                    rewards = torch.zeros(config.group_size, dtype=torch.float32, device=device)
                    feedbacks: List[Optional[str]] = []
                    # –ü–æ–ª—É—á–∞–µ–º metadata –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
                    prompt_metadata = metadata_list[batch_idx * batch_size + i] if metadata_list else {}
                    for j, completion in enumerate(completions):
                        try:
                            result = reward_fn(
                                completion=completion,
                                reference_answer=ref_answer,
                                reasoning_format=config.reasoning_format,
                                is_truncated=is_truncated[j],
                                metadata=prompt_metadata,
                            )
                            # üî• SDPO: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º RewardResult —Å feedback
                            if isinstance(result, RewardResult):
                                rewards[j] = float(result.score)
                                feedbacks.append(result.feedback)
                            elif isinstance(result, (int, float)):
                                rewards[j] = float(result)
                                feedbacks.append(None)
                            else:
                                rewards[j] = 0.0
                                feedbacks.append(None)
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ reward –¥–ª—è completion {j}: {e}")
                            rewards[j] = 0.0
                            feedbacks.append(f"Error computing reward: {str(e)}")
                    
                    # –°–æ–∑–¥–∞—ë–º Rollout
                    # –ù—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å prompt_ids –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
                    prompt_inputs_single = tokenizer(
                        prompt,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=config.max_prompt_length,
                    ).to(device)
                    
                    rollout = Rollout(
                        prompt=prompt,
                        prompt_ids=prompt_inputs_single["input_ids"][0],
                        completions=completions,
                        completion_ids=[generated_ids[j, prompt_length:] for j in range(config.group_size)],
                        rewards=rewards,
                        is_truncated=is_truncated,
                        feedbacks=feedbacks,  # üî• SDPO
                        metadata={
                            "reference_answer": ref_answer,
                            "prompt_idx": prompt_idx,
                            "prompt_id": (prompt_ids[prompt_idx] if prompt_ids is not None and prompt_idx < len(prompt_ids) else prompt_idx),
                        }
                    )
                    rollouts.append(rollout)
                
                if batch_start == 0:
                    logger.info(f"  ‚úÖ First batch completed")
            
            return rollouts
        
        # ============================================================
        # SINGLE-PROMPT: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–æ–º–ø—Ç—É (—Å Prefix Grouper)
        # ============================================================
        for prompt_idx, (prompt, ref_answer) in enumerate(zip(prompts, reference_answers)):
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞.
            prompt_inputs = tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=config.max_prompt_length,
            ).to(device)
            
            prompt_length = prompt_inputs["input_ids"].size(1)
            
            if prompt_idx == 0:
                logger.info(f"  üîÑ –ü–µ—Ä–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: is_zero3={is_zero3}, device={device}, group_size={config.group_size}")
                logger.info(f"  üìä Prompt length: {prompt_length}, max_new_tokens={config.max_new_tokens}")
                logger.info(f"  üöÄ Prefix Grouper (shared KV-cache): {'ON' if use_prefix_grouper else 'OFF'}")
                logger.info(f"  üîß ds3_gather_for_generation: {'ON' if use_ds3_gather else 'OFF'}")
                if is_zero3 and not use_ds3_gather:
                    logger.warning("  ‚ö†Ô∏è ZeRO-3 –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –û–ß–ï–ù–¨ –º–µ–¥–ª–µ–Ω–Ω–æ–π! –í–∫–ª—é—á–∏—Ç–µ ds3_gather_for_generation")
            
            with autocast_ctx:
                if use_prefix_grouper:
                    # ============================================================
                    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Prefix Grouper - shared KV-cache
                    # ============================================================
                    # –ò–¥–µ—è: prompt –ø—Ä–æ–≥–æ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –û–î–ò–ù —Ä–∞–∑, –ø–æ–ª—É—á–∞–µ–º KV-cache,
                    # –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º G completions —Å —ç—Ç–∏–º –∫—ç—à–µ–º.
                    # –≠–∫–æ–Ω–æ–º–∏—è: prompt_length * (G-1) forward passes
                    # ============================================================
                    
                    try:
                        from transformers.cache_utils import DynamicCache
                        
                        # –®–∞–≥ 1: –ü—Ä–æ–≥–Ω–∞—Ç—å prompt (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞) –æ–¥–∏–Ω —Ä–∞–∑, –ø–æ–ª—É—á–∏—Ç—å KV-cache
                        # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è –Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                        with torch.no_grad():
                            past_key_values = DynamicCache()
                            
                            # –ü—Ä–æ–≥–æ–Ω—è–µ–º prompt[:-1] —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∫—ç—à
                            # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω –≤ generate() –∫–∞–∫ –Ω–∞—á–∞–ª—å–Ω—ã–π
                            if prompt_length > 1:
                                prefix_ids = prompt_inputs["input_ids"][:, :-1]
                                prefix_mask = prompt_inputs["attention_mask"][:, :-1]
                                cached_seq_len = prefix_ids.size(1)
                                
                                # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º cache_position –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –Ω–æ–≤—ã–º Cache API
                                cache_position = torch.arange(cached_seq_len, device=device)
                                
                                prefix_outputs = generate_model(
                                    input_ids=prefix_ids,
                                    attention_mask=prefix_mask,
                                    past_key_values=past_key_values,
                                    cache_position=cache_position,
                                    use_cache=True,
                                    return_dict=True,
                                )
                                # past_key_values —Ç–µ–ø–µ—Ä—å –∑–∞–ø–æ–ª–Ω–µ–Ω –¥–ª—è prefix
                            else:
                                # –ï—Å–ª–∏ prompt –≤—Å–µ–≥–æ 1 —Ç–æ–∫–µ–Ω, –Ω–µ—Ç —Å–º—ã—Å–ª–∞ –≤ prefix grouper
                                raise ValueError("Prompt too short for prefix grouper")
                        
                        # –®–∞–≥ 2: –†–∞—Å—à–∏—Ä–∏—Ç—å KV-cache –¥–ª—è G –≥–µ–Ω–µ—Ä–∞—Ü–∏–π
                        legacy_cache = past_key_values.to_legacy_cache()
                        
                        expanded_legacy = []
                        for layer_kv in legacy_cache:
                            expanded_key = layer_kv[0].expand(config.group_size, -1, -1, -1).contiguous()
                            expanded_value = layer_kv[1].expand(config.group_size, -1, -1, -1).contiguous()
                            expanded_legacy.append((expanded_key, expanded_value))
                        expanded_legacy = tuple(expanded_legacy)
                        
                        expanded_cache = DynamicCache.from_legacy_cache(expanded_legacy)
                        
                        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å shared KV-cache
                        # input_ids = —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω prompt'–∞ (G —Ä–∞–∑)
                        last_token = prompt_inputs["input_ids"][:, -1:].repeat(config.group_size, 1)
                        
                        # attention_mask –¥–æ–ª–∂–µ–Ω –ø–æ–∫—Ä—ã–≤–∞—Ç—å –≤–µ—Å—å prefix + –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
                        gen_attention_mask = torch.ones(
                            config.group_size, cached_seq_len + 1,
                            dtype=prompt_inputs["attention_mask"].dtype,
                            device=device
                        )
                        
                        # –í–ê–ñ–ù–û: cache_position –¥–ª—è generate() –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Å–ª–µ –∫—ç—à–∞
                        gen_cache_position = torch.tensor([cached_seq_len], device=device)
                        
                        outputs = generate_model.generate(
                            input_ids=last_token,
                            attention_mask=gen_attention_mask,
                            past_key_values=expanded_cache,
                            cache_position=gen_cache_position,
                            generation_config=generation_config,
                            return_dict_in_generate=True,
                            output_scores=False,
                        )
                        
                        # –†–µ–∑—É–ª—å—Ç–∞—Ç: sequences –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å last_token, –∑–∞—Ç–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ
                        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: prefix + generated
                        prefix_expanded = prompt_inputs["input_ids"][:, :-1].repeat(config.group_size, 1)
                        generated_ids = torch.cat([prefix_expanded, outputs.sequences], dim=1)
                        
                        if prompt_idx == 0:
                            logger.info(f"  ‚úÖ Prefix Grouper: cached {cached_seq_len} tokens, generated {outputs.sequences.size(1)} tokens")
                    
                    except Exception as e:
                        # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                        if prompt_idx == 0:
                            logger.warning(f"  ‚ö†Ô∏è Prefix Grouper failed: {e}, using standard generation")
                        
                        input_ids = prompt_inputs["input_ids"].repeat(config.group_size, 1)
                        attention_mask = prompt_inputs["attention_mask"].repeat(config.group_size, 1)
                        
                        outputs = generate_model.generate(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            generation_config=generation_config,
                            return_dict_in_generate=True,
                            output_scores=False,
                        )
                        generated_ids = outputs.sequences
                
                else:
                    # ============================================================
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–±–µ–∑ Prefix Grouper)
                    # ============================================================
                    input_ids = prompt_inputs["input_ids"].repeat(config.group_size, 1)
                    attention_mask = prompt_inputs["attention_mask"].repeat(config.group_size, 1)
                    
                    outputs = generate_model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        generation_config=generation_config,
                        return_dict_in_generate=True,
                        output_scores=False,
                    )
                    generated_ids = outputs.sequences
            
            if prompt_idx == 0:
                logger.info(f"  ‚úÖ –ü–µ—Ä–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, tokens: {generated_ids.shape}")
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ completions
            completions = tokenizer.batch_decode(
                generated_ids[:, prompt_length:],
                skip_special_tokens=True,
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º truncated –æ—Ç–≤–µ—Ç—ã
            is_truncated = []
            for i in range(config.group_size):
                completion_length = (generated_ids[i, prompt_length:] != tokenizer.pad_token_id).sum().item()
                is_truncated.append(completion_length >= config.max_new_tokens)
            
            # üî• SDPO: –í—ã—á–∏—Å–ª—è–µ–º rewards –∏ —Å–æ–±–∏—Ä–∞–µ–º feedback
            rewards = torch.zeros(config.group_size, dtype=torch.float32, device=device)
            feedbacks: List[Optional[str]] = []
            # –ü–æ–ª—É—á–∞–µ–º metadata –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            prompt_metadata = metadata_list[prompt_idx] if metadata_list else {}
            for i, completion in enumerate(completions):
                try:
                    result = reward_fn(
                        completion=completion,
                        reference_answer=ref_answer,
                        reasoning_format=config.reasoning_format,
                        is_truncated=is_truncated[i],
                        metadata=prompt_metadata,
                    )
                    # üî• SDPO: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º RewardResult —Å feedback
                    if isinstance(result, RewardResult):
                        rewards[i] = float(result.score)
                        feedbacks.append(result.feedback)
                    elif isinstance(result, (int, float)):
                        rewards[i] = float(result)
                        feedbacks.append(None)
                    else:
                        logger.warning(
                            f"Reward –Ω–µ —á–∏—Å–ª–æ: {type(result)} = {result} –¥–ª—è completion: {completion[:100]}..."
                        )
                        rewards[i] = 0.0
                        feedbacks.append(None)
                except Exception as e:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ reward –¥–ª—è completion {i}: {e}\n"
                        f"Completion: {completion[:200]}...\n"
                        f"Reference: {ref_answer[:100]}..."
                    )
                    rewards[i] = 0.0
                    feedbacks.append(f"Error computing reward: {str(e)}")
            
            # –°–æ–∑–¥–∞—ë–º Rollout
            rollout = Rollout(
                prompt=prompt,
                prompt_ids=prompt_inputs["input_ids"][0],
                completions=completions,
                completion_ids=[generated_ids[i, prompt_length:] for i in range(config.group_size)],
                rewards=rewards,
                is_truncated=is_truncated,
                feedbacks=feedbacks,  # üî• SDPO
                metadata={
                    "reference_answer": ref_answer,
                    "prompt_idx": prompt_idx,
                    "prompt_id": (prompt_ids[prompt_idx] if prompt_ids is not None and prompt_idx < len(prompt_ids) else prompt_idx),
                }
            )
            rollouts.append(rollout)
    
    return rollouts


@torch.no_grad()
def generate_rollouts_vllm(
    *,
    vllm_engine,
    tokenizer: PreTrainedTokenizer,
    prompts: List[str],
    reference_answers: List[str],
    reward_fn: Callable,
    config: GRPOConfig,
    prompt_ids: Optional[List[int]] = None,
    metadata_list: Optional[List[Dict[str, Any]]] = None,
) -> List[Rollout]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è rollouts —á–µ—Ä–µ–∑ vLLM.

    –ú—ã –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ—Ç –∂–µ —Ñ–æ—Ä–º–∞—Ç Rollout, —á—Ç–æ –∏ generate_rollouts (HF).
    old_logprobs/ref_logprobs —Å—á–∏—Ç–∞—é—Ç—Å—è –ø–æ–∑–∂–µ –Ω–∞ training-–º–æ–¥–µ–ª–∏ (teacher-forcing),
    –ø–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω—ã –∏ —Ç–µ–∫—Å—Ç completions.
    """
    if len(prompts) != len(reference_answers):
        raise ValueError("prompts –∏ reference_answers –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã")

    eos_id = tokenizer.eos_token_id
    stop_ids = [int(eos_id)] if eos_id is not None else None
    sampling_params = vllm_engine.make_sampling_params(
        n=int(config.group_size),
        temperature=float(config.temperature),
        top_p=float(config.top_p),
        max_tokens=int(config.max_new_tokens),
        stop_token_ids=stop_ids,
    )

    # vLLM batched generation: –æ–¥–∏–Ω –≤—ã–∑–æ–≤ –Ω–∞ batch prompts
    outputs = vllm_engine.generate(prompts, sampling_params)
    if len(outputs) != len(prompts):
        raise RuntimeError(f"vLLM –≤–µ—Ä–Ω—É–ª {len(outputs)} outputs –Ω–∞ {len(prompts)} prompts")

    rollouts: List[Rollout] = []
    for prompt_idx, (prompt, ref_answer, out) in enumerate(zip(prompts, reference_answers, outputs)):
        # prompt token ids (–¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Å–∫–ª–µ–∏–≤–∞–Ω–∏—è)
        prompt_tok = tokenizer(prompt, return_tensors="pt", add_special_tokens=False)
        prompt_ids_tensor = prompt_tok["input_ids"][0]
        prompt_len = int(prompt_ids_tensor.size(0))

        completions: List[str] = []
        completion_ids: List[torch.Tensor] = []
        is_truncated: List[bool] = []

        # out.outputs: list of candidates (n == group_size)
        cand_list = getattr(out, "outputs", None)
        if cand_list is None:
            raise RuntimeError("vLLM output missing .outputs")

        for cand in cand_list:
            text = getattr(cand, "text", "")
            tok_ids = getattr(cand, "token_ids", None)
            if tok_ids is None:
                # fallback: tokenize text (–º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ –ø—Ä–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤)
                tok_ids = tokenizer(text, add_special_tokens=False).input_ids
            completions.append(text)
            completion_ids.append(torch.tensor(tok_ids, dtype=torch.long))
            finish_reason = getattr(cand, "finish_reason", None)
            is_truncated.append(finish_reason == "length")

        # üî• SDPO: Rewards –∏ feedback
        rewards = torch.zeros(len(completions), dtype=torch.float)
        feedbacks: List[Optional[str]] = []
        # –ü–æ–ª—É—á–∞–µ–º metadata –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        prompt_metadata = metadata_list[prompt_idx] if metadata_list else {}
        for i, comp in enumerate(completions):
            try:
                result = reward_fn(
                    completion=comp,
                    reference_answer=ref_answer,
                    reasoning_format=config.reasoning_format,
                    is_truncated=is_truncated[i],
                    metadata=prompt_metadata,
                )
                # üî• SDPO: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º RewardResult —Å feedback
                if isinstance(result, RewardResult):
                    rewards[i] = float(result.score)
                    feedbacks.append(result.feedback)
                elif isinstance(result, (int, float)):
                    rewards[i] = float(result)
                    feedbacks.append(None)
                else:
                    rewards[i] = 0.0
                    feedbacks.append(None)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ reward_fn: {e}")
                rewards[i] = 0.0
                feedbacks.append(f"Error computing reward: {str(e)}")

        rollouts.append(
            Rollout(
                prompt=prompt,
                prompt_ids=prompt_ids_tensor,
                completions=completions,
                completion_ids=completion_ids,
                rewards=rewards,
                is_truncated=is_truncated,
                feedbacks=feedbacks,  # üî• SDPO
                metadata={
                    "reference_answer": ref_answer,
                    "prompt_idx": prompt_idx,
                    "prompt_id": (prompt_ids[prompt_idx] if prompt_ids is not None and prompt_idx < len(prompt_ids) else prompt_idx),
                },
            )
        )

    return rollouts


def rollout_to_experiences(
    rollout: Rollout,
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    config: GRPOConfig,
    reference_model: Optional[PreTrainedModel] = None,
    device: Optional[torch.device] = None,
    accelerator=None,
) -> List[Experience]:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Rollout –≤ —Å–ø–∏—Å–æ–∫ Experience –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
    
    Args:
        rollout: –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        model: –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å (–ø–æ–ª–∏—Ç–∏–∫–∞)
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        reference_model: –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è KL
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        
    Returns:
        –°–ø–∏—Å–æ–∫ Experience –¥–ª—è –∫–∞–∂–¥–æ–≥–æ completion –≤ –≥—Ä—É–ø–ø–µ
    """
    if device is None:
        # –í–ê–ñ–ù–û: –ø—Ä–∏ ZeRO-3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å sharded/offloaded, device –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –Ω–∞–¥—ë–∂–µ–Ω
        if accelerator is not None:
            device = accelerator.device
        else:
            try:
                device = next(model.parameters()).device
            except StopIteration:
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    experiences = []
    prompt_length = rollout.prompt_ids.size(0)
    
    # –í—ã—á–∏—Å–ª—è–µ–º advantages –¥–ª—è –≥—Ä—É–ø–ø—ã
    from .loss import compute_advantages
    
    advantages = compute_advantages(
        rollout.rewards,
        use_std_normalization=config.use_std_normalization,
    )
    
    # ============================================================
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: batched logprobs –¥–ª—è –≤—Å–µ–π –≥—Ä—É–ø–ø—ã (G completions)
    # –í–º–µ—Å—Ç–æ G –æ—Ç–¥–µ–ª—å–Ω—ã—Ö forward pass –¥–µ–ª–∞–µ–º 1 forward –Ω–∞ –±–∞—Ç—á.
    # ============================================================
    seq_tensors: List[torch.Tensor] = []
    seq_lens: List[int] = []
    cleaned_completion_ids: List[torch.Tensor] = []
    for i in range(len(rollout.completions)):
        completion_ids = rollout.completion_ids[i]
        
        # –í–ê–ñ–ù–û: –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–π padding, –ù–ï EOS!
        # EOS - —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–º—É –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —É—á–∏—Ç—å—Å—è
        pad_token_id = tokenizer.pad_token_id
        if pad_token_id is None:
            pad_token_id = tokenizer.eos_token_id
        
        # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–π padding (–ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ EOS –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ)
        # –ù–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º EOS —Ç–æ–∫–µ–Ω—ã, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è –º–æ–¥–µ–ª–∏
        non_pad_mask = completion_ids != pad_token_id
        # –ï—Å–ª–∏ pad_token == eos_token, —Ç–æ –Ω–µ –º–∞—Å–∫–∏—Ä—É–µ–º EOS (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ)
        if pad_token_id == tokenizer.eos_token_id:
            # –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ pad_token == eos_token, –º–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ padding –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ EOS
            # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π EOS
            eos_positions = (completion_ids == tokenizer.eos_token_id).nonzero(as_tuple=True)[0]
            if len(eos_positions) > 0:
                first_eos = eos_positions[0].item()
                # –ú–∞—Å–∫–∏—Ä—É–µ–º –≤—Å—ë –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ EOS –∫–∞–∫ padding
                non_pad_mask[first_eos + 1:] = False
                # –ù–æ —Å–∞–º EOS –æ—Å—Ç–∞–≤–ª—è–µ–º
                non_pad_mask[first_eos] = True
        
        completion_ids = completion_ids[non_pad_mask]
        cleaned_completion_ids.append(completion_ids)

        sequence_ids = torch.cat([rollout.prompt_ids.to(device), completion_ids.to(device)])
        seq_tensors.append(sequence_ids)
        seq_lens.append(int(sequence_ids.numel()))
    
    # padding —Å–ø—Ä–∞–≤–∞: prompt –≤—Å–µ–≥–¥–∞ –≤ –Ω–∞—á–∞–ª–µ, –ø—Ä–æ—â–µ —Å—Ç—Ä–æ–∏—Ç—å action_mask
    pad_token_id = tokenizer.pad_token_id
    if pad_token_id is None:
        pad_token_id = tokenizer.eos_token_id
    if pad_token_id is None:
        pad_token_id = 0

    max_len = max(seq_lens) if seq_lens else 0
    batch_size = len(seq_tensors)
    if batch_size == 0 or max_len < 2:
        return []

    batch_ids = torch.full((batch_size, max_len), int(pad_token_id), dtype=torch.long, device=device)
    attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)
    for i, seq in enumerate(seq_tensors):
        L = int(seq.numel())
        batch_ids[i, :L] = seq
        attention_mask[i, :L] = 1

    # Log probs —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (old_logprobs) ‚Äî –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    with torch.no_grad():
        batch_log_probs = compute_log_probs(
            model,
            batch_ids,
            attention_mask,
            accelerator=accelerator,
        )  # [B, max_len-1]

    # Log probs —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–¥–ª—è KL) ‚Äî —Ç–æ–∂–µ –±–∞—Ç—á–µ–≤–æ –∏ –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    batch_log_probs_ref = None
    if reference_model is not None and config.kl_weight > 0:
        with torch.no_grad():
            batch_log_probs_ref = compute_log_probs(
                reference_model,
                batch_ids,
                attention_mask,
                accelerator=accelerator,
            )

    # –¢–µ–ø–µ—Ä—å —Å–æ–±–∏—Ä–∞–µ–º Experience –ø–æ –∫–∞–∂–¥–æ–º—É completion
    for i in range(batch_size):
        sequence_ids = seq_tensors[i]
        L = seq_lens[i]
        # attention_mask –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ–º–ø–ª–∞ (–±–µ–∑ –ø–∞–¥–¥–∏–Ω–≥–∞)
        attn = torch.ones(L, dtype=torch.long, device=device)
        
        # Action mask (—Ç–æ–ª—å–∫–æ –¥–ª—è completion —Ç–æ–∫–µ–Ω–æ–≤, –≤–∫–ª—é—á–∞—è EOS)
        # –í–ê–ñ–ù–û: EOS —Ç–æ–∫–µ–Ω—ã –ù–ï –º–∞—Å–∫–∏—Ä—É—é—Ç—Å—è - —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è –º–æ–¥–µ–ª–∏
        action_mask = torch.zeros(L - 1, dtype=torch.bool, device=device)
        action_mask[prompt_length - 1 :] = True

        log_probs = batch_log_probs[i, : L - 1]
        log_probs_ref = batch_log_probs_ref[i, : L - 1] if batch_log_probs_ref is not None else None
        
        # üî• SDPO: –ø–µ—Ä–µ–¥–∞—ë–º prompt –¥–ª—è teacher reprompting
        prompt_id = None
        if rollout.metadata:
            prompt_id = rollout.metadata.get('prompt_id')
        
        exp = Experience(
            sequences=sequence_ids,
            prompt_length=prompt_length,
            action_log_probs=log_probs,
            log_probs_ref=log_probs_ref,
            returns=rollout.rewards[i].unsqueeze(0),
            advantages=advantages[i].unsqueeze(0),
            attention_mask=attn,
            action_mask=action_mask,
            completion_text=rollout.completions[i],
            prompts=[rollout.prompt],  # üî• SDPO: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            prompt_ids=[prompt_id] if prompt_id is not None else None,  # üî• SDPO
        )
        experiences.append(exp)
    
    return experiences
