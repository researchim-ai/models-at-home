"""
–ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–π.

üî• SDPO Support: Reward —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç RewardResult —Å feedback
–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ self-distillation (rich environment feedback).
"""
from abc import ABC, abstractmethod
from typing import List, Optional, Any, Dict, Callable, Union, NamedTuple
from dataclasses import dataclass
import torch
import re


@dataclass
class RewardResult:
    """
    –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è reward —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º feedback.
    
    üî• –î–ª—è SDPO: feedback –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ reprompting –¥–ª—è self-distillation.
    –ú–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç feedback —Å–≤–æ–µ–π –ø–æ–ø—ã—Ç–∫–∏ –∏ —É—á–∏—Ç—Å—è –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—à–∏–±–∫–∏.
    
    Attributes:
        score: –ß–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ reward (0-1)
        feedback: –¢–µ–∫—Å—Ç–æ–≤—ã–π feedback –¥–ª—è SDPO (–æ—à–∏–±–∫–∏, –ø–æ—è—Å–Ω–µ–Ω–∏—è)
        is_correct: –ë—ã–ª–∞ –ª–∏ –ø–æ–ø—ã—Ç–∫–∞ —É—Å–ø–µ—à–Ω–æ–π
        metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    score: float
    feedback: Optional[str] = None
    is_correct: bool = False
    metadata: Optional[Dict[str, Any]] = None
    
    def __float__(self) -> float:
        """–î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ float."""
        return self.score
    
    def __add__(self, other: Union["RewardResult", float]) -> float:
        if isinstance(other, RewardResult):
            return self.score + other.score
        return self.score + other
    
    def __radd__(self, other: Union["RewardResult", float]) -> float:
        return self.__add__(other)
    
    def __mul__(self, other: float) -> float:
        return self.score * other
    
    def __rmul__(self, other: float) -> float:
        return self.__mul__(other)


class RewardFunction(ABC):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–π.
    
    üî• SDPO Support: –ú–µ—Ç–æ–¥ __call__ –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å RewardResult —Å feedback.
    –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤–æ–∑–≤—Ä–∞—Ç float.
    """
    
    def __init__(self, weight: float = 1.0):
        """
        Args:
            weight: –í–µ—Å —ç—Ç–æ–π reward —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏
        """
        self.weight = weight
    
    @abstractmethod
    def __call__(
        self,
        completion: str,
        reference_answer: str,
        reasoning_format: str = "deepseek",
        is_truncated: bool = False,
        **kwargs,
    ) -> Union[float, RewardResult]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç reward –¥–ª—è –æ–¥–Ω–æ–≥–æ completion.
        
        Args:
            completion: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            reference_answer: –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            reasoning_format: –§–æ—Ä–º–∞—Ç reasoning —Ç–µ–≥–æ–≤
            is_truncated: –ë—ã–ª –ª–∏ –æ—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            float –∏–ª–∏ RewardResult —Å feedback –¥–ª—è SDPO
        """
        pass
    
    def _to_reward_result(
        self,
        result: Union[float, RewardResult],
    ) -> RewardResult:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ RewardResult."""
        if isinstance(result, RewardResult):
            return result
        return RewardResult(score=float(result), is_correct=float(result) >= 0.99)
    
    def batch_call(
        self,
        completions: List[str],
        reference_answers: List[str],
        return_feedback: bool = False,
        **kwargs,
    ) -> Union[torch.Tensor, tuple]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç rewards –¥–ª—è –±–∞—Ç—á–∞.
        
        Args:
            completions: –°–ø–∏—Å–æ–∫ completions
            reference_answers: –°–ø–∏—Å–æ–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            return_feedback: üî• –ï—Å–ª–∏ True, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∞–∫–∂–µ feedback list
            
        Returns:
            –ï—Å–ª–∏ return_feedback=False: Tensor rewards [batch_size]
            –ï—Å–ª–∏ return_feedback=True: (Tensor rewards, List[str|None] feedbacks)
        """
        rewards = []
        feedbacks = []
        
        for completion, ref in zip(completions, reference_answers):
            result = self(completion, ref, **kwargs)
            reward_result = self._to_reward_result(result)
            rewards.append(reward_result.score)
            feedbacks.append(reward_result.feedback)
        
        reward_tensor = torch.tensor(rewards, dtype=torch.float32)
        
        if return_feedback:
            return reward_tensor, feedbacks
        return reward_tensor


class CombinedReward(RewardFunction):
    """
    –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö reward —Ñ—É–Ω–∫—Ü–∏–π.
    
    üî• SDPO Support: –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç feedback –æ—Ç –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π.
    
    –ò—Ç–æ–≥–æ–≤—ã–π reward = sum(weight_i * reward_i) / sum(weights)
    –∏–ª–∏ –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
    """
    
    def __init__(
        self,
        reward_functions: List[RewardFunction],
        normalize: bool = True,
    ):
        """
        Args:
            reward_functions: –°–ø–∏—Å–æ–∫ reward —Ñ—É–Ω–∫—Ü–∏–π
            normalize: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–∏ –Ω–∞ —Å—É–º–º—É –≤–µ—Å–æ–≤
        """
        super().__init__(weight=1.0)
        self.reward_functions = reward_functions
        self.normalize = normalize
        
        self._total_weight = sum(rf.weight for rf in reward_functions)
    
    def __call__(
        self,
        completion: str,
        reference_answer: str,
        **kwargs,
    ) -> RewardResult:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π reward —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º feedback.
        
        üî• SDPO: Feedback –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç—Å—è –æ—Ç –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π (–Ω–µ–ø—É—Å—Ç—ã–µ).
        """
        total_reward = 0.0
        feedbacks = []
        all_correct = True
        metadata = {}
        
        for rf in self.reward_functions:
            result = rf(completion, reference_answer, **kwargs)
            reward_result = self._to_reward_result(result)
            
            total_reward += rf.weight * reward_result.score
            
            # –°–æ–±–∏—Ä–∞–µ–º feedback
            if reward_result.feedback:
                feedbacks.append(reward_result.feedback)
            
            # –°–æ–±–∏—Ä–∞–µ–º metadata
            if reward_result.metadata:
                metadata[rf.__class__.__name__] = reward_result.metadata
            
            if not reward_result.is_correct:
                all_correct = False
        
        if self.normalize and self._total_weight > 0:
            total_reward = total_reward / self._total_weight
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º feedback
        combined_feedback = None
        if feedbacks:
            combined_feedback = "\n\n".join(feedbacks)
        
        return RewardResult(
            score=total_reward,
            feedback=combined_feedback,
            is_correct=all_correct,
            metadata=metadata if metadata else None,
        )
    
    def get_component_rewards(
        self,
        completion: str,
        reference_answer: str,
        **kwargs,
    ) -> Dict[str, RewardResult]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç RewardResults –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è).
        """
        component_rewards = {}
        for rf in self.reward_functions:
            name = rf.__class__.__name__
            result = rf(completion, reference_answer, **kwargs)
            component_rewards[name] = self._to_reward_result(result)
        return component_rewards


class ConstantReward(RewardFunction):
    """
    –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π reward (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è).
    """
    
    def __init__(self, value: float = 0.0, weight: float = 1.0):
        super().__init__(weight)
        self.value = value
    
    def __call__(self, completion: str, reference_answer: str, **kwargs) -> float:
        return self.value


class BinaryReward(RewardFunction):
    """
    –ë–∏–Ω–∞—Ä–Ω—ã–π reward: 1 –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ, 0 –∏–Ω–∞—á–µ.
    """
    
    def __init__(
        self, 
        condition_fn,
        weight: float = 1.0,
        true_value: float = 1.0,
        false_value: float = 0.0,
    ):
        """
        Args:
            condition_fn: –§—É–Ω–∫—Ü–∏—è (completion, reference) -> bool
            true_value: Reward –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ True
            false_value: Reward –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ False
        """
        super().__init__(weight)
        self.condition_fn = condition_fn
        self.true_value = true_value
        self.false_value = false_value
    
    def __call__(
        self,
        completion: str,
        reference_answer: str,
        **kwargs,
    ) -> float:
        if self.condition_fn(completion, reference_answer):
            return self.true_value
        return self.false_value


class UniversalRuleReward(RewardFunction):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è reward —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –∏–∑ Reward Designer.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - Regex —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
    - –£—Å–ª–æ–≤–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏ (contains, matches, equals, etc.)
    - –õ–æ–≥–∏–∫—É AND/OR –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —É—Å–ª–æ–≤–∏–π
    - Python —Ñ–æ—Ä–º—É–ª—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è reward
    """
    
    def __init__(
        self,
        rules: List[Dict[str, Any]],
        normalize: bool = False,
    ):
        """
        Args:
            rules: –°–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–∏–ª –∏–∑ Reward Designer
            normalize: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–∏ –Ω–∞ —Å—É–º–º—É –≤–µ—Å–æ–≤
        """
        super().__init__(weight=1.0)
        self.rules = rules
        self.normalize = normalize
        self._total_weight = sum(r.get("weight", 1.0) for r in rules if r.get("enabled", True))
    
    def _substitute_vars(
        self,
        text: str,
        response: str,
        reference: str,
        prompt: str,
        extracted: Dict[str, str],
        metadata: Dict[str, Any] = None,
    ) -> str:
        """–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —Å—Ç—Ä–æ–∫—É."""
        if not isinstance(text, str):
            return text
        text = text.replace("{{response}}", response)
        text = text.replace("{{reference}}", reference)
        text = text.replace("{{prompt}}", prompt)
        for k, v in extracted.items():
            text = text.replace(f"{{{{extracted.{k}}}}}", str(v) if v else "")
        # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º metadata –ø–æ–ª—è
        if metadata:
            for k, v in metadata.items():
                text = text.replace(f"{{{{metadata.{k}}}}}", str(v) if v is not None else "")
        return text
    
    def _evaluate_condition(
        self,
        cond: Dict[str, Any],
        response: str,
        reference: str,
        prompt: str,
        extracted: Dict[str, str],
        metadata: Dict[str, Any] = None,
    ) -> bool:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–¥–Ω–æ —É—Å–ª–æ–≤–∏–µ."""
        cond_type = cond.get("type", "contains")
        
        target = self._substitute_vars(
            cond.get("target") or cond.get("left", "{{response}}"),
            response, reference, prompt, extracted, metadata
        )
        
        try:
            if cond_type == "contains":
                value = self._substitute_vars(cond.get("value", ""), response, reference, prompt, extracted, metadata)
                return value in target
            
            elif cond_type == "not_contains":
                value = self._substitute_vars(cond.get("value", ""), response, reference, prompt, extracted, metadata)
                return value not in target
            
            elif cond_type == "matches":
                pattern = cond.get("pattern", "")
                return bool(re.search(pattern, target))
            
            elif cond_type == "not_matches":
                pattern = cond.get("pattern", "")
                return not bool(re.search(pattern, target))
            
            elif cond_type == "equals":
                value = self._substitute_vars(cond.get("value", ""), response, reference, prompt, extracted, metadata)
                return target.strip() == value.strip()
            
            elif cond_type == "equals_numeric":
                right = self._substitute_vars(cond.get("right", ""), response, reference, prompt, extracted, metadata)
                tolerance = float(cond.get("tolerance", 0.01))
                try:
                    left_num = float(re.sub(r"[^\d.\-]", "", str(target)))
                    right_num = float(re.sub(r"[^\d.\-]", "", str(right)))
                    return abs(left_num - right_num) <= tolerance
                except:
                    return False
            
            elif cond_type == "greater":
                value = float(cond.get("value", 0))
                try:
                    num = float(re.sub(r"[^\d.\-]", "", str(target)))
                    return num > value
                except:
                    return False
            
            elif cond_type == "less":
                value = float(cond.get("value", 0))
                try:
                    num = float(re.sub(r"[^\d.\-]", "", str(target)))
                    return num < value
                except:
                    return False
            
            elif cond_type == "length_between":
                length = len(target)
                min_len = int(cond.get("min", 0))
                max_len = int(cond.get("max", 99999))
                return min_len <= length <= max_len
            
            elif cond_type == "length_min":
                return len(target) >= int(cond.get("min", 0))
            
            elif cond_type == "length_max":
                return len(target) <= int(cond.get("max", 99999))
            
        except Exception:
            return False
        
        return False
    
    def _evaluate_formula(
        self,
        formula: str,
        response: str,
        reference: str,
        prompt: str,
        extracted: Dict[str, str],
        metadata: Dict[str, Any] = None,
    ) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ñ–æ—Ä–º—É–ª—É reward."""
        # –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        formula = formula.replace("{{response}}", f"'''{response}'''")
        formula = formula.replace("{{reference}}", f"'''{reference}'''")
        formula = formula.replace("{{prompt}}", f"'''{prompt}'''")
        for k, v in extracted.items():
            safe_v = str(v).replace("'", "\\'") if v else ""
            formula = formula.replace(f"{{{{extracted.{k}}}}}", f"'''{safe_v}'''")
        # –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ metadata –ø–æ–ª–µ–π
        if metadata:
            for k, v in metadata.items():
                safe_v = str(v).replace("'", "\\'") if v is not None else ""
                formula = formula.replace(f"{{{{metadata.{k}}}}}", f"'''{safe_v}'''")
        
        try:
            safe_globals = {
                "__builtins__": {
                    "len": len, "min": min, "max": max, "abs": abs,
                    "float": float, "int": int, "str": str, "bool": bool,
                }
            }
            return float(eval(formula, safe_globals))
        except Exception:
            return 0.0
    
    def __call__(
        self,
        completion: str,
        reference_answer: str,
        prompt: str = "",
        **kwargs,
    ) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç reward –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª.
        
        Args:
            completion: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
            reference_answer: –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            prompt: –ü—Ä–æ–º–ø—Ç
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≤–∫–ª—é—á–∞—è metadata –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        """
        total_reward = 0.0
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º metadata –∏–∑ kwargs (–ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∏–∑ RLSample)
        metadata = kwargs.get("metadata", {})
        
        for rule in self.rules:
            if not rule.get("enabled", True):
                continue
            
            weight = rule.get("weight", 1.0)
            extracted = {}
            
            # 1. –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã
            for ext in rule.get("extractors", []):
                source = ext.get("source", "{{response}}")
                source_text = self._substitute_vars(source, completion, reference_answer, prompt, {}, metadata)
                
                pattern = ext.get("pattern", "")
                flags = 0
                if "DOTALL" in ext.get("flags", ""):
                    flags |= re.DOTALL
                if "IGNORECASE" in ext.get("flags", ""):
                    flags |= re.IGNORECASE
                
                try:
                    match = re.search(pattern, source_text, flags)
                    if match:
                        if match.groups():
                            extracted[ext["name"]] = match.group(1)
                        else:
                            extracted[ext["name"]] = match.group(0)
                    else:
                        extracted[ext["name"]] = ""
                except re.error:
                    extracted[ext["name"]] = ""
            
            # 2. –£—Å–ª–æ–≤–∏—è
            conditions = rule.get("conditions", [])
            logic = rule.get("condition_logic", "all")
            
            if not conditions:
                all_conditions_true = True
            else:
                results = [
                    self._evaluate_condition(c, completion, reference_answer, prompt, extracted, metadata)
                    for c in conditions
                ]
                if logic == "all":
                    all_conditions_true = all(results)
                else:
                    all_conditions_true = any(results)
            
            # 3. –§–æ—Ä–º—É–ª–∞ reward
            if all_conditions_true:
                formula = rule.get("reward_formula", "1.0")
            else:
                formula = rule.get("else_reward", "0.0")
            
            rule_reward = self._evaluate_formula(formula, completion, reference_answer, prompt, extracted, metadata)
            total_reward += weight * rule_reward
        
        if self.normalize and self._total_weight > 0:
            return total_reward / self._total_weight
        return total_reward
    
    @classmethod
    def from_config(cls, reward_rules: List[Dict[str, Any]]) -> "UniversalRuleReward":
        """
        –°–æ–∑–¥–∞—ë—Ç UniversalRuleReward –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Reward Designer.
        """
        return cls(rules=reward_rules, normalize=False)
