"""
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è GRPO/RL –æ–±—É—á–µ–Ω–∏—è.
"""
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional, Literal
from pathlib import Path


class RLAlgorithm(str, Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã RL."""
    GRPO = "grpo"           # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ std
    DRGRPO = "drgrpo"       # Dr.GRPO - –±–µ–∑ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ std, —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    DAPO = "dapo"           # DAPO - clip higher + dynamic sampling + token-level loss


@dataclass
class GRPOConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GRPO/RL.
    
    –°–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤:
    - DeepSeek-R1 (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π GRPO)
    - Dr.GRPO (Understanding R1-Zero)
    - DAPO (GRPO++)
    
    Attributes:
        algorithm: –ê–ª–≥–æ—Ä–∏—Ç–º RL (grpo, drgrpo, dapo)
        
        # –†–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        group_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç (G –≤ —Å—Ç–∞—Ç—å—è—Ö)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω —à–∞–≥
        train_batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ—Å–ª–µ —Å–±–æ—Ä–∞ –æ–ø—ã—Ç–∞)
        gradient_accumulation_steps: –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        top_p: Top-p –¥–ª—è nucleus sampling
        
        # –ö–ª–∏–ø–ø–∏–Ω–≥ (PPO-style)
        clip_eps_low: –ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –∫–ª–∏–ø–ø–∏–Ω–≥–∞ (default: 0.2)
        clip_eps_high: –í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –∫–ª–∏–ø–ø–∏–Ω–≥–∞ (DAPO —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç 0.28)
        
        # KL —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        kl_weight: –í–µ—Å KL-—à—Ç—Ä–∞—Ñ–∞ (–≤ reasoning-RL –æ–±—ã—á–Ω–æ 0 –∏–ª–∏ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        weight_decay: Weight decay –¥–ª—è AdamW
        max_grad_norm: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –∫–ª–∏–ø–ø–∏–Ω–≥–∞
        
        # –û–±—É—á–µ–Ω–∏–µ
        num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö RL –æ–±—É—á–µ–Ω–∏—è
        epochs_per_step: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ–¥–∏–Ω –±–∞—Ç—á —Ä–æ–ª–ª–∞—É—Ç–æ–≤
        warmup_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ warmup
        
        # Dr.GRPO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        use_std_normalization: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ std (True –¥–ª—è GRPO, False –¥–ª—è DrGRPO)
        fixed_length_normalizer: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ–ª–∏—Ç–µ–ª—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ loss (DrGRPO)
        
        # DAPO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        dynamic_sampling: –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å zero-gradient –≥—Ä—É–ø–ø—ã
        token_level_loss: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å token-level –∞–≥—Ä–µ–≥–∞—Ü–∏—é (vs sample-level)
        overlong_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
        overlong_buffer: –ë—É—Ñ–µ—Ä–Ω–∞—è –∑–æ–Ω–∞ –¥–ª—è —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –¥–ª–∏–Ω—É
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        save_steps: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª—å –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
        log_steps: –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
        use_wandb: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Weights & Biases
        wandb_project: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ W&B
        
        # –°–∏—Å—Ç–µ–º–∞
        mixed_precision: Mixed precision —Ä–µ–∂–∏–º ("no" | "fp16" | "bf16"). –î–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å UI.
        use_lora: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
        lora_r: –†–∞–Ω–≥ LoRA
        lora_alpha: Alpha –¥–ª—è LoRA
        use_4bit: 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (bitsandbytes)
        use_8bit: 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
        use_flash_attention: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Flash Attention 2
    """
    
    # –ê–ª–≥–æ—Ä–∏—Ç–º (DAPO —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    algorithm: RLAlgorithm = RLAlgorithm.DAPO
    
    # –†–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–∞
    group_size: int = 8
    batch_size: int = 8  # –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ —à–∞–≥ —Ä–æ–ª–ª–∞—É—Ç–∞
    train_batch_size: int = 2  # –¥–ª—è –æ–±—É—á–µ–Ω–∏—è - —É–º–µ–Ω—å—à–µ–Ω–æ –¥–æ 2 —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å OOM –ø—Ä–∏ 1200 —Ç–æ–∫–µ–Ω–∞—Ö
    gradient_accumulation_steps: int = 4
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    max_new_tokens: int = 1024
    max_prompt_length: int = 512
    temperature: float = 0.7
    top_p: float = 0.95
    
    # –ö–ª–∏–ø–ø–∏–Ω–≥
    clip_eps_low: float = 0.2
    clip_eps_high: float = 0.2  # –î–ª—è DAPO: 0.28
    
    # KL —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    kl_weight: float = 0.0  # –î–ª—è reasoning-RL –æ–±—ã—á–Ω–æ 0
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    learning_rate: float = 5e-6
    # –ù–∏–∂–Ω–∏–π –ø—Ä–µ–¥–µ–ª LR: lr = base_lr * min_lr_ratio –≤ –∫–æ–Ω—Ü–µ cosine (0.0 = –¥–æ –Ω—É–ª—è)
    min_lr_ratio: float = 0.1
    weight_decay: float = 0.01
    max_grad_norm: float = 1.0
    
    # –û–±—É—á–µ–Ω–∏–µ
    num_epochs: int = 1
    epochs_per_step: int = 1
    warmup_steps: int = 100
    max_steps: Optional[int] = None
    # –õ–∏–º–∏—Ç –ø–æ –¥–∞–Ω–Ω—ã–º (—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ–≥–æ). –ï—Å–ª–∏ None ‚Äî –∏–¥—ë–º –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É (—Å —É—á—ë—Ç–æ–º num_epochs).
    # –≠—Ç–æ –±–ª–∏–∂–µ –∫ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –æ–∂–∏–¥–∞–Ω–∏—é "—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–æ—à–ª–æ".
    max_prompts: Optional[int] = None
    
    # Dr.GRPO
    use_std_normalization: bool = True  # False –¥–ª—è DrGRPO
    fixed_length_normalizer: Optional[int] = None  # max_new_tokens –¥–ª—è DrGRPO
    
    # DAPO
    dynamic_sampling: bool = False  # True –¥–ª—è DAPO
    max_refill_rounds: int = 3  # –ú–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫ –¥–æ–±–æ—Ä–∞ –≥—Ä—É–ø–ø (0 = –±–µ–∑ –¥–æ–±–æ—Ä–∞, 8 = –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)
    token_level_loss: bool = False  # True –¥–ª—è DAPO
    overlong_penalty: float = -1.0
    overlong_buffer: int = 0  # 4000 –≤ DAPO –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
    
    # ============================================================
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ì–ï–ù–ï–†–ê–¶–ò–ò
    # ============================================================
    
    # Prefix Grouper: shared KV-cache –¥–ª—è G completions –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
    # –î–∞—ë—Ç 2-3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å ZeRO-3)
    use_prefix_grouper: bool = True
    
    # Multi-prompt batching: —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
    # 1 = –æ—Ç–∫–ª—é—á–µ–Ω–æ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é). 2-4 = —Ö–æ—Ä–æ—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è GPU —Å –∑–∞–ø–∞—Å–æ–º –ø–∞–º—è—Ç–∏.
    # –ù–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å Prefix Grouper (–≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ).
    rollout_batch_size: int = 1
    
    # ds3_gather_for_generation: —Å–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ZeRO-3 –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
    # –î–∞—ë—Ç 10-100x —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–ª—è ZeRO-3 (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∫–ª—é—á–∞–µ—Ç—Å—è –ø—Ä–∏ ZeRO-3)
    ds3_gather_for_generation: bool = True

    # ============================================================
    # ROLLOUT ENGINE (–æ—Ç–¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
    # ============================================================
    # –ï—Å–ª–∏ True ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (rollout) –±—É–¥–µ—Ç –¥–µ–ª–∞—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é (HF / vLLM),
    # –∞ training-–º–æ–¥–µ–ª—å (DDP/ZeRO/FSDP) –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è teacher-forcing logprobs + backprop.
    use_rollout_engine: bool = False

    # Backend –¥–ª—è rollout engine:
    # - "hf": –æ—Ç–¥–µ–ª—å–Ω–∞—è HF-–º–æ–¥–µ–ª—å (–±—ã—Å—Ç—Ä–æ –¥–ª—è DDP, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å ZeRO-3 —á–µ—Ä–µ–∑ —Ä–µ–¥–∫–∏–π sync –≤–µ—Å–æ–≤)
    # - "vllm": (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ) vLLM rollout (–ª—É—á—à–∏–π throughput, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏)
    rollout_engine_backend: Literal["hf", "vllm"] = "hf"

    # –ö–∞–∫ —á–∞—Å—Ç–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ training->rollout (–≤ –µ–¥–∏–Ω–∏—Ü–∞—Ö rollout_step).
    # 1 = –∫–∞–∂–¥—ã–π rollout-step (—Å–∞–º–æ–µ on-policy), 2-10 = –±—ã—Å—Ç—Ä–µ–µ (–º–µ–Ω—å—à–µ overhead), –Ω–æ —Å–ª–µ–≥–∫–∞ "stale".
    rollout_sync_interval: int = 1

    # –ï—Å–ª–∏ True ‚Äî —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä LoRA), –∞ –Ω–µ –≤–µ—Å—å base model.
    # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è ZeRO-3/FSDP, —Ç.–∫. –ø–æ–ª–Ω—ã–π state_dict –¥–æ—Ä–æ–≥–æ–π.
    rollout_sync_trainable_only: bool = True

    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é: –º–æ–∂–Ω–æ –¥–µ—Ä–∂–∞—Ç—å rollout –º–æ–¥–µ–ª—å –Ω–∞ CPU –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏—è–º–∏, —á—Ç–æ–±—ã –æ—Å–≤–æ–±–æ–∂–¥–∞—Ç—å VRAM.
    rollout_offload_to_cpu: bool = False

    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è rollout-–º–æ–¥–µ–ª–∏:
    # - "auto": local accelerator.device (cuda:local_rank)
    # - "cpu": –≤—Å–µ–≥–¥–∞ CPU (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ)
    rollout_device: str = "auto"
    
    # vLLM GPU memory utilization (0.0-1.0)
    # –°–∫–æ–ª—å–∫–æ % –ø–∞–º—è—Ç–∏ GPU –≤—ã–¥–µ–ª–∏—Ç—å –ø–æ–¥ vLLM (KV-cache –∏ –º–æ–¥–µ–ª—å)
    vllm_gpu_memory_utilization: float = 0.85
    
    # vLLM device: –Ω–∞ –∫–∞–∫–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –∑–∞–ø—É—Å–∫–∞—Ç—å vLLM
    # - "cuda:0", "cuda:1", ... ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è GPU
    # - "cpu" ‚Äî –Ω–∞ CPU (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç GPU –¥–ª—è training)
    # vLLM –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¢–û–õ–¨–ö–û –Ω–∞ main process (rank 0), —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã broadcast'—è—Ç—Å—è
    vllm_device: str = "cuda:0"
    
    # ============================================================
    # LIGER KERNEL OPTIMIZATIONS
    # ============================================================
    # Liger Kernel ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Triton kernels –¥–ª—è LLM —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    # –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å (–Ω–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits) –∏ —É—Å–∫–æ—Ä—è–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
    # https://github.com/linkedin/Liger-Kernel
    
    # –í–∫–ª—é—á–∏—Ç—å Liger –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (CrossEntropy, RMSNorm, MLP –∏ —Ç.–¥.)
    use_liger: bool = True
    
    # –ü–∞—Ç—á–∏—Ç—å –º–æ–¥–µ–ª—å Liger kernels (RMSNorm -> LigerRMSNorm, MLP -> LigerSwiGLU –∏ —Ç.–¥.)
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: Qwen2, Llama, Mistral, Gemma, Phi3
    liger_patch_model: bool = True
    
    # –†–∞–∑–º–µ—Ä chunk'–∞ –¥–ª—è chunked cross-entropy (—É–º–µ–Ω—å—à–∏—Ç—å –µ—Å–ª–∏ OOM)
    liger_chunk_size: int = 4096
    
    # üî• LigerFusedLinearGRPOLoss ‚Äî –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits (—ç–∫–æ–Ω–æ–º–∏—è –¥–æ 80% –ø–∞–º—è—Ç–∏!)
    # Fused: hidden_states -> lm_head -> GRPO loss –≤ –æ–¥–Ω–æ–º kernel
    # –í–ê–ñ–ù–û: —Ç—Ä–µ–±—É–µ—Ç output_hidden_states=True –≤ forward pass
    liger_fused_grpo: bool = True
    
    # –¢–∏–ø loss –¥–ª—è Liger GRPO (–µ—Å–ª–∏ liger_fused_grpo=True):
    # - "grpo": —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO (sample-level –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
    # - "dapo": DAPO —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    # - "dr_grpo": DrGRPO —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
    # - "bnpo": Batch Normalized Per-token loss
    liger_grpo_loss_type: str = "dapo"  # DAPO —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–∞–∫ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    output_dir: str = "./output/grpo"
    save_steps: int = 100
    log_steps: int = 10
    # –ï—Å–ª–∏ True ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º `final_model/` –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –±—ã–ª–æ —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å.
    export_on_checkpoint: bool = True
    use_wandb: bool = False
    wandb_project: str = "homellm-grpo"
    
    # LoRA
    use_lora: bool = True
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ])
    # –ï—Å–ª–∏ lora_target_modules –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω "all-linear" (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
    
    # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
    use_4bit: bool = False
    use_8bit: bool = False
    quantize_reference_model: bool = False  # –ö–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–∏ reference –º–æ–¥–µ–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ KL)
    use_flash_attention: bool = True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True; flash-attn –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω (–≤ Docker —Å—Ç–∞–≤–∏—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)
    
    # –§–æ—Ä–º–∞—Ç reasoning
    reasoning_format: str = "deepseek"  # "deepseek" (<think>), "simple" (<reasoning>)
    
    # Seed
    seed: int = 42

    # Precision (–¥–æ–ª–∂–Ω–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –∏–∑ UI/distributed_config)
    mixed_precision: str = "bf16"  # "no" | "fp16" | "bf16"
    # –ï—Å–ª–∏ True –∏ mixed_precision="fp16": "pure fp16" (–≤–µ—Å–∞ fp16, –±–µ–∑ GradScaler).
    # –≠—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç VRAM (—Å—Ä–∞–≤–Ω–∏–º–æ —Å bf16), –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º, —á–µ–º AMP fp16.
    fp16_pure: bool = False
    
    # Memory
    grad_checkpoint: bool = False

    # UI/monitoring plumbing (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    # –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –∏–∑ Streamlit, UI —Å–æ–∑–¥–∞—ë—Ç run_dir (RUNS_DIR/<run_id>).
    # –ß—Ç–æ–±—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–ª "–∂–µ–ª–µ–∑–Ω–æ" –¥–∞–∂–µ –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–∏—è—Ö –ø—É—Ç–µ–π (/app vs host),
    # –º–æ–∂–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å metrics.jsonl/samples.jsonl –≤ —ç—Ç—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.
    ui_run_dir: Optional[str] = None
    
    def __post_init__(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞."""
        if isinstance(self.algorithm, str):
            self.algorithm = RLAlgorithm(self.algorithm)
        
        if self.algorithm == RLAlgorithm.DRGRPO:
            # Dr.GRPO: –±–µ–∑ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ std, —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            self.use_std_normalization = False
            if self.fixed_length_normalizer is None:
                self.fixed_length_normalizer = self.max_new_tokens
                
        elif self.algorithm == RLAlgorithm.DAPO:
            # DAPO: clip higher, dynamic sampling, token-level loss
            self.clip_eps_high = max(self.clip_eps_high, 0.28)
            self.dynamic_sampling = True
            self.token_level_loss = True
            self.use_std_normalization = False
            if self.fixed_length_normalizer is None:
                self.fixed_length_normalizer = self.max_new_tokens
    
    @classmethod
    def from_preset(cls, preset: str) -> "GRPOConfig":
        """
        –°–æ–∑–¥–∞—ë—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫–∏.
        
        Args:
            preset: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫–∏:
                - "grpo": –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO
                - "drgrpo": Dr.GRPO —Å —Ñ–∏–∫—Å–∞–º–∏
                - "dapo": –ü–æ–ª–Ω—ã–π DAPO —Å clip higher –∏ dynamic sampling
                - "reasoning_small": –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π (0.5-3B)
                - "reasoning_large": –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (7B+)
        """
        presets = {
            "grpo": cls(
                algorithm=RLAlgorithm.GRPO,
                use_std_normalization=True,
                clip_eps_high=0.2,
            ),
            "drgrpo": cls(
                algorithm=RLAlgorithm.DRGRPO,
                use_std_normalization=False,
                clip_eps_high=0.2,
            ),
            "dapo": cls(
                algorithm=RLAlgorithm.DAPO,
                use_std_normalization=False,
                clip_eps_high=0.28,
                dynamic_sampling=True,
                token_level_loss=True,
            ),
            "reasoning_small": cls(
                algorithm=RLAlgorithm.DRGRPO,
                group_size=8,
                batch_size=4,
                train_batch_size=2,
                max_new_tokens=512,
                learning_rate=5e-6,
                use_lora=True,
                use_4bit=True,
            ),
            "reasoning_large": cls(
                algorithm=RLAlgorithm.DAPO,
                group_size=16,
                batch_size=8,
                train_batch_size=4,
                max_new_tokens=2048,
                learning_rate=1e-6,
                use_lora=True,
                use_4bit=True,
                dynamic_sampling=True,
            ),
        }
        
        if preset not in presets:
            raise ValueError(f"Unknown preset: {preset}. Available: {list(presets.keys())}")
        
        return presets[preset]
    
    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        return {
            "algorithm": self.algorithm.value,
            "group_size": self.group_size,
            "batch_size": self.batch_size,
            "train_batch_size": self.train_batch_size,
            "max_new_tokens": self.max_new_tokens,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "clip_eps_low": self.clip_eps_low,
            "clip_eps_high": self.clip_eps_high,
            "kl_weight": self.kl_weight,
            "learning_rate": self.learning_rate,
            "min_lr_ratio": self.min_lr_ratio,
            "max_steps": self.max_steps,
            "max_prompts": self.max_prompts,
            "mixed_precision": self.mixed_precision,
            "grad_checkpoint": self.grad_checkpoint,
            "save_steps": self.save_steps,
            "log_steps": self.log_steps,
            "export_on_checkpoint": self.export_on_checkpoint,
            "use_std_normalization": self.use_std_normalization,
            "fixed_length_normalizer": self.fixed_length_normalizer,
            "dynamic_sampling": self.dynamic_sampling,
            "token_level_loss": self.token_level_loss,
            "use_lora": self.use_lora,
            "use_4bit": self.use_4bit,
        }
