#!/usr/bin/env python3
"""
Standalone vLLM worker script.
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å CUDA_VISIBLE_DEVICES —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –î–û —Å—Ç–∞—Ä—Ç–∞ Python.
–ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ stdin/stdout (JSON lines).

–í–ê–ñ–ù–û: stdout –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è JSON –ø—Ä–æ—Ç–æ–∫–æ–ª–∞!
–í—Å–µ –ª–æ–≥–∏ –∏–¥—É—Ç –≤ stderr.
"""
import json
import sys
import os

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π stdout –¥–ª—è JSON –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
_json_out = sys.stdout

# –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º stdout –≤ stderr —á—Ç–æ–±—ã vLLM –ª–æ–≥–∏ –Ω–µ –ø–æ–ø–∞–¥–∞–ª–∏ –≤ JSON –∫–∞–Ω–∞–ª
sys.stdout = sys.stderr


def send_json(data: dict) -> None:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç JSON –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π stdout (–ø—Ä–æ—Ç–æ–∫–æ–ª)."""
    _json_out.write(json.dumps(data) + "\n")
    _json_out.flush()


def main():
    # –õ–æ–≥–∏—Ä—É–µ–º environment
    gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "not set")
    print(f"üß© vLLM Worker: CUDA_VISIBLE_DEVICES={gpu_id}", flush=True)
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º torch –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º GPU
    import torch
    print(f"üß© vLLM Worker: torch.cuda.device_count()={torch.cuda.device_count()}", flush=True)
    if torch.cuda.is_available():
        print(f"üß© vLLM Worker: cuda:0 = {torch.cuda.get_device_name(0)}", flush=True)
        free_mem, total_mem = torch.cuda.mem_get_info(0)
        print(f"üß© vLLM Worker: GPU memory: {free_mem/1e9:.1f}GB free / {total_mem/1e9:.1f}GB total", flush=True)
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º vLLM
    from vllm import LLM, SamplingParams
    try:
        from vllm.lora.request import LoRARequest
    except ImportError:
        from vllm.lora import LoRARequest
    
    print(f"üß© vLLM Worker: vLLM imported", flush=True)
    
    # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏ stdin
    # stdin –æ—Å—Ç–∞–ª—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º, —á–∏—Ç–∞–µ–º –∏–∑ –Ω–µ–≥–æ
    import sys as _sys
    config_line = _sys.__stdin__.readline()
    config = json.loads(config_line)
    
    model_path = config["model_path"]
    dtype_str = config["dtype"]
    max_model_len = config["max_model_len"]
    gpu_memory_utilization = config["gpu_memory_utilization"]
    enable_lora = config["enable_lora"]
    max_lora_rank = config.get("max_lora_rank", 64)  # –î–µ—Ñ–æ–ª—Ç 64, —á—Ç–æ–±—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å rank –¥–æ 64
    
    print(f"üß© vLLM Worker: loading model {model_path}", flush=True)
    print(f"üß© vLLM Worker: enable_lora={enable_lora}, max_lora_rank={max_lora_rank}", flush=True)
    
    llm_kwargs = {
        "model": model_path,
        "trust_remote_code": True,
        "dtype": dtype_str,
        "tensor_parallel_size": 1,
        "max_model_len": max_model_len,
        "gpu_memory_utilization": gpu_memory_utilization,
        "enforce_eager": True,  # –û—Ç–∫–ª—é—á–∞–µ–º CUDA graphs
    }
    if enable_lora:
        llm_kwargs["enable_lora"] = True
        llm_kwargs["max_loras"] = 1
        llm_kwargs["max_lora_rank"] = max_lora_rank  # –ö–†–ò–¢–ò–ß–ù–û! –î–æ–ª–∂–µ–Ω –±—ã—Ç—å >= lora_r
    
    try:
        llm = LLM(**llm_kwargs)
        print(f"üß© vLLM Worker: model loaded!", flush=True)
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º ready signal
        send_json({"status": "ready"})
    except Exception as e:
        print(f"üß© vLLM Worker: failed to load: {e}", flush=True)
        send_json({"status": "error", "error": str(e)})
        _sys.exit(1)
    
    current_lora_request = None
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    for line in _sys.__stdin__:
        try:
            request = json.loads(line.strip())
            cmd = request.get("cmd")
            
            if cmd == "shutdown":
                print(f"üß© vLLM Worker: shutting down", flush=True)
                break
            
            elif cmd == "set_lora":
                lora_path = request.get("lora_path")
                lora_name = request.get("lora_name", "rollout_lora")
                lora_int_id = request.get("lora_int_id", 1)
                
                if lora_path:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∞–¥–∞–ø—Ç–µ—Ä —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                    adapter_config_path = os.path.join(lora_path, "adapter_config.json")
                    adapter_model_path = os.path.join(lora_path, "adapter_model.safetensors")
                    adapter_model_bin_path = os.path.join(lora_path, "adapter_model.bin")
                    
                    if not os.path.exists(adapter_config_path):
                        print(f"üß© vLLM Worker: ERROR - adapter_config.json not found at {lora_path}", flush=True)
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –µ—Å—Ç—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                        if os.path.isdir(lora_path):
                            files = os.listdir(lora_path)
                            print(f"üß© vLLM Worker: Files in {lora_path}: {files}", flush=True)
                        send_json({"status": "error", "error": f"adapter_config.json not found at {lora_path}"})
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤
                    has_weights = os.path.exists(adapter_model_path) or os.path.exists(adapter_model_bin_path)
                    if not has_weights:
                        print(f"üß© vLLM Worker: WARNING - no adapter weights found (safetensors or bin)", flush=True)
                    
                    # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    try:
                        with open(adapter_config_path, 'r') as f:
                            adapter_cfg = json.load(f)
                        print(f"üß© vLLM Worker: adapter_config: r={adapter_cfg.get('r')}, alpha={adapter_cfg.get('lora_alpha')}, modules={adapter_cfg.get('target_modules', [])[:3]}...", flush=True)
                    except Exception as e:
                        print(f"üß© vLLM Worker: couldn't read adapter_config: {e}", flush=True)
                    
                    # –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π lora_int_id —á—Ç–æ–±—ã vLLM –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏–ª –∞–¥–∞–ø—Ç–µ—Ä
                    current_lora_request = LoRARequest(str(lora_name), int(lora_int_id), str(lora_path))
                    print(f"üß© vLLM Worker: LoRA set to {lora_path} (name={lora_name}, id={lora_int_id})", flush=True)
                else:
                    current_lora_request = None
                    print(f"üß© vLLM Worker: LoRA disabled", flush=True)
                send_json({"status": "ok"})
            
            elif cmd == "generate":
                prompts = request.get("prompts", [])
                sampling_params_dict = request.get("sampling_params", {})
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º None
                filtered_params = {k: v for k, v in sampling_params_dict.items() if v is not None}
                print(f"üß© vLLM Worker: generating {len(prompts)} prompts", flush=True)
                
                sampling_params = SamplingParams(**filtered_params)
                outputs = llm.generate(prompts, sampling_params, lora_request=current_lora_request)
                
                # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results = []
                for output in outputs:
                    result = {
                        "prompt": output.prompt,
                        "outputs": [
                            {
                                "text": o.text,
                                "token_ids": list(o.token_ids) if o.token_ids else [],
                                "finish_reason": str(o.finish_reason) if o.finish_reason else None,
                            }
                            for o in output.outputs
                        ]
                    }
                    results.append(result)
                
                print(f"üß© vLLM Worker: generated {len(results)} outputs", flush=True)
                send_json({"status": "ok", "outputs": results})
            
            else:
                send_json({"status": "error", "error": f"unknown cmd: {cmd}"})
        
        except Exception as e:
            print(f"üß© vLLM Worker: error: {e}", flush=True)
            send_json({"status": "error", "error": str(e)})


if __name__ == "__main__":
    main()
