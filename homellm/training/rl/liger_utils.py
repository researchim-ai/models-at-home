"""
Liger Kernel –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è GRPO —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.

Liger Kernel ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö Triton kernels –¥–ª—è LLM —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:

üî• CHUNKED LOSS (—ç–∫–æ–Ω–æ–º–∏—è –¥–æ 80% –ø–∞–º—è—Ç–∏):
- LigerFusedLinearGRPOLoss: GRPO –±–µ–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ logits
- LigerFusedLinearCrossEntropyLoss: CE –±–µ–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ logits (pretrain/SFT)
- LigerFusedLinearDPOLoss, LigerFusedLinearCPOLoss, etc.

‚ö° LOW-LEVEL OPS:
- LigerCrossEntropyLoss: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π cross-entropy
- LigerRMSNorm: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RMSNorm
- LigerKLDIVLoss: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π KL divergence
- –ü–∞—Ç—á–∏–Ω–≥ HF –º–æ–¥–µ–ª–µ–π (Qwen, Llama, Mistral –∏ –¥—Ä.)

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://github.com/linkedin/Liger-Kernel
"""

from __future__ import annotations

import logging
from typing import Optional, Callable, TYPE_CHECKING, Tuple, Any

import torch
import torch.nn as nn
import torch.nn.functional as F

if TYPE_CHECKING:
    from transformers import PreTrainedModel

logger = logging.getLogger(__name__)

# ============================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ö–≠–®–ò
# ============================================================
_LIGER_AVAILABLE: Optional[bool] = None
_LIGER_CE_LOSS: Optional[Callable] = None
_LIGER_FUSED_LINEAR_CE: Optional[type] = None
_LIGER_FUSED_LINEAR_GRPO: Optional[type] = None
_LIGER_KL_DIV: Optional[type] = None

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ç—á–∏–Ω–≥–∞
LIGER_SUPPORTED_MODELS = {"qwen2", "qwen", "llama", "mistral", "gemma", "gemma2", "phi", "phi3", "mixtral"}


def is_liger_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Liger Kernel."""
    global _LIGER_AVAILABLE
    if _LIGER_AVAILABLE is not None:
        return _LIGER_AVAILABLE
    
    try:
        import liger_kernel
        _LIGER_AVAILABLE = True
        logger.info(f"‚úÖ Liger Kernel –¥–æ—Å—Ç—É–ø–µ–Ω: v{getattr(liger_kernel, '__version__', 'unknown')}")
    except ImportError:
        _LIGER_AVAILABLE = False
        logger.warning("‚ö†Ô∏è Liger Kernel –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install liger-kernel")
    
    return _LIGER_AVAILABLE


def get_liger_cross_entropy_loss() -> Optional[Callable]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç LigerCrossEntropyLoss –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    - –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞ GPU
    - –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ —á–µ–º F.cross_entropy
    """
    global _LIGER_CE_LOSS
    
    if _LIGER_CE_LOSS is not None:
        return _LIGER_CE_LOSS
    
    if not is_liger_available():
        return None
    
    try:
        from liger_kernel.ops.cross_entropy import LigerCrossEntropyFunction
        
        def liger_ce_loss(
            logits: torch.Tensor,
            targets: torch.Tensor,
            ignore_index: int = -100,
            reduction: str = "none",
        ) -> torch.Tensor:
            """
            Wrapper –¥–ª—è LigerCrossEntropyFunction.
            
            Args:
                logits: [batch, vocab] –∏–ª–∏ [batch*seq, vocab]
                targets: [batch] –∏–ª–∏ [batch*seq]
                ignore_index: –∏–Ω–¥–µ–∫—Å –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
                reduction: "none", "mean", "sum"
            """
            # Liger CE –æ–∂–∏–¥–∞–µ—Ç 2D input
            if logits.dim() == 3:
                # [batch, seq, vocab] -> [batch*seq, vocab]
                logits = logits.reshape(-1, logits.size(-1))
                targets = targets.reshape(-1)
            
            # LigerCrossEntropyFunction.apply signature:
            # (_input, target, weight, ignore_index, lse_square_scale, 
            #  label_smoothing, reduction, softcap, return_z_loss, return_token_accuracy)
            loss, z_loss, token_accuracy = LigerCrossEntropyFunction.apply(
                logits.contiguous(),  # _input: [BT, V]
                targets.contiguous(), # target: [BT]
                None,                 # weight: Optional[Tensor] ‚Äî –ù–ï ignore_index!
                ignore_index,         # ignore_index: int
                0.0,                  # lse_square_scale: float
                0.0,                  # label_smoothing: float
                reduction,            # reduction: str ("none", "mean", "sum")
                None,                 # softcap: Optional[float]
                False,                # return_z_loss: bool
                False,                # return_token_accuracy: bool
            )
            
            return loss
        
        _LIGER_CE_LOSS = liger_ce_loss
        logger.info("‚úÖ LigerCrossEntropyLoss –∑–∞–≥—Ä—É–∂–µ–Ω")
        return _LIGER_CE_LOSS
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LigerCrossEntropyLoss: {e}")
        return None


def get_liger_fused_linear_ce() -> Optional[type]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç LigerFusedLinearCrossEntropyLoss –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    –ì–õ–ê–í–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –¥–ª—è LLM (pretrain/SFT):
    - –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π logits —Ç–µ–Ω–∑–æ—Ä [batch, seq, vocab]
    - –î–ª—è vocab=150k —ç—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –≥–∏–≥–∞–±–∞–π—Ç—ã –ø–∞–º—è—Ç–∏!
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        loss_fn = LigerFusedLinearCrossEntropyLoss()
        loss = loss_fn(lm_head.weight, hidden_states, targets, lm_head.bias)
    """
    global _LIGER_FUSED_LINEAR_CE
    
    if _LIGER_FUSED_LINEAR_CE is not None:
        return _LIGER_FUSED_LINEAR_CE
    
    if not is_liger_available():
        return None
    
    try:
        from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss
        _LIGER_FUSED_LINEAR_CE = LigerFusedLinearCrossEntropyLoss
        logger.info("‚úÖ LigerFusedLinearCrossEntropyLoss –∑–∞–≥—Ä—É–∂–µ–Ω")
        return _LIGER_FUSED_LINEAR_CE
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LigerFusedLinearCrossEntropyLoss: {e}")
        return None


def get_liger_fused_linear_grpo() -> Optional[type]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç LigerFusedLinearGRPOLoss –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    üî• –ì–õ–ê–í–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –¥–ª—è GRPO:
    - Fused lm_head + GRPO loss computation
    - –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π logits —Ç–µ–Ω–∑–æ—Ä [batch, seq, vocab]
    - –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞: grpo, dapo, dr_grpo, bnpo loss types
    - –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π KL penalty
    - –î–æ 80% —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏!
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        loss_fn = LigerFusedLinearGRPOLoss(
            beta=0.04,  # KL penalty weight
            loss_type="grpo",  # –∏–ª–∏ "dapo", "dr_grpo", "bnpo"
            epsilon_low=0.2,
            epsilon_high=0.2,
        )
        loss, metrics = loss_fn(
            hidden_states,  # [batch*seq, hidden]
            lm_head.weight,  # [vocab, hidden]
            selected_token_ids,  # [batch, seq]
            attention_mask,  # [batch, seq]
            advantages,  # [batch]
            bias=lm_head.bias,
            ref_per_token_logps=ref_logprobs,  # [batch, seq] (optional)
            old_per_token_logps=old_logprobs,  # [batch, seq] (optional)
        )
    """
    global _LIGER_FUSED_LINEAR_GRPO
    
    if _LIGER_FUSED_LINEAR_GRPO is not None:
        return _LIGER_FUSED_LINEAR_GRPO
    
    if not is_liger_available():
        return None
    
    try:
        from liger_kernel.chunked_loss import LigerFusedLinearGRPOLoss
        _LIGER_FUSED_LINEAR_GRPO = LigerFusedLinearGRPOLoss
        logger.info("‚úÖ LigerFusedLinearGRPOLoss –∑–∞–≥—Ä—É–∂–µ–Ω (fused GRPO –±–µ–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ logits)")
        return _LIGER_FUSED_LINEAR_GRPO
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LigerFusedLinearGRPOLoss: {e}")
        return None


def get_liger_kl_div() -> Optional[type]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç LigerKLDIVLoss –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π KL divergence –¥–ª—è KL penalty –≤ RL.
    """
    global _LIGER_KL_DIV
    
    if _LIGER_KL_DIV is not None:
        return _LIGER_KL_DIV
    
    if not is_liger_available():
        return None
    
    try:
        from liger_kernel.transformers import LigerKLDIVLoss
        _LIGER_KL_DIV = LigerKLDIVLoss
        logger.info("‚úÖ LigerKLDIVLoss –∑–∞–≥—Ä—É–∂–µ–Ω")
        return _LIGER_KL_DIV
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LigerKLDIVLoss: {e}")
        return None


def apply_liger_patch_to_model(
    model: "PreTrainedModel",
    patch_rms_norm: bool = True,
    patch_rope: bool = True,
    patch_mlp: bool = True,
    patch_fused_linear_ce: bool = False,  # –î–ª—è GRPO –ª—É—á—à–µ –æ—Ç–∫–ª—é—á–∏—Ç—å (–º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π loss)
) -> bool:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç Liger –ø–∞—Ç—á–∏ –∫ HuggingFace –º–æ–¥–µ–ª–∏.
    
    –í–ê–ñ–ù–û –¥–ª—è Liger 0.6.x:
    - cross_entropy –∏ fused_linear_cross_entropy –Ω–µ–ª—å–∑—è –≤–∫–ª—é—á–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    - –î–ª—è GRPO –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π loss, –ø–æ—ç—Ç–æ–º—É –ø–∞—Ç—á–∏–º —Ç–æ–ª—å–∫–æ RMSNorm/RoPE/MLP
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
    - Qwen2
    - Llama / Llama2 / Llama3
    - Mistral
    - Gemma / Gemma2
    - Phi3
    
    Args:
        model: HuggingFace –º–æ–¥–µ–ª—å
        patch_rms_norm: –ø–∞—Ç—á–∏—Ç—å RMSNorm –Ω–∞ LigerRMSNorm
        patch_rope: –ø–∞—Ç—á–∏—Ç—å RoPE embeddings
        patch_mlp: –ø–∞—Ç—á–∏—Ç—å MLP –Ω–∞ fused SwiGLU/GeGLU
        patch_fused_linear_ce: –ø–∞—Ç—á–∏—Ç—å CrossEntropy –Ω–∞ FusedLinearCrossEntropy
    
    Returns:
        True –µ—Å–ª–∏ –ø–∞—Ç—á –ø—Ä–∏–º–µ–Ω—ë–Ω, False –µ—Å–ª–∏ –Ω–µ—Ç
    """
    if not is_liger_available():
        return False
    
    model_type = getattr(model.config, "model_type", "").lower()
    
    # –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
    # –í–ê–ñ–ù–û: cross_entropy=False —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å fused_linear_cross_entropy
    common_kwargs = {
        "rms_norm": patch_rms_norm,
        "rope": patch_rope,
        "cross_entropy": False,  # –û—Ç–∫–ª—é—á–∞–µ–º ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π chunked CE –∏–ª–∏ Liger CE –æ—Ç–¥–µ–ª—å–Ω–æ
        "fused_linear_cross_entropy": patch_fused_linear_ce,
    }
    
    try:
        # Qwen2
        if "qwen2" in model_type or "qwen" in model_type:
            from liger_kernel.transformers import apply_liger_kernel_to_qwen2
            apply_liger_kernel_to_qwen2(
                **common_kwargs,
                swiglu=patch_mlp,
            )
            logger.info(f"‚úÖ Liger –ø–∞—Ç—á –ø—Ä–∏–º–µ–Ω—ë–Ω –∫ Qwen2 (rms={patch_rms_norm}, rope={patch_rope}, mlp={patch_mlp})")
            return True
        
        # Llama
        elif "llama" in model_type:
            from liger_kernel.transformers import apply_liger_kernel_to_llama
            apply_liger_kernel_to_llama(
                **common_kwargs,
                swiglu=patch_mlp,
            )
            logger.info(f"‚úÖ Liger –ø–∞—Ç—á –ø—Ä–∏–º–µ–Ω—ë–Ω –∫ Llama (rms={patch_rms_norm}, rope={patch_rope}, mlp={patch_mlp})")
            return True
        
        # Mistral
        elif "mistral" in model_type:
            from liger_kernel.transformers import apply_liger_kernel_to_mistral
            apply_liger_kernel_to_mistral(
                **common_kwargs,
                swiglu=patch_mlp,
            )
            logger.info(f"‚úÖ Liger –ø–∞—Ç—á –ø—Ä–∏–º–µ–Ω—ë–Ω –∫ Mistral (rms={patch_rms_norm}, rope={patch_rope}, mlp={patch_mlp})")
            return True
        
        # Gemma
        elif "gemma" in model_type:
            from liger_kernel.transformers import apply_liger_kernel_to_gemma
            apply_liger_kernel_to_gemma(
                rms_norm=patch_rms_norm,
                rope=patch_rope,
                cross_entropy=False,
                fused_linear_cross_entropy=patch_fused_linear_ce,
                geglu=patch_mlp,
            )
            logger.info(f"‚úÖ Liger –ø–∞—Ç—á –ø—Ä–∏–º–µ–Ω—ë–Ω –∫ Gemma (rms={patch_rms_norm}, rope={patch_rope}, mlp={patch_mlp})")
            return True
        
        # Phi3
        elif "phi" in model_type:
            from liger_kernel.transformers import apply_liger_kernel_to_phi3
            apply_liger_kernel_to_phi3(
                **common_kwargs,
                swiglu=patch_mlp,
            )
            logger.info(f"‚úÖ Liger –ø–∞—Ç—á –ø—Ä–∏–º–µ–Ω—ë–Ω –∫ Phi3 (rms={patch_rms_norm}, rope={patch_rope}, mlp={patch_mlp})")
            return True
        
        else:
            # INFO –∞ –Ω–µ WARNING ‚Äî –¥–ª—è HomeModel –∏ –¥—Ä—É–≥–∏—Ö –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ,
            # –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç Liger –Ω–∞–ø—Ä—è–º—É—é (LigerRMSNorm, LigerSiLUMul, LigerFusedCE)
            logger.info(f"‚ÑπÔ∏è Liger: –∞–≤—Ç–æ–ø–∞—Ç—á HF-—Å—Ç–∏–ª—è –Ω–µ –ø—Ä–∏–º–µ–Ω—ë–Ω –∫ '{model_type}' (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä—è–º—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è Liger –ø–∞—Ç—á–∞: {e}")
        return False


def liger_cross_entropy(
    logits: torch.Tensor,
    targets: torch.Tensor,
    ignore_index: int = -100,
    reduction: str = "none",
) -> torch.Tensor:
    """
    Cross-entropy —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback –Ω–∞ Liger –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LigerCrossEntropyLoss –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ F.cross_entropy.
    
    Args:
        logits: [batch, seq, vocab] –∏–ª–∏ [batch*seq, vocab]
        targets: [batch, seq] –∏–ª–∏ [batch*seq]
        ignore_index: –∏–Ω–¥–µ–∫—Å –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
        reduction: "none", "mean", "sum"
    
    Returns:
        Loss tensor
    """
    liger_ce = get_liger_cross_entropy_loss()
    
    if liger_ce is not None:
        try:
            return liger_ce(logits, targets, ignore_index=ignore_index, reduction=reduction)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Liger CE failed, fallback to F.cross_entropy: {e}")
    
    # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π cross_entropy
    if logits.dim() == 3:
        logits = logits.reshape(-1, logits.size(-1))
        targets = targets.reshape(-1)
    
    return F.cross_entropy(logits, targets, ignore_index=ignore_index, reduction=reduction)


def chunked_cross_entropy(
    logits: torch.Tensor,
    targets: torch.Tensor,
    chunk_size: int = 4096,
    ignore_index: int = -100,
) -> torch.Tensor:
    """
    Chunked cross-entropy –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö batch/seq.
    
    –†–∞–∑–±–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ —á—Ç–æ–±—ã –Ω–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤–µ—Å—å logits —Å—Ä–∞–∑—É.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Liger –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    Args:
        logits: [batch, seq, vocab] –∏–ª–∏ [batch*seq, vocab]
        targets: [batch, seq] –∏–ª–∏ [batch*seq]
        chunk_size: —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (tokens)
        ignore_index: –∏–Ω–¥–µ–∫—Å –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Returns:
        Per-token loss [batch*seq] –∏–ª–∏ [batch, seq]
    """
    original_shape = logits.shape[:-1]  # [batch, seq] –∏–ª–∏ [batch*seq]
    
    # Flatten –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if logits.dim() == 3:
        logits = logits.reshape(-1, logits.size(-1))
        targets = targets.reshape(-1)
    
    total_tokens = logits.size(0)
    
    if total_tokens <= chunk_size:
        # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–π ‚Äî —Å—á–∏—Ç–∞–µ–º —Å—Ä–∞–∑—É
        loss = liger_cross_entropy(logits, targets, ignore_index=ignore_index, reduction="none")
        return loss.reshape(original_shape) if len(original_shape) == 2 else loss
    
    # Chunked computation
    all_losses = []
    for start in range(0, total_tokens, chunk_size):
        end = min(start + chunk_size, total_tokens)
        chunk_logits = logits[start:end]
        chunk_targets = targets[start:end]
        
        chunk_loss = liger_cross_entropy(
            chunk_logits, 
            chunk_targets, 
            ignore_index=ignore_index, 
            reduction="none"
        )
        all_losses.append(chunk_loss)
    
    loss = torch.cat(all_losses, dim=0)
    return loss.reshape(original_shape) if len(original_shape) == 2 else loss


class LigerOptimizedLogProbs:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ log-probabilities —Å Liger.
    
    –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    1. Chunked forward pass –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    2. Liger CrossEntropy –≤–º–µ—Å—Ç–æ F.cross_entropy
    3. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π gradient checkpointing
    """
    
    def __init__(
        self,
        chunk_size: int = 2048,
        use_liger: bool = True,
        use_gradient_checkpointing: bool = False,
    ):
        self.chunk_size = chunk_size
        self.use_liger = use_liger and is_liger_available()
        self.use_gradient_checkpointing = use_gradient_checkpointing
    
    @torch.no_grad()
    def compute_log_probs(
        self,
        model: "PreTrainedModel",
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
    ) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç log-probabilities –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
        
        Args:
            model: –ú–æ–¥–µ–ª—å
            input_ids: [batch, seq]
            attention_mask: [batch, seq]
        
        Returns:
            log_probs: [batch, seq-1]
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        # Position IDs
        position_ids = attention_mask.long().cumsum(dim=-1) - 1
        position_ids.masked_fill_(attention_mask == 0, 1)
        
        # –ï—Å–ª–∏ batch –º–∞–ª–µ–Ω—å–∫–∏–π ‚Äî —Å—á–∏—Ç–∞–µ–º —Å—Ä–∞–∑—É
        if batch_size * seq_len <= self.chunk_size:
            return self._forward_and_compute_logprobs(
                model, input_ids, attention_mask, position_ids
            )
        
        # Chunked –ø–æ batch dimension
        all_log_probs = []
        for i in range(0, batch_size, max(1, self.chunk_size // seq_len)):
            end_i = min(i + max(1, self.chunk_size // seq_len), batch_size)
            
            chunk_log_probs = self._forward_and_compute_logprobs(
                model,
                input_ids[i:end_i],
                attention_mask[i:end_i],
                position_ids[i:end_i],
            )
            all_log_probs.append(chunk_log_probs)
            
            # –û—á–∏—â–∞–µ–º cache –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return torch.cat(all_log_probs, dim=0)
    
    def _forward_and_compute_logprobs(
        self,
        model: "PreTrainedModel",
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        position_ids: torch.Tensor,
    ) -> torch.Tensor:
        """Forward pass + log probs computation."""
        output = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            use_cache=False,
        )
        
        logits = output.logits[:, :-1]  # [batch, seq-1, vocab]
        targets = input_ids[:, 1:]  # [batch, seq-1]
        
        # –í—ã—á–∏—Å–ª—è–µ–º log probs
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunked CE –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –Ω–∞ vocab dimension
        nll = chunked_cross_entropy(
            logits,
            targets,
            chunk_size=self.chunk_size,
            ignore_index=-100,
        )
        
        return -nll  # NLL -> log probs


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ rollout.py
_liger_log_probs_computer: Optional[LigerOptimizedLogProbs] = None


def get_liger_log_probs_computer(
    chunk_size: int = 2048,
    use_liger: bool = True,
) -> LigerOptimizedLogProbs:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å LigerOptimizedLogProbs."""
    global _liger_log_probs_computer
    
    if _liger_log_probs_computer is None:
        _liger_log_probs_computer = LigerOptimizedLogProbs(
            chunk_size=chunk_size,
            use_liger=use_liger,
        )
    
    return _liger_log_probs_computer


# ============================================================
# FUSED GRPO LOSS MODULE (–¥–ª—è trainer.py)
# ============================================================

class LigerGRPOLossModule(nn.Module):
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ LigerFusedLinearGRPOLoss –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ GRPOTrainer.
    
    üî• –ì–ª–∞–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GRPO:
    - –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits [batch, seq, vocab] ‚Äî —ç–∫–æ–Ω–æ–º–∏—è –≥–∏–≥–∞–±–∞–π—Ç!
    - Fused forward: hidden_states -> lm_head -> loss –≤ –æ–¥–Ω–æ–º kernel
    - –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π KL penalty (k3 estimator)
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö loss types: grpo, dapo, dr_grpo, bnpo
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        loss_module = LigerGRPOLossModule(model, config)
        loss, metrics = loss_module(
            hidden_states,  # outputs.hidden_states[-1]
            selected_token_ids,
            attention_mask,
            advantages,
            ref_per_token_logps=...,
            old_per_token_logps=...,
        )
    """
    
    def __init__(
        self,
        model: "PreTrainedModel",
        beta: float = 0.04,
        loss_type: str = "grpo",
        epsilon: float = 0.2,
        max_completion_length: Optional[int] = None,
        chunk_size: int = 1,
        use_ref_model: bool = False,
        compiled: bool = True,
    ):
        super().__init__()
        
        LigerFusedLinearGRPOLoss = get_liger_fused_linear_grpo()
        if LigerFusedLinearGRPOLoss is None:
            raise RuntimeError("LigerFusedLinearGRPOLoss –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
        
        self.loss_fn = LigerFusedLinearGRPOLoss(
            beta=beta,
            loss_type=loss_type,
            epsilon_low=epsilon,
            epsilon_high=epsilon,
            max_completion_length=max_completion_length,
            chunk_size=chunk_size,
            use_ref_model=use_ref_model,
            compiled=compiled,
            importance_sampling_level="token",
            temperature=1.0,
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ lm_head
        self.lm_head_weight = model.lm_head.weight
        self.lm_head_bias = getattr(model.lm_head, 'bias', None)
        
        self.beta = beta
        self.loss_type = loss_type
        logger.info(f"‚úÖ LigerGRPOLossModule –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (loss_type={loss_type}, beta={beta})")
    
    def forward(
        self,
        hidden_states: torch.Tensor,  # [batch, seq, hidden] –∏–ª–∏ [batch*seq, hidden]
        selected_token_ids: torch.Tensor,  # [batch, seq]
        attention_mask: torch.Tensor,  # [batch, seq]
        advantages: torch.Tensor,  # [batch]
        ref_per_token_logps: Optional[torch.Tensor] = None,  # [batch, seq]
        old_per_token_logps: Optional[torch.Tensor] = None,  # [batch, seq]
    ) -> Tuple[torch.Tensor, dict]:
        """
        Forward pass –¥–ª—è GRPO loss.
        
        Returns:
            loss: scalar loss
            metrics: dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (kl_div, clip_ratio)
        """
        batch_size = selected_token_ids.shape[0]
        seq_len = selected_token_ids.shape[1]
        
        # Reshape hidden_states –µ—Å–ª–∏ –Ω—É–∂–Ω–æ: [batch, seq, hidden] -> [batch*seq, hidden]
        if hidden_states.dim() == 3:
            hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))
        
        # –í—ã–∑—ã–≤–∞–µ–º fused loss
        result = self.loss_fn(
            hidden_states,           # [batch*seq, hidden]
            self.lm_head_weight,     # [vocab, hidden]
            selected_token_ids,      # [batch, seq]
            attention_mask,          # [batch, seq]
            advantages,              # [batch]
            bias=self.lm_head_bias,
            ref_per_token_logps=ref_per_token_logps,
            old_per_token_logps=old_per_token_logps,
        )
        
        # LigerFusedLinearGRPOLoss –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (loss, [kl_div, clip_ratio]) –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ loss
        if isinstance(result, tuple):
            loss = result[0]
            metrics_list = result[1] if len(result) > 1 else []
        else:
            loss = result
            metrics_list = []
        
        # –ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {}
        if len(metrics_list) >= 1 and self.beta != 0.0:
            metrics["kl_div"] = metrics_list[0].item() if hasattr(metrics_list[0], 'item') else float(metrics_list[0])
        if len(metrics_list) >= 2:
            clip_idx = 1 if self.beta != 0.0 else 0
            if clip_idx < len(metrics_list):
                metrics["clip_ratio"] = metrics_list[clip_idx].item() if hasattr(metrics_list[clip_idx], 'item') else float(metrics_list[clip_idx])
        
        return loss, metrics


# ============================================================
# FUSED LINEAR CROSS-ENTROPY –î–õ–Ø PRETRAIN/SFT
# ============================================================

class LigerFusedCEModule(nn.Module):
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ LigerFusedLinearCrossEntropyLoss –¥–ª—è pretrain/SFT.
    
    üî• –ì–ª–∞–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è language modeling:
    - –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits [batch, seq, vocab]
    - Fused forward: hidden_states -> lm_head -> CE loss –≤ –æ–¥–Ω–æ–º kernel
    - –î–æ 80% —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –Ω–∞ vocab dimension!
    
    ‚ö†Ô∏è –í–ê–ñ–ù–û: –î–ª—è Causal LM –º—ã –¥–µ–ª–∞–µ–º –°–î–í–ò–ì (shift):
    - hidden_states[:-1] –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç labels[1:]
    - –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –¥–ª—è language modeling (next-token prediction)
    
    ‚ö†Ô∏è DeepSpeed ZeRO-3: –ò—Å–ø–æ–ª—å–∑—É–µ–º GatheredParameters –¥–ª—è —Å–±–æ—Ä–∞ lm_head.weight
    –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º Liger, —Ç–∞–∫ –∫–∞–∫ Triton kernels –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        # –í–º–µ—Å—Ç–æ:
        logits = model.lm_head(hidden_states)
        shift_logits = logits[..., :-1, :]
        shift_labels = labels[..., 1:]
        loss = F.cross_entropy(shift_logits.view(-1, vocab), shift_labels.view(-1))
        
        # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:
        loss_module = LigerFusedCEModule(model, accelerator=accelerator)
        loss = loss_module(hidden_states, labels)  # –°–¥–≤–∏–≥ –¥–µ–ª–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏!
    """
    
    def __init__(
        self,
        model: "PreTrainedModel",
        ignore_index: int = -100,
        label_smoothing: float = 0.0,
        reduction: str = "mean",
        accelerator: Optional[Any] = None,
    ):
        super().__init__()
        
        LigerFusedLinearCE = get_liger_fused_linear_ce()
        if LigerFusedLinearCE is None:
            raise RuntimeError("LigerFusedLinearCrossEntropyLoss –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
        
        self.loss_fn = LigerFusedLinearCE(
            ignore_index=ignore_index,
            label_smoothing=label_smoothing,
            reduction=reduction,
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –º–æ–¥–µ–ª—å (–Ω–µ –Ω–∞ weight!) –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ ZeRO-3
        self.model = model
        self.accelerator = accelerator
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ ZeRO-3
        self.is_zero3 = False
        if accelerator is not None:
            try:
                ds_plugin = getattr(accelerator.state, 'deepspeed_plugin', None)
                if ds_plugin is not None:
                    zero_stage = getattr(ds_plugin, 'zero_stage', 0)
                    self.is_zero3 = zero_stage == 3
                    if self.is_zero3:
                        logger.info("‚úÖ LigerFusedCEModule: –æ–±–Ω–∞—Ä—É–∂–µ–Ω ZeRO-3, –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GatheredParameters")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å ZeRO stage: {e}")
        
        logger.info(f"‚úÖ LigerFusedCEModule –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (ignore_index={ignore_index}, causal_shift=True)")
    
    def _get_lm_head_params(self):
        """–ü–æ–ª—É—á–∞–µ—Ç weight –∏ bias –∏–∑ lm_head –º–æ–¥–µ–ª–∏."""
        lm_head = self.model.lm_head
        weight = lm_head.weight
        bias = getattr(lm_head, 'bias', None)
        return weight, bias
    
    def forward(
        self,
        hidden_states: torch.Tensor,  # [batch, seq, hidden]
        labels: torch.Tensor,  # [batch, seq]
    ) -> torch.Tensor:
        """
        Forward pass –¥–ª—è Fused Linear CrossEntropy —Å CAUSAL SHIFT.
        
        ‚ö†Ô∏è –í–ê–ñ–ù–û: –î–ª—è Causal LM (next-token prediction) –º—ã –¥–µ–ª–∞–µ–º —Å–¥–≤–∏–≥:
        - hidden_states[:, :-1] ‚Üí –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç ‚Üí labels[:, 1:]
        - –≠—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ shift_logits[:-1] vs shift_labels[1:] –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º CE
        
        Args:
            hidden_states: –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state –º–æ–¥–µ–ª–∏ [batch, seq, hidden]
            labels: target token ids [batch, seq]
        
        Returns:
            loss: scalar (–µ—Å–ª–∏ reduction="mean") –∏–ª–∏ [batch*(seq-1)] (–µ—Å–ª–∏ reduction="none")
        """
        if hidden_states.dim() != 3:
            raise ValueError(f"hidden_states –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 3D [batch, seq, hidden], –ø–æ–ª—É—á–µ–Ω {hidden_states.dim()}D")
        
        batch_size, seq_len, hidden_size = hidden_states.shape
        
        # ============================================================
        # CAUSAL SHIFT –¥–ª—è next-token prediction
        # ============================================================
        # hidden_states[:, :-1] –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç labels[:, 1:]
        # –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è language modeling
        shift_hidden = hidden_states[:, :-1, :].contiguous()  # [batch, seq-1, hidden]
        shift_labels = labels[:, 1:].contiguous()              # [batch, seq-1]
        
        # Reshape: [batch, seq-1, hidden] -> [batch*(seq-1), hidden]
        shift_hidden = shift_hidden.reshape(-1, hidden_size)
        shift_labels = shift_labels.reshape(-1)
        
        # ============================================================
        # ZeRO-3: —Å–æ–±–∏—Ä–∞–µ–º lm_head.weight —á–µ—Ä–µ–∑ GatheredParameters
        # ============================================================
        if self.is_zero3:
            return self._forward_with_gathered_params(shift_hidden, shift_labels)
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å: –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
            weight, bias = self._get_lm_head_params()
            return self.loss_fn(weight, shift_hidden, shift_labels, bias)
    
    def _forward_with_gathered_params(
        self,
        shift_hidden: torch.Tensor,
        shift_labels: torch.Tensor,
    ) -> torch.Tensor:
        """Forward pass —Å GatheredParameters –¥–ª—è ZeRO-3."""
        try:
            from deepspeed.runtime.zero.partition_parameters import GatheredParameters
            
            lm_head = self.model.lm_head
            params_to_gather = [lm_head.weight]
            if hasattr(lm_head, 'bias') and lm_head.bias is not None:
                params_to_gather.append(lm_head.bias)
            
            # modifier_rank=None: –≤—Å–µ —Ä–∞–Ω–∫–∏ –º–æ–≥—É—Ç —á–∏—Ç–∞—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            with GatheredParameters(params_to_gather, modifier_rank=None):
                weight = lm_head.weight
                bias = getattr(lm_head, 'bias', None)
                return self.loss_fn(weight, shift_hidden, shift_labels, bias)
                
        except ImportError:
            logger.warning("‚ö†Ô∏è DeepSpeed –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º")
            weight, bias = self._get_lm_head_params()
            return self.loss_fn(weight, shift_hidden, shift_labels, bias)


def create_liger_grpo_loss(
    model: "PreTrainedModel",
    config: Any,  # GRPOConfig
) -> Optional[LigerGRPOLossModule]:
    """
    –°–æ–∑–¥–∞—ë—Ç LigerGRPOLossModule –µ—Å–ª–∏ Liger –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –≤–∫–ª—é—á—ë–Ω.
    
    Args:
        model: HuggingFace –º–æ–¥–µ–ª—å
        config: GRPOConfig —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    
    Returns:
        LigerGRPOLossModule –∏–ª–∏ None
    """
    if not getattr(config, 'use_liger', False):
        return None
    
    if not is_liger_available():
        logger.warning("‚ö†Ô∏è Liger –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO loss")
        return None
    
    if get_liger_fused_linear_grpo() is None:
        logger.warning("‚ö†Ô∏è LigerFusedLinearGRPOLoss –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO loss")
        return None
    
    try:
        loss_module = LigerGRPOLossModule(
            model=model,
            beta=getattr(config, 'kl_weight', 0.04),
            loss_type=getattr(config, 'liger_grpo_loss_type', 'dapo'),
            epsilon=getattr(config, 'epsilon', 0.2),
            max_completion_length=getattr(config, 'max_new_tokens', 512),
            chunk_size=getattr(config, 'liger_chunk_size', 1),
            use_ref_model=getattr(config, 'kl_weight', 0) > 0,
            compiled=True,
        )
        logger.info("ü¶Å LigerFusedLinearGRPOLoss –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω ‚Äî logits –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É—é—Ç—Å—è!")
        return loss_module
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LigerGRPOLossModule: {e}")
        return None


def create_liger_fused_ce(
    model: "PreTrainedModel",
    ignore_index: int = -100,
    label_smoothing: float = 0.0,
    accelerator: Optional[Any] = None,
) -> Optional[LigerFusedCEModule]:
    """
    –°–æ–∑–¥–∞—ë—Ç LigerFusedCEModule –µ—Å–ª–∏ Liger –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    ‚ö†Ô∏è –í–ê–ñ–ù–û: –î–ª—è DeepSpeed ZeRO-3 –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å accelerator!
    –ë–µ–∑ —ç—Ç–æ–≥–æ Liger –Ω–µ —Å–º–æ–∂–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–±—Ä–∞—Ç—å —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ lm_head.
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è pretrain/SFT:
        loss_fn = create_liger_fused_ce(model, accelerator=accelerator)
        if loss_fn:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º fused loss (–Ω–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits)
            outputs = model(input_ids, output_hidden_states=True)
            hidden = outputs.hidden_states[-1]
            loss = loss_fn(hidden, labels)
        else:
            # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥
            outputs = model(input_ids, labels=labels)
            loss = outputs.loss
    """
    if not is_liger_available():
        return None
    
    if get_liger_fused_linear_ce() is None:
        return None
    
    try:
        return LigerFusedCEModule(
            model=model,
            ignore_index=ignore_index,
            label_smoothing=label_smoothing,
            accelerator=accelerator,
        )
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LigerFusedCEModule: {e}")
        return None
