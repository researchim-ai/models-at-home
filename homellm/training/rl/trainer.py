"""
GRPOTrainer - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GRPO/RL.

–†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:
1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è rollout'–æ–≤ (completions)
2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ rewards –∏ advantages
3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ (–º–æ–¥–µ–ª–∏)
4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
"""
import logging
import math
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, List, Dict, Any, Callable, Union
from datetime import datetime

import torch
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from tqdm import tqdm

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
    get_cosine_schedule_with_warmup,
)

from .config import GRPOConfig, RLAlgorithm
from .experience import Experience, ReplayBuffer, join_experience_batch
from .loss import GRPOLoss, compute_advantages, compute_entropy
from .rollout import (
    generate_rollouts,
    rollout_to_experiences,
    build_reasoning_prompt,
    compute_log_probs,
)
from .rewards.base import RewardFunction, CombinedReward
from .rewards.math import GSM8KReward
from .rewards.format import FormatReward, ReasoningQualityReward
from .data.base import RLDataset, RLSample

logger = logging.getLogger(__name__)


def setup_logging(log_level: str = "INFO"):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    logging.basicConfig(
        level=getattr(logging, log_level),
        format="[%(asctime)s] [%(levelname)s] %(name)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def set_seed(seed: int):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏."""
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


class GRPOTrainer:
    """
    Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM —Å GRPO.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO
    - Dr.GRPO (–±–µ–∑ std –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
    - DAPO (clip higher, dynamic sampling)
    - LoRA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    - Multi-GPU —á–µ—Ä–µ–∑ accelerate
    - W&B –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    
    Example:
        >>> from homellm.training.rl import GRPOConfig, GRPOTrainer
        >>> from homellm.training.rl.data import load_gsm8k
        >>> 
        >>> config = GRPOConfig.from_preset("reasoning_small")
        >>> dataset = load_gsm8k(split="train", max_samples=1000)
        >>> 
        >>> trainer = GRPOTrainer(
        ...     model_name="Qwen/Qwen2.5-0.5B-Instruct",
        ...     config=config,
        ... )
        >>> trainer.train(dataset)
    """
    
    def __init__(
        self,
        model_name: str,
        config: Optional[GRPOConfig] = None,
        tokenizer: Optional[PreTrainedTokenizer] = None,
        reward_fn: Optional[RewardFunction] = None,
        device: Optional[torch.device] = None,
        use_accelerate: bool = True,
    ):
        """
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—É—Ç—å
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            reward_fn: –§—É–Ω–∫—Ü–∏—è reward (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            use_accelerate: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å accelerate –¥–ª—è multi-GPU
        """
        self.model_name = model_name
        self.config = config or GRPOConfig()
        self.use_accelerate = use_accelerate
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
        set_seed(self.config.seed)
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∏–∑ accelerator –≤ setup()
        # –ù–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º device –∑–¥–µ—Å—å, —á—Ç–æ–±—ã accelerator –º–æ–≥ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å multi-GPU
        self._device = device  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è fallback –µ—Å–ª–∏ accelerate –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = tokenizer
        self.model = None
        self.reference_model = None
        
        # Reward —Ñ—É–Ω–∫—Ü–∏—è
        if reward_fn is None:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è format + correctness
            self.reward_fn = CombinedReward([
                FormatReward(weight=1.0),
                ReasoningQualityReward(weight=0.5),
                GSM8KReward(weight=2.0),
            ])
        else:
            self.reward_fn = reward_fn
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ setup())
        self.optimizer = None
        self.scheduler = None
        self.loss_fn = None
        self.replay_buffer = None
        self.accelerator = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.global_step = 0
        # –û—Ç–¥–µ–ª—å–Ω—ã–π —Å—á—ë—Ç—á–∏–∫ –¥–ª—è rollout-–±–∞—Ç—á–µ–π (prompts/step). –ù—É–∂–µ–Ω –¥–ª—è –ø–æ–Ω—è—Ç–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
        self.rollout_step = 0
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è –≥—Ä—É–ø–ø (–Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å dataset index –ø—Ä–∏ dynamic sampling —Å –¥–æ–±–æ—Ä–æ–º)
        self._group_uid = 0
        # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Å—á—ë—Ç—á–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ "—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–æ—à–ª–æ"
        self.cum_prompts_generated = 0
        self.cum_prompts_used = 0
        self.cum_completions_generated = 0
        self.cum_experiences_tuned = 0

        # –ü—Ä–æ—á–∏–µ –º–µ—Ç—Ä–∏–∫–∏/—Å—Ç–∞—Ç—É—Å—ã
        self.total_rollouts = 0
        self.best_mean_reward = float("-inf")
        
        # W&B
        self.wandb_run = None

    def _next_group_uids(self, n: int) -> List[int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç n —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö group_id –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ Experience."""
        start = self._group_uid
        self._group_uid += int(n)
        return list(range(start, start + int(n)))
    
    def setup(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GRPOTrainer...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –î–û —Å–æ–∑–¥–∞–Ω–∏—è accelerator
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            logger.info(f"üñ•Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {num_gpus} —É—Å—Ç—Ä–æ–π—Å—Ç–≤")
            for i in range(num_gpus):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)
                logger.info(f"  - GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            logger.info("üñ•Ô∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        
        # Accelerate - —Å–æ–∑–¥–∞–µ–º –ü–ï–†–ï–î –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ pretrain/SFT)
        if self.use_accelerate:
            try:
                from accelerate import Accelerator
                
                # Mixed precision –¥–æ–ª–∂–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ UI.
                mixed_precision = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                if mixed_precision not in ("no", "fp16", "bf16"):
                    logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π mixed_precision='{mixed_precision}', fallback -> bf16")
                    mixed_precision = "bf16"
                if mixed_precision == "bf16" and torch.cuda.is_available() and not torch.cuda.is_bf16_supported():
                    logger.warning("bf16 –≤—ã–±—Ä–∞–Ω –≤ UI, –Ω–æ GPU –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç bf16. Fallback -> fp16")
                    mixed_precision = "fp16"

                # "Pure fp16" (–≤–µ—Å–∞ fp16, –±–µ–∑ GradScaler): –¥–ª—è accelerate –Ω—É–∂–Ω–æ mixed_precision='no',
                # –∏–Ω–∞—á–µ –æ–Ω –≤–∫–ª—é—á–∏—Ç GradScaler –∏ —É–ø–∞–¥—ë—Ç –ø—Ä–∏ fp16 –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö.
                accel_mp = mixed_precision
                if mixed_precision == "fp16" and bool(getattr(self.config, "fp16_pure", False)):
                    accel_mp = "no"
                    logger.info("üß™ FP16 Pure —Ä–µ–∂–∏–º: Accelerator(mixed_precision='no'), –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç torch.float16")
                
                logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Accelerator...")
                logger.info(f"  - gradient_accumulation_steps: {self.config.gradient_accumulation_steps}")
                logger.info(f"  - mixed_precision (UI): {mixed_precision}")
                logger.info(f"  - mixed_precision (accelerate): {accel_mp}")
                
                self.accelerator = Accelerator(
                    gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                    mixed_precision=accel_mp,
                )
                
                # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±–µ—Ä–µ–º –∏–∑ accelerator (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multi-GPU)
                self.device = self.accelerator.device
                self.is_main_process = self.accelerator.is_main_process
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏
                if self.accelerator.num_processes > 1:
                    logger.info(f"‚úÖ Multi-GPU —Ä–µ–∂–∏–º: {self.accelerator.num_processes} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")
                    logger.info(f"  - –¢–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å: {self.accelerator.process_index} / {self.accelerator.num_processes - 1}")
                    logger.info(f"  - Main process: {self.is_main_process}")
                else:
                    logger.info(f"‚úÖ Single GPU —Ä–µ–∂–∏–º")
                
                logger.info(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
                
            except ImportError:
                logger.warning("‚ö†Ô∏è  accelerate –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º single GPU")
                self.accelerator = None
                self.device = self._device if self._device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
                self.is_main_process = True
        else:
            logger.info("‚ÑπÔ∏è  Accelerate –æ—Ç–∫–ª—é—á–µ–Ω (use_accelerate=False)")
            self.accelerator = None
            self.device = self._device if self._device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.is_main_process = True
            logger.info(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        if self.tokenizer is None:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self._load_model()
        
        # Loss —Ñ—É–Ω–∫—Ü–∏—è
        self.loss_fn = GRPOLoss(config=self.config)
        
        # Replay buffer
        self.replay_buffer = ReplayBuffer()
        
        # W&B
        if self.config.use_wandb and self.is_main_process:
            self._setup_wandb()
        
        logger.info(f"GRPOTrainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
        logger.info(f"–ê–ª–≥–æ—Ä–∏—Ç–º: {self.config.algorithm.value}")
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –∏ LoRA."""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        logger.info(f"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
        logger.info(f"  - use_4bit: {self.config.use_4bit}")
        logger.info(f"  - use_8bit: {self.config.use_8bit}")
        logger.info(f"  - use_lora: {self.config.use_lora}")
        if self.config.use_lora:
            logger.info(f"  - lora_r: {self.config.lora_r}")
            logger.info(f"  - lora_alpha: {self.config.lora_alpha}")
            logger.info(f"  - lora_target_modules: {self.config.lora_target_modules}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏
        memory_before = 0.0
        if torch.cuda.is_available():
            memory_before = torch.cuda.memory_allocated() / (1024 ** 2)
            logger.info(f"üíæ –ü–∞–º—è—Ç—å CUDA –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {memory_before:.1f} MB")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        quantization_config = None
        if self.config.use_4bit or self.config.use_8bit:
            try:
                from transformers import BitsAndBytesConfig
                
                if self.config.use_4bit:
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_use_double_quant=True,
                    )
                    logger.info("‚úÖ –°–æ–∑–¥–∞–Ω BitsAndBytesConfig –¥–ª—è 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏")
                else:
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                    )
                    logger.info("‚úÖ –°–æ–∑–¥–∞–Ω BitsAndBytesConfig –¥–ª—è 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏")
            except ImportError:
                logger.warning("‚ùå bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
                quantization_config = None
        else:
            logger.info("‚ÑπÔ∏è  –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (use_4bit=False, use_8bit=False)")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model_kwargs = {
            "trust_remote_code": True,
            "device_map": "auto" if quantization_config else None,
        }
        
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        else:
            # –í–ê–ñ–ù–û:
            # - bf16: –º–æ–∂–Ω–æ –≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –≤ bf16 (–Ω–µ—Ç GradScaler).
            # - fp16: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —ç—Ç–æ AMP fp16 (fp32 master-–≤–µ—Å–∞ + GradScaler) => –≤–µ—Å–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º fp32.
            #   –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å "pure fp16" (–≤–µ—Å–∞ fp16, –±–µ–∑ GradScaler) —á–µ—Ä–µ–∑ config.fp16_pure.
            mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
            if mp == "bf16" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                model_kwargs["dtype"] = torch.bfloat16
            elif mp == "fp16":
                if bool(getattr(self.config, "fp16_pure", False)):
                    model_kwargs["dtype"] = torch.float16
                else:
                    # AMP fp16: –æ—Å—Ç–∞–≤–ª—è–µ–º fp32 –≤–µ—Å–∞ (GradScaler —Ç—Ä–µ–±—É–µ—Ç fp32 master weights)
                    pass
            elif mp == "no":
                # –û—Å—Ç–∞–≤–ª—è–µ–º fp32 (–¥–µ—Ñ–æ–ª—Ç HF)
                pass
            else:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ flash_attn –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
        # –í–ê–ñ–ù–û: Flash Attention –º–æ–∂–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö
        # –î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π attention
        if self.config.use_flash_attention and not quantization_config:
            try:
                import flash_attn
                mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                if mp == "no":
                    logger.info("Flash Attention 2 –æ—Ç–∫–ª—é—á–µ–Ω: mixed_precision='no' (fp32 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è flash-attn)")
                else:
                    model_kwargs["attn_implementation"] = "flash_attention_2"
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Flash Attention 2")
            except ImportError:
                logger.warning(
                    "Flash Attention 2 –∑–∞–ø—Ä–æ—à–µ–Ω, –Ω–æ –ø–∞–∫–µ—Ç flash_attn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è attention. "
                    "–î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install flash-attn"
                )
                # –ù–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º attn_implementation, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω–∞—è
        elif self.config.use_flash_attention and quantization_config:
            logger.info(
                "Flash Attention –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ "
                "(–º–æ–∂–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å bitsandbytes). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π attention."
            )
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏
        logger.info(f"üì¶ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏:")
        logger.info(f"  - quantization_config: {'‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è' if quantization_config else '‚ùå –ù–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è'}")
        logger.info(f"  - device_map: {model_kwargs.get('device_map', 'None')}")
        if quantization_config:
            logger.info(f"  - –¢–∏–ø –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏: {'4-bit' if self.config.use_4bit else '8-bit'}")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            **model_kwargs,
        )

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ dtype –º–æ–¥–µ–ª–∏ (–ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É fp16 –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–ª—è—Ç—å –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ —á–µ–º bf16)
        try:
            first_param = next(self.model.parameters(), None)
            if first_param is not None:
                logger.info(f"üîé DType –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–º–µ—Ä): {first_param.dtype}")
        except Exception:
            pass

        # Gradient checkpointing (—É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∏–∑ UI)
        if getattr(self.config, "grad_checkpoint", False) and hasattr(self.model, "gradient_checkpointing_enable"):
            try:
                self.model.gradient_checkpointing_enable()
                logger.info("‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω (–∏–∑ UI)")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å gradient checkpointing: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        if torch.cuda.is_available():
            memory_after_load = torch.cuda.memory_allocated() / (1024 ** 2)
            logger.info(f"üíæ –ü–∞–º—è—Ç—å CUDA –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {memory_after_load:.1f} MB (+{memory_after_load - memory_before:.1f} MB)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
        if quantization_config:
            is_quantized = False
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                for name, param in self.model.named_parameters():
                    if hasattr(param, 'quant_state') or str(param.dtype) == 'torch.uint8':
                        is_quantized = True
                        break
                    # –î–ª—è bitsandbytes –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                    if hasattr(param, 'data') and hasattr(param.data, 'quant_state'):
                        is_quantized = True
                        break
                
                if is_quantized:
                    logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (–Ω–∞–π–¥–µ–Ω—ã –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)")
                else:
                    logger.warning("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ BitsAndBytesConfig.")
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é: {e}")
        
        # LoRA
        # –í–ê–ñ–ù–û: –ï—Å–ª–∏ use_lora=True, –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã (–±–µ–∑ fallback)
        if self.config.use_lora:
            if self.config.lora_r is None:
                raise ValueError(
                    "‚ùå use_lora=True –Ω–æ lora_r=None! "
                    "lora_r –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_r."
                )
            if self.config.lora_alpha is None:
                raise ValueError(
                    "‚ùå use_lora=True –Ω–æ lora_alpha=None! "
                    "lora_alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_alpha."
                )
            self._apply_lora()
        else:
            # –ï—Å–ª–∏ LoRA –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –≤–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            # (–¥–ª—è full fine-tuning)
            # –í–ê–ñ–ù–û: –ü—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–µ–∑ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã!
            if quantization_config:
                raise RuntimeError(
                    "‚ùå –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (4bit/8bit) –±–µ–∑ LoRA –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è! "
                    "–ü—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã. "
                    "–í–∫–ª—é—á–∏—Ç–µ use_lora=True –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."
                )
            
            logger.info("LoRA –æ—Ç–∫–ª—é—á–µ–Ω, –≤–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (full fine-tuning)...")
            for param in self.model.parameters():
                param.requires_grad = True
        
        # –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å (–¥–ª—è KL)
        # –í–ê–ñ–ù–û: Reference –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è forward pass (–±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
        # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞, –Ω–æ –º–æ–∂–µ—Ç —ç–∫–æ–Ω–æ–º–∏—Ç—å –ø–∞–º—è—Ç—å
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ù–ï –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É–µ–º –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ KL divergence
        if self.config.kl_weight > 0:
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è KL...")
            
            # –°–æ–∑–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ model_kwargs –¥–ª—è reference –º–æ–¥–µ–ª–∏
            ref_model_kwargs = {
                "trust_remote_code": True,
                "device_map": "auto" if (self.config.quantize_reference_model and quantization_config) else None,
            }
            
            # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è reference –º–æ–¥–µ–ª–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞
            if self.config.quantize_reference_model and quantization_config:
                ref_model_kwargs["quantization_config"] = quantization_config
                logger.info("‚ö†Ô∏è –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–π KL)")
            else:
                # –ù–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É–µ–º reference –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ KL
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ dtype —á—Ç–æ –∏ –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å (–∏–ª–∏ bfloat16 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                if not quantization_config:
                    # Reference –º–æ–¥–µ–ª—å –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: –º–æ–∂–Ω–æ –≥—Ä—É–∑–∏—Ç—å –≤ mp dtype –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
                    mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                    if mp == "bf16" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                        ref_model_kwargs["dtype"] = torch.bfloat16
                    elif mp == "fp16" and torch.cuda.is_available():
                        ref_model_kwargs["dtype"] = torch.float16
                    else:
                        # fp32
                        pass
                logger.info("‚úÖ –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –ù–ï –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (—Ç–æ—á–Ω—ã–π KL divergence)")
            
            # Flash Attention –¥–ª—è reference –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞)
            if self.config.use_flash_attention and not (self.config.quantize_reference_model and quantization_config):
                try:
                    import flash_attn
                    ref_model_kwargs["attn_implementation"] = "flash_attention_2"
                except ImportError:
                    pass
            
            self.reference_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **ref_model_kwargs,
            )
            self.reference_model.eval()
            for param in self.reference_model.parameters():
                param.requires_grad = False
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –µ—Å–ª–∏ –Ω–µ device_map
            if not (self.config.quantize_reference_model and quantization_config):
                self.reference_model = self.reference_model.to(self.device)
        else:
            logger.info("KL weight = 0, —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–µ—Å–ª–∏ –Ω–µ device_map)
        if not quantization_config:
            self.model = self.model.to(self.device)
            if self.reference_model:
                self.reference_model = self.reference_model.to(self.device)
        
        # –í–ê–ñ–ù–û: –ü–æ—Å–ª–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏–ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA,
        # —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Å—ë –µ—â—ë —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if self.config.use_lora:
            # –î–ª—è LoRA –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            # PEFT –¥–æ–ª–∂–µ–Ω —ç—Ç–æ –¥–µ–ª–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –Ω–æ –ø—Ä–æ–≤–µ—Ä–∏–º
            try:
                from peft import PeftModel
                if isinstance(self.model, PeftModel):
                    # PEFT –º–æ–¥–µ–ª—å - –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    pass  # PEFT –¥–æ–ª–∂–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å requires_grad
            except:
                pass
        else:
            # –î–ª—è full fine-tuning —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            for param in self.model.parameters():
                if not param.requires_grad:
                    logger.warning(f"–ü–∞—Ä–∞–º–µ—Ç—Ä –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, –≤–∫–ª—é—á–∞–µ–º: {param.shape}")
                    param.requires_grad = True
        
        # –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        # –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            try:
                # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–∏
                # –î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: ~0.5 bytes/param (4-bit)
                # –î–ª—è fp16: 2 bytes/param, –¥–ª—è fp32: 4 bytes/param
                if quantization_config:
                    if self.config.use_4bit:
                        bytes_per_param = 0.5  # 4-bit = 0.5 bytes
                        quant_type = "4-bit"
                    else:
                        bytes_per_param = 1.0  # 8-bit = 1 byte
                        quant_type = "8-bit"
                    model_memory_mb = (total_params * bytes_per_param) / (1024 ** 2)
                else:
                    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º bfloat16/fp16
                    try:
                        first_param = next(self.model.parameters(), None)
                        dt = getattr(first_param, "dtype", None)
                        if dt == torch.float32:
                            bytes_per_param = 4.0
                            quant_type = "fp32"
                        elif dt == torch.bfloat16:
                            bytes_per_param = 2.0
                            quant_type = "bf16"
                        elif dt == torch.float16:
                            bytes_per_param = 2.0
                            quant_type = "fp16"
                        else:
                            bytes_per_param = 2.0
                            quant_type = "fp16/bf16"
                    except Exception:
                        bytes_per_param = 2.0
                        quant_type = "fp16/bf16"
                    model_memory_mb = (total_params * bytes_per_param) / (1024 ** 2)
                
                logger.info(
                    f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,} –≤—Å–µ–≥–æ, {trainable_params:,} –æ–±—É—á–∞–µ–º—ã—Ö "
                    f"({100*trainable_params/total_params:.2f}%)"
                )
                logger.info(
                    f"üíæ –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–∏: ~{model_memory_mb:.1f} MB ({quant_type})"
                )
                
                # –î–ª—è LoRA –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –ø–∞–º—è—Ç–∏ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
                if self.config.use_lora:
                    # LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã: r * (input_dim + output_dim) * 2 (A –∏ B –º–∞—Ç—Ä–∏—Ü—ã) * 2 bytes (fp16)
                    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: r * 2 * avg_dim * 2 bytes
                    # –î–ª—è r=16, avg_dim=1024: ~16 * 2 * 1024 * 2 = 64KB –Ω–∞ –º–æ–¥—É–ª—å
                    # –ù–æ —ç—Ç–æ –æ—á–µ–Ω—å –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞, —Ä–µ–∞–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
                    lora_memory_mb = (trainable_params * 2.0) / (1024 ** 2)  # fp16 –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
                    logger.info(f"üíæ –ü–∞–º—è—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: ~{lora_memory_mb:.1f} MB")
                    
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –ø–∞–º—è—Ç—å: {e}")
        else:
            logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,} –≤—Å–µ–≥–æ, {trainable_params:,} –æ–±—É—á–∞–µ–º—ã—Ö")
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if trainable_params == 0:
            raise RuntimeError(
                "‚ùå –ù–µ—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏! "
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: use_lora, use_4bit, use_8bit. "
                "–î–ª—è full fine-tuning –Ω—É–∂–µ–Ω use_lora=False –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏."
            )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ç–µ—Å—Ç–æ–≤—ã–π forward pass –¥–æ–ª–∂–µ–Ω —Ç—Ä–µ–±–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        # –í–ê–ñ–ù–û: –ø—Ä–∏ flash_attention_2 –∏ mixed_precision fp16/bf16 –¥–µ–ª–∞–µ–º forward –ø–æ–¥ autocast,
        # –∏–Ω–∞—á–µ FlashAttention –º–æ–∂–µ—Ç —Ä—É–≥–∞—Ç—å—Å—è –Ω–∞ fp32 dtype.
        self.model.train()  # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤ train —Ä–µ–∂–∏–º–µ
        test_input = torch.randint(0, 1000, (1, 10), device=self.device)
        test_mask = torch.ones_like(test_input)
        mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
        use_autocast = torch.cuda.is_available() and mp in ("bf16", "fp16")
        if use_autocast:
            amp_dtype = torch.bfloat16 if mp == "bf16" else torch.float16
            autocast_ctx = torch.amp.autocast("cuda", enabled=True, dtype=amp_dtype)
        else:
            from contextlib import nullcontext
            autocast_ctx = nullcontext()
        if self.accelerator is not None:
            try:
                logger.info(
                    "üîé AMP/Precision: "
                    f"mixed_precision={mp}, "
                    f"autocast={'on' if use_autocast else 'off'}, "
                    f"autocast_dtype={('bf16' if mp=='bf16' else 'fp16') if use_autocast else 'n/a'}, "
                    f"grad_scaler={'on' if getattr(self.accelerator, 'scaler', None) is not None else 'off'}"
                )
            except Exception:
                pass
        with torch.enable_grad():
            with autocast_ctx:
                test_output = self.model(input_ids=test_input, attention_mask=test_mask, use_cache=False)
        if not test_output.logits.requires_grad:
            logger.warning("‚ö†Ô∏è –¢–µ—Å—Ç–æ–≤—ã–π forward pass –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤! –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–æ–π.")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            for param in self.model.parameters():
                param.requires_grad = True
            logger.info("–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–µ–Ω—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            memory_final = torch.cuda.memory_allocated() / (1024 ** 2)
            memory_reserved = torch.cuda.memory_reserved() / (1024 ** 2)
            logger.info("=" * 60)
            logger.info("üìä –ò–¢–û–ì–û–í–û–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ü–ê–ú–Ø–¢–ò –ü–û–°–õ–ï –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò:")
            logger.info(f"  - –í—ã–¥–µ–ª–µ–Ω–æ (allocated): {memory_final:.1f} MB")
            logger.info(f"  - –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ (reserved): {memory_reserved:.1f} MB")
            logger.info(f"  - –í—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Å –Ω–∞—á–∞–ª–∞: +{memory_final - memory_before:.1f} MB")
            logger.info("=" * 60)
    
    def _apply_lora(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∫ –º–æ–¥–µ–ª–∏."""
        logger.info("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA
        memory_before_lora = 0.0
        if torch.cuda.is_available():
            memory_before_lora = torch.cuda.memory_allocated() / (1024 ** 2)
        
        try:
            from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
            
            # –í–∫–ª—é—á–∞–µ–º gradient checkpointing –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–ª—è –≤—Å–µ—Ö LoRA —Ä–µ–∂–∏–º–æ–≤ (–∫–∞–∫ –≤ re-grpo)
            # –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç: casting layernorm to fp32, enabling gradient checkpointing, input_require_grads
            logger.info("üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è LoRA training (prepare_model_for_kbit_training)...")
            self.model.gradient_checkpointing_enable()
            self.model = prepare_model_for_kbit_training(
                self.model,
                use_gradient_checkpointing=True,
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (–µ—Å–ª–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–æ—Å—å)
            if self.config.use_4bit or self.config.use_8bit:
                try:
                    from transformers import BitsAndBytesConfig
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    quantized_params = sum(
                        1 for p in self.model.parameters() 
                        if hasattr(p, 'quant_state') or str(p.dtype) == 'torch.uint8'
                    )
                    if quantized_params > 0:
                        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: –Ω–∞–π–¥–µ–Ω–æ {quantized_params} –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                    else:
                        logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ BitsAndBytesConfig.")
                except:
                    pass
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º "all-linear" –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥—É–ª–µ–π (–∫–∞–∫ –≤ re-grpo)
            # –≠—Ç–æ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ —á–µ–º —Ä—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
            if isinstance(self.config.lora_target_modules, list) and len(self.config.lora_target_modules) > 0:
                target_modules = self.config.lora_target_modules
                logger.info(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º target_modules –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞: {target_modules}")
            else:
                # Fallback –Ω–∞ "all-linear" –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
                target_modules = "all-linear"
                logger.info("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º target_modules='all-linear' –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥—É–ª–µ–π")
            
            # –í–ê–ñ–ù–û: –í–∞–ª–∏–¥–∞—Ü–∏—è LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã (–±–µ–∑ fallback)
            lora_r = self.config.lora_r
            lora_alpha = self.config.lora_alpha
            lora_dropout = self.config.lora_dropout
            
            # –°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã None - —ç—Ç–æ –æ—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            if lora_r is None:
                raise ValueError(
                    "‚ùå lora_r = None! "
                    "lora_r –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_r."
                )
            
            if lora_alpha is None:
                raise ValueError(
                    "‚ùå lora_alpha = None! "
                    "lora_alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_alpha."
                )
            
            # lora_dropout –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç –∏–∑ GRPOConfig (0.1)
            if lora_dropout is None:
                lora_dropout = self.config.lora_dropout  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç –∏–∑ dataclass
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –∏ –∑–Ω–∞—á–µ–Ω–∏–π
            if not isinstance(lora_r, int) or lora_r <= 0:
                raise ValueError(
                    f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π lora_r: {lora_r} (—Ç–∏–ø: {type(lora_r)}). "
                    f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ. "
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤ UI –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–∏—Å–ª–æ, –∞ –Ω–µ None –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞."
                )
            
            if not isinstance(lora_alpha, (int, float)) or lora_alpha <= 0:
                raise ValueError(
                    f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π lora_alpha: {lora_alpha} (—Ç–∏–ø: {type(lora_alpha)}). "
                    f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ. "
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤ UI –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–∏—Å–ª–æ, –∞ –Ω–µ None –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞."
                )
            
            logger.info(f"üîß –°–æ–∑–¥–∞–Ω–∏–µ LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
            logger.info(f"  - r (rank): {lora_r}")
            logger.info(f"  - alpha: {lora_alpha}")
            logger.info(f"  - dropout: {lora_dropout}")
            logger.info(f"  - target_modules: {target_modules}")
            
            lora_config = LoraConfig(
                r=lora_r,
                lora_alpha=lora_alpha,
                target_modules=target_modules,
                lora_dropout=lora_dropout,
                bias="none",
                task_type="CAUSAL_LM",
            )
            
            logger.info("üì¶ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∫ –º–æ–¥–µ–ª–∏...")
            self.model = get_peft_model(self.model, lora_config)
            logger.info("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω—ã!")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA
            if torch.cuda.is_available():
                memory_after_lora = torch.cuda.memory_allocated() / (1024 ** 2)
                logger.info(f"üíæ –ü–∞–º—è—Ç—å CUDA –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA: {memory_after_lora:.1f} MB (+{memory_after_lora - memory_before_lora:.1f} MB)")
            
            # PEFT –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
            logger.info("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö (–æ—Ç PEFT):")
            self.model.print_trainable_parameters()
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–æ–ª—å–∫–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã trainable
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())
            frozen_params = total_params - trainable_params
            trainable_percent = 100 * trainable_params / total_params if total_params > 0 else 0
            
            logger.info(f"üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
            logger.info(f"  - –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
            logger.info(f"  - Trainable (LoRA): {trainable_params:,} ({trainable_percent:.2f}%)")
            logger.info(f"  - Frozen (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å): {frozen_params:,} ({100 - trainable_percent:.2f}%)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–æ–ª—å–∫–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            non_lora_trainable = 0
            lora_trainable = 0
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    if 'lora' in name.lower():
                        lora_trainable += param.numel()
                    else:
                        non_lora_trainable += param.numel()
            
            if non_lora_trainable > 0:
                logger.warning(
                    f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {non_lora_trainable:,} trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ë–ï–ó 'lora' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏! "
                    f"–≠—Ç–æ –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å —á—Ç–æ LoRA –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ."
                )
            else:
                logger.info(f"‚úÖ –í—Å–µ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã - —ç—Ç–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã ({lora_trainable:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
            
            if trainable_percent > 5.0:
                logger.warning(
                    f"‚ö†Ô∏è  –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({trainable_percent:.2f}%)! "
                    f"–í–æ–∑–º–æ–∂–Ω–æ LoRA –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –û–∂–∏–¥–∞–µ—Ç—Å—è < 1% –¥–ª—è LoRA."
                )
            elif trainable_percent < 0.1:
                logger.warning(
                    f"‚ö†Ô∏è  –°–ª–∏—à–∫–æ–º –º–∞–ª–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({trainable_percent:.2f}%)! "
                    f"–í–æ–∑–º–æ–∂–Ω–æ LoRA –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ."
                )
            else:
                logger.info(f"‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –Ω–æ—Ä–º–µ ({trainable_percent:.2f}%)")
            
        except ImportError:
            logger.warning("peft –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, LoRA –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            self.config.use_lora = False
    
    def _setup_wandb(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç Weights & Biases –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        try:
            import wandb
            
            self.wandb_run = wandb.init(
                project=self.config.wandb_project,
                config=self.config.to_dict(),
                name=f"grpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            )
            logger.info(f"W&B –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {wandb.run.name}")
            
        except ImportError:
            logger.warning("wandb –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            self.config.use_wandb = False
    
    def _setup_optimizer(self, num_training_steps: int):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ scheduler."""
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        num_trainable = sum(p.numel() for p in trainable_params)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        logger.info(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞:")
        logger.info(f"  - Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_trainable:,} / {total_params:,} ({100*num_trainable/total_params:.2f}%)")
        logger.info(f"  - –ì—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(trainable_params)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if len(trainable_params) == 0:
            raise RuntimeError("‚ùå –ù–µ—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.")
        
        try:
            from bitsandbytes.optim import AdamW8bit
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è AdamW8bit (8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)")
            self.optimizer = AdamW8bit(
                trainable_params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
            )
        except ImportError:
            logger.info("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π AdamW (bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)")
            self.optimizer = torch.optim.AdamW(
                trainable_params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
            )
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        # AdamW —Ö—Ä–∞–Ω–∏—Ç: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (fp16), momentum (fp16), variance (fp16) = 3x trainable_params
        optimizer_memory_mb = (num_trainable * 3 * 2) / (1024 ** 2)  # 3 —Å–æ—Å—Ç–æ—è–Ω–∏—è * 2 bytes (fp16)
        logger.info(f"üíæ –ü—Ä–∏–º–µ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: ~{optimizer_memory_mb:.1f} MB")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        optimizer_param_count = sum(p.numel() for group in self.optimizer.param_groups for p in group['params'])
        if optimizer_param_count != num_trainable:
            logger.warning(
                f"‚ö†Ô∏è  –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç {optimizer_param_count:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, "
                f"–∞ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {num_trainable:,}"
            )
        else:
            logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ({optimizer_param_count:,})")
        
        # Scheduler
        # –í–ê–ñ–ù–û: scheduler.step() –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ optimizer-step, –ø–æ—ç—Ç–æ–º—É num_training_steps –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ optim-—à–∞–≥–∞—Ö.
        min_lr_ratio = float(getattr(self.config, "min_lr_ratio", 0.0) or 0.0)
        if min_lr_ratio > 0:
            from torch.optim.lr_scheduler import LambdaLR
            warmup = int(self.config.warmup_steps or 0)
            total = max(int(num_training_steps), 1)

            def lr_lambda(step: int):
                # warmup: 0 -> 1
                if warmup > 0 and step < warmup:
                    return float(step) / float(max(1, warmup))
                # cosine with floor
                denom = max(1, total - warmup)
                progress = float(step - warmup) / float(denom)
                progress = min(max(progress, 0.0), 1.0)
                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))
                return min_lr_ratio + (1.0 - min_lr_ratio) * cosine

            self.scheduler = LambdaLR(self.optimizer, lr_lambda=lr_lambda)
        else:
            self.scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=self.config.warmup_steps,
                num_training_steps=num_training_steps,
            )
        
        # Accelerate prepare
        if self.accelerator:
            self.model, self.optimizer, self.scheduler = self.accelerator.prepare(
                self.model, self.optimizer, self.scheduler
            )
            try:
                def _strip_fp32_convert(m):
                    if m is None:
                        return
                    fwd = getattr(m, "forward", None)
                    if fwd is not None and hasattr(fwd, "model_forward"):
                        m.forward = fwd.model_forward  # type: ignore[attr-defined]

                # accelerate –º–æ–∂–µ—Ç –Ω–∞–≤–µ—Å–∏—Ç—å ConvertOutputsToFp32 –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –æ–±—ë—Ä—Ç–æ–∫
                _strip_fp32_convert(self.model)
                _strip_fp32_convert(getattr(self.model, "module", None))
                base = self.accelerator.unwrap_model(self.model)
                _strip_fp32_convert(base)
                _strip_fp32_convert(getattr(base, "module", None))
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫–ª—é—á–∏—Ç—å accelerate convert_to_fp32: {e}")
    
    def train(
        self,
        dataset: RLDataset,
        eval_dataset: Optional[RLDataset] = None,
    ):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è GRPO.
        
        Args:
            dataset: –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            eval_dataset: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.setup()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        num_prompts = len(dataset)
        # –û—Ü–µ–Ω–∫–∞ rollout-—à–∞–≥–æ–≤ (–¥–ª—è –ª–æ–≥–æ–≤/—à–µ–¥—É–ª–µ—Ä–∞). –í multi-gpu –≥–ª–æ–±–∞–ª—å–Ω–æ –∑–∞ —à–∞–≥ –ø—Ä–æ—Ö–æ–¥–∏—Ç batch_size * num_processes.
        world = int(self.accelerator.num_processes) if self.accelerator is not None else 1
        denom = max(int(self.config.batch_size) * max(world, 1), 1)
        steps_per_epoch = math.ceil(num_prompts / denom)
        total_steps_uncapped = steps_per_epoch * self.config.num_epochs
        
        # –õ–∏–º–∏—Ç "–ø–æ –¥–∞–Ω–Ω—ã–º": —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ —Ä–µ–∞–ª—å–Ω–æ —Ö–æ—Ç–∏–º –ø—Ä–æ–π—Ç–∏ (–ø–æ–Ω—è—Ç–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞).
        planned_prompts = int(num_prompts) * int(self.config.num_epochs)
        if getattr(self.config, "max_prompts", None):
            try:
                planned_prompts = min(planned_prompts, int(self.config.max_prompts))
            except Exception:
                pass
        rollout_total_steps = math.ceil(planned_prompts / denom) if planned_prompts > 0 else 0
        
        if self.config.max_steps:
            rollout_total_steps = rollout_total_steps  # max_steps ‚Äî —ç—Ç–æ –ª–∏–º–∏—Ç optim_step, –Ω–µ rollout_step
        
        # –î–ª—è UI/ETA: —Ñ–∏–∫—Å–∏—Ä—É–µ–º –ø–ª–∞–Ω–æ–≤—ã–µ —à–∞–≥–∏ (–Ω–µ "max_steps", –∞ —Ä–µ–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç/–ª–∏–º–∏—Ç).
        self.planned_total_steps = int(rollout_total_steps) if rollout_total_steps else 0
        self.planned_total_steps_uncapped = int(total_steps_uncapped) if total_steps_uncapped else 0

        # –î–ª—è scheduler: –æ—Ü–µ–Ω–∏–≤–∞–µ–º —á–∏—Å–ª–æ optimizer steps.
        # 1 rollout (–Ω–∞ –û–î–ò–ù –ø—Ä–æ—Ü–µ—Å—Å) –¥–∞—ë—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ batch_size * group_size –æ–ø—ã—Ç–æ–≤.
        # exp_loader drop_last=True => —á–∏—Å–ª–æ –º–∏–∫—Ä–æ–±–∞—Ç—á–µ–π = floor(exps / train_batch_size)
        est_exps = int(self.config.batch_size) * int(self.config.group_size)
        est_micro_batches = max(1, est_exps // max(1, int(self.config.train_batch_size)))
        est_optim_steps_per_rollout = math.ceil(est_micro_batches / max(1, int(self.config.gradient_accumulation_steps)))
        est_optim_steps_per_rollout *= max(1, int(self.config.epochs_per_step))

        planned_optim_steps = int(rollout_total_steps) * int(est_optim_steps_per_rollout)
        if self.config.max_steps:
            # max_steps ‚Äî —è–≤–Ω—ã–π –ª–∏–º–∏—Ç optim_step –∏–∑ UI
            planned_optim_steps = min(int(planned_optim_steps), int(self.config.max_steps))
        self.planned_optim_total_steps = max(int(planned_optim_steps), 1)
        
        logger.info(
            f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {num_prompts} –ø—Ä–æ–º–ø—Ç–æ–≤, ~{int(rollout_total_steps)} rollout-—à–∞–≥–æ–≤, "
            f"~{int(self.planned_optim_total_steps)} optim-—à–∞–≥–æ–≤"
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        self._setup_optimizer(self.planned_optim_total_steps)
        
        # –°–æ–∑–¥–∞—ë–º output –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # DataLoader –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
        prompt_loader = DataLoader(
            list(range(len(dataset))),
            batch_size=self.config.batch_size,
            shuffle=True,
            drop_last=False,
        )
        # –í–ê–ñ–ù–û (–∫–∞–∫ –≤ re-grpo accelerate): –ø—Ä–∏ multi-gpu –¥–µ–ª–∏–º –ø—Ä–æ–º–ø—Ç—ã –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
        if self.accelerator is not None:
            prompt_loader = self.accelerator.prepare(prompt_loader)
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
        for epoch in range(self.config.num_epochs):
            epoch_metrics = self._train_epoch(
                dataset=dataset,
                prompt_loader=prompt_loader,
                epoch=epoch,
                eval_dataset=eval_dataset,
            )
            
            logger.info(f"Epoch {epoch + 1}/{self.config.num_epochs} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            logger.info(f"  Mean reward: {epoch_metrics.get('mean_reward', 0):.4f}")
            
            if self.config.max_steps and self.global_step >= self.config.max_steps:
                logger.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç max_steps, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
                break
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if self.is_main_process:
            self._save_checkpoint(output_dir / "final", is_final=True)
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        if self.wandb_run:
            self.wandb_run.finish()
    
    def _train_epoch(
        self,
        dataset: RLDataset,
        prompt_loader: DataLoader,
        epoch: int,
        eval_dataset: Optional[RLDataset] = None,
    ) -> Dict[str, float]:
        """–û–¥–∏–Ω epoch –æ–±—É—á–µ–Ω–∏—è."""
        epoch_rewards = []
        epoch_losses = []
        
        pbar = tqdm(
            prompt_loader,
            desc=f"Epoch {epoch + 1}",
            disable=not self.is_main_process,
        )
        
        for batch_idx, prompt_indices in enumerate(pbar):
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –∏ –æ—Ç–≤–µ—Ç—ã
            batch_samples = [dataset[i] for i in prompt_indices]
            prompts = [
                build_reasoning_prompt(
                    s.prompt,
                    self.tokenizer,
                    self.config.reasoning_format,
                )
                for s in batch_samples
            ]
            reference_answers = [s.reference_answer for s in batch_samples]
            # group_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã (–æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ dynamic sampling —Å –¥–æ–±–æ—Ä–æ–º)
            desired_groups = len(batch_samples)
            group_ids = self._next_group_uids(desired_groups)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è rollout'–æ–≤
            self.replay_buffer.clear()
            batch_rewards = self._generate_and_collect(
                prompts=prompts,
                reference_answers=reference_answers,
                prompt_ids=group_ids,
            )
            refill_rounds = 0
            # DAPO dynamic sampling: –¥–æ–±–æ—Ä –≥—Ä—É–ø–ø –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (–ù–ï —É–º–µ–Ω—å—à–∞–µ–º batch –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            if self.config.dynamic_sampling:
                import random
                max_refill_rounds = 8  # –∑–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
                while self.replay_buffer.get_stats().get("num_groups", 0) < desired_groups and refill_rounds < max_refill_rounds:
                    missing = desired_groups - int(self.replay_buffer.get_stats().get("num_groups", 0))
                    if missing <= 0:
                        break
                    # –¥–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã (—Å replacement –¥–æ–ø—É—Å—Ç–∏–º–æ, –Ω–æ group_id —É–Ω–∏–∫–∞–ª—å–Ω—ã–π)
                    extra_indices = [random.randrange(0, len(dataset)) for _ in range(missing)]
                    extra_samples = [dataset[i] for i in extra_indices]
                    extra_prompts = [
                        build_reasoning_prompt(s.prompt, self.tokenizer, self.config.reasoning_format)
                        for s in extra_samples
                    ]
                    extra_refs = [s.reference_answer for s in extra_samples]
                    extra_group_ids = self._next_group_uids(len(extra_samples))
                    extra_rewards = self._generate_and_collect(
                        prompts=extra_prompts,
                        reference_answers=extra_refs,
                        prompt_ids=extra_group_ids,
                    )
                    batch_rewards.extend(extra_rewards)
                    refill_rounds += 1

                if self.replay_buffer.get_stats().get("num_groups", 0) < desired_groups:
                    logger.warning(
                        f"‚ö†Ô∏è dynamic_sampling: –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±—Ä–∞—Ç—å –≥—Ä—É–ø–ø—ã –¥–æ {desired_groups}. "
                        f"–ü–æ–ª—É—á–∏–ª–æ—Å—å {self.replay_buffer.get_stats().get('num_groups', 0)} –ø–æ—Å–ª–µ {refill_rounds} –¥–æ–±–æ—Ä–æ–≤. "
                        f"–í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞: –º–æ–¥–µ–ª—å –¥–∞—ë—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π reward –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤."
                    )

            epoch_rewards.extend(batch_rewards)
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω–æ–º –æ–ø—ã—Ç–µ
            buffer_size = len(self.replay_buffer)
            if buffer_size == 0:
                logger.warning(
                    f"‚ö†Ô∏è –ë—É—Ñ–µ—Ä –ø—É—Å—Ç –Ω–∞ —à–∞–≥–µ {self.global_step}! "
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ dynamic_sampling –∏ reward —Ñ—É–Ω–∫—Ü–∏—é."
                )
                train_metrics = {"loss": 0.0, "kl": 0.0, "grad_norm": 0.0}
            else:
                train_metrics = self._train_on_buffer()
            
            epoch_losses.append(train_metrics.get("loss", 0))
            
            # ---- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ----
            # –í–ê–ñ–ù–û: UI –¥–æ–ª–∂–µ–Ω –ø–æ–ª—É—á–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ä–∞–∑—É, –∏–Ω–∞—á–µ –æ–Ω "–∑–∞–≤–∏—Å–∞–µ—Ç" –Ω–∞ STARTING.
            # –ü–æ—ç—Ç–æ–º—É –ø–∏—à–µ–º heartbeat –º–µ—Ç—Ä–∏–∫–∏ –ö–ê–ñ–î–´–ô rollout, –∞ –≤ –∫–æ–Ω—Å–æ–ª—å/W&B ‚Äî –ø–æ log_steps.
            batch_reward_mean = sum(batch_rewards) / len(batch_rewards) if batch_rewards else 0.0
            group_size = max(int(self.config.group_size), 1)
            prompts_generated = int(len(batch_rewards) // group_size) if group_size > 0 else 0
            num_groups_used = int(self.replay_buffer.get_stats().get("num_groups", 0))
            completions_generated = int(len(batch_rewards))
            experiences_tuned = int(len(self.replay_buffer))
            filtered_groups = max(0, prompts_generated - num_groups_used)

            # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Å—á—ë—Ç—á–∏–∫–∏ (–Ω–∞ –∫–∞–∂–¥—ã–π rollout, —á—Ç–æ–±—ã UI –ø–æ–∫–∞–∑—ã–≤–∞–ª "–ø–æ —Ñ–∞–∫—Ç—É")
            self.cum_prompts_generated += prompts_generated
            self.cum_prompts_used += num_groups_used
            self.cum_completions_generated += completions_generated
            self.cum_experiences_tuned += experiences_tuned

            heartbeat = {
                "step": self.global_step,
                "epoch": epoch,
                "batch_reward_mean": batch_reward_mean,
                "buffer_size": buffer_size,
                "rollouts_count": len(batch_rewards),
                "prompts_generated": prompts_generated,
                "prompts_used": num_groups_used,
                "filtered_groups": filtered_groups,
                "completions_generated": completions_generated,
                "experiences_tuned": experiences_tuned,
                "refill_rounds": refill_rounds,
                "cum_prompts_generated": int(self.cum_prompts_generated),
                "cum_prompts_used": int(self.cum_prompts_used),
                "cum_completions_generated": int(self.cum_completions_generated),
                "cum_experiences_tuned": int(self.cum_experiences_tuned),
                **train_metrics,
            }

            # –ü–∏—à–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∂–¥—ã–π rollout (–¥–ª—è UI), –∞ –∫–æ–Ω—Å–æ–ª—å/W&B ‚Äî —Ç–æ–ª—å–∫–æ –ø–æ log_steps.
            should_log = (self.global_step % max(int(self.config.log_steps), 1) == 0)
            self._log_metrics(heartbeat, jsonl_only=(not should_log))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º progress bar
            pbar.set_postfix({
                "reward": f"{sum(batch_rewards) / max(len(batch_rewards), 1):.3f}",
                "loss": f"{train_metrics.get('loss', 0):.4f}",
            })
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            if self.global_step > 0 and self.global_step % self.config.save_steps == 0:
                # –í–ê–ñ–ù–û: –≤ distributed —Ä–µ–∂–∏–º–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –í–°–ï–ú–ò –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏,
                # –∏–Ω–∞—á–µ –≤–æ–∑–º–æ–∂–Ω—ã —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏/—Ç–∞–π–º–∞—É—Ç—ã –Ω–∞ collectives.
                self._save_checkpoint(Path(self.config.output_dir) / f"step_{self.global_step}")

            # Rollout-step –∑–∞–≤–µ—Ä—à—ë–Ω (1 batch –ø—Ä–æ–º–ø—Ç–æ–≤ -> —Å–±–æ—Ä rollout -> train on buffer)
            self.rollout_step += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º max_steps
            if self.config.max_steps and self.global_step >= self.config.max_steps:
                break

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –ø–æ –¥–∞–Ω–Ω—ã–º (—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å)
            if getattr(self.config, "max_prompts", None):
                try:
                    world = int(self.accelerator.num_processes) if self.accelerator is not None else 1
                    prompts_seen = int(self.rollout_step) * int(self.config.batch_size) * max(world, 1)
                    if prompts_seen >= int(self.config.max_prompts):
                        logger.info(
                            f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç max_prompts={int(self.config.max_prompts)} "
                            f"(–æ—Ü–µ–Ω–∫–∞ prompts_seen={prompts_seen}), –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ"
                        )
                        break
                except Exception:
                    # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ —Ç–∞–∫ ‚Äî –Ω–µ –ª–æ–º–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
                    pass
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if eval_dataset and self.is_main_process:
            eval_metrics = self._evaluate(eval_dataset)
            logger.info(f"Validation: {eval_metrics}")
            self._log_metrics({"val/" + k: v for k, v in eval_metrics.items()})
        
        return {
            "mean_reward": sum(epoch_rewards) / max(len(epoch_rewards), 1),
            "mean_loss": sum(epoch_losses) / max(len(epoch_losses), 1),
        }
    
    def _generate_and_collect(
        self,
        prompts: List[str],
        reference_answers: List[str],
        prompt_ids: Optional[List[int]] = None,
    ) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç rollout'—ã –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –æ–ø—ã—Ç –≤ –±—É—Ñ–µ—Ä.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö rewards
        """
        self.model.eval()
        all_rewards = []
        
        # –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–∏
        def reward_wrapper(completion, reference_answer, reasoning_format, is_truncated):
            return self.reward_fn(
                completion=completion,
                reference_answer=reference_answer,
                reasoning_format=reasoning_format,
                is_truncated=is_truncated,
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º rollout'—ã
        # –í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º accelerator –¥–ª—è unwrap –º–æ–¥–µ–ª–∏ (DDP –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç generate –Ω–∞–ø—Ä—è–º—É—é)
        rollouts = generate_rollouts(
            model=self.model,
            tokenizer=self.tokenizer,
            prompts=prompts,
            reference_answers=reference_answers,
            reward_fn=reward_wrapper,
            config=self.config,
            accelerator=self.accelerator,
            reference_model=self.reference_model,
            device=self.device,
            prompt_ids=prompt_ids,
        )
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Experience –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä
        # –í–ê–ñ–ù–û: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ —Å—Ä–∞–∑—É —É–¥–∞–ª—è–µ–º, —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ç—å –ø–∞–º—è—Ç—å
        num_rollouts = len(rollouts)
        for i in range(num_rollouts):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ —É–¥–∞–ª—è–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞, —á—Ç–æ–±—ã –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —Å—Å—ã–ª–∫—É
            rollout = rollouts.pop(0)
            
            # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º rewards –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –¥–∞–∂–µ –µ—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞
            rollout_rewards = rollout.rewards.tolist()
            all_rewards.extend(rollout_rewards)
            
            # –û—Ç–ª–∞–¥–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö rollout'–æ–≤)
            if i < 2:
                logger.debug(
                    f"Rollout {rollout.metadata.get('prompt_idx', 0)}: "
                    f"rewards={[f'{r:.3f}' for r in rollout_rewards]}, "
                    f"mean={sum(rollout_rewards)/len(rollout_rewards):.3f}, "
                    f"completions_len={[len(c) for c in rollout.completions[:2]]}"
                )
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–µ–º–ø–ª—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏)
            if self.global_step % max(self.config.log_steps, 1) == 0 and rollout.metadata.get("prompt_idx", 0) == 0:
                self._log_sample(rollout)
            
            experiences = rollout_to_experiences(
                rollout=rollout,
                model=self.model,
                tokenizer=self.tokenizer,
                config=self.config,
                reference_model=self.reference_model,
                device=self.device,
                accelerator=self.accelerator,
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º rollout
            prompt_idx = rollout.metadata.get("prompt_id", rollout.metadata.get("prompt_idx", 0))
            rollout_completions_len = len(rollout.completions)
            
            # –Ø–≤–Ω–æ —É–¥–∞–ª—è–µ–º rollout –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            del rollout
            
            # –í–ê–ñ–ù–û: –ü–µ—Ä–µ–º–µ—â–∞–µ–º –æ–ø—ã—Ç—ã –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM (–∫–∞–∫ –≤ re-grpo)
            # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è Multi-GPU –∏ –±–æ–ª—å—à–∏—Ö –±—É—Ñ–µ—Ä–æ–≤
            cpu_device = torch.device("cpu")
            experiences_cpu = [exp.to(cpu_device) for exp in experiences]
            
            # Dynamic sampling: —Ñ–∏–ª—å—Ç—Ä—É–µ–º zero-gradient –≥—Ä—É–ø–ø—ã
            filter_zero = self.config.dynamic_sampling
            added = self.replay_buffer.append_group(
                experiences_cpu,
                prompt_id=prompt_idx,
                filter_zero_gradient=filter_zero,
            )
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º GPU –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –Ω–∞ CPU
            del experiences
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            if not added and filter_zero:
                logger.debug(
                    f"–ì—Ä—É–ø–ø–∞ {prompt_idx} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞ "
                    f"(zero-gradient, rewards={rollout_rewards})"
                )
            
            self.total_rollouts += rollout_completions_len
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ rewards
        if all_rewards:
            logger.debug(
                f"Batch rewards: mean={sum(all_rewards)/len(all_rewards):.4f}, "
                f"min={min(all_rewards):.4f}, max={max(all_rewards):.4f}, "
                f"count={len(all_rewards)}"
            )
        else:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç rewards –≤ batch! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ reward —Ñ—É–Ω–∫—Ü–∏—é –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.")
        
        return all_rewards
    
    def _train_on_buffer(self) -> Dict[str, float]:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω–æ–º –æ–ø—ã—Ç–µ –≤ –±—É—Ñ–µ—Ä–µ.
        
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        self.model.train()
        
        buffer_size = len(self.replay_buffer)
        if buffer_size == 0:
            logger.warning(
                "‚ö†Ô∏è –ë—É—Ñ–µ—Ä –ø—É—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ. "
                "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –≤—Å–µ –≥—Ä—É–ø–ø—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã (dynamic_sampling) –∏–ª–∏ –Ω–µ—Ç –æ–ø—ã—Ç–∞."
            )
            return {"loss": 0.0, "kl": 0.0, "grad_norm": 0.0}
        
        logger.debug(f"–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±—É—Ñ–µ—Ä–µ: {buffer_size} –æ–ø—ã—Ç–æ–≤")
        
        # DataLoader –¥–ª—è experience
        exp_loader = DataLoader(
            self.replay_buffer.items,
            batch_size=self.config.train_batch_size,
            shuffle=True,
            drop_last=True,
            collate_fn=join_experience_batch,
        )
        
        epoch_losses = []
        epoch_kls = []
        epoch_grad_norms = []
        
        from contextlib import nullcontext

        for epoch_idx in range(self.config.epochs_per_step):
            for batch_idx, exp_batch in enumerate(exp_loader):
                exp_batch = exp_batch.to(self.device)
                accumulate_ctx = (
                    self.accelerator.accumulate(self.model)
                    if self.accelerator is not None
                    else nullcontext()
                )
                
                # –í–ê–ñ–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ OOM
                batch_size = exp_batch.sequences.size(0)
                max_seq_len = exp_batch.sequences.size(1)
                total_tokens = batch_size * max_seq_len
                
                if batch_idx == 0 and epoch_idx == 0:
                    # –í–ê–ñ–ù–û: –î–ª—è DDP –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ unwrap –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ config
                    if self.accelerator is not None:
                        unwrapped_model = self.accelerator.unwrap_model(self.model)
                        vocab_size = unwrapped_model.config.vocab_size
                    else:
                        vocab_size = self.model.config.vocab_size
                    
                    estimated_logits_memory = total_tokens * vocab_size * 2 / (1024**3)  # GB (fp16)
                    logger.info(
                        f"üìä –†–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: "
                        f"batch_size={batch_size}, max_seq_len={max_seq_len}, "
                        f"total_tokens={total_tokens:,}, "
                        f"–ø—Ä–∏–º–µ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è logits: ~{estimated_logits_memory:.2f} GB"
                    )
                
                # –ó–∞—â–∏—Ç–∞ –æ—Ç –æ—á–µ–≤–∏–¥–Ω–æ–≥–æ OOM: –æ—Ü–µ–Ω–∏–≤–∞–µ–º –º–∏–Ω–∏–º—É–º –ø–æ–¥ logits + —Ä–∞–∑—É–º–Ω—ã–π overhead –∏ —Å–≤–µ—Ä—è–µ–º —Å–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç—å—é.
                # –≠—Ç–æ –ù–ï "–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞" ‚Äî –ø—Ä–æ—Å—Ç–æ —Ä–∞–Ω–Ω—è—è, –ø–æ–Ω—è—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
                if torch.cuda.is_available():
                    try:
                        free_bytes, total_bytes = torch.cuda.mem_get_info(self.device)
                        free_gb = free_bytes / (1024**3)
                        # logits fp16/bf16 + –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±—É—Ñ–µ—Ä—ã + –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ => –æ—á–µ–Ω—å –≥—Ä—É–±–æ 2.2x
                        # (–¥–ª—è Qwen —Å –±–æ–ª—å—à–∏–º vocab –∏ –¥–ª–∏–Ω–Ω–æ–π seq —ç—Ç–æ –±–ª–∏–∂–µ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏).
                        required_gb = estimated_logits_memory * 2.2
                        # –û—Å—Ç–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ –≤–æ–∑–¥—É—Ö–∞ –ø–æ–¥ allocator/—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é
                        if required_gb > free_gb * 0.9:
                            raise RuntimeError(
                                "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM –¥–ª—è —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è GRPO.\n"
                                f"  - train_batch_size={batch_size}\n"
                                f"  - max_seq_len={max_seq_len}\n"
                                f"  - –æ—Ü–µ–Ω–∫–∞ logits(fp16/bf16)‚âà{estimated_logits_memory:.2f} GB\n"
                                f"  - –æ—Ü–µ–Ω–∫–∞ –ø–∏–∫–∞ (—Å overhead)‚âà{required_gb:.2f} GB\n"
                                f"  - —Å–≤–æ–±–æ–¥–Ω–æ —Å–µ–π—á–∞—Å‚âà{free_gb:.2f} GB (–∏–∑ {total_bytes/(1024**3):.2f} GB)\n\n"
                                "–ß—Ç–æ –¥–µ–ª–∞—Ç—å (–±–µ–∑ –∞–≤—Ç–æ-–ø–æ–¥—Å—Ç—Ä–æ–µ–∫):\n"
                                "  - –£–º–µ–Ω—å—à–∏—Ç–µ **Train Batch Size** (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: 1‚Äì4)\n"
                                "  - –£–º–µ–Ω—å—à–∏—Ç–µ **Max new tokens**\n"
                                "  - –í–∫–ª—é—á–∏—Ç–µ **LoRA/QLoRA** –≤–º–µ—Å—Ç–æ full fine-tuning\n"
                                "  - –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–∫–ª—é—á–∏—Ç–µ/—É–≤–µ–ª–∏—á—å—Ç–µ gradient checkpointing (–µ—Å–ª–∏ –¥–æ–±–∞–≤–∏—Ç–µ –≤ UI)\n"
                            )
                    except Exception:
                        # –ï—Å–ª–∏ mem_get_info –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω/–ø–∞–¥–∞–µ—Ç ‚Äî –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–∏–µ
                        pass
                
                # Forward pass –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π API –¥–ª—è autocast (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º deprecated warning)
                # –í–ê–ñ–ù–û: autocast –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ CUDA
                mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                use_autocast = (self.accelerator is not None and torch.cuda.is_available() and mp != "no")
                
                if use_autocast:
                    amp_dtype = torch.bfloat16 if mp == "bf16" else torch.float16
                    autocast_context = torch.amp.autocast("cuda", enabled=True, dtype=amp_dtype)
                else:
                    from contextlib import nullcontext
                    autocast_context = nullcontext()
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –≤ train —Ä–µ–∂–∏–º–µ
                if not self.model.training:
                    self.model.train()
                
                # –í–ê–ñ–ù–û: –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ forward pass (–Ω–∞ —Å–ª—É—á–∞–π –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è)
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                with accumulate_ctx:
                    with autocast_context:
                        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–µ—Ä–µ–¥ —Ç—è–∂–µ–ª–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ª–æ–≥–∏—Ç–æ–≤
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            
                        log_probs = compute_log_probs(
                            self.model,
                            exp_batch.sequences,
                            exp_batch.attention_mask,
                            accelerator=self.accelerator,
                        )
                        
                        loss, metrics = self.loss_fn(
                            log_probs=log_probs,
                            experience=exp_batch,
                        )
                
                    # –í–ê–ñ–ù–û: –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ—Å–ª–µ forward pass
                    # –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
                    del log_probs
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–í–ï–†–ö–ò –ø–µ—Ä–µ–¥ backward
                    if not loss.isfinite():
                        logger.warning(f"Loss –Ω–µ finite: {loss.item()}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º batch")
                        continue
                    
                    if not loss.requires_grad:
                        # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                        trainable_count = sum(1 for p in self.model.parameters() if p.requires_grad)
                        total_count = sum(1 for _ in self.model.parameters())
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å forward pass
                        test_seq = exp_batch.sequences[:1, :5]
                        test_mask = exp_batch.attention_mask[:1, :5]
                        with torch.enable_grad():
                            test_output = self.model(input_ids=test_seq, attention_mask=test_mask)
                            test_logits_grad = test_output.logits.requires_grad
                        
                        raise RuntimeError(
                            f"‚ùå Loss –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤!\n"
                            f"  - loss.requires_grad: {loss.requires_grad}\n"
                            f"  - loss.dtype: {loss.dtype}\n"
                            f"  - –ú–æ–¥–µ–ª—å training: {self.model.training}\n"
                            f"  - Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_count}/{total_count}\n"
                            f"  - Test logits requires_grad: {test_logits_grad}\n"
                            f"  - use_lora: {self.config.use_lora}\n"
                            f"  - use_4bit: {self.config.use_4bit}\n"
                            f"  - use_8bit: {self.config.use_8bit}\n"
                            f"  - use_autocast: {use_autocast}\n"
                        )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º loss –¥–ª—è –º–µ—Ç—Ä–∏–∫ –ü–ï–†–ï–î backward
                    loss_value = loss.item()
                    
                    # Backward
                    if self.accelerator is not None:
                        self.accelerator.backward(loss)
                    else:
                        loss.backward()
                    
                    # –í–ê–ñ–ù–û: –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º loss –ø–æ—Å–ª–µ backward –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    del loss
                    if torch.cuda.is_available() and batch_idx % 5 == 0:
                        torch.cuda.empty_cache()
                    
                    # Optimizer step –¥–µ–ª–∞–µ–º –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–∏ –Ω—É–∂–Ω–æ–µ —á–∏—Å–ª–æ micro-steps.
                    do_step = True
                    if self.accelerator is not None:
                        do_step = bool(self.accelerator.sync_gradients)
                    
                    if do_step:
                        # Gradient clipping
                        if self.accelerator is not None:
                            grad_norm = self.accelerator.clip_grad_norm_(
                                self.model.parameters(),
                                self.config.max_grad_norm,
                            )
                        else:
                            grad_norm = clip_grad_norm_(
                                self.model.parameters(),
                                self.config.max_grad_norm,
                            )
                        
                        self.optimizer.step()
                        self.scheduler.step()
                        self.optimizer.zero_grad()
                        
                        self.global_step += 1
                    else:
                        grad_norm = 0.0
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    epoch_losses.append(loss_value)
                    epoch_kls.append(metrics.get("kl_mean", 0))
                    epoch_grad_norms.append(
                        grad_norm.item() if torch.is_tensor(grad_norm) else grad_norm
                    )
        
        return {
            "loss": sum(epoch_losses) / max(len(epoch_losses), 1),
            "kl": sum(epoch_kls) / max(len(epoch_kls), 1),
            "grad_norm": sum(epoch_grad_norms) / max(len(epoch_grad_norms), 1),
        }
    
    @torch.no_grad()
    def _evaluate(
        self,
        dataset: RLDataset,
        max_samples: int = 100,
    ) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ."""
        self.model.eval()
        
        # –ë–µ—Ä—ë–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É
        indices = list(range(min(len(dataset), max_samples)))
        samples = [dataset[i] for i in indices]
        
        correct = 0
        total = 0
        rewards = []
        
        for sample in samples:
            prompt = build_reasoning_prompt(
                sample.prompt,
                self.tokenizer,
                self.config.reasoning_format,
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç (greedy)
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.config.max_prompt_length,
            ).to(self.device)
            
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                do_sample=False,  # Greedy –¥–ª—è eval
                pad_token_id=self.tokenizer.pad_token_id,
            )
            
            completion = self.tokenizer.decode(
                outputs[0, inputs["input_ids"].size(1):],
                skip_special_tokens=True,
            )
            
            reward = self.reward_fn(
                completion=completion,
                reference_answer=sample.reference_answer,
                reasoning_format=self.config.reasoning_format,
            )
            rewards.append(reward)
            
            if reward >= 0.5:  # Threshold –¥–ª—è "–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ" –æ—Ç–≤–µ—Ç–∞
                correct += 1
            total += 1
        
        return {
            "accuracy": correct / max(total, 1),
            "mean_reward": sum(rewards) / max(len(rewards), 1),
            "samples": total,
        }
    
    def _log_metrics(self, metrics: Dict[str, Any], *, jsonl_only: bool = False):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.

        –í–∞–∂–Ω–æ –¥–ª—è UI: `metrics.jsonl` –¥–æ–ª–∂–µ–Ω –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ, –∏–Ω–∞—á–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ "–∑–∞–≤–∏—Å–∞–µ—Ç" –Ω–∞ STARTING.
        –ü–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å JSONL –¥–∞–∂–µ —á–∞—Å—Ç–æ (–∫–∞–∂–¥—ã–π rollout), –∞ –∫–æ–Ω—Å–æ–ª—å/W&B ‚Äî —Ä–µ–∂–µ.
        """
        # –í distributed —Ä–µ–∂–∏–º–µ –ø–∏—à–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ —Å main –ø—Ä–æ—Ü–µ—Å—Å–∞, –∏–Ω–∞—á–µ jsonl –±—É–¥–µ—Ç –ø–µ—Ä–µ–º–µ—à–∞–Ω.
        if self.accelerator is not None and not self.accelerator.is_main_process:
            return
        if (not jsonl_only) and self.config.use_wandb and self.wandb_run:
            import wandb
            wandb.log(metrics, step=self.global_step)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ JSONL –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–∑ UI (–≤—Å–µ–≥–¥–∞ –Ω–∞ main process)
        metrics_file = Path(self.config.output_dir) / "metrics.jsonl"
        ui_metrics_file = None
        try:
            if getattr(self.config, "ui_run_dir", None):
                ui_metrics_file = Path(str(self.config.ui_run_dir)) / "metrics.jsonl"
        except Exception:
            ui_metrics_file = None
        try:
            import json
            from datetime import datetime
            log_entry = {
                    # optim_step: —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (—Ä–∞—Å—Ç—ë—Ç –≤–Ω—É—Ç—Ä–∏ _train_on_buffer)
                    "step": self.global_step,
                    "optim_step": self.global_step,
                    # rollout_step: —Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤ (prompts/step) —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                    "rollout_step": getattr(self, "rollout_step", 0),
                    # current_step –¥–ª—è UI: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ GRPO —Å—á–∏—Ç–∞–µ–º –ø–æ rollout_step (–ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞)
                    "current_step": int(getattr(self, "rollout_step", 0)),
                    # total_steps –¥–ª—è UI/ETA: –ø–ª–∞–Ω –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ (–Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç/–ª–∏–º–∏—Ç—ã), –∞ –Ω–µ —Ç–æ–ª—å–∫–æ max_steps.
                    "total_steps": int(getattr(self, "planned_total_steps", 0)) or None,
                    # planned_total_steps: "–ø–ª–∞–Ω –Ω–∞ —ç–ø–æ—Ö—É" –±–µ–∑ –ª–∏–º–∏—Ç–æ–≤ –ø–æ max_prompts/max_steps (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
                    "planned_total_steps": int(getattr(self, "planned_total_steps_uncapped", 0)) or None,
                    "reward": metrics.get("batch_reward_mean", metrics.get("reward", 0)),
                    "loss": metrics.get("loss", 0),
                    "kl": metrics.get("kl", 0),
                    "grad_norm": metrics.get("grad_norm", 0),
                    "epoch": metrics.get("epoch", 0),
                    "learning_rate": self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.config.learning_rate,
                    "timestamp": datetime.now().isoformat(),
                    # –î–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É/—Å–∫–æ—Ä–æ—Å—Ç–∏:
                    # batch_size –∑–¥–µ—Å—å = prompts/step –Ω–∞ –û–î–ò–ù –ø—Ä–æ—Ü–µ—Å—Å; –≤ UI —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ num_gpus (–∏–∑ config.json)
                    "prompt_batch_size": int(self.config.batch_size),
                    "group_size": int(self.config.group_size),
                    "train_batch_size": int(self.config.train_batch_size),
                    "epochs_per_step": int(self.config.epochs_per_step),
            }
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            for k, v in metrics.items():
                if k not in log_entry and isinstance(v, (int, float)):
                    log_entry[k] = v
            
            with open(metrics_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry) + "\n")
            # –î—É–±–ª–∏—Ä—É–µ–º –≤ run_dir UI (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω), —á—Ç–æ–±—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–ª –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø—É—Ç–µ–π output_dir.
            if ui_metrics_file is not None:
                ui_metrics_file.parent.mkdir(parents=True, exist_ok=True)
                with open(ui_metrics_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(log_entry) + "\n")
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ JSONL: {e}")

        # –¢–∞–∫–∂–µ –ª–æ–≥–∏—Ä—É–µ–º –≤ –∫–æ–Ω—Å–æ–ª—å (–æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏) ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        if (not jsonl_only) and self.is_main_process:
            log_str = " | ".join([
                f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}"
                for k, v in metrics.items()
                if k in ["step", "loss", "kl", "batch_reward_mean", "buffer_size", "rollouts_count"]
            ])
            if log_str:
                logger.info(f"Step {self.global_step}: {log_str}")
    
    def _log_sample(self, rollout):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å–µ–º–ø–ª (–ø—Ä–æ–º–ø—Ç –∏ –æ—Ç–≤–µ—Ç—ã) –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ UI."""
        if self.accelerator is not None and not self.accelerator.is_main_process:
            return
        try:
            import json
            from pathlib import Path
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ output_dir/samples.jsonl (UI –±—É–¥–µ—Ç —á–∏—Ç–∞—Ç—å –∏–∑ run_dir)
            samples_file = Path(self.config.output_dir) / "samples.jsonl"
            ui_samples_file = None
            try:
                if getattr(self.config, "ui_run_dir", None):
                    ui_samples_file = Path(str(self.config.ui_run_dir)) / "samples.jsonl"
            except Exception:
                ui_samples_file = None
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (–ø—Ä–æ–º–ø—Ç + completion) –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            full_texts = []
            for completion in rollout.completions:
                full_text = rollout.prompt + completion
                full_texts.append(full_text)
            
            sample_entry = {
                "step": self.global_step,
                "prompt": rollout.prompt,
                "reference_answer": rollout.metadata.get("reference_answer", ""),
                "completions": rollout.completions,
                "full_texts": full_texts,  # –ü—Ä–æ–º–ø—Ç + completion –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                "rewards": rollout.rewards.tolist(),
                "timestamp": datetime.now().isoformat(),
            }
            
            with open(samples_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(sample_entry, ensure_ascii=False) + "\n")
            # –î—É–±–ª–∏—Ä—É–µ–º –≤ run_dir UI (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)
            if ui_samples_file is not None:
                ui_samples_file.parent.mkdir(parents=True, exist_ok=True)
                with open(ui_samples_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(sample_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å —Å–µ–º–ø–ª: {e}")
    
    def _save_checkpoint(self, path: Path, is_final: bool = False):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç."""
        # DDP-safe —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ:
        # 1) –≤—Å–µ —Ä–∞–Ω–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –¥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        # 2) —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ main process
        # 3) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ç–æ–º–∞—Ä–Ω–æ–µ: –ø–∏—à–µ–º –≤ tmp-dir –∏ –¥–µ–ª–∞–µ–º rename
        # 4) –≤—Å–µ —Ä–∞–Ω–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        if self.accelerator is not None:
            self.accelerator.wait_for_everyone()

        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            tmp_path = path.with_name(path.name + "_tmp")

            # —á–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–π tmp (–µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –æ—Ç –ø–∞–¥–µ–Ω–∏—è) ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞ main
            if self.accelerator is None or self.is_main_process:
                if tmp_path.exists():
                    import shutil
                    shutil.rmtree(tmp_path, ignore_errors=True)
            # —Å–æ–∑–¥–∞—ë–º tmp-dir –Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö
            tmp_path.mkdir(parents=True, exist_ok=True)

            if self.accelerator is None:
                # Single-process: —Å–æ—Ö—Ä–∞–Ω—è–µ–º state –º–æ–¥–µ–ª–∏ –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ.
                self.model.save_pretrained(tmp_path)
            else:
                # Distributed (DDP/FSDP/DeepSpeed): —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è resume.
                self.accelerator.save_state(tmp_path)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ—Å—à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–ª—å–∫–æ –Ω–∞ main
            if self.accelerator is None or self.is_main_process:
                self.tokenizer.save_pretrained(tmp_path)
                import json
                with open(tmp_path / "grpo_config.json", "w", encoding="utf-8") as f:
                    json.dump(self.config.to_dict(), f, indent=2, ensure_ascii=False)

            # –í—Å–µ –¥–æ–∂–¥–∞–ª–∏—Å—å –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–æ–≤, –∑–∞—Ç–µ–º main –¥–µ–ª–∞–µ—Ç —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—é
            if self.accelerator is not None:
                self.accelerator.wait_for_everyone()

            if self.accelerator is None or self.is_main_process:
                if path.exists():
                    import shutil
                    shutil.rmtree(path, ignore_errors=True)
                tmp_path.rename(path)
                logger.info(f"–ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")

            # –û–±–Ω–æ–≤–ª—è–µ–º "usable" –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º final_model), –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ.
            if bool(getattr(self.config, "export_on_checkpoint", False)):
                final_dir = Path(self.config.output_dir) / "final_model"
                final_tmp = final_dir.with_name(final_dir.name + "_tmp")

                # —á–∏—Å—Ç–∏–º tmp –Ω–∞ main
                if self.accelerator is None or self.is_main_process:
                    if final_tmp.exists():
                        import shutil
                        shutil.rmtree(final_tmp, ignore_errors=True)
                final_tmp.mkdir(parents=True, exist_ok=True)

                if self.accelerator is None:
                    # single-process
                    self.model.save_pretrained(final_tmp, safe_serialization=True)
                else:
                    # distributed: —Å–æ–±—Ä–∞–Ω–Ω—ã–π state_dict —á–µ—Ä–µ–∑ accelerate (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –¥–ª—è FSDP/ZeRO)
                    self.accelerator.save_model(self.model, final_tmp, safe_serialization=True)

                if self.accelerator is None or self.is_main_process:
                    self.tokenizer.save_pretrained(final_tmp)
                if self.accelerator is not None:
                    self.accelerator.wait_for_everyone()

                if self.accelerator is None or self.is_main_process:
                    if final_dir.exists():
                        import shutil
                        shutil.rmtree(final_dir, ignore_errors=True)
                    final_tmp.rename(final_dir)
                    logger.info(f"final_model –æ–±–Ω–æ–≤–ª—ë–Ω: {final_dir}")
        finally:
            if self.accelerator is not None:
                self.accelerator.wait_for_everyone()
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: float = 0.7,
        do_sample: bool = True,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt: –í–æ–ø—Ä–æ—Å/–∑–∞–¥–∞—á–∞
            max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            do_sample: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        self.model.eval()
        
        formatted_prompt = build_reasoning_prompt(
            prompt,
            self.tokenizer,
            self.config.reasoning_format,
        )
        
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.config.max_prompt_length,
        ).to(self.device)
        
        # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DDP, –∏—Å–ø–æ–ª—å–∑—É–µ–º unwrapped –º–æ–¥–µ–ª—å –¥–ª—è generate()
        if self.accelerator is not None:
            unwrapped_model = self.accelerator.unwrap_model(self.model)
        elif hasattr(self.model, 'module'):
            unwrapped_model = self.model.module
        else:
            unwrapped_model = self.model
        
        with torch.no_grad():
            outputs = unwrapped_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens or self.config.max_new_tokens,
                temperature=temperature if do_sample else 1.0,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        completion = self.tokenizer.decode(
            outputs[0, inputs["input_ids"].size(1):],
            skip_special_tokens=True,
        )
        
        return completion
