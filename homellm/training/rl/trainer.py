"""
GRPOTrainer - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GRPO/RL.

–†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:
1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è rollout'–æ–≤ (completions)
2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ rewards –∏ advantages
3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ (–º–æ–¥–µ–ª–∏)
4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
"""
import logging
import math
import os
import random
from contextlib import contextmanager
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, List, Dict, Any, Callable, Union, Tuple
from datetime import datetime

import torch
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from tqdm import tqdm

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
    get_cosine_schedule_with_warmup,
)

from .legacy_config import GRPOConfig, RLAlgorithm
from .experience import Experience, ReplayBuffer, join_experience_batch
from .loss import GRPOLoss, SDPOLoss, LigerFusedGRPOLoss, compute_advantages, compute_entropy, create_loss_function
from .rollout import (
    generate_rollouts,
    generate_rollouts_vllm,
    rollout_to_experiences,
    build_reasoning_prompt,
    compute_log_probs,
)
from .rollout_engine import HFRolloutEngine, VLLMRolloutEngine, VLLMSubprocessEngine
from .rewards.base import RewardFunction, CombinedReward
from .rewards.math import GSM8KReward
from .rewards.format import FormatReward, ReasoningQualityReward
from .data.base import RLDataset, RLSample

logger = logging.getLogger(__name__)


def setup_logging(log_level: str = "INFO"):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    logging.basicConfig(
        level=getattr(logging, log_level),
        format="[%(asctime)s] [%(levelname)s] %(name)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def set_seed(seed: int):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏."""
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


class GRPOTrainer:
    """
    Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM —Å GRPO.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO
    - Dr.GRPO (–±–µ–∑ std –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
    - DAPO (clip higher, dynamic sampling)
    - LoRA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    - Multi-GPU —á–µ—Ä–µ–∑ accelerate
    - W&B –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    
    Example:
        >>> from homellm.training.rl import GRPOConfig, GRPOTrainer
        >>> from homellm.training.rl.data import load_gsm8k
        >>> 
        >>> config = GRPOConfig.from_preset("reasoning_small")
        >>> dataset = load_gsm8k(split="train", max_samples=1000)
        >>> 
        >>> trainer = GRPOTrainer(
        ...     model_name="Qwen/Qwen2.5-0.5B-Instruct",
        ...     config=config,
        ... )
        >>> trainer.train(dataset)
    """
    
    def __init__(
        self,
        model_name: str,
        config: Optional[GRPOConfig] = None,
        tokenizer: Optional[PreTrainedTokenizer] = None,
        reward_fn: Optional[RewardFunction] = None,
        device: Optional[torch.device] = None,
        use_accelerate: bool = True,
    ):
        """
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—É—Ç—å
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            reward_fn: –§—É–Ω–∫—Ü–∏—è reward (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            use_accelerate: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å accelerate –¥–ª—è multi-GPU
        """
        self.model_name = model_name
        self.config = config or GRPOConfig()
        self.use_accelerate = use_accelerate
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
        set_seed(self.config.seed)
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∏–∑ accelerator –≤ setup()
        # –ù–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º device –∑–¥–µ—Å—å, —á—Ç–æ–±—ã accelerator –º–æ–≥ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å multi-GPU
        self._device = device  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è fallback –µ—Å–ª–∏ accelerate –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = tokenizer
        self.model = None
        self.reference_model = None

        # Rollout engine (–æ—Ç–¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
        self.rollout_engine: Optional[HFRolloutEngine] = None
        self._rollout_last_sync_step: int = -10**9
        
        # Reward —Ñ—É–Ω–∫—Ü–∏—è
        if reward_fn is None:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è format + correctness
            self.reward_fn = CombinedReward([
                FormatReward(weight=1.0),
                ReasoningQualityReward(weight=0.5),
                GSM8KReward(weight=2.0),
            ])
        else:
            self.reward_fn = reward_fn
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ setup())
        self.optimizer = None
        self.scheduler = None
        self.loss_fn = None
        self.replay_buffer = None
        self.accelerator = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.global_step = 0
        # –û—Ç–¥–µ–ª—å–Ω—ã–π —Å—á—ë—Ç—á–∏–∫ –¥–ª—è rollout-–±–∞—Ç—á–µ–π (prompts/step). –ù—É–∂–µ–Ω –¥–ª—è –ø–æ–Ω—è—Ç–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
        self.rollout_step = 0
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è –≥—Ä—É–ø–ø (–Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å dataset index –ø—Ä–∏ dynamic sampling —Å –¥–æ–±–æ—Ä–æ–º)
        self._group_uid = 0
        # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Å—á—ë—Ç—á–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ "—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–æ—à–ª–æ"
        self.cum_prompts_generated = 0
        self.cum_prompts_used = 0
        self.cum_completions_generated = 0
        self.cum_experiences_tuned = 0

        # –ü—Ä–æ—á–∏–µ –º–µ—Ç—Ä–∏–∫–∏/—Å—Ç–∞—Ç—É—Å—ã
        self.total_rollouts = 0
        self.best_mean_reward = float("-inf")
        
        # W&B
        self.wandb_run = None

    # ---------------------------------------------------------------------
    # Rollout engine helpers
    # ---------------------------------------------------------------------
    def _get_train_module_for_sync(self) -> PreTrainedModel:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç "—Ä–µ–∞–ª—å–Ω—É—é" –º–æ–¥–µ–ª—å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ named_parameters/state_dict.
        –î–ª—è DDP/DeepSpeed –æ–±—ë—Ä—Ç–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º accelerator.unwrap_model.
        """
        if self.accelerator is not None:
            try:
                return self.accelerator.unwrap_model(self.model)  # type: ignore[arg-type]
            except Exception:
                pass
        # fallback
        return getattr(self.model, "module", self.model)

    def _sync_rollout_engine_weights(self, *, force: bool = False) -> None:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ training->rollout.
        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (LoRA), —á—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞–ª–æ –±—ã—Å—Ç—Ä–æ –∏ —Å ZeRO-3.
        """
        if not getattr(self.config, "use_rollout_engine", False):
            return
        if self.rollout_engine is None:
            return

        interval = max(int(getattr(self.config, "rollout_sync_interval", 1)), 1)
        if (not force) and (self.rollout_step - self._rollout_last_sync_step) < interval:
            return

        backend = getattr(self.config, "rollout_engine_backend", "hf")

        trainable_only = bool(getattr(self.config, "rollout_sync_trainable_only", True))
        train_mod = self._get_train_module_for_sync()

        state_dict = None

        # Distributed broadcast (rank0 -> all). –î–ª—è single-GPU: –ø—Ä–æ—Å—Ç–æ –ª–æ–∫–∞–ª—å–Ω–æ.
        is_dist = torch.distributed.is_available() and torch.distributed.is_initialized()
        rank = torch.distributed.get_rank() if is_dist else 0

        # vLLM backend: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä —á–µ—Ä–µ–∑ save_pretrained (rank0) –∏ broadcast path
        if backend == "vllm":
            if not bool(getattr(self.config, "use_lora", False)):
                raise RuntimeError("vLLM rollout backend —Å–µ–π—á–∞—Å –ø–æ–¥–¥–µ—Ä–∂–∞–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è LoRA (use_lora=True).")
            if not trainable_only:
                raise RuntimeError("vLLM rollout backend: full weight sync –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ trainable-only (LoRA).")

            adapter_path = None
            adapter_name = None
            adapter_int_id = None

            if rank == 0:
                # –°–æ–±–∏—Ä–∞–µ–º trainable params (LoRA) –Ω–∞ rank0 –ø—Ä–∏ ZeRO-3 –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä.
                try:
                    from peft.utils import get_peft_model_state_dict
                except Exception as e:
                    raise RuntimeError("peft –Ω—É–∂–µ–Ω –¥–ª—è vLLM LoRA sync. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ peft.") from e

                peft_model = train_mod
                if not hasattr(peft_model, "peft_config"):
                    raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ PEFT/LoRA –º–æ–¥–µ–ª—å, –Ω–æ use_lora=True. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É LoRA.")

                out_dir = Path(self.config.output_dir)
                out_dir.mkdir(parents=True, exist_ok=True)
                adapter_dir = out_dir / "rollout_engine" / "vllm_adapters" / f"step_{int(self.rollout_step)}"
                adapter_dir.mkdir(parents=True, exist_ok=True)

                # Gather trainable params for ZeRO-3
                params = [(n, p) for n, p in peft_model.named_parameters() if getattr(p, "requires_grad", False)]
                if getattr(self, "is_deepspeed_zero3", False) and params:
                    from deepspeed.runtime.zero.partition_parameters import GatheredParameters
                    with GatheredParameters([p for _, p in params], modifier_rank=0):
                        lora_sd = get_peft_model_state_dict(peft_model)
                else:
                    lora_sd = get_peft_model_state_dict(peft_model)

                # save_pretrained with provided state_dict avoids reading partitioned weights
                peft_model.save_pretrained(str(adapter_dir), state_dict=lora_sd, safe_serialization=True)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∞–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è
                adapter_config_path = adapter_dir / "adapter_config.json"
                if adapter_config_path.exists():
                    logger.info(f"üß© LoRA adapter saved to {adapter_dir}")
                    # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    saved_files = list(adapter_dir.iterdir())
                    logger.info(f"üß© Saved files: {[f.name for f in saved_files]}")
                else:
                    logger.error(f"üß© ERROR: adapter_config.json not found after save_pretrained!")

                adapter_path = str(adapter_dir)
                adapter_name = "rollout_lora"
                # –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ô id=1 —á—Ç–æ–±—ã vLLM –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–ª –∞–¥–∞–ø—Ç–µ—Ä
                # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ id –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏ vLLM –∏ –≤—ã–∑—ã–≤–∞—é—Ç CUDA OOM
                adapter_int_id = 1

            if is_dist:
                obj_list = [(adapter_path, adapter_name, adapter_int_id)]
                torch.distributed.broadcast_object_list(obj_list, src=0)
                adapter_path, adapter_name, adapter_int_id = obj_list[0]

            if adapter_path is None:
                raise RuntimeError("vLLM adapter sync failed: adapter_path is None")

            # Apply LoRA adapter to vLLM engine (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ —Ç–∏–ø–∞)
            if isinstance(self.rollout_engine, (VLLMRolloutEngine, VLLMSubprocessEngine)):
                self.rollout_engine.set_lora_adapter(
                    lora_path=adapter_path,
                    lora_name=adapter_name,
                    lora_int_id=int(adapter_int_id or 1),
                )
            else:
                raise RuntimeError(f"rollout_engine backend mismatch (expected VLLMRolloutEngine or VLLMSubprocessEngine, got {type(self.rollout_engine).__name__})")

            if self.is_main_process:
                logger.info(f"üß© RolloutEngine(vLLM) sync: adapter={adapter_path}")

            self._rollout_last_sync_step = int(self.rollout_step)
            return

        # HF backend: state_dict sync
        if rank == 0:
            if trainable_only:
                params = [(n, p) for n, p in train_mod.named_parameters() if getattr(p, "requires_grad", False)]
                if not params:
                    logger.warning("RolloutEngine sync: –Ω–µ—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, fallback -> full state_dict")
                    state_dict = {k: v.detach().cpu() for k, v in train_mod.state_dict().items()}
                else:
                    if getattr(self, "is_deepspeed_zero3", False):
                        try:
                            from deepspeed.runtime.zero.partition_parameters import GatheredParameters
                            with GatheredParameters([p for _, p in params], modifier_rank=0):
                                state_dict = {n: p.detach().cpu() for n, p in params}
                        except Exception as e:
                            logger.warning(f"RolloutEngine sync (ZeRO-3 trainable-only) failed: {e}. Fallback -> full state_dict")
                            state_dict = {k: v.detach().cpu() for k, v in train_mod.state_dict().items()}
                    else:
                        state_dict = {n: p.detach().cpu() for n, p in params}
            else:
                if self.accelerator is not None:
                    try:
                        state_dict = {k: v.detach().cpu() for k, v in self.accelerator.get_state_dict(self.model).items()}
                    except Exception:
                        state_dict = {k: v.detach().cpu() for k, v in train_mod.state_dict().items()}
                else:
                    state_dict = {k: v.detach().cpu() for k, v in train_mod.state_dict().items()}

        if is_dist:
            obj_list = [state_dict]
            torch.distributed.broadcast_object_list(obj_list, src=0)
            state_dict = obj_list[0]

        if state_dict is None:
            raise RuntimeError("RolloutEngine sync failed: state_dict is None")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º state_dict –≤ rollout engine (HF)
        strict = not trainable_only
        stats = self.rollout_engine.apply_state_dict(state_dict, strict=strict)

        if self.is_main_process:
            mode = "trainable-only" if trainable_only else "full"
            logger.info(
                f"üß© RolloutEngine sync: mode={mode}, keys={stats.synced_keys}, "
                f"~numel={stats.total_numel:,}, interval={interval}"
            )

        self._rollout_last_sync_step = int(self.rollout_step)

    def _create_loss_function(self) -> None:
        """
        –°–æ–∑–¥–∞—ë—Ç loss —Ñ—É–Ω–∫—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞.
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:
        - SDPO: SDPOLoss (GRPO + self-distillation)
        - GRPO/DrGRPO/DAPO —Å Liger: LigerFusedGRPOLoss
        - GRPO/DrGRPO/DAPO –±–µ–∑ Liger: GRPOLoss
        
        –í–ê–ñ–ù–û: –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ü–û–°–õ–ï accelerator.prepare() –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–ª—è Liger Fused Loss
        –Ω—É–∂–Ω–∞ unwrapped –º–æ–¥–µ–ª—å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ lm_head.weight.
        """
        # –ü–æ–ª—É—á–∞–µ–º unwrapped –º–æ–¥–µ–ª—å –¥–ª—è Liger (–Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∫ lm_head.weight)
        if self.accelerator:
            unwrapped_model = self.accelerator.unwrap_model(self.model)
        else:
            unwrapped_model = self.model
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        is_sdpo = (
            getattr(self.config, 'algorithm', None) == RLAlgorithm.SDPO or
            getattr(self.config, 'use_self_distillation', False)
        )
        
        if is_sdpo:
            # SDPO: –∏—Å–ø–æ–ª—å–∑—É–µ–º SDPOLoss
            self.loss_fn = SDPOLoss(config=self.config)
            logger.info("üéì SDPOLoss –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!")
            logger.info("   - GRPO loss + Self-Distillation")
            logger.info(f"   - success_threshold: {getattr(self.config, 'sdpo_success_threshold', 0.5)}")
            logger.info(f"   - alpha (KL type): {getattr(self.config, 'sdpo_alpha', 0.5)}")
            
            # ============================================================
            # üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Teacher Module Setup (–∏–∑ verl)
            # ============================================================
            self.sdpo_ema_rate = getattr(self.config, 'sdpo_ema_rate', 0.0)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é teacher:
            # 1. LoRA + EMA > 0: Teacher = Student —Å EMA LoRA –≤–µ—Å–∞–º–∏ (—á–µ—Ä–µ–∑ context manager)
            # 2. LoRA –±–µ–∑ EMA: Teacher = Student (detached)  
            # 3. Full FT + reference: Teacher = Reference Model
            # 4. Full FT –±–µ–∑ reference: Teacher = Student (detached)
            
            use_lora = getattr(self.config, 'use_lora', False)
            
            if use_lora:
                # –ü—Ä–∏ LoRA: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º reference_model –∫–∞–∫ teacher!
                # Reference model –Ω–µ –∏–º–µ–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã
                self.teacher_module = None  # –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è student —á–µ—Ä–µ–∑ context manager
                
                if self.sdpo_ema_rate > 0:
                    logger.info("   üî• Teacher = Student + EMA LoRA (—á–µ—Ä–µ–∑ context manager)")
                else:
                    logger.info("   ‚ÑπÔ∏è Teacher = Student (detached, EMA –æ—Ç–∫–ª—é—á–µ–Ω)")
            else:
                # Full Fine-tuning: –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å reference_model
                self.teacher_module = self.reference_model  # üî• –®–∞—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏!
                
                if self.teacher_module is not None:
                    logger.info("   üî• Teacher = Reference Model (—à–∞—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏!)")
                else:
                    logger.info("   ‚ö†Ô∏è –ù–µ—Ç reference –º–æ–¥–µ–ª–∏, teacher = student (detached)")
            
            # Top-K Distillation –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self.sdpo_distillation_topk = getattr(self.config, 'sdpo_distillation_topk', None)
            self.sdpo_full_logit_distillation = getattr(self.config, 'sdpo_full_logit_distillation', False)
            
            if self.sdpo_distillation_topk is not None:
                logger.info(f"   üî• Top-K Distillation: k={self.sdpo_distillation_topk}")
                logger.info(f"      –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏: ~99.97% vs full vocab!")
            
            if self.sdpo_ema_rate > 0:
                logger.info(f"   üìà EMA Teacher: rate={self.sdpo_ema_rate}")
                
                # üî• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º EMA –¥–ª—è LoRA (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
                if getattr(self.config, 'use_lora', False):
                    self._init_ema_for_lora()
                    logger.info("   üî• EMA LoRA —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —É—Å–ø–µ—à–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
            self._successful_trajectories: Dict[int, List[str]] = {}
            return
        
        # GRPO/DrGRPO/DAPO —Å Liger Fused Loss
        if self.use_liger_fused_loss:
            try:
                from .liger_utils import is_liger_available, get_liger_fused_linear_grpo
                
                if is_liger_available() and get_liger_fused_linear_grpo() is not None:
                    self.loss_fn = LigerFusedGRPOLoss(
                        model=unwrapped_model,
                        config=self.config,
                    )
                    logger.info("ü¶Å LigerFusedGRPOLoss –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!")
                    logger.info("   ‚ö° Logits –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É—é—Ç—Å—è ‚Äî —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏!")
                    return
                else:
                    logger.warning("‚ö†Ô∏è Liger –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LigerFusedGRPOLoss: {e}")
                logger.warning("   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss")
        
        # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss
        self.loss_fn = GRPOLoss(config=self.config)
        logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPOLoss")

    # =========================================================================
    # üéì SDPO: Teacher Model –∏ EMA Update (–∏–∑ verl)
    # =========================================================================
    
    def _init_ema_for_lora(self) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç EMA state dict –¥–ª—è LoRA –≤–µ—Å–æ–≤.
        
        –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ LoRA –º—ã —Ö—Ä–∞–Ω–∏–º EMA –∫–æ–ø–∏—é —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (~50-200 MB),
        –∞ –Ω–µ –≤—Å–µ–π –º–æ–¥–µ–ª–∏ (~–ì–ë). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å EMA Teacher —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º
        overhead –ø–æ –ø–∞–º—è—Ç–∏.
        """
        if not getattr(self.config, 'use_lora', False):
            return
        
        if not hasattr(self, 'sdpo_ema_rate') or self.sdpo_ema_rate <= 0:
            return
        
        # Unwrap –º–æ–¥–µ–ª–∏
        if self.accelerator:
            model = self.accelerator.unwrap_model(self.model)
        else:
            model = self.model
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ PEFT –º–æ–¥–µ–ª—å
        try:
            from peft import PeftModel
            if not isinstance(model, PeftModel):
                logger.warning("‚ö†Ô∏è use_lora=True, –Ω–æ –º–æ–¥–µ–ª—å –Ω–µ PeftModel. EMA LoRA –ø—Ä–æ–ø—É—â–µ–Ω.")
                return
        except ImportError:
            logger.warning("‚ö†Ô∏è peft –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. EMA LoRA –ø—Ä–æ–ø—É—â–µ–Ω.")
            return
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º EMA state dict –¥–ª—è LoRA –≤–µ—Å–æ–≤
        self._ema_lora_state_dict: Dict[str, torch.Tensor] = {}
        self._original_lora_state_dict: Dict[str, torch.Tensor] = {}  # –î–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        
        lora_param_count = 0
        lora_memory_bytes = 0
        
        for name, param in model.named_parameters():
            # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–º–µ—é—Ç 'lora_' –≤ –∏–º–µ–Ω–∏ –∏ —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            if 'lora_' in name.lower() and param.requires_grad:
                # –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞ –∫–∞–∫ –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ EMA
                self._ema_lora_state_dict[name] = param.data.clone().detach()
                lora_param_count += param.numel()
                lora_memory_bytes += param.numel() * param.element_size()
        
        if lora_param_count > 0:
            lora_memory_mb = lora_memory_bytes / (1024 ** 2)
            logger.info(f"‚úÖ EMA LoRA –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
            logger.info(f"   - LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {lora_param_count:,}")
            logger.info(f"   - EMA –ø–∞–º—è—Ç—å: ~{lora_memory_mb:.1f} MB")
            logger.info(f"   - EMA rate: {self.sdpo_ema_rate}")
        else:
            logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è EMA!")
            self._ema_lora_state_dict = {}
    
    def _update_teacher_ema(self) -> None:
        """
        üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: EMA Update –¥–ª—è Teacher –º–æ–¥–µ–ª–∏ (–∏–∑ verl).
        
        Teacher = EMA(Student) ‚Äî –º–µ–¥–ª–µ–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º–∞—è –∫–æ–ø–∏—è student –º–æ–¥–µ–ª–∏.
        –≠—Ç–æ –¥–∞—ë—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π target –¥–ª—è distillation.
        
        –§–æ—Ä–º—É–ª–∞: teacher = (1 - ema_rate) * teacher + ema_rate * student
        
        –î–ª—è LoRA: –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏!)
        –î–ª—è Full Fine-tuning: –æ–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã teacher –º–æ–¥–µ–ª–∏
        """
        if not hasattr(self, 'sdpo_ema_rate') or self.sdpo_ema_rate <= 0:
            return
        
        ema_rate = self.sdpo_ema_rate
        
        # Unwrap –º–æ–¥–µ–ª–∏
        if self.accelerator:
            student_model = self.accelerator.unwrap_model(self.model)
        else:
            student_model = self.model
        
        # ============================================================
        # –†–ï–ñ–ò–ú 1: LoRA ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ EMA LoRA –≤–µ—Å–æ–≤
        # ============================================================
        if getattr(self.config, 'use_lora', False) and hasattr(self, '_ema_lora_state_dict'):
            if not self._ema_lora_state_dict:
                return  # EMA –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
            
            with torch.no_grad():
                for name, param in student_model.named_parameters():
                    if name in self._ema_lora_state_dict:
                        # EMA update: ema = (1 - rate) * ema + rate * current
                        ema_tensor = self._ema_lora_state_dict[name]
                        student_data = param.data.to(device=ema_tensor.device, dtype=ema_tensor.dtype)
                        ema_tensor.mul_(1.0 - ema_rate).add_(student_data, alpha=ema_rate)
            
            logger.debug(f"üéì SDPO EMA LoRA –æ–±–Ω–æ–≤–ª—ë–Ω (rate={ema_rate}, params={len(self._ema_lora_state_dict)})")
            return
        
        # ============================================================
        # –†–ï–ñ–ò–ú 2: Full Fine-tuning ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º teacher_module
        # ============================================================
        if not hasattr(self, 'teacher_module') or self.teacher_module is None:
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ teacher != student (–∏–Ω–∞—á–µ EMA –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–µ–Ω)
        if self.teacher_module is self.model:
            return
        
        with torch.no_grad():
            for teacher_param, student_param in zip(
                self.teacher_module.parameters(),
                student_model.parameters()
            ):
                # EMA update: teacher = (1 - ema) * teacher + ema * student
                student_data = student_param.data.to(device=teacher_param.device)
                teacher_param.data.mul_(1.0 - ema_rate).add_(student_data, alpha=ema_rate)
        
        logger.debug(f"üéì SDPO EMA Teacher –æ–±–Ω–æ–≤–ª—ë–Ω (rate={ema_rate})")
    
    def _apply_ema_lora_weights(self) -> None:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç EMA LoRA –≤–µ—Å–∞ –∫ –º–æ–¥–µ–ª–∏ (—Å–æ—Ö—Ä–∞–Ω—è—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è).
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–¥ forward pass teacher –¥–ª—è SDPO.
        –ü–æ—Å–ª–µ forward pass –Ω—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å _restore_lora_weights().
        """
        if not hasattr(self, '_ema_lora_state_dict') or not self._ema_lora_state_dict:
            return
        
        # Unwrap –º–æ–¥–µ–ª–∏
        if self.accelerator:
            model = self.accelerator.unwrap_model(self.model)
        else:
            model = self.model
        
        self._original_lora_state_dict = {}
        
        with torch.no_grad():
            for name, param in model.named_parameters():
                if name in self._ema_lora_state_dict:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
                    self._original_lora_state_dict[name] = param.data.clone()
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º EMA –≤–µ—Å–∞
                    param.data.copy_(self._ema_lora_state_dict[name])
    
    def _restore_lora_weights(self) -> None:
        """
        –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ LoRA –≤–µ—Å–∞ –ø–æ—Å–ª–µ forward pass teacher.
        """
        if not hasattr(self, '_original_lora_state_dict') or not self._original_lora_state_dict:
            return
        
        # Unwrap –º–æ–¥–µ–ª–∏
        if self.accelerator:
            model = self.accelerator.unwrap_model(self.model)
        else:
            model = self.model
        
        with torch.no_grad():
            for name, param in model.named_parameters():
                if name in self._original_lora_state_dict:
                    param.data.copy_(self._original_lora_state_dict[name])
        
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self._original_lora_state_dict = {}
    
    @contextmanager
    def _with_ema_lora_weights(self):
        """
        Context manager –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è EMA LoRA –≤–µ—Å–æ–≤.
        
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
            with self._with_ema_lora_weights():
                output = model(input_ids)  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç EMA –≤–µ—Å–∞
            # –ó–¥–µ—Å—å –≤–µ—Å–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
        """
        use_ema_lora = (
            getattr(self.config, 'use_lora', False) and
            hasattr(self, '_ema_lora_state_dict') and
            bool(self._ema_lora_state_dict) and
            getattr(self, 'sdpo_ema_rate', 0) > 0
        )
        
        if use_ema_lora:
            self._apply_ema_lora_weights()
        
        try:
            yield
        finally:
            if use_ema_lora:
                self._restore_lora_weights()
    
    # =========================================================================
    # üéì SDPO: Reprompting –º–µ—Ç–æ–¥—ã
    # =========================================================================
    
    def _create_reprompted_input(
        self,
        original_prompt: str,
        successful_solution: str,
        feedback: Optional[str] = None,
    ) -> str:
        """
        –°–æ–∑–¥–∞—ë—Ç reprompted –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è teacher (SDPO).
        
        –§–æ—Ä–º–∞—Ç:
            Here is the problem:
            {original_question}
            
            Here is a successful solution for reference:
            {successful_solution}
            
            Now solve this problem step by step.
        
        Args:
            original_prompt: –ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –≤–æ–ø—Ä–æ—Å–æ–º
            successful_solution: –£—Å–ø–µ—à–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ (completion)
            feedback: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π feedback (–æ—à–∏–±–∫–∏ –∏ —Ç.–¥.)
            
        Returns:
            Reprompted —Å—Ç—Ä–æ–∫–∞ –¥–ª—è teacher
        """
        # –ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        reprompt_template = getattr(
            self.config, 
            'sdpo_reprompt_template',
            """Here is the problem:
{question}

Here is a successful solution for reference:
{successful_solution}

Now solve this problem step by step."""
        )
        
        feedback_template = getattr(
            self.config,
            'sdpo_feedback_template',
            """
Previous attempt feedback:
{feedback}
"""
        )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ original_prompt
        # –û–±—ã—á–Ω–æ prompt —Å–æ–¥–µ—Ä–∂–∏—Ç system message + user question
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å
        question = original_prompt
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤–æ–ø—Ä–æ—Å –≤ chat template —Ñ–æ—Ä–º–∞—Ç–µ
        if "User:" in original_prompt:
            parts = original_prompt.split("User:")
            if len(parts) > 1:
                question = parts[-1].split("Assistant:")[0].strip()
        elif "<|user|>" in original_prompt:
            parts = original_prompt.split("<|user|>")
            if len(parts) > 1:
                question = parts[-1].split("<|assistant|>")[0].strip()
        elif "[INST]" in original_prompt:
            parts = original_prompt.split("[INST]")
            if len(parts) > 1:
                question = parts[-1].split("[/INST]")[0].strip()
        
        # –°–æ–∑–¥–∞—ë–º reprompted —Ç–µ–∫—Å—Ç
        reprompted = reprompt_template.format(
            question=question,
            successful_solution=successful_solution,
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º feedback –µ—Å–ª–∏ –µ—Å—Ç—å
        include_feedback = getattr(self.config, 'sdpo_include_feedback', True)
        if include_feedback and feedback:
            feedback_text = feedback_template.format(feedback=feedback)
            reprompted = reprompted + feedback_text
        
        # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
        max_len = getattr(self.config, 'sdpo_max_reprompt_len', 4096)
        if len(reprompted) > max_len:
            reprompted = reprompted[:max_len]
        
        return reprompted
    
    def _get_teacher_log_probs(
        self,
        exp_batch: "Experience",
        device: torch.device,
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –í—ã—á–∏—Å–ª—è–µ—Ç teacher_log_probs –¥–ª—è SDPO (–∏–∑ verl).
        
        –ö–ª—é—á–µ–≤—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
        1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç teacher_module (—à–∞—Ä–∏–Ω–≥ —Å reference_model) ‚Äî —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏!
        2. Top-K Distillation ‚Äî –≤–º–µ—Å—Ç–æ vocab=152k –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ k=50-100 —Ç–æ–∫–µ–Ω–æ–≤
        3. Chunked processing ‚Äî –ø–æ –æ–¥–Ω–æ–º—É reprompt –∑–∞ —Ä–∞–∑
        
        –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞ –≤ batch:
        1. –ò—â–µ–º —É—Å–ø–µ—à–Ω—É—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –µ–≥–æ prompt_id
        2. –°–æ–∑–¥–∞—ë–º reprompted –∫–æ–Ω—Ç–µ–∫—Å—Ç (prompt + successful_solution)
        3. –î–µ–ª–∞–µ–º forward pass —á–µ—Ä–µ–∑ teacher_module
        4. –ü–æ–ª—É—á–∞–µ–º log_probs (–∏ top-k –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        
        Args:
            exp_batch: Batch Experience –æ–±—ä–µ–∫—Ç–æ–≤
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            
        Returns:
            (teacher_log_probs, distillation_mask, student_topk, teacher_topk) –∏–ª–∏ (None, None, None, None)
        """
        if not hasattr(self, '_successful_trajectories'):
            return None, None, None, None
        
        batch_size = exp_batch.sequences.size(0)
        seq_len = exp_batch.action_log_probs.size(1)
        
        # ============================================================
        # üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –í—ã–±–æ—Ä Teacher Model
        # ============================================================
        # –î–ª—è LoRA + EMA: –∏—Å–ø–æ–ª—å–∑—É–µ–º student –º–æ–¥–µ–ª—å —Å EMA –≤–µ—Å–∞–º–∏
        # –î–ª—è Full Fine-tuning: –∏—Å–ø–æ–ª—å–∑—É–µ–º teacher_module (reference model)
        use_ema_lora = (
            getattr(self.config, 'use_lora', False) and
            hasattr(self, '_ema_lora_state_dict') and
            bool(self._ema_lora_state_dict) and
            getattr(self, 'sdpo_ema_rate', 0) > 0
        )
        
        if use_ema_lora:
            # LoRA + EMA: –∏—Å–ø–æ–ª—å–∑—É–µ–º student —Å EMA –≤–µ—Å–∞–º–∏
            if self.accelerator:
                teacher_model = self.accelerator.unwrap_model(self.model)
            else:
                teacher_model = self.model
            logger.debug("üéì Teacher: Student + EMA LoRA –≤–µ—Å–∞")
        else:
            # Full Fine-tuning –∏–ª–∏ LoRA –±–µ–∑ EMA: –∏—Å–ø–æ–ª—å–∑—É–µ–º teacher_module
            teacher_model = getattr(self, 'teacher_module', None)
            if teacher_model is None:
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º student model
                if self.accelerator:
                    teacher_model = self.accelerator.unwrap_model(self.model)
                else:
                    teacher_model = self.model
        
        # Top-K –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        use_topk = getattr(self, 'sdpo_full_logit_distillation', False) and \
                   getattr(self, 'sdpo_distillation_topk', None) is not None
        topk = getattr(self, 'sdpo_distillation_topk', 50) if use_topk else None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ —Å—ç–º–ø–ª—ã –∏–º–µ—é—Ç —É—Å–ø–µ—à–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        has_teacher = torch.zeros(batch_size, dtype=torch.bool, device=device)
        reprompted_inputs = []
        sample_to_reprompt_idx = {}
        
        # –ü–æ–ª—É—á–∞–µ–º prompt_ids –∏–∑ batch –µ—Å–ª–∏ –µ—Å—Ç—å
        prompt_ids = getattr(exp_batch, 'prompt_ids', None)
        if prompt_ids is None:
            prompt_ids = [None] * batch_size
        
        for idx in range(batch_size):
            pid = prompt_ids[idx] if prompt_ids is not None else None
            if pid is None:
                continue
                
            pid_int = int(pid) if torch.is_tensor(pid) else pid
            
            if pid_int in self._successful_trajectories and self._successful_trajectories[pid_int]:
                import random
                trajectory = random.choice(self._successful_trajectories[pid_int])
                
                reprompted = self._create_reprompted_input(
                    original_prompt=trajectory['prompt'],
                    successful_solution=trajectory['completion'],
                )
                
                has_teacher[idx] = True
                sample_to_reprompt_idx[idx] = len(reprompted_inputs)
                reprompted_inputs.append(reprompted)
        
        if not reprompted_inputs:
            return None, None, None, None
        
        # üî• –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        teacher_log_probs = torch.zeros(batch_size, seq_len, device=device)
        
        # Top-K tensors (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º full_logit_distillation)
        student_topk_log_probs = None
        teacher_topk_log_probs = None
        if use_topk and topk is not None:
            student_topk_log_probs = torch.zeros(batch_size, seq_len, topk, device=device)
            teacher_topk_log_probs = torch.zeros(batch_size, seq_len, topk, device=device)
        
        # üî• Forward pass —á–µ—Ä–µ–∑ Teacher (—Å EMA LoRA –≤–µ—Å–∞–º–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        with torch.no_grad():
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º context manager –¥–ª—è EMA LoRA –≤–µ—Å–æ–≤
            with self._with_ema_lora_weights():
                for idx in range(batch_size):
                    if not has_teacher[idx]:
                        continue
                    
                    reprompt_idx = sample_to_reprompt_idx[idx]
                    reprompt_text = reprompted_inputs[reprompt_idx]
                    
                    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –û–î–ò–ù reprompt
                    reprompt_encoding = self.tokenizer(
                        reprompt_text,
                        return_tensors="pt",
                        truncation=True,
                        max_length=getattr(self.config, 'sdpo_max_reprompt_len', 4096),
                    ).to(device)
                    
                    # üî• Forward pass —á–µ—Ä–µ–∑ TEACHER MODEL
                    # –ü—Ä–∏ LoRA + EMA: student –º–æ–¥–µ–ª—å —Å EMA –≤–µ—Å–∞–º–∏
                    # –ü—Ä–∏ Full FT: reference –º–æ–¥–µ–ª—å
                    output = teacher_model(
                        input_ids=reprompt_encoding['input_ids'],
                        attention_mask=reprompt_encoding['attention_mask'],
                        use_cache=False,
                    )
                    
                    logits = output.logits[0]  # [reprompt_seq, vocab]
                    reprompt_seq_len = logits.size(0)
                    completion_tokens = exp_batch.sequences[idx, 1:seq_len+1]
                    
                    for t in range(min(seq_len, reprompt_seq_len - 1)):
                        pos = reprompt_seq_len - seq_len - 1 + t if reprompt_seq_len > seq_len else t
                        if pos >= 0 and pos < reprompt_seq_len - 1:
                            token_id = completion_tokens[t].item()
                            if token_id < logits.size(-1):
                                log_probs_pos = F.log_softmax(logits[pos], dim=-1)
                                teacher_log_probs[idx, t] = log_probs_pos[token_id]
                                
                                # üî• TOP-K DISTILLATION
                                if use_topk and teacher_topk_log_probs is not None:
                                    topk_vals, topk_idxs = torch.topk(log_probs_pos, topk)
                                    teacher_topk_log_probs[idx, t] = topk_vals
                    
                    del output, logits, reprompt_encoding
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
        
        # üî• TOP-K –¥–ª—è Student (–Ω—É–∂–Ω–æ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º full_logit_distillation)
        # –í—ã—á–∏—Å–ª—è–µ–º top-k student log_probs –¥–ª—è —Ç–µ—Ö –∂–µ –ø–æ–∑–∏—Ü–∏–π
        if use_topk and student_topk_log_probs is not None:
            # Unwrap student model
            if self.accelerator:
                student_model = self.accelerator.unwrap_model(self.model)
            else:
                student_model = self.model
            
            with torch.no_grad():
                # Forward pass student –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö sequences
                student_output = student_model(
                    input_ids=exp_batch.sequences,
                    attention_mask=exp_batch.attention_mask,
                    use_cache=False,
                )
                student_logits = student_output.logits[:, :-1]  # [batch, seq, vocab]
                
                for idx in range(batch_size):
                    if has_teacher[idx]:
                        for t in range(seq_len):
                            log_probs_t = F.log_softmax(student_logits[idx, t], dim=-1)
                            topk_vals, _ = torch.topk(log_probs_t, topk)
                            student_topk_log_probs[idx, t] = topk_vals
                
                del student_output, student_logits
        
        distillation_mask = has_teacher.float()
        
        logger.debug(
            f"üéì SDPO: {has_teacher.sum().item()}/{batch_size} —Å—ç–º–ø–ª–æ–≤ —Å teacher, "
            f"top-k={topk if use_topk else 'off'}"
        )
        
        return teacher_log_probs, distillation_mask, student_topk_log_probs, teacher_topk_log_probs

    def _setup_rollout_engine(self) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (rollout engine).
        """
        if not getattr(self.config, "use_rollout_engine", False):
            return

        backend = getattr(self.config, "rollout_engine_backend", "hf")

        mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
        if mp == "bf16":
            dtype = torch.bfloat16
        elif mp == "fp16":
            dtype = torch.float16
        else:
            dtype = torch.float32

        offload = bool(getattr(self.config, "rollout_offload_to_cpu", False))
        rollout_device = getattr(self.config, "rollout_device", "auto")
        if rollout_device == "cpu":
            device = torch.device("cpu")
            offload = True
        else:
            device = self.device

        if backend == "hf":
            self.rollout_engine = HFRolloutEngine(
                base_model_path=self.model_name,
                device=device,
                torch_dtype=dtype,
                use_flash_attention=bool(getattr(self.config, "use_flash_attention", True)),
                trust_remote_code=True,
                offload_to_cpu=offload,
            )
            self.rollout_engine.ensure_loaded()
            self._sync_rollout_engine_weights(force=True)
        elif backend == "vllm":
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            num_processes = getattr(self.accelerator, "num_processes", 1)
            
            # vLLM + Multi-GPU DDP –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (–∫–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ)
            if num_processes > 1:
                logger.warning(
                    f"‚ö†Ô∏è vLLM + Multi-GPU DDP ({num_processes} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤) –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. "
                    f"–ò—Å–ø–æ–ª—å–∑—É–µ–º HF backend —Å Prefix Grouper –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                )
                self.rollout_engine = None
                return
            
            # vLLM —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            vllm_device_str = getattr(self.config, "vllm_device", "cuda:0")
            
            # vLLM –Ω–∞ CPU –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            if vllm_device_str == "cpu":
                raise RuntimeError(
                    "vLLM –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CPU. –í—ã–±–µ—Ä–∏—Ç–µ GPU (cuda:X) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ HF backend."
                )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä GPU –¥–ª—è vLLM (—Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å –∏–∑ UI)
            if vllm_device_str.startswith("cuda:"):
                vllm_physical_gpu = int(vllm_device_str.split(":")[1])
            else:
                vllm_physical_gpu = 0
            
            # –í–ê–ñ–ù–û: –†–µ–º–∞–ø–∏–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å GPU –≤ –∏–Ω–¥–µ–∫—Å –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
            # CUDA_VISIBLE_DEVICES=0,1 –æ–∑–Ω–∞—á–∞–µ—Ç cuda:0=physical0, cuda:1=physical1
            # CUDA_VISIBLE_DEVICES=1,0 –æ–∑–Ω–∞—á–∞–µ—Ç cuda:0=physical1, cuda:1=physical0
            cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "")
            available_gpus = torch.cuda.device_count()
            
            if cuda_visible:
                visible_gpus = [int(x.strip()) for x in cuda_visible.split(",") if x.strip().isdigit()]
                if vllm_physical_gpu in visible_gpus:
                    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
                    vllm_gpu_id = visible_gpus.index(vllm_physical_gpu)
                    logger.info(f"üîÑ vLLM GPU: physical {vllm_physical_gpu} ‚Üí process cuda:{vllm_gpu_id}")
                else:
                    logger.error(
                        f"‚ùå vLLM GPU (physical {vllm_physical_gpu}) –Ω–µ –≤ CUDA_VISIBLE_DEVICES={cuda_visible}! "
                        f"–ò—Å–ø–æ–ª—å–∑—É–µ–º cuda:0."
                    )
                    vllm_gpu_id = 0
            else:
                # –ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å –Ω–∞–ø—Ä—è–º—É—é
                vllm_gpu_id = vllm_physical_gpu
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ GPU —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
            if vllm_gpu_id >= available_gpus:
                logger.error(
                    f"‚ùå vLLM GPU cuda:{vllm_gpu_id} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç! "
                    f"–î–æ—Å—Ç—É–ø–Ω–æ —Ç–æ–ª—å–∫–æ {available_gpus} GPU.\n"
                    f"CUDA_VISIBLE_DEVICES={cuda_visible}\n"
                    f"–ò—Å–ø–æ–ª—å–∑—É–µ–º cuda:0 –¥–ª—è vLLM."
                )
                vllm_gpu_id = 0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é GPU training –º–æ–¥–µ–ª–∏
            training_device = self.device
            if hasattr(training_device, 'index') and training_device.index is not None:
                training_gpu_id = training_device.index
            elif str(training_device).startswith("cuda:"):
                training_gpu_id = int(str(training_device).split(":")[1])
            elif str(training_device) == "cuda":
                training_gpu_id = torch.cuda.current_device()
            else:
                training_gpu_id = 0
            
            # GPU memory utilization –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            vllm_gpu_util = float(getattr(self.config, "vllm_gpu_memory_utilization", 0.85))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞ —Ç–æ–π –∂–µ GPU –∏–ª–∏ –Ω–∞ –¥—Ä—É–≥–æ–π
            same_gpu = (vllm_gpu_id == training_gpu_id)
            
            # max_model_len: prompt + response
            max_len = int(getattr(self.config, "max_prompt_length", 512)) + int(getattr(self.config, "max_new_tokens", 1024))
            
            # –ü–æ–ª—É—á–∞–µ–º lora_r –¥–ª—è vLLM max_lora_rank
            lora_rank_for_vllm = self.config.lora_r if self.config.use_lora else 16
            
            if same_gpu:
                # –ù–∞ —Ç–æ–π –∂–µ GPU ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º VLLMRolloutEngine –Ω–∞–ø—Ä—è–º—É—é
                if vllm_gpu_util > 0.5:
                    logger.warning(
                        f"‚ö†Ô∏è vLLM –Ω–∞ —Ç–æ–π –∂–µ GPU —á—Ç–æ training (cuda:{training_gpu_id}). "
                        f"gpu_memory_utilization={vllm_gpu_util:.0%} –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–º! "
                        f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 30-50%."
                    )
                logger.info(f"üß© vLLM: –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ cuda:{vllm_gpu_id} (—Ç–∞ –∂–µ GPU —á—Ç–æ training, memory={vllm_gpu_util:.0%})")
                
                self.rollout_engine = VLLMRolloutEngine(
                    base_model_path=self.model_name,
                    torch_dtype=dtype,
                    trust_remote_code=True,
                    tensor_parallel_size=1,
                    max_model_len=max_len,
                    gpu_memory_utilization=vllm_gpu_util,
                    max_lora_rank=lora_rank_for_vllm,  # –î–ª—è vLLM max_lora_rank!
                )
                self.rollout_engine.ensure_loaded()
            else:
                # –ù–∞ –û–¢–î–ï–õ–¨–ù–û–ô GPU ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º VLLMSubprocessEngine!
                # –≠—Ç–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç vLLM –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º CUDA_VISIBLE_DEVICES
                logger.info(f"üß© vLLM: –∑–∞–ø—É—Å–∫ –Ω–∞ cuda:{vllm_gpu_id} —á–µ—Ä–µ–∑ SUBPROCESS (–æ—Ç–¥–µ–ª—å–Ω–∞—è GPU, memory={vllm_gpu_util:.0%})")
                logger.info(f"   Training –Ω–∞ cuda:{training_gpu_id}, vLLM –Ω–∞ cuda:{vllm_physical_gpu}")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –§–ò–ó–ò–ß–ï–°–ö–ò–ô –∏–Ω–¥–µ–∫—Å GPU –¥–ª—è subprocess
                # (–≤–Ω—É—Ç—Ä–∏ subprocess –±—É–¥–µ—Ç CUDA_VISIBLE_DEVICES={vllm_physical_gpu})
                self.rollout_engine = VLLMSubprocessEngine(
                    base_model_path=self.model_name,
                    torch_dtype=dtype,
                    gpu_id=vllm_physical_gpu,  # –§–∏–∑–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å GPU!
                    max_model_len=max_len,
                    gpu_memory_utilization=vllm_gpu_util,
                    enable_lora=True,
                    max_lora_rank=lora_rank_for_vllm,  # –î–ª—è vLLM max_lora_rank!
                    output_dir=getattr(self.config, "output_dir", None),
                )
                self.rollout_engine.ensure_loaded()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º GPU IDs
            self._vllm_gpu_id = vllm_gpu_id
            self._training_gpu_id = training_gpu_id
            self._use_vllm_subprocess = not same_gpu
            
            # –ü–µ—Ä–≤–∏—á–Ω—ã–π sync LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
            self._sync_rollout_engine_weights(force=True)
        else:
            raise NotImplementedError(f"Unknown rollout_engine_backend='{backend}'")

    def _next_group_uids(self, n: int) -> List[int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç n —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö group_id –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ Experience."""
        start = self._group_uid
        self._group_uid += int(n)
        return list(range(start, start + int(n)))
    
    def setup(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GRPOTrainer...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –î–û —Å–æ–∑–¥–∞–Ω–∏—è accelerator
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            logger.info(f"üñ•Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {num_gpus} —É—Å—Ç—Ä–æ–π—Å—Ç–≤")
            for i in range(num_gpus):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)
                logger.info(f"  - GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            logger.info("üñ•Ô∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        
        # Accelerate - —Å–æ–∑–¥–∞–µ–º –ü–ï–†–ï–î –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ pretrain/SFT)
        if self.use_accelerate:
            try:
                from accelerate import Accelerator
                
                # Mixed precision –¥–æ–ª–∂–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ UI.
                mixed_precision = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                if mixed_precision not in ("no", "fp16", "bf16"):
                    logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π mixed_precision='{mixed_precision}', fallback -> bf16")
                    mixed_precision = "bf16"
                if mixed_precision == "bf16" and torch.cuda.is_available() and not torch.cuda.is_bf16_supported():
                    logger.warning("bf16 –≤—ã–±—Ä–∞–Ω –≤ UI, –Ω–æ GPU –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç bf16. Fallback -> fp16")
                    mixed_precision = "fp16"

                # "Pure fp16" (–≤–µ—Å–∞ fp16, –±–µ–∑ GradScaler): –¥–ª—è accelerate –Ω—É–∂–Ω–æ mixed_precision='no',
                # –∏–Ω–∞—á–µ –æ–Ω –≤–∫–ª—é—á–∏—Ç GradScaler –∏ —É–ø–∞–¥—ë—Ç –ø—Ä–∏ fp16 –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö.
                accel_mp = mixed_precision
                if mixed_precision == "fp16" and bool(getattr(self.config, "fp16_pure", False)):
                    accel_mp = "no"
                    logger.info("üß™ FP16 Pure —Ä–µ–∂–∏–º: Accelerator(mixed_precision='no'), –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç torch.float16")
                
                logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Accelerator...")
                logger.info(f"  - gradient_accumulation_steps: {self.config.gradient_accumulation_steps}")
                logger.info(f"  - mixed_precision (UI): {mixed_precision}")
                logger.info(f"  - mixed_precision (accelerate): {accel_mp}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–µ–Ω –ª–∏ find_unused_parameters –¥–ª—è DDP
                # lm_head/embed_tokens —Å–≤—è–∑–∞–Ω—ã —á–µ—Ä–µ–∑ tie_word_embeddings –∏ –º–æ–≥—É—Ç –Ω–µ –ø–æ–ª—É—á–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                target_modules = getattr(self.config, "lora_target_modules", None) or []
                needs_find_unused = any(m in target_modules for m in ["lm_head", "embed_tokens"])
                
                ddp_kwargs = None
                if needs_find_unused:
                    from accelerate import DistributedDataParallelKwargs
                    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
                    logger.info(f"  - find_unused_parameters: True (lm_head/embed_tokens –≤ target_modules)")
                
                self.accelerator = Accelerator(
                    gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                    mixed_precision=accel_mp,
                    kwargs_handlers=[ddp_kwargs] if ddp_kwargs else None,
                )
                
                # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±–µ—Ä–µ–º –∏–∑ accelerator (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multi-GPU)
                self.device = self.accelerator.device
                self.is_main_process = self.accelerator.is_main_process
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏
                if self.accelerator.num_processes > 1:
                    logger.info(f"‚úÖ Multi-GPU —Ä–µ–∂–∏–º: {self.accelerator.num_processes} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")
                    logger.info(f"  - –¢–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å: {self.accelerator.process_index} / {self.accelerator.num_processes - 1}")
                    logger.info(f"  - Main process: {self.is_main_process}")
                else:
                    logger.info(f"‚úÖ Single GPU —Ä–µ–∂–∏–º")
                
                logger.info(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º DeepSpeed –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                self._log_and_setup_deepspeed_config()
                
            except ImportError:
                logger.warning("‚ö†Ô∏è  accelerate –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º single GPU")
                self.accelerator = None
                self.device = self._device if self._device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
                self.is_main_process = True
        else:
            logger.info("‚ÑπÔ∏è  Accelerate –æ—Ç–∫–ª—é—á–µ–Ω (use_accelerate=False)")
            self.accelerator = None
            self.device = self._device if self._device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.is_main_process = True
            logger.info(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        if self.tokenizer is None:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self._load_model()
        
        # Loss —Ñ—É–Ω–∫—Ü–∏—è ‚Äî —Å–æ–∑–¥–∞—ë—Ç—Å—è –ü–û–°–õ–ï –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ (–Ω—É–∂–Ω–∞ –¥–ª—è Liger Fused Loss)
        # –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –≤ setup() –ø–æ—Å–ª–µ accelerator.prepare()
        self.loss_fn = None
        self.use_liger_fused_loss = getattr(self.config, 'liger_fused_grpo', False) and getattr(self.config, 'use_liger', False)
        
        # Replay buffer
        self.replay_buffer = ReplayBuffer()
        
        # W&B
        if self.config.use_wandb and self.is_main_process:
            self._setup_wandb()
        
        logger.info(f"GRPOTrainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
        logger.info(f"–ê–ª–≥–æ—Ä–∏—Ç–º: {self.config.algorithm.value}")
        if self.config.dynamic_sampling:
            logger.info(f"  üéØ Dynamic sampling: ON (max_refill_rounds={self.config.max_refill_rounds})")
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è multi-GPU
            if self.accelerator.num_processes > 1:
                logger.warning(
                    f"  ‚ö†Ô∏è Dynamic sampling + Multi-GPU ({self.accelerator.num_processes} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤) "
                    f"–º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é! –î–æ–±–∞–≤–ª–µ–Ω—ã –±–∞—Ä—å–µ—Ä—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏."
                )
        else:
            logger.info(f"  üéØ Dynamic sampling: OFF (–±—ã—Å—Ç—Ä–µ–µ)")
        if self.config.token_level_loss:
            logger.info(f"  üìä Token-level loss: ON")
        else:
            logger.info(f"  üìä Sample-level loss: ON")
    
    def _log_and_setup_deepspeed_config(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç DeepSpeed –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é."""
        if self.accelerator is None:
            return
        
        ds_plugin = getattr(self.accelerator.state, 'deepspeed_plugin', None)
        if ds_plugin is None:
            logger.info("üìã DeepSpeed: –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (DDP/FSDP —Ä–µ–∂–∏–º)")
            return
        
        logger.info("=" * 60)
        logger.info("üìã –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø DEEPSPEED:")
        logger.info("=" * 60)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        zero_stage = getattr(ds_plugin, 'zero_stage', 'N/A')
        logger.info(f"  - ZeRO Stage: {zero_stage}")
        
        # Offload –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        offload_optimizer = getattr(ds_plugin, 'offload_optimizer_device', None)
        offload_param = getattr(ds_plugin, 'offload_param_device', None)
        logger.info(f"  - Offload Optimizer: {offload_optimizer or 'none'}")
        logger.info(f"  - Offload Param: {offload_param or 'none'}")
        
        # –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        ds_config = getattr(ds_plugin, 'deepspeed_config', {})
        if ds_config:
            logger.info("  - –ü–æ–ª–Ω—ã–π DeepSpeed –∫–æ–Ω—Ñ–∏–≥:")
            for key, value in ds_config.items():
                if isinstance(value, dict):
                    logger.info(f"    {key}:")
                    for k, v in value.items():
                        logger.info(f"      {k}: {v}")
                else:
                    logger.info(f"    {key}: {value}")
        
        # –í–ê–ñ–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º train_micro_batch_size_per_gpu –¥–ª—è DeepSpeed
        # DeepSpeed —Ç—Ä–µ–±—É–µ—Ç —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏ accelerator.prepare() –±–µ–∑ dataloader
        # –î–ª—è GRPO –∏—Å–ø–æ–ª—å–∑—É–µ–º train_batch_size –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        micro_batch_size = getattr(self.config, 'train_batch_size', None)
        if micro_batch_size is None:
            micro_batch_size = getattr(self.config, 'batch_size', 1) or 1
        micro_batch_size = max(1, int(micro_batch_size))
        
        logger.info(f"  - –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º train_micro_batch_size_per_gpu: {micro_batch_size}")
        logger.info(f"  - gradient_accumulation_steps: {self.config.gradient_accumulation_steps}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ DeepSpeed –∫–æ–Ω—Ñ–∏–≥
        try:
            from accelerate.state import AcceleratorState
            state = AcceleratorState()
            if hasattr(state, 'deepspeed_plugin') and state.deepspeed_plugin is not None:
                ds_cfg = state.deepspeed_plugin.deepspeed_config
                if ds_cfg is not None:
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º batch sizes
                    ds_cfg['train_micro_batch_size_per_gpu'] = micro_batch_size
                    ds_cfg['gradient_accumulation_steps'] = self.config.gradient_accumulation_steps
                    # train_batch_size = micro_batch_size * gradient_accumulation * num_gpus
                    num_gpus = self.accelerator.num_processes
                    ds_cfg['train_batch_size'] = micro_batch_size * self.config.gradient_accumulation_steps * num_gpus
                    logger.info(f"  ‚úÖ DeepSpeed batch sizes —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:")
                    logger.info(f"    - train_micro_batch_size_per_gpu: {ds_cfg['train_micro_batch_size_per_gpu']}")
                    logger.info(f"    - gradient_accumulation_steps: {ds_cfg['gradient_accumulation_steps']}")
                    logger.info(f"    - train_batch_size: {ds_cfg['train_batch_size']}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å DeepSpeed batch sizes: {e}")
        
        logger.info("=" * 60)
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –∏ LoRA."""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        logger.info(f"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
        logger.info(f"  - use_4bit: {self.config.use_4bit}")
        logger.info(f"  - use_8bit: {self.config.use_8bit}")
        logger.info(f"  - use_lora: {self.config.use_lora}")
        if self.config.use_lora:
            logger.info(f"  - lora_r: {self.config.lora_r}")
            logger.info(f"  - lora_alpha: {self.config.lora_alpha}")
            logger.info(f"  - lora_target_modules: {self.config.lora_target_modules}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ DeepSpeed ZeRO-3 –≤ –Ω–∞—á–∞–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏
        # –ü—Ä–∏ ZeRO-3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã sharded –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ –∏ –Ω–µ–ª—å–∑—è –¥–µ–ª–∞—Ç—å .to(device)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç –∫–ª–∞—Å—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–∞—Ö
        self.is_deepspeed_zero3 = False
        if self.accelerator is not None:
            ds_plugin = getattr(self.accelerator.state, 'deepspeed_plugin', None)
            if ds_plugin is not None:
                zero_stage = getattr(ds_plugin, 'zero_stage', 0)
                self.is_deepspeed_zero3 = zero_stage == 3
                logger.info(f"üîß DeepSpeed ZeRO stage: {zero_stage}")
                if self.is_deepspeed_zero3:
                    logger.info("‚ö° ZeRO-3 —Ä–µ–∂–∏–º: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±—É–¥—É—Ç sharded, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º .to(device)")
        is_deepspeed_zero3 = self.is_deepspeed_zero3  # –ª–æ–∫–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏
        memory_before = 0.0
        if torch.cuda.is_available():
            memory_before = torch.cuda.memory_allocated() / (1024 ** 2)
            logger.info(f"üíæ –ü–∞–º—è—Ç—å CUDA –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {memory_before:.1f} MB")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        quantization_config = None
        if self.config.use_4bit or self.config.use_8bit:
            try:
                from transformers import BitsAndBytesConfig
                
                if self.config.use_4bit:
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_use_double_quant=True,
                    )
                    logger.info("‚úÖ –°–æ–∑–¥–∞–Ω BitsAndBytesConfig –¥–ª—è 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏")
                else:
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                    )
                    logger.info("‚úÖ –°–æ–∑–¥–∞–Ω BitsAndBytesConfig –¥–ª—è 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏")
            except ImportError:
                logger.warning("‚ùå bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
                quantization_config = None
        else:
            logger.info("‚ÑπÔ∏è  –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (use_4bit=False, use_8bit=False)")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model_kwargs = {
            "trust_remote_code": True,
            "device_map": "auto" if quantization_config else None,
        }
        
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        else:
            # –í–ê–ñ–ù–û:
            # - bf16: –º–æ–∂–Ω–æ –≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –≤ bf16 (–Ω–µ—Ç GradScaler).
            # - fp16: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —ç—Ç–æ AMP fp16 (fp32 master-–≤–µ—Å–∞ + GradScaler) => –≤–µ—Å–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º fp32.
            #   –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å "pure fp16" (–≤–µ—Å–∞ fp16, –±–µ–∑ GradScaler) —á–µ—Ä–µ–∑ config.fp16_pure.
            mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
            if mp == "bf16" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                model_kwargs["dtype"] = torch.bfloat16
            elif mp == "fp16":
                if bool(getattr(self.config, "fp16_pure", False)):
                    model_kwargs["dtype"] = torch.float16
                else:
                    # AMP fp16: –æ—Å—Ç–∞–≤–ª—è–µ–º fp32 –≤–µ—Å–∞ (GradScaler —Ç—Ä–µ–±—É–µ—Ç fp32 master weights)
                    pass
            elif mp == "no":
                # –û—Å—Ç–∞–≤–ª—è–µ–º fp32 (–¥–µ—Ñ–æ–ª—Ç HF)
                pass
            else:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ flash_attn –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
        # –í–ê–ñ–ù–û: Flash Attention –º–æ–∂–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö
        # –î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π attention
        if self.config.use_flash_attention and not quantization_config:
            try:
                import flash_attn
                mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                if mp == "no":
                    logger.info("Flash Attention 2 –æ—Ç–∫–ª—é—á–µ–Ω: mixed_precision='no' (fp32 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è flash-attn)")
                else:
                    model_kwargs["attn_implementation"] = "flash_attention_2"
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Flash Attention 2")
            except ImportError:
                logger.warning(
                    "Flash Attention 2 –∑–∞–ø—Ä–æ—à–µ–Ω, –Ω–æ –ø–∞–∫–µ—Ç flash_attn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è attention. "
                    "–î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install flash-attn"
                )
                # –ù–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º attn_implementation, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω–∞—è
        elif self.config.use_flash_attention and quantization_config:
            logger.info(
                "Flash Attention –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ "
                "(–º–æ–∂–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å bitsandbytes). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π attention."
            )
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏
        logger.info(f"üì¶ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏:")
        logger.info(f"  - quantization_config: {'‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è' if quantization_config else '‚ùå –ù–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è'}")
        logger.info(f"  - device_map: {model_kwargs.get('device_map', 'None')}")
        if quantization_config:
            logger.info(f"  - –¢–∏–ø –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏: {'4-bit' if self.config.use_4bit else '8-bit'}")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            **model_kwargs,
        )

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ dtype –º–æ–¥–µ–ª–∏ (–ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É fp16 –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–ª—è—Ç—å –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ —á–µ–º bf16)
        try:
            first_param = next(self.model.parameters(), None)
            if first_param is not None:
                logger.info(f"üîé DType –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–º–µ—Ä): {first_param.dtype}")
        except Exception:
            pass

        # Gradient checkpointing (—É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∏–∑ UI)
        if getattr(self.config, "grad_checkpoint", False) and hasattr(self.model, "gradient_checkpointing_enable"):
            try:
                self.model.gradient_checkpointing_enable()
                logger.info("‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω (–∏–∑ UI)")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å gradient checkpointing: {e}")
        
        # Liger Kernel –ø–∞—Ç—á–∏–Ω–≥ –º–æ–¥–µ–ª–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Triton kernels)
        if getattr(self.config, "use_liger", True) and getattr(self.config, "liger_patch_model", True):
            try:
                from homellm.training.rl.liger_utils import apply_liger_patch_to_model, is_liger_available
                if is_liger_available():
                    # –ü–∞—Ç—á–∏–º RMSNorm, RoPE, MLP ‚Äî –Ω–æ –ù–ï CrossEntropy
                    # –î–ª—è GRPO –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π chunked cross-entropy –≤ rollout.py
                    patched = apply_liger_patch_to_model(
                        self.model,
                        patch_rms_norm=True,
                        patch_rope=True,
                        patch_mlp=True,
                        patch_fused_linear_ce=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π loss
                    )
                    if patched:
                        logger.info("‚úÖ Liger Kernel –ø–∞—Ç—á–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã (RMSNorm, RoPE, MLP)")
                    else:
                        logger.info("‚ÑπÔ∏è Liger: –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –ø–∞—Ç—á–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ kernels")
                else:
                    logger.info("‚ÑπÔ∏è Liger Kernel –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ kernels")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å Liger –ø–∞—Ç—á–∏: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        if torch.cuda.is_available():
            memory_after_load = torch.cuda.memory_allocated() / (1024 ** 2)
            logger.info(f"üíæ –ü–∞–º—è—Ç—å CUDA –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {memory_after_load:.1f} MB (+{memory_after_load - memory_before:.1f} MB)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
        if quantization_config:
            is_quantized = False
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                for name, param in self.model.named_parameters():
                    if hasattr(param, 'quant_state') or str(param.dtype) == 'torch.uint8':
                        is_quantized = True
                        break
                    # –î–ª—è bitsandbytes –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                    if hasattr(param, 'data') and hasattr(param.data, 'quant_state'):
                        is_quantized = True
                        break
                
                if is_quantized:
                    logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (–Ω–∞–π–¥–µ–Ω—ã –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)")
                else:
                    logger.warning("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ BitsAndBytesConfig.")
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é: {e}")
        
        # LoRA
        # –í–ê–ñ–ù–û: –ï—Å–ª–∏ use_lora=True, –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã (–±–µ–∑ fallback)
        if self.config.use_lora:
            if self.config.lora_r is None:
                raise ValueError(
                    "‚ùå use_lora=True –Ω–æ lora_r=None! "
                    "lora_r –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_r."
                )
            if self.config.lora_alpha is None:
                raise ValueError(
                    "‚ùå use_lora=True –Ω–æ lora_alpha=None! "
                    "lora_alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_alpha."
                )
            self._apply_lora()
        else:
            # –ï—Å–ª–∏ LoRA –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –≤–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            # (–¥–ª—è full fine-tuning)
            # –í–ê–ñ–ù–û: –ü—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–µ–∑ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã!
            if quantization_config:
                raise RuntimeError(
                    "‚ùå –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (4bit/8bit) –±–µ–∑ LoRA –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è! "
                    "–ü—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã. "
                    "–í–∫–ª—é—á–∏—Ç–µ use_lora=True –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."
                )
            
            logger.info("LoRA –æ—Ç–∫–ª—é—á–µ–Ω, –≤–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (full fine-tuning)...")
            for param in self.model.parameters():
                param.requires_grad = True
        
        # –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å (–¥–ª—è KL)
        # –í–ê–ñ–ù–û: Reference –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è forward pass (–±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
        # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞, –Ω–æ –º–æ–∂–µ—Ç —ç–∫–æ–Ω–æ–º–∏—Ç—å –ø–∞–º—è—Ç—å
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ù–ï –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É–µ–º –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ KL divergence
        if self.config.kl_weight > 0:
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è KL...")
            
            # –°–æ–∑–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ model_kwargs –¥–ª—è reference –º–æ–¥–µ–ª–∏
            ref_model_kwargs = {
                "trust_remote_code": True,
                "device_map": "auto" if (self.config.quantize_reference_model and quantization_config) else None,
            }
            
            # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è reference –º–æ–¥–µ–ª–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞
            if self.config.quantize_reference_model and quantization_config:
                ref_model_kwargs["quantization_config"] = quantization_config
                logger.info("‚ö†Ô∏è –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–π KL)")
            else:
                # –ù–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É–µ–º reference –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ KL
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ dtype —á—Ç–æ –∏ –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å (–∏–ª–∏ bfloat16 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                if not quantization_config:
                    # Reference –º–æ–¥–µ–ª—å –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: –º–æ–∂–Ω–æ –≥—Ä—É–∑–∏—Ç—å –≤ mp dtype –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
                    mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                    if mp == "bf16" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                        ref_model_kwargs["dtype"] = torch.bfloat16
                    elif mp == "fp16" and torch.cuda.is_available():
                        ref_model_kwargs["dtype"] = torch.float16
                    else:
                        # fp32
                        pass
                logger.info("‚úÖ –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –ù–ï –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (—Ç–æ—á–Ω—ã–π KL divergence)")
            
            # Flash Attention –¥–ª—è reference –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞)
            if self.config.use_flash_attention and not (self.config.quantize_reference_model and quantization_config):
                try:
                    import flash_attn
                    ref_model_kwargs["attn_implementation"] = "flash_attention_2"
                except ImportError:
                    pass
            
            self.reference_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **ref_model_kwargs,
            )
            self.reference_model.eval()
            for param in self.reference_model.parameters():
                param.requires_grad = False
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º reference –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –µ—Å–ª–∏ –Ω–µ device_map –∏ –Ω–µ ZeRO-3
            if not (self.config.quantize_reference_model and quantization_config) and not is_deepspeed_zero3:
                self.reference_model = self.reference_model.to(self.device)
        else:
            logger.info("KL weight = 0, —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–µ—Å–ª–∏ –Ω–µ device_map –∏ –Ω–µ ZeRO-3)
        # –ü—Ä–∏ ZeRO-3 DeepSpeed —Å–∞–º —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        # Reference –º–æ–¥–µ–ª—å —É–∂–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –≤—ã—à–µ
        if not quantization_config and not is_deepspeed_zero3:
            self.model = self.model.to(self.device)
        
        # –í–ê–ñ–ù–û: –ü–æ—Å–ª–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏–ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA,
        # —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Å—ë –µ—â—ë —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        # –î–ª—è ZeRO-3 –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É –ø—Ä–æ–≤–µ—Ä–∫—É - DeepSpeed —Å–∞–º —É–ø—Ä–∞–≤–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
        if not is_deepspeed_zero3:
            if self.config.use_lora:
                # –î–ª—è LoRA –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                # PEFT –¥–æ–ª–∂–µ–Ω —ç—Ç–æ –¥–µ–ª–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –Ω–æ –ø—Ä–æ–≤–µ—Ä–∏–º
                try:
                    from peft import PeftModel
                    if isinstance(self.model, PeftModel):
                        # PEFT –º–æ–¥–µ–ª—å - –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                        pass  # PEFT –¥–æ–ª–∂–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å requires_grad
                except:
                    pass
            else:
                # –î–ª—è full fine-tuning —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                for param in self.model.parameters():
                    if not param.requires_grad:
                        logger.warning(f"–ü–∞—Ä–∞–º–µ—Ç—Ä –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, –≤–∫–ª—é—á–∞–µ–º: {param.shape}")
                        param.requires_grad = True
        else:
            logger.info("‚ö° ZeRO-3: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä—É—á–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É requires_grad")
        
        # –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞
        # –ü—Ä–∏ ZeRO-3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã sharded, –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        if is_deepspeed_zero3:
            # –î–ª—è ZeRO-3: –∏—Å–ø–æ–ª—å–∑—É–µ–º num_parameters() –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥ –º–æ–¥–µ–ª–∏
            try:
                # DeepSpeed –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–¥—Å—á—ë—Ç–∞
                if hasattr(self.model, 'num_parameters'):
                    total_params = self.model.num_parameters()
                    trainable_params = self.model.num_parameters(only_trainable=True)
                else:
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –ø–æ–¥—Å—á—ë—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ (HuggingFace)
                    from transformers import AutoConfig
                    model_config = AutoConfig.from_pretrained(self.model_name)
                    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤: 
                    # vocab_size * hidden_size + num_layers * (4 * hidden_size^2 + ...)
                    hidden = getattr(model_config, 'hidden_size', 768)
                    layers = getattr(model_config, 'num_hidden_layers', 12)
                    vocab = getattr(model_config, 'vocab_size', 32000)
                    # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞: embedding + transformer layers
                    total_params = vocab * hidden + layers * 12 * hidden * hidden
                    trainable_params = total_params  # full fine-tuning = –≤—Å–µ trainable
                    logger.info(f"‚ö° ZeRO-3: –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ZeRO-3: {e}")
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º sharded —Ä–∞–∑–º–µ—Ä—ã * world_size
                world_size = self.accelerator.num_processes if self.accelerator else 1
                total_params = sum(p.numel() for p in self.model.parameters()) * world_size
                trainable_params = total_params  # –ø—Ä–∏ full fine-tuning –≤—Å–µ trainable
                logger.info(f"‚ö° ZeRO-3: –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ = sharded * world_size ({world_size})")
        else:
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        # –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            try:
                # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–∏
                # –î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: ~0.5 bytes/param (4-bit)
                # –î–ª—è fp16: 2 bytes/param, –¥–ª—è fp32: 4 bytes/param
                if quantization_config:
                    if self.config.use_4bit:
                        bytes_per_param = 0.5  # 4-bit = 0.5 bytes
                        quant_type = "4-bit"
                    else:
                        bytes_per_param = 1.0  # 8-bit = 1 byte
                        quant_type = "8-bit"
                    model_memory_mb = (total_params * bytes_per_param) / (1024 ** 2)
                else:
                    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º bfloat16/fp16
                    try:
                        first_param = next(self.model.parameters(), None)
                        dt = getattr(first_param, "dtype", None)
                        if dt == torch.float32:
                            bytes_per_param = 4.0
                            quant_type = "fp32"
                        elif dt == torch.bfloat16:
                            bytes_per_param = 2.0
                            quant_type = "bf16"
                        elif dt == torch.float16:
                            bytes_per_param = 2.0
                            quant_type = "fp16"
                        else:
                            bytes_per_param = 2.0
                            quant_type = "fp16/bf16"
                    except Exception:
                        bytes_per_param = 2.0
                        quant_type = "fp16/bf16"
                    model_memory_mb = (total_params * bytes_per_param) / (1024 ** 2)
                
                logger.info(
                    f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,} –≤—Å–µ–≥–æ, {trainable_params:,} –æ–±—É—á–∞–µ–º—ã—Ö "
                    f"({100*trainable_params/total_params:.2f}%)"
                )
                logger.info(
                    f"üíæ –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–∏: ~{model_memory_mb:.1f} MB ({quant_type})"
                )
                
                # –î–ª—è LoRA –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –ø–∞–º—è—Ç–∏ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
                if self.config.use_lora:
                    # LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã: r * (input_dim + output_dim) * 2 (A –∏ B –º–∞—Ç—Ä–∏—Ü—ã) * 2 bytes (fp16)
                    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: r * 2 * avg_dim * 2 bytes
                    # –î–ª—è r=16, avg_dim=1024: ~16 * 2 * 1024 * 2 = 64KB –Ω–∞ –º–æ–¥—É–ª—å
                    # –ù–æ —ç—Ç–æ –æ—á–µ–Ω—å –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞, —Ä–µ–∞–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
                    lora_memory_mb = (trainable_params * 2.0) / (1024 ** 2)  # fp16 –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
                    logger.info(f"üíæ –ü–∞–º—è—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: ~{lora_memory_mb:.1f} MB")
                    
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –ø–∞–º—è—Ç—å: {e}")
        else:
            logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,} –≤—Å–µ–≥–æ, {trainable_params:,} –æ–±—É—á–∞–µ–º—ã—Ö")
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        # –î–ª—è ZeRO-3 –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É –ø—Ä–æ–≤–µ—Ä–∫—É - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã sharded –∏ —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if trainable_params == 0 and not is_deepspeed_zero3:
            raise RuntimeError(
                "‚ùå –ù–µ—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏! "
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: use_lora, use_4bit, use_8bit. "
                "–î–ª—è full fine-tuning –Ω—É–∂–µ–Ω use_lora=False –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏."
            )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ç–µ—Å—Ç–æ–≤—ã–π forward pass –¥–æ–ª–∂–µ–Ω —Ç—Ä–µ–±–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        # –í–ê–ñ–ù–û: –ø—Ä–∏ flash_attention_2 –∏ mixed_precision fp16/bf16 –¥–µ–ª–∞–µ–º forward –ø–æ–¥ autocast,
        # –∏–Ω–∞—á–µ FlashAttention –º–æ–∂–µ—Ç —Ä—É–≥–∞—Ç—å—Å—è –Ω–∞ fp32 dtype.
        # –î–ª—è ZeRO-3 –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π forward - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—â—ë –Ω–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω—ã
        if is_deepspeed_zero3:
            logger.info("‚ö° ZeRO-3: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π forward pass (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã sharded)")
            self.model.train()
            return  # –í—ã—Ö–æ–¥–∏–º –∏–∑ _load_model –¥–ª—è ZeRO-3
        
        self.model.train()  # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤ train —Ä–µ–∂–∏–º–µ
        test_input = torch.randint(0, 1000, (1, 10), device=self.device)
        test_mask = torch.ones_like(test_input)
        mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
        use_autocast = torch.cuda.is_available() and mp in ("bf16", "fp16")
        if use_autocast:
            amp_dtype = torch.bfloat16 if mp == "bf16" else torch.float16
            autocast_ctx = torch.amp.autocast("cuda", enabled=True, dtype=amp_dtype)
        else:
            from contextlib import nullcontext
            autocast_ctx = nullcontext()
        if self.accelerator is not None:
            try:
                logger.info(
                    "üîé AMP/Precision: "
                    f"mixed_precision={mp}, "
                    f"autocast={'on' if use_autocast else 'off'}, "
                    f"autocast_dtype={('bf16' if mp=='bf16' else 'fp16') if use_autocast else 'n/a'}, "
                    f"grad_scaler={'on' if getattr(self.accelerator, 'scaler', None) is not None else 'off'}"
                )
            except Exception:
                pass
        with torch.enable_grad():
            with autocast_ctx:
                test_output = self.model(input_ids=test_input, attention_mask=test_mask, use_cache=False)
        if not test_output.logits.requires_grad:
            logger.warning("‚ö†Ô∏è –¢–µ—Å—Ç–æ–≤—ã–π forward pass –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤! –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–æ–π.")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            for param in self.model.parameters():
                param.requires_grad = True
            logger.info("–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–µ–Ω—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            memory_final = torch.cuda.memory_allocated() / (1024 ** 2)
            memory_reserved = torch.cuda.memory_reserved() / (1024 ** 2)
            logger.info("=" * 60)
            logger.info("üìä –ò–¢–û–ì–û–í–û–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ü–ê–ú–Ø–¢–ò –ü–û–°–õ–ï –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò:")
            logger.info(f"  - –í—ã–¥–µ–ª–µ–Ω–æ (allocated): {memory_final:.1f} MB")
            logger.info(f"  - –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ (reserved): {memory_reserved:.1f} MB")
            logger.info(f"  - –í—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Å –Ω–∞—á–∞–ª–∞: +{memory_final - memory_before:.1f} MB")
            logger.info("=" * 60)
    
    def _apply_lora(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∫ –º–æ–¥–µ–ª–∏."""
        logger.info("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA
        memory_before_lora = 0.0
        if torch.cuda.is_available():
            memory_before_lora = torch.cuda.memory_allocated() / (1024 ** 2)
        
        try:
            from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
            
            # –í–ê–ñ–ù–û:
            # `prepare_model_for_kbit_training` –Ω—É–∂–Ω–æ –¢–û–õ–¨–ö–û –¥–ª—è QLoRA (4/8bit).
            # –î–ª—è –æ–±—ã—á–Ω–æ–π LoRA –Ω–∞ fp16/bf16 –æ–Ω–æ –º–æ–∂–µ—Ç –∫–∞—Å—Ç–∏—Ç—å LayerNorm –≤ fp32 => FlashAttention –ø–∞–¥–∞–µ—Ç.
            if self.config.use_4bit or self.config.use_8bit:
                logger.info("üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è QLoRA (prepare_model_for_kbit_training)...")
                self.model = prepare_model_for_kbit_training(
                    self.model,
                    use_gradient_checkpointing=bool(getattr(self.config, "grad_checkpoint", False)),
                )
            else:
                # –û–±—ã—á–Ω–∞—è LoRA: gradient checkpointing ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ UI
                if bool(getattr(self.config, "grad_checkpoint", False)) and hasattr(self.model, "gradient_checkpointing_enable"):
                    try:
                        self.model.gradient_checkpointing_enable()
                        logger.info("‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω –¥–ª—è LoRA (–∏–∑ UI)")
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å gradient checkpointing –¥–ª—è LoRA: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (–µ—Å–ª–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–æ—Å—å)
            if self.config.use_4bit or self.config.use_8bit:
                try:
                    from transformers import BitsAndBytesConfig
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    quantized_params = sum(
                        1 for p in self.model.parameters() 
                        if hasattr(p, 'quant_state') or str(p.dtype) == 'torch.uint8'
                    )
                    if quantized_params > 0:
                        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: –Ω–∞–π–¥–µ–Ω–æ {quantized_params} –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                    else:
                        logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ BitsAndBytesConfig.")
                except:
                    pass
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º "all-linear" –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥—É–ª–µ–π (–∫–∞–∫ –≤ re-grpo)
            # –≠—Ç–æ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ —á–µ–º —Ä—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
            if isinstance(self.config.lora_target_modules, list) and len(self.config.lora_target_modules) > 0:
                target_modules = self.config.lora_target_modules
                logger.info(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º target_modules –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞: {target_modules}")
            else:
                # Fallback –Ω–∞ "all-linear" –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
                target_modules = "all-linear"
                logger.info("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º target_modules='all-linear' –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥—É–ª–µ–π")
            
            # –í–ê–ñ–ù–û: –í–∞–ª–∏–¥–∞—Ü–∏—è LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã (–±–µ–∑ fallback)
            lora_r = self.config.lora_r
            lora_alpha = self.config.lora_alpha
            lora_dropout = self.config.lora_dropout
            
            # –°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã None - —ç—Ç–æ –æ—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            if lora_r is None:
                raise ValueError(
                    "‚ùå lora_r = None! "
                    "lora_r –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_r."
                )
            
            if lora_alpha is None:
                raise ValueError(
                    "‚ùå lora_alpha = None! "
                    "lora_alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. "
                    "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç lora_alpha."
                )
            
            # lora_dropout –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç –∏–∑ GRPOConfig (0.1)
            if lora_dropout is None:
                lora_dropout = self.config.lora_dropout  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç –∏–∑ dataclass
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –∏ –∑–Ω–∞—á–µ–Ω–∏–π
            if not isinstance(lora_r, int) or lora_r <= 0:
                raise ValueError(
                    f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π lora_r: {lora_r} (—Ç–∏–ø: {type(lora_r)}). "
                    f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ. "
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤ UI –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–∏—Å–ª–æ, –∞ –Ω–µ None –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞."
                )
            
            if not isinstance(lora_alpha, (int, float)) or lora_alpha <= 0:
                raise ValueError(
                    f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π lora_alpha: {lora_alpha} (—Ç–∏–ø: {type(lora_alpha)}). "
                    f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ. "
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –≤ UI –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–∏—Å–ª–æ, –∞ –Ω–µ None –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞."
                )
            
            logger.info(f"üîß –°–æ–∑–¥–∞–Ω–∏–µ LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
            logger.info(f"  - r (rank): {lora_r}")
            logger.info(f"  - alpha: {lora_alpha}")
            logger.info(f"  - dropout: {lora_dropout}")
            logger.info(f"  - target_modules: {target_modules}")
            
            # –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –±–µ–∑ —á–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –ø—Ä–æ—Ç–µ–∫–∞—é—Ç —á–µ—Ä–µ–∑ LoRA!
            logger.info("üîß –í–∫–ª—é—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ (enable_input_require_grads)...")
            self.model.enable_input_require_grads()
            
            lora_config = LoraConfig(
                r=lora_r,
                lora_alpha=lora_alpha,
                target_modules=target_modules,
                lora_dropout=lora_dropout,
                bias="none",
                task_type="CAUSAL_LM",
            )
            
            logger.info("üì¶ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∫ –º–æ–¥–µ–ª–∏...")
            self.model = get_peft_model(self.model, lora_config)
            logger.info("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω—ã!")

            # –î–ª—è FlashAttention –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã hidden_states –±—ã–ª–∏ fp16/bf16.
            # –ü–æ—Å–ª–µ LoRA –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥—É–ª–∏/–≤–µ—Å–∞ –º–æ–≥—É—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è –≤ fp32 –∏ –ø—Ä–æ–º–æ—É—Ç–∏—Ç—å dtype –≤ forward.
            # –î–ª—è –ù–ï-–∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏–≤–æ–¥–∏–º LoRA-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ AMP dtype (bf16/fp16).
            if not (self.config.use_4bit or self.config.use_8bit):
                mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                if mp in ("bf16", "fp16") and torch.cuda.is_available():
                    target_dtype = torch.bfloat16 if mp == "bf16" else torch.float16
                    try:
                        casted = 0
                        for name, p in self.model.named_parameters():
                            if "lora" in name.lower() and getattr(p, "dtype", None) != target_dtype:
                                p.data = p.data.to(target_dtype)
                                casted += 1
                        if casted > 0:
                            logger.info(f"‚úÖ –ü—Ä–∏–≤–µ–ª–∏ {casted} LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫ dtype={target_dtype} (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å flash-attn)")
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–≤–µ—Å—Ç–∏ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ AMP dtype: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA
            if torch.cuda.is_available():
                memory_after_lora = torch.cuda.memory_allocated() / (1024 ** 2)
                logger.info(f"üíæ –ü–∞–º—è—Ç—å CUDA –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA: {memory_after_lora:.1f} MB (+{memory_after_lora - memory_before_lora:.1f} MB)")
            
            # PEFT –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
            logger.info("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö (–æ—Ç PEFT):")
            self.model.print_trainable_parameters()
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–æ–ª—å–∫–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã trainable
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())
            frozen_params = total_params - trainable_params
            trainable_percent = 100 * trainable_params / total_params if total_params > 0 else 0
            
            logger.info(f"üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
            logger.info(f"  - –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
            logger.info(f"  - Trainable (LoRA): {trainable_params:,} ({trainable_percent:.2f}%)")
            logger.info(f"  - Frozen (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å): {frozen_params:,} ({100 - trainable_percent:.2f}%)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–æ–ª—å–∫–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–±—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            non_lora_trainable = 0
            lora_trainable = 0
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    if 'lora' in name.lower():
                        lora_trainable += param.numel()
                    else:
                        non_lora_trainable += param.numel()
            
            if non_lora_trainable > 0:
                logger.warning(
                    f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {non_lora_trainable:,} trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ë–ï–ó 'lora' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏! "
                    f"–≠—Ç–æ –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å —á—Ç–æ LoRA –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ."
                )
            else:
                logger.info(f"‚úÖ –í—Å–µ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã - —ç—Ç–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã ({lora_trainable:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
            
            if trainable_percent > 5.0:
                logger.warning(
                    f"‚ö†Ô∏è  –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({trainable_percent:.2f}%)! "
                    f"–í–æ–∑–º–æ–∂–Ω–æ LoRA –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –û–∂–∏–¥–∞–µ—Ç—Å—è < 1% –¥–ª—è LoRA."
                )
            elif trainable_percent < 0.1:
                logger.warning(
                    f"‚ö†Ô∏è  –°–ª–∏—à–∫–æ–º –º–∞–ª–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({trainable_percent:.2f}%)! "
                    f"–í–æ–∑–º–æ–∂–Ω–æ LoRA –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ."
                )
            else:
                logger.info(f"‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –Ω–æ—Ä–º–µ ({trainable_percent:.2f}%)")
            
        except ImportError:
            logger.warning("peft –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, LoRA –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            self.config.use_lora = False
    
    def _setup_wandb(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç Weights & Biases –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        try:
            import wandb
            
            self.wandb_run = wandb.init(
                project=self.config.wandb_project,
                config=self.config.to_dict(),
                name=f"grpo_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            )
            logger.info(f"W&B –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {wandb.run.name}")
            
        except ImportError:
            logger.warning("wandb –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            self.config.use_wandb = False
    
    def _setup_optimizer(self, num_training_steps: int):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ scheduler."""
        logger.info(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞:")
        
        # –ü—Ä–∏ ZeRO-3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã sharded - –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–¥—Å—á—ë—Ç
        if getattr(self, 'is_deepspeed_zero3', False):
            # –î–ª—è ZeRO-3: –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã trainable –ø—Ä–∏ full fine-tuning
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ü–µ–Ω–∫—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –º–æ–¥–µ–ª–∏
            try:
                from transformers import AutoConfig
                model_config = AutoConfig.from_pretrained(self.model_name)
                hidden = getattr(model_config, 'hidden_size', 768)
                layers = getattr(model_config, 'num_hidden_layers', 12)
                vocab = getattr(model_config, 'vocab_size', 32000)
                total_params = vocab * hidden + layers * 12 * hidden * hidden
                num_trainable = total_params  # full fine-tuning
                logger.info(f"  ‚ö° ZeRO-3: –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞")
                logger.info(f"  - Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ~{num_trainable:,} (–æ—Ü–µ–Ω–∫–∞)")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ZeRO-3: {e}")
                num_trainable = 1  # placeholder –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è division by zero
                total_params = 1
            trainable_params = list(self.model.parameters())  # –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ZeRO-3
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Å—á—ë—Ç –¥–ª—è –Ω–µ-ZeRO-3
            trainable_params = [p for p in self.model.parameters() if p.requires_grad]
            num_trainable = sum(p.numel() for p in trainable_params)
            total_params = sum(p.numel() for p in self.model.parameters())
            
            if total_params > 0:
                logger.info(f"  - Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_trainable:,} / {total_params:,} ({100*num_trainable/total_params:.2f}%)")
            else:
                logger.info(f"  - Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_trainable:,}")
            logger.info(f"  - –ì—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(trainable_params)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        # –î–ª—è ZeRO-3 –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É –ø—Ä–æ–≤–µ—Ä–∫—É - DeepSpeed —Å–∞–º —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        if len(trainable_params) == 0 and not getattr(self, 'is_deepspeed_zero3', False):
            raise RuntimeError("‚ùå –ù–µ—Ç trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ DeepSpeed (–¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞)
        uses_deepspeed = (
            self.accelerator is not None and 
            getattr(self.accelerator.state, 'deepspeed_plugin', None) is not None
        )
        
        # –ü—Ä–∏ DeepSpeed –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π AdamW - bitsandbytes –º–æ–∂–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å CPU offload
        if uses_deepspeed:
            logger.info("‚ö° DeepSpeed —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π AdamW (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å ZeRO offload)")
            self.optimizer = torch.optim.AdamW(
                trainable_params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
            )
        else:
            try:
                from bitsandbytes.optim import AdamW8bit
                logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è AdamW8bit (8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)")
                self.optimizer = AdamW8bit(
                    trainable_params,
                    lr=self.config.learning_rate,
                    weight_decay=self.config.weight_decay,
                )
            except ImportError:
                logger.info("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π AdamW (bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)")
                self.optimizer = torch.optim.AdamW(
                    trainable_params,
                    lr=self.config.learning_rate,
                    weight_decay=self.config.weight_decay,
                )
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        # AdamW —Ö—Ä–∞–Ω–∏—Ç: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (fp16), momentum (fp16), variance (fp16) = 3x trainable_params
        optimizer_memory_mb = (num_trainable * 3 * 2) / (1024 ** 2)  # 3 —Å–æ—Å—Ç–æ—è–Ω–∏—è * 2 bytes (fp16)
        logger.info(f"üíæ –ü—Ä–∏–º–µ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: ~{optimizer_memory_mb:.1f} MB")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        # –î–ª—è ZeRO-3 –ø—Ä–æ–ø—É—Å–∫–∞–µ–º - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã sharded
        if not getattr(self, 'is_deepspeed_zero3', False):
            optimizer_param_count = sum(p.numel() for group in self.optimizer.param_groups for p in group['params'])
            if optimizer_param_count != num_trainable:
                logger.warning(
                    f"‚ö†Ô∏è  –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç {optimizer_param_count:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, "
                    f"–∞ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {num_trainable:,}"
                )
            else:
                logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ({optimizer_param_count:,})")
        else:
            logger.info(f"‚ö° ZeRO-3: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (sharded)")
        
        # Scheduler
        # –í–ê–ñ–ù–û: scheduler.step() –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ optimizer-step, –ø–æ—ç—Ç–æ–º—É num_training_steps –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ optim-—à–∞–≥–∞—Ö.
        min_lr_ratio = float(getattr(self.config, "min_lr_ratio", 0.0) or 0.0)
        if min_lr_ratio > 0:
            from torch.optim.lr_scheduler import LambdaLR
            warmup = int(self.config.warmup_steps or 0)
            total = max(int(num_training_steps), 1)

            def lr_lambda(step: int):
                # warmup: 0 -> 1
                if warmup > 0 and step < warmup:
                    return float(step) / float(max(1, warmup))
                # cosine with floor
                denom = max(1, total - warmup)
                progress = float(step - warmup) / float(denom)
                progress = min(max(progress, 0.0), 1.0)
                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))
                return min_lr_ratio + (1.0 - min_lr_ratio) * cosine

            self.scheduler = LambdaLR(self.optimizer, lr_lambda=lr_lambda)
        else:
            self.scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=self.config.warmup_steps,
                num_training_steps=num_training_steps,
            )
        
        # Accelerate prepare
        if self.accelerator:
            self.model, self.optimizer, self.scheduler = self.accelerator.prepare(
                self.model, self.optimizer, self.scheduler
            )
            try:
                def _strip_fp32_convert(m):
                    if m is None:
                        return
                    fwd = getattr(m, "forward", None)
                    if fwd is not None and hasattr(fwd, "model_forward"):
                        m.forward = fwd.model_forward  # type: ignore[attr-defined]

                # accelerate –º–æ–∂–µ—Ç –Ω–∞–≤–µ—Å–∏—Ç—å ConvertOutputsToFp32 –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –æ–±—ë—Ä—Ç–æ–∫
                _strip_fp32_convert(self.model)
                _strip_fp32_convert(getattr(self.model, "module", None))
                base = self.accelerator.unwrap_model(self.model)
                _strip_fp32_convert(base)
                _strip_fp32_convert(getattr(base, "module", None))
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫–ª—é—á–∏—Ç—å accelerate convert_to_fp32: {e}")

        # ü¶Å –°–æ–∑–¥–∞—ë–º Loss —Ñ—É–Ω–∫—Ü–∏—é –ü–û–°–õ–ï prepare() ‚Äî –Ω—É–∂–Ω–∞ unwrapped –º–æ–¥–µ–ª—å –¥–ª—è Liger Fused Loss
        self._create_loss_function()

        # Rollout engine (–æ—Ç–¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏) –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ü–û–°–õ–ï prepare(),
        # —á—Ç–æ–±—ã training –º–æ–¥–µ–ª—å —É–∂–µ –±—ã–ª–∞ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ–±—ë—Ä—Ç–∫–µ (DDP/DeepSpeed).
        self._setup_rollout_engine()
    
    def train(
        self,
        dataset: RLDataset,
        eval_dataset: Optional[RLDataset] = None,
    ):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è GRPO.
        
        Args:
            dataset: –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            eval_dataset: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.setup()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        num_prompts = len(dataset)
        # –û—Ü–µ–Ω–∫–∞ rollout-—à–∞–≥–æ–≤ (–¥–ª—è –ª–æ–≥–æ–≤/—à–µ–¥—É–ª–µ—Ä–∞). –í multi-gpu –≥–ª–æ–±–∞–ª—å–Ω–æ –∑–∞ —à–∞–≥ –ø—Ä–æ—Ö–æ–¥–∏—Ç batch_size * num_processes.
        world = int(self.accelerator.num_processes) if self.accelerator is not None else 1
        denom = max(int(self.config.batch_size) * max(world, 1), 1)
        steps_per_epoch = math.ceil(num_prompts / denom)
        total_steps_uncapped = steps_per_epoch * self.config.num_epochs
        
        # –õ–∏–º–∏—Ç "–ø–æ –¥–∞–Ω–Ω—ã–º": —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ —Ä–µ–∞–ª—å–Ω–æ —Ö–æ—Ç–∏–º –ø—Ä–æ–π—Ç–∏ (–ø–æ–Ω—è—Ç–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞).
        planned_prompts = int(num_prompts) * int(self.config.num_epochs)
        if getattr(self.config, "max_prompts", None):
            try:
                planned_prompts = min(planned_prompts, int(self.config.max_prompts))
            except Exception:
                pass
        rollout_total_steps = math.ceil(planned_prompts / denom) if planned_prompts > 0 else 0
        
        if self.config.max_steps:
            rollout_total_steps = rollout_total_steps  # max_steps ‚Äî —ç—Ç–æ –ª–∏–º–∏—Ç optim_step, –Ω–µ rollout_step
        
        # –î–ª—è UI/ETA: —Ñ–∏–∫—Å–∏—Ä—É–µ–º –ø–ª–∞–Ω–æ–≤—ã–µ —à–∞–≥–∏ (–Ω–µ "max_steps", –∞ —Ä–µ–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç/–ª–∏–º–∏—Ç).
        self.planned_total_steps = int(rollout_total_steps) if rollout_total_steps else 0
        self.planned_total_steps_uncapped = int(total_steps_uncapped) if total_steps_uncapped else 0

        # –î–ª—è scheduler: –æ—Ü–µ–Ω–∏–≤–∞–µ–º —á–∏—Å–ª–æ optimizer steps.
        # 1 rollout (–Ω–∞ –û–î–ò–ù –ø—Ä–æ—Ü–µ—Å—Å) –¥–∞—ë—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ batch_size * group_size –æ–ø—ã—Ç–æ–≤.
        # exp_loader drop_last=True => —á–∏—Å–ª–æ –º–∏–∫—Ä–æ–±–∞—Ç—á–µ–π = floor(exps / train_batch_size)
        est_exps = int(self.config.batch_size) * int(self.config.group_size)
        est_micro_batches = max(1, est_exps // max(1, int(self.config.train_batch_size)))
        est_optim_steps_per_rollout = math.ceil(est_micro_batches / max(1, int(self.config.gradient_accumulation_steps)))
        est_optim_steps_per_rollout *= max(1, int(self.config.epochs_per_step))

        planned_optim_steps = int(rollout_total_steps) * int(est_optim_steps_per_rollout)
        if self.config.max_steps:
            # max_steps ‚Äî —è–≤–Ω—ã–π –ª–∏–º–∏—Ç optim_step –∏–∑ UI
            planned_optim_steps = min(int(planned_optim_steps), int(self.config.max_steps))
        self.planned_optim_total_steps = max(int(planned_optim_steps), 1)
        
        logger.info(
            f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {num_prompts} –ø—Ä–æ–º–ø—Ç–æ–≤, ~{int(rollout_total_steps)} rollout-—à–∞–≥–æ–≤, "
            f"~{int(self.planned_optim_total_steps)} optim-—à–∞–≥–æ–≤"
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        self._setup_optimizer(self.planned_optim_total_steps)
        
        # –°–æ–∑–¥–∞—ë–º output –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –í–ê–ñ–ù–û: –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π heartbeat –≤ metrics.jsonl
        # –≠—Ç–æ —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç UI —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ä—Ç–æ–≤–∞–ª–æ
        if self.is_main_process:
            initial_metrics = {
                "step": 0,
                "status": "training_started",
                "epoch": 0,
                "total_prompts": num_prompts,
                "planned_optim_steps": self.planned_optim_total_steps,
            }
            self._log_metrics(initial_metrics, jsonl_only=True)
            logger.info("üìù –ó–∞–ø–∏—Å–∞–Ω –Ω–∞—á–∞–ª—å–Ω—ã–π heartbeat –≤ metrics.jsonl")
        
        # DataLoader –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
        prompt_loader = DataLoader(
            list(range(len(dataset))),
            batch_size=self.config.batch_size,
            shuffle=True,
            drop_last=False,
        )
        # –í–ê–ñ–ù–û (–∫–∞–∫ –≤ re-grpo accelerate): –ø—Ä–∏ multi-gpu –¥–µ–ª–∏–º –ø—Ä–æ–º–ø—Ç—ã –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
        if self.accelerator is not None:
            prompt_loader = self.accelerator.prepare(prompt_loader)
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
        for epoch in range(self.config.num_epochs):
            epoch_metrics = self._train_epoch(
                dataset=dataset,
                prompt_loader=prompt_loader,
                epoch=epoch,
                eval_dataset=eval_dataset,
            )
            
            logger.info(f"Epoch {epoch + 1}/{self.config.num_epochs} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            logger.info(f"  Mean reward: {epoch_metrics.get('mean_reward', 0):.4f}")
            
            if self.config.max_steps and self.global_step >= self.config.max_steps:
                logger.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç max_steps, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
                break
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–≤—Å–µ —Ä–∞–Ω–∫–∏ –¥–æ–ª–∂–Ω—ã –≤–æ–π—Ç–∏ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏)
        self._save_checkpoint(output_dir / "final", is_final=True)
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å –≤ metrics.jsonl –¥–ª—è UI
        if self.is_main_process:
            world = int(self.accelerator.num_processes) if self.accelerator is not None else 1
            prompts_processed = int(self.rollout_step) * int(self.config.batch_size) * max(world, 1)
            final_metrics = {
                "step": self.global_step,
                "rollout_step": self.rollout_step,
                "status": "completed",
                "total_prompts_processed": prompts_processed,
            }
            self._log_metrics(final_metrics, jsonl_only=True)
            logger.info("üìù –ó–∞–ø–∏—Å–∞–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å 'completed' –≤ metrics.jsonl")
        
        if self.wandb_run:
            self.wandb_run.finish()
    
    def _train_epoch(
        self,
        dataset: RLDataset,
        prompt_loader: DataLoader,
        epoch: int,
        eval_dataset: Optional[RLDataset] = None,
    ) -> Dict[str, float]:
        """–û–¥–∏–Ω epoch –æ–±—É—á–µ–Ω–∏—è."""
        epoch_rewards = []
        epoch_losses = []
        
        pbar = tqdm(
            prompt_loader,
            desc=f"Epoch {epoch + 1}",
            disable=not self.is_main_process,
        )
        
        for batch_idx, prompt_indices in enumerate(pbar):
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –∏ –æ—Ç–≤–µ—Ç—ã
            batch_samples = [dataset[i] for i in prompt_indices]
            prompts = [
                build_reasoning_prompt(
                    s.prompt,
                    self.tokenizer,
                    self.config.reasoning_format,
                    system_prompt=getattr(self.config, 'user_system_prompt', None),
                )
                for s in batch_samples
            ]
            reference_answers = [s.reference_answer for s in batch_samples]
            metadata_list = [s.metadata if hasattr(s, 'metadata') else {} for s in batch_samples]
            # group_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã (–æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ dynamic sampling —Å –¥–æ–±–æ—Ä–æ–º)
            desired_groups = len(batch_samples)
            group_ids = self._next_group_uids(desired_groups)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è rollout'–æ–≤
            logger.info(f"üé≤ Batch {batch_idx}: –Ω–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤...")
            self.replay_buffer.clear()
            batch_rewards = self._generate_and_collect(
                prompts=prompts,
                reference_answers=reference_answers,
                prompt_ids=group_ids,
                metadata_list=metadata_list,
            )
            logger.info(f"‚úÖ Batch {batch_idx}: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, rewards={len(batch_rewards)}")
            refill_rounds = 0
            # DAPO dynamic sampling: –¥–æ–±–æ—Ä –≥—Ä—É–ø–ø –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (–ù–ï —É–º–µ–Ω—å—à–∞–µ–º batch –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            if self.config.dynamic_sampling and self.config.max_refill_rounds > 0:
                import random
                max_refill_rounds = self.config.max_refill_rounds  # –∏–∑ UI/config (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)
                while self.replay_buffer.get_stats().get("num_groups", 0) < desired_groups and refill_rounds < max_refill_rounds:
                    missing = desired_groups - int(self.replay_buffer.get_stats().get("num_groups", 0))
                    if missing <= 0:
                        break
                    # –¥–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã (—Å replacement –¥–æ–ø—É—Å—Ç–∏–º–æ, –Ω–æ group_id —É–Ω–∏–∫–∞–ª—å–Ω—ã–π)
                    extra_indices = [random.randrange(0, len(dataset)) for _ in range(missing)]
                    extra_samples = [dataset[i] for i in extra_indices]
                    extra_prompts = [
                        build_reasoning_prompt(
                            s.prompt, 
                            self.tokenizer, 
                            self.config.reasoning_format,
                            system_prompt=getattr(self.config, 'user_system_prompt', None),
                        )
                        for s in extra_samples
                    ]
                    extra_refs = [s.reference_answer for s in extra_samples]
                    extra_metadata = [s.metadata if hasattr(s, 'metadata') else {} for s in extra_samples]
                    extra_group_ids = self._next_group_uids(len(extra_samples))
                    extra_rewards = self._generate_and_collect(
                        prompts=extra_prompts,
                        reference_answers=extra_refs,
                        prompt_ids=extra_group_ids,
                        metadata_list=extra_metadata,
                    )
                    batch_rewards.extend(extra_rewards)
                    refill_rounds += 1

                if self.replay_buffer.get_stats().get("num_groups", 0) < desired_groups:
                    logger.warning(
                        f"‚ö†Ô∏è dynamic_sampling: –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±—Ä–∞—Ç—å –≥—Ä—É–ø–ø—ã –¥–æ {desired_groups}. "
                        f"–ü–æ–ª—É—á–∏–ª–æ—Å—å {self.replay_buffer.get_stats().get('num_groups', 0)} –ø–æ—Å–ª–µ {refill_rounds} –¥–æ–±–æ—Ä–æ–≤. "
                        f"–í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞: –º–æ–¥–µ–ª—å –¥–∞—ë—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π reward –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤."
                    )
                
                # üî• Barrier –ø–æ—Å–ª–µ refills —á—Ç–æ–±—ã –≤—Å–µ GPU –∑–∞–∫–æ–Ω—á–∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                if self.accelerator.num_processes > 1:
                    self.accelerator.wait_for_everyone()

            epoch_rewards.extend(batch_rewards)
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω–æ–º –æ–ø—ã—Ç–µ
            buffer_size = len(self.replay_buffer)
            
            # ============================================================
            # üî• –ö–†–ò–¢–ò–ß–ù–û: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –º–µ–∂–¥—É GPU –ø–µ—Ä–µ–¥ training!
            # ============================================================
            # –ü—Ä–∏ multi-GPU dynamic_sampling –º–æ–∂–µ—Ç –¥–∞—Ç—å —Ä–∞–∑–Ω—ã–µ buffer_size –Ω–∞ —Ä–∞–∑–Ω—ã—Ö GPU.
            # –ë–µ–∑ barrier –æ–¥–∏–Ω GPU –∂–¥—ë—Ç –¥—Ä—É–≥–æ–π –≤ DDP forward ‚Üí NCCL timeout!
            if self.accelerator.num_processes > 1:
                # –°–æ–±–∏—Ä–∞–µ–º buffer_size —Å–æ –≤—Å–µ—Ö GPU
                buffer_tensor = torch.tensor([buffer_size], device=self.device)
                all_buffers = self.accelerator.gather(buffer_tensor)
                min_buffer = int(all_buffers.min().item())
                max_buffer = int(all_buffers.max().item())
                
                # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω GPU –∏–º–µ–µ—Ç –ø—É—Å—Ç–æ–π –±—É—Ñ–µ—Ä ‚Äî –≤—Å–µ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç training
                if min_buffer == 0:
                    if buffer_size > 0:
                        logger.warning(
                            f"‚ö†Ô∏è Multi-GPU sync: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º training (–¥—Ä—É–≥–æ–π GPU –∏–º–µ–µ—Ç –ø—É—Å—Ç–æ–π –±—É—Ñ–µ—Ä). "
                            f"Local buffer: {buffer_size}, all buffers: {all_buffers.tolist()}"
                        )
                    buffer_size = 0  # Force skip
                
                # Barrier –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ training
                self.accelerator.wait_for_everyone()
            
            if buffer_size == 0:
                logger.warning(
                    f"‚ö†Ô∏è –ë—É—Ñ–µ—Ä –ø—É—Å—Ç –Ω–∞ —à–∞–≥–µ {self.global_step}! "
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ dynamic_sampling –∏ reward —Ñ—É–Ω–∫—Ü–∏—é."
                )
                train_metrics = {"loss": 0.0, "kl": 0.0, "grad_norm": 0.0}
            else:
                train_metrics = self._train_on_buffer()
            
            epoch_losses.append(train_metrics.get("loss", 0))
            
            # ---- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ----
            # –í–ê–ñ–ù–û: UI –¥–æ–ª–∂–µ–Ω –ø–æ–ª—É—á–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ä–∞–∑—É, –∏–Ω–∞—á–µ –æ–Ω "–∑–∞–≤–∏—Å–∞–µ—Ç" –Ω–∞ STARTING.
            # –ü–æ—ç—Ç–æ–º—É –ø–∏—à–µ–º heartbeat –º–µ—Ç—Ä–∏–∫–∏ –ö–ê–ñ–î–´–ô rollout, –∞ –≤ –∫–æ–Ω—Å–æ–ª—å/W&B ‚Äî –ø–æ log_steps.
            batch_reward_mean = sum(batch_rewards) / len(batch_rewards) if batch_rewards else 0.0
            group_size = max(int(self.config.group_size), 1)
            prompts_generated = int(len(batch_rewards) // group_size) if group_size > 0 else 0
            num_groups_used = int(self.replay_buffer.get_stats().get("num_groups", 0))
            completions_generated = int(len(batch_rewards))
            experiences_tuned = int(len(self.replay_buffer))
            filtered_groups = max(0, prompts_generated - num_groups_used)

            # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Å—á—ë—Ç—á–∏–∫–∏ (–Ω–∞ –∫–∞–∂–¥—ã–π rollout, —á—Ç–æ–±—ã UI –ø–æ–∫–∞–∑—ã–≤–∞–ª "–ø–æ —Ñ–∞–∫—Ç—É")
            self.cum_prompts_generated += prompts_generated
            self.cum_prompts_used += num_groups_used
            self.cum_completions_generated += completions_generated
            self.cum_experiences_tuned += experiences_tuned

            heartbeat = {
                "step": self.global_step,
                "epoch": epoch,
                "batch_reward_mean": batch_reward_mean,
                "buffer_size": buffer_size,
                "rollouts_count": len(batch_rewards),
                "prompts_generated": prompts_generated,
                "prompts_used": num_groups_used,
                "filtered_groups": filtered_groups,
                "completions_generated": completions_generated,
                "experiences_tuned": experiences_tuned,
                "refill_rounds": refill_rounds,
                "cum_prompts_generated": int(self.cum_prompts_generated),
                "cum_prompts_used": int(self.cum_prompts_used),
                "cum_completions_generated": int(self.cum_completions_generated),
                "cum_experiences_tuned": int(self.cum_experiences_tuned),
                **train_metrics,
            }

            # –ü–∏—à–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∂–¥—ã–π rollout (–¥–ª—è UI), –∞ –∫–æ–Ω—Å–æ–ª—å/W&B ‚Äî —Ç–æ–ª—å–∫–æ –ø–æ log_steps.
            should_log = (self.global_step % max(int(self.config.log_steps), 1) == 0)
            self._log_metrics(heartbeat, jsonl_only=(not should_log))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º progress bar
            pbar.set_postfix({
                "reward": f"{sum(batch_rewards) / max(len(batch_rewards), 1):.3f}",
                "loss": f"{train_metrics.get('loss', 0):.4f}",
            })
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            if self.global_step > 0 and self.global_step % self.config.save_steps == 0:
                # –í–ê–ñ–ù–û: –≤ distributed —Ä–µ–∂–∏–º–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –í–°–ï–ú–ò –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏,
                # –∏–Ω–∞—á–µ –≤–æ–∑–º–æ–∂–Ω—ã —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏/—Ç–∞–π–º–∞—É—Ç—ã –Ω–∞ collectives.
                self._save_checkpoint(Path(self.config.output_dir) / f"step_{self.global_step}")

            # Rollout-step –∑–∞–≤–µ—Ä—à—ë–Ω (1 batch –ø—Ä–æ–º–ø—Ç–æ–≤ -> —Å–±–æ—Ä rollout -> train on buffer)
            self.rollout_step += 1
            
            # üî• Barrier –≤ –∫–æ–Ω—Ü–µ batch –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ multi-GPU
            if self.accelerator.num_processes > 1:
                self.accelerator.wait_for_everyone()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º max_steps
            if self.config.max_steps and self.global_step >= self.config.max_steps:
                break

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –ø–æ –¥–∞–Ω–Ω—ã–º (—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å)
            if getattr(self.config, "max_prompts", None):
                try:
                    world = int(self.accelerator.num_processes) if self.accelerator is not None else 1
                    prompts_seen = int(self.rollout_step) * int(self.config.batch_size) * max(world, 1)
                    if prompts_seen >= int(self.config.max_prompts):
                        logger.info(
                            f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç max_prompts={int(self.config.max_prompts)} "
                            f"(–æ—Ü–µ–Ω–∫–∞ prompts_seen={prompts_seen}), –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ"
                        )
                        break
                except Exception:
                    # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ —Ç–∞–∫ ‚Äî –Ω–µ –ª–æ–º–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
                    pass
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if eval_dataset and self.is_main_process:
            eval_metrics = self._evaluate(eval_dataset)
            logger.info(f"Validation: {eval_metrics}")
            self._log_metrics({"val/" + k: v for k, v in eval_metrics.items()})
        
        return {
            "mean_reward": sum(epoch_rewards) / max(len(epoch_rewards), 1),
            "mean_loss": sum(epoch_losses) / max(len(epoch_losses), 1),
        }
    
    def _generate_and_collect(
        self,
        prompts: List[str],
        reference_answers: List[str],
        prompt_ids: Optional[List[int]] = None,
        metadata_list: Optional[List[Dict[str, Any]]] = None,
    ) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç rollout'—ã –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –æ–ø—ã—Ç –≤ –±—É—Ñ–µ—Ä.
        
        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
            reference_answers: –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            prompt_ids: ID –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            metadata_list: Metadata –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–π)
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö rewards
        """
        # –í–ê–ñ–ù–û: –ü—Ä–∏ ZeRO-3 –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–æ–ª–∂–Ω—ã –≤–æ–π—Ç–∏ –≤ generate() —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        # –ò–Ω–∞—á–µ –±—É–¥–µ—Ç deadlock –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if getattr(self, 'is_deepspeed_zero3', False) and self.accelerator is not None:
            logger.info("‚ö° ZeRO-3: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π...")
            self.accelerator.wait_for_everyone()
            logger.info("‚ö° ZeRO-3: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é")
        
        self.model.eval()
        all_rewards = []
        
        # –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–∏
        def reward_wrapper(completion, reference_answer, reasoning_format, is_truncated, metadata=None):
            return self.reward_fn(
                completion=completion,
                reference_answer=reference_answer,
                reasoning_format=reasoning_format,
                is_truncated=is_truncated,
                metadata=metadata or {},
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º rollout'—ã.
        # –í–ê–ñ–ù–û: –î–ª—è ZeRO-3/FSDP generation –≤–Ω—É—Ç—Ä–∏ training engine –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ –ø–æ—Ä—è–¥–∫–∏ –º–µ–¥–ª–µ–Ω–Ω–µ–µ.
        # –ï—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω rollout_engine ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é, –∞ training –º–æ–¥–µ–ª—å
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è teacher-forcing logprobs + backprop.
        use_rollout_engine = bool(getattr(self.config, "use_rollout_engine", False))
        backend = getattr(self.config, "rollout_engine_backend", "hf")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å rollout engine
        # –ü—Ä–∏ vLLM + multi-GPU –æ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ main process
        rollout_engine_available = use_rollout_engine and self.rollout_engine is not None
        
        if rollout_engine_available:
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞ training -> rollout (–æ–±—ã—á–Ω–æ trainable-only, —Ç.–µ. LoRA)
            self._sync_rollout_engine_weights(force=False)
            
            if backend == "hf":
                self.rollout_engine.ensure_on_device()
                rollouts = generate_rollouts(
                    model=self.rollout_engine.model,  # type: ignore[arg-type]
                    tokenizer=self.tokenizer,
                    prompts=prompts,
                    reference_answers=reference_answers,
                    reward_fn=reward_wrapper,
                    config=self.config,
                    accelerator=None,          # rollout –º–æ–¥–µ–ª—å –Ω–µ DeepSpeed/DDP wrapper
                    reference_model=None,      # ref logprobs —Å—á–∏—Ç–∞–µ–º –Ω–∞ training —Å—Ç–æ—Ä–æ–Ω–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    device=self.rollout_engine.device,
                    prompt_ids=prompt_ids,
                    metadata_list=metadata_list,
                )
                self.rollout_engine.maybe_offload()
            elif backend == "vllm":
                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ —Ç–∏–ø–∞: VLLMRolloutEngine –∏ VLLMSubprocessEngine
                if not isinstance(self.rollout_engine, (VLLMRolloutEngine, VLLMSubprocessEngine)):
                    raise RuntimeError("rollout_engine backend mismatch (expected VLLMRolloutEngine or VLLMSubprocessEngine)")
                
                # VLLMSubprocessEngine —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ IPC ‚Äî –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å GPU
                # VLLMRolloutEngine –Ω–∞ —Ç–æ–π –∂–µ GPU ‚Äî —Ç–æ–∂–µ –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å
                rollouts = generate_rollouts_vllm(
                    vllm_engine=self.rollout_engine,
                    tokenizer=self.tokenizer,
                    prompts=prompts,
                    reference_answers=reference_answers,
                    reward_fn=reward_wrapper,
                    config=self.config,
                    prompt_ids=prompt_ids,
                    metadata_list=metadata_list,
                )
            else:
                raise NotImplementedError(f"Unknown rollout_engine_backend='{backend}'")
        else:
            # –í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º accelerator –¥–ª—è unwrap –º–æ–¥–µ–ª–∏ (DDP –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç generate –Ω–∞–ø—Ä—è–º—É—é)
            rollouts = generate_rollouts(
                model=self.model,
                tokenizer=self.tokenizer,
                prompts=prompts,
                reference_answers=reference_answers,
                reward_fn=reward_wrapper,
                config=self.config,
                accelerator=self.accelerator,
                reference_model=self.reference_model,
                device=self.device,
                prompt_ids=prompt_ids,
                metadata_list=metadata_list,
            )
        
        # –í–ê–ñ–ù–û: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è ZeRO-3
        if getattr(self, 'is_deepspeed_zero3', False) and self.accelerator is not None:
            logger.debug("‚ö° ZeRO-3: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
            self.accelerator.wait_for_everyone()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Experience –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä
        # –í–ê–ñ–ù–û: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ —Å—Ä–∞–∑—É —É–¥–∞–ª—è–µ–º, —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ç—å –ø–∞–º—è—Ç—å
        num_rollouts = len(rollouts)
        for i in range(num_rollouts):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ —É–¥–∞–ª—è–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞, —á—Ç–æ–±—ã –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —Å—Å—ã–ª–∫—É
            rollout = rollouts.pop(0)
            
            # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º rewards –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –¥–∞–∂–µ –µ—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞
            rollout_rewards = rollout.rewards.tolist()
            all_rewards.extend(rollout_rewards)
            
            # –û—Ç–ª–∞–¥–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö rollout'–æ–≤)
            if i < 2:
                logger.debug(
                    f"Rollout {rollout.metadata.get('prompt_idx', 0)}: "
                    f"rewards={[f'{r:.3f}' for r in rollout_rewards]}, "
                    f"mean={sum(rollout_rewards)/len(rollout_rewards):.3f}, "
                    f"completions_len={[len(c) for c in rollout.completions[:2]]}"
                )
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–µ–º–ø–ª—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏)
            if self.global_step % max(self.config.log_steps, 1) == 0 and rollout.metadata.get("prompt_idx", 0) == 0:
                self._log_sample(rollout)
            
            experiences = rollout_to_experiences(
                rollout=rollout,
                model=self.model,
                tokenizer=self.tokenizer,
                config=self.config,
                reference_model=self.reference_model,
                device=self.device,
                accelerator=self.accelerator,
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º rollout
            prompt_idx = rollout.metadata.get("prompt_id", rollout.metadata.get("prompt_idx", 0))
            rollout_completions_len = len(rollout.completions)
            
            # üéì SDPO: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è self-distillation
            if hasattr(self, '_successful_trajectories') and isinstance(self.loss_fn, SDPOLoss):
                sdpo_threshold = getattr(self.config, 'sdpo_success_threshold', 0.5)
                for comp_idx, (reward, completion) in enumerate(zip(rollout_rewards, rollout.completions)):
                    if reward >= sdpo_threshold:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä—É (prompt, completion) –¥–ª—è reprompting
                        if prompt_idx not in self._successful_trajectories:
                            self._successful_trajectories[prompt_idx] = []
                        
                        trajectory_data = {
                            'prompt': rollout.prompt,  # –ò—Å—Ö–æ–¥–Ω—ã–π prompt (—Ç–µ–∫—Å—Ç)
                            'completion': completion,   # –£—Å–ø–µ—à–Ω—ã–π completion
                            'reward': reward,
                        }
                        
                        # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —É—Å–ø–µ—à–Ω—ã—Ö (—á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–¥—É–≤–∞—Ç—å –ø–∞–º—è—Ç—å)
                        if len(self._successful_trajectories[prompt_idx]) < 5:
                            self._successful_trajectories[prompt_idx].append(trajectory_data)
                        else:
                            # –ó–∞–º–µ–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Å—Ç–∞—Ä—ã–π
                            import random
                            replace_idx = random.randint(0, 4)
                            self._successful_trajectories[prompt_idx][replace_idx] = trajectory_data
            
            # –Ø–≤–Ω–æ —É–¥–∞–ª—è–µ–º rollout –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            del rollout
            
            # –í–ê–ñ–ù–û: –ü–µ—Ä–µ–º–µ—â–∞–µ–º –æ–ø—ã—Ç—ã –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM (–∫–∞–∫ –≤ re-grpo)
            # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è Multi-GPU –∏ –±–æ–ª—å—à–∏—Ö –±—É—Ñ–µ—Ä–æ–≤
            cpu_device = torch.device("cpu")
            experiences_cpu = [exp.to(cpu_device) for exp in experiences]
            
            # Dynamic sampling: —Ñ–∏–ª—å—Ç—Ä—É–µ–º zero-gradient –≥—Ä—É–ø–ø—ã
            filter_zero = self.config.dynamic_sampling
            added = self.replay_buffer.append_group(
                experiences_cpu,
                prompt_id=prompt_idx,
                filter_zero_gradient=filter_zero,
            )
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º GPU –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –Ω–∞ CPU
            del experiences
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            if not added and filter_zero:
                logger.debug(
                    f"–ì—Ä—É–ø–ø–∞ {prompt_idx} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞ "
                    f"(zero-gradient, rewards={rollout_rewards})"
                )
            
            self.total_rollouts += rollout_completions_len
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ rewards
        if all_rewards:
            logger.debug(
                f"Batch rewards: mean={sum(all_rewards)/len(all_rewards):.4f}, "
                f"min={min(all_rewards):.4f}, max={max(all_rewards):.4f}, "
                f"count={len(all_rewards)}"
            )
        else:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç rewards –≤ batch! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ reward —Ñ—É–Ω–∫—Ü–∏—é –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.")
        
        return all_rewards
    
    def _train_on_buffer(self) -> Dict[str, float]:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω–æ–º –æ–ø—ã—Ç–µ –≤ –±—É—Ñ–µ—Ä–µ.
        
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        self.model.train()
        
        buffer_size = len(self.replay_buffer)
        if buffer_size == 0:
            logger.warning(
                "‚ö†Ô∏è –ë—É—Ñ–µ—Ä –ø—É—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ. "
                "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –≤—Å–µ –≥—Ä—É–ø–ø—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã (dynamic_sampling) –∏–ª–∏ –Ω–µ—Ç –æ–ø—ã—Ç–∞."
            )
            return {"loss": 0.0, "kl": 0.0, "grad_norm": 0.0}
        
        logger.debug(f"–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±—É—Ñ–µ—Ä–µ: {buffer_size} –æ–ø—ã—Ç–æ–≤")
        
        # DataLoader –¥–ª—è experience
        exp_loader = DataLoader(
            self.replay_buffer.items,
            batch_size=self.config.train_batch_size,
            shuffle=True,
            drop_last=True,
            collate_fn=join_experience_batch,
        )
        
        # ============================================================
        # üî• –ö–†–ò–¢–ò–ß–ù–û: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–∞—Ç—á–µ–π –º–µ–∂–¥—É GPU!
        # ============================================================
        # –ü—Ä–∏ multi-GPU –∫–∞–∂–¥—ã–π GPU –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –±—É—Ñ–µ—Ä–µ.
        # –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ä–∞–∑–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∏—Ç–µ—Ä–∞—Ü–∏–π ‚Üí DDP deadlock!
        local_num_batches = len(exp_loader)
        if self.accelerator is not None and self.accelerator.num_processes > 1:
            # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π —Å–æ –≤—Å–µ—Ö GPU
            num_batches_tensor = torch.tensor([local_num_batches], device=self.device)
            all_num_batches = self.accelerator.gather(num_batches_tensor)
            min_batches = int(all_num_batches.min().item())
            
            if min_batches != local_num_batches:
                logger.info(
                    f"üîÑ Multi-GPU sync: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–æ {min_batches} "
                    f"(local={local_num_batches}, all={all_num_batches.tolist()})"
                )
            local_num_batches = min_batches
        
        epoch_losses = []
        epoch_kls = []
        epoch_grad_norms = []
        
        from contextlib import nullcontext

        for epoch_idx in range(self.config.epochs_per_step):
            for batch_idx, exp_batch in enumerate(exp_loader):
                # üî• –ü—Ä–µ—Ä—ã–≤–∞–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ min_batches (–¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ multi-GPU)
                if batch_idx >= local_num_batches:
                    break
                exp_batch = exp_batch.to(self.device)
                accumulate_ctx = (
                    self.accelerator.accumulate(self.model)
                    if self.accelerator is not None
                    else nullcontext()
                )
                
                # –í–ê–ñ–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ OOM
                batch_size = exp_batch.sequences.size(0)
                max_seq_len = exp_batch.sequences.size(1)
                total_tokens = batch_size * max_seq_len
                
                if batch_idx == 0 and epoch_idx == 0:
                    # –í–ê–ñ–ù–û: –î–ª—è DDP –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ unwrap –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ config
                    if self.accelerator is not None:
                        unwrapped_model = self.accelerator.unwrap_model(self.model)
                        vocab_size = unwrapped_model.config.vocab_size
                    else:
                        vocab_size = self.model.config.vocab_size
                    
                    estimated_logits_memory = total_tokens * vocab_size * 2 / (1024**3)  # GB (fp16)
                    logger.info(
                        f"üìä –†–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: "
                        f"batch_size={batch_size}, max_seq_len={max_seq_len}, "
                        f"total_tokens={total_tokens:,}, "
                        f"–ø—Ä–∏–º–µ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è logits: ~{estimated_logits_memory:.2f} GB"
                    )
                
                # –ó–∞—â–∏—Ç–∞ –æ—Ç –æ—á–µ–≤–∏–¥–Ω–æ–≥–æ OOM: –æ—Ü–µ–Ω–∏–≤–∞–µ–º –º–∏–Ω–∏–º—É–º –ø–æ–¥ logits + —Ä–∞–∑—É–º–Ω—ã–π overhead –∏ —Å–≤–µ—Ä—è–µ–º —Å–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç—å—é.
                # –≠—Ç–æ –ù–ï "–∞–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞" ‚Äî –ø—Ä–æ—Å—Ç–æ —Ä–∞–Ω–Ω—è—è, –ø–æ–Ω—è—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
                if torch.cuda.is_available():
                    try:
                        free_bytes, total_bytes = torch.cuda.mem_get_info(self.device)
                        free_gb = free_bytes / (1024**3)
                        # logits fp16/bf16 + –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±—É—Ñ–µ—Ä—ã + –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ => –æ—á–µ–Ω—å –≥—Ä—É–±–æ 2.2x
                        # (–¥–ª—è Qwen —Å –±–æ–ª—å—à–∏–º vocab –∏ –¥–ª–∏–Ω–Ω–æ–π seq —ç—Ç–æ –±–ª–∏–∂–µ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏).
                        required_gb = estimated_logits_memory * 2.2
                        # –û—Å—Ç–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ –≤–æ–∑–¥—É—Ö–∞ –ø–æ–¥ allocator/—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é
                        if required_gb > free_gb * 0.9:
                            raise RuntimeError(
                                "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM –¥–ª—è —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è GRPO.\n"
                                f"  - train_batch_size={batch_size}\n"
                                f"  - max_seq_len={max_seq_len}\n"
                                f"  - –æ—Ü–µ–Ω–∫–∞ logits(fp16/bf16)‚âà{estimated_logits_memory:.2f} GB\n"
                                f"  - –æ—Ü–µ–Ω–∫–∞ –ø–∏–∫–∞ (—Å overhead)‚âà{required_gb:.2f} GB\n"
                                f"  - —Å–≤–æ–±–æ–¥–Ω–æ —Å–µ–π—á–∞—Å‚âà{free_gb:.2f} GB (–∏–∑ {total_bytes/(1024**3):.2f} GB)\n\n"
                                "–ß—Ç–æ –¥–µ–ª–∞—Ç—å (–±–µ–∑ –∞–≤—Ç–æ-–ø–æ–¥—Å—Ç—Ä–æ–µ–∫):\n"
                                "  - –£–º–µ–Ω—å—à–∏—Ç–µ **Train Batch Size** (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: 1‚Äì4)\n"
                                "  - –£–º–µ–Ω—å—à–∏—Ç–µ **Max new tokens**\n"
                                "  - –í–∫–ª—é—á–∏—Ç–µ **LoRA/QLoRA** –≤–º–µ—Å—Ç–æ full fine-tuning\n"
                                "  - –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–∫–ª—é—á–∏—Ç–µ/—É–≤–µ–ª–∏—á—å—Ç–µ gradient checkpointing (–µ—Å–ª–∏ –¥–æ–±–∞–≤–∏—Ç–µ –≤ UI)\n"
                            )
                    except Exception:
                        # –ï—Å–ª–∏ mem_get_info –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω/–ø–∞–¥–∞–µ—Ç ‚Äî –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–∏–µ
                        pass
                
                # Forward pass –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π API –¥–ª—è autocast (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º deprecated warning)
                # –í–ê–ñ–ù–û: autocast –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ CUDA
                mp = (getattr(self.config, "mixed_precision", None) or "bf16").lower()
                use_autocast = (self.accelerator is not None and torch.cuda.is_available() and mp != "no")
                
                if use_autocast:
                    amp_dtype = torch.bfloat16 if mp == "bf16" else torch.float16
                    autocast_context = torch.amp.autocast("cuda", enabled=True, dtype=amp_dtype)
                else:
                    from contextlib import nullcontext
                    autocast_context = nullcontext()
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –≤ train —Ä–µ–∂–∏–º–µ
                if not self.model.training:
                    self.model.train()
                
                # –í–ê–ñ–ù–û: –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ forward pass (–Ω–∞ —Å–ª—É—á–∞–π –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è)
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                with accumulate_ctx:
                    with autocast_context:
                        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–µ—Ä–µ–¥ —Ç—è–∂–µ–ª–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ª–æ–≥–∏—Ç–æ–≤
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        
                        # ü¶Å –†–∞–∑–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è Liger Fused Loss –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ loss
                        if self.use_liger_fused_loss and isinstance(self.loss_fn, LigerFusedGRPOLoss):
                            # LIGER FUSED PATH: hidden_states -> fused loss (–ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ–º logits!)
                            # Forward pass —Å output_hidden_states=True
                            outputs = self.model(
                                input_ids=exp_batch.sequences,
                                attention_mask=exp_batch.attention_mask,
                                output_hidden_states=True,
                                use_cache=False,
                            )
                            
                            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state
                            hidden_states = outputs.hidden_states[-1]
                            
                            # –í—ã—á–∏—Å–ª—è–µ–º loss —á–µ—Ä–µ–∑ Liger Fused Loss
                            loss, metrics = self.loss_fn.forward_with_experience(
                                hidden_states=hidden_states,
                                experience=exp_batch,
                            )
                            
                            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                            del outputs, hidden_states
                        else:
                            # STANDARD PATH: logits -> log_probs -> loss
                            log_probs = compute_log_probs(
                                self.model,
                                exp_batch.sequences,
                                exp_batch.attention_mask,
                                accelerator=self.accelerator,
                            )
                            
                            # üéì SDPO: –ø–æ–ª—É—á–∞–µ–º teacher_log_probs —á–µ—Ä–µ–∑ reprompting
                            teacher_log_probs = None
                            distillation_mask = None
                            student_topk_log_probs = None
                            teacher_topk_log_probs = None
                            
                            if isinstance(self.loss_fn, SDPOLoss) and hasattr(self, '_successful_trajectories'):
                                # üî• –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Top-K Distillation + Teacher Module (–∏–∑ verl)
                                teacher_log_probs, distillation_mask, student_topk_log_probs, teacher_topk_log_probs = \
                                    self._get_teacher_log_probs(
                                        exp_batch=exp_batch,
                                        device=exp_batch.sequences.device,
                                    )
                            
                            # –í—ã–∑—ã–≤–∞–µ–º loss —Ñ—É–Ω–∫—Ü–∏—é
                            if isinstance(self.loss_fn, SDPOLoss):
                                loss, metrics = self.loss_fn(
                                    log_probs=log_probs,
                                    experience=exp_batch,
                                    teacher_log_probs=teacher_log_probs,
                                    distillation_mask=distillation_mask,
                                    student_topk_log_probs=student_topk_log_probs,  # üî• Top-K
                                    teacher_topk_log_probs=teacher_topk_log_probs,  # üî• Top-K
                                )
                            else:
                                loss, metrics = self.loss_fn(
                                    log_probs=log_probs,
                                    experience=exp_batch,
                                )
                            
                            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                            del log_probs
                            if teacher_log_probs is not None:
                                del teacher_log_probs
                            if student_topk_log_probs is not None:
                                del student_topk_log_probs, teacher_topk_log_probs
                
                    # –í–ê–ñ–ù–û: –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ—Å–ª–µ forward pass
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–í–ï–†–ö–ò –ø–µ—Ä–µ–¥ backward
                    if not loss.isfinite():
                        logger.warning(f"Loss –Ω–µ finite: {loss.item()}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º batch")
                        continue
                    
                    if not loss.requires_grad:
                        # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                        trainable_count = sum(1 for p in self.model.parameters() if p.requires_grad)
                        total_count = sum(1 for _ in self.model.parameters())
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å forward pass
                        test_seq = exp_batch.sequences[:1, :5]
                        test_mask = exp_batch.attention_mask[:1, :5]
                        with torch.enable_grad():
                            test_output = self.model(input_ids=test_seq, attention_mask=test_mask)
                            test_logits_grad = test_output.logits.requires_grad
                        
                        raise RuntimeError(
                            f"‚ùå Loss –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤!\n"
                            f"  - loss.requires_grad: {loss.requires_grad}\n"
                            f"  - loss.dtype: {loss.dtype}\n"
                            f"  - –ú–æ–¥–µ–ª—å training: {self.model.training}\n"
                            f"  - Trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_count}/{total_count}\n"
                            f"  - Test logits requires_grad: {test_logits_grad}\n"
                            f"  - use_lora: {self.config.use_lora}\n"
                            f"  - use_4bit: {self.config.use_4bit}\n"
                            f"  - use_8bit: {self.config.use_8bit}\n"
                            f"  - use_autocast: {use_autocast}\n"
                        )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º loss –¥–ª—è –º–µ—Ç—Ä–∏–∫ –ü–ï–†–ï–î backward
                    loss_value = loss.item()
                    
                    # Backward
                    if self.accelerator is not None:
                        self.accelerator.backward(loss)
                    else:
                        loss.backward()
                    
                    # –í–ê–ñ–ù–û: –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º loss –ø–æ—Å–ª–µ backward –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    del loss
                    if torch.cuda.is_available() and batch_idx % 5 == 0:
                        torch.cuda.empty_cache()
                    
                    # Optimizer step –¥–µ–ª–∞–µ–º –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–∏ –Ω—É–∂–Ω–æ–µ —á–∏—Å–ª–æ micro-steps.
                    do_step = True
                    if self.accelerator is not None:
                        do_step = bool(self.accelerator.sync_gradients)
                    
                    if do_step:
                        # Gradient clipping
                        if self.accelerator is not None:
                            grad_norm = self.accelerator.clip_grad_norm_(
                                self.model.parameters(),
                                self.config.max_grad_norm,
                            )
                        else:
                            grad_norm = clip_grad_norm_(
                                self.model.parameters(),
                                self.config.max_grad_norm,
                            )
                        
                        # DEBUG: –ø—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã LoRA –¥–æ optimizer.step()
                        if self.is_main_process and self.config.use_lora:
                            lora_grads = []
                            lora_total_numel = 0
                            for name, p in self.model.named_parameters():
                                if p.grad is not None and 'lora' in name.lower():
                                    grad_norm_p = p.grad.norm().item()
                                    lora_grads.append((name, grad_norm_p, p.numel()))
                                    lora_total_numel += p.numel()
                            
                            if lora_grads:
                                avg_lora_grad = sum(g for _, g, _ in lora_grads) / len(lora_grads)
                                max_lora_grad = max(g for _, g, _ in lora_grads)
                                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º: –º–∞—Ç—Ä–∏—Ü, –æ–±—â–µ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, avg/max grad norm
                                logger.info(
                                    f"üîç LoRA grads: {len(lora_grads)} matrices, "
                                    f"{lora_total_numel:,} params, "
                                    f"avg={avg_lora_grad:.6f}, max={max_lora_grad:.6f}"
                                )
                            else:
                                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!")
                        
                        self.optimizer.step()
                        self.scheduler.step()
                        self.optimizer.zero_grad()
                        
                        # üî• SDPO: EMA Update –¥–ª—è Teacher –º–æ–¥–µ–ª–∏ (–∏–∑ verl)
                        if isinstance(self.loss_fn, SDPOLoss):
                            self._update_teacher_ema()
                        
                        self.global_step += 1
                    else:
                        grad_norm = 0.0
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    epoch_losses.append(loss_value)
                    epoch_kls.append(metrics.get("kl_mean", 0))
                    epoch_grad_norms.append(
                        grad_norm.item() if torch.is_tensor(grad_norm) else grad_norm
                    )
            
            # üî• Barrier –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π epoch –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ multi-GPU
            if self.accelerator is not None and self.accelerator.num_processes > 1:
                self.accelerator.wait_for_everyone()
        
        return {
            "loss": sum(epoch_losses) / max(len(epoch_losses), 1),
            "kl": sum(epoch_kls) / max(len(epoch_kls), 1),
            "grad_norm": sum(epoch_grad_norms) / max(len(epoch_grad_norms), 1),
        }
    
    @torch.no_grad()
    def _evaluate(
        self,
        dataset: RLDataset,
        max_samples: int = 100,
    ) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ."""
        self.model.eval()
        
        # –ë–µ—Ä—ë–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É
        indices = list(range(min(len(dataset), max_samples)))
        samples = [dataset[i] for i in indices]
        
        correct = 0
        total = 0
        rewards = []
        
        for sample in samples:
            prompt = build_reasoning_prompt(
                sample.prompt,
                self.tokenizer,
                self.config.reasoning_format,
                system_prompt=getattr(self.config, 'user_system_prompt', None),
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç (greedy)
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.config.max_prompt_length,
            ).to(self.device)
            
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                do_sample=False,  # Greedy –¥–ª—è eval
                pad_token_id=self.tokenizer.pad_token_id,
            )
            
            completion = self.tokenizer.decode(
                outputs[0, inputs["input_ids"].size(1):],
                skip_special_tokens=True,
            )
            
            reward = self.reward_fn(
                completion=completion,
                reference_answer=sample.reference_answer,
                reasoning_format=self.config.reasoning_format,
                metadata=sample.metadata if hasattr(sample, 'metadata') else {},
            )
            rewards.append(reward)
            
            if reward >= 0.5:  # Threshold –¥–ª—è "–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ" –æ—Ç–≤–µ—Ç–∞
                correct += 1
            total += 1
        
        return {
            "accuracy": correct / max(total, 1),
            "mean_reward": sum(rewards) / max(len(rewards), 1),
            "samples": total,
        }
    
    def _log_metrics(self, metrics: Dict[str, Any], *, jsonl_only: bool = False):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.

        –í–∞–∂–Ω–æ –¥–ª—è UI: `metrics.jsonl` –¥–æ–ª–∂–µ–Ω –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ, –∏–Ω–∞—á–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ "–∑–∞–≤–∏—Å–∞–µ—Ç" –Ω–∞ STARTING.
        –ü–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å JSONL –¥–∞–∂–µ —á–∞—Å—Ç–æ (–∫–∞–∂–¥—ã–π rollout), –∞ –∫–æ–Ω—Å–æ–ª—å/W&B ‚Äî —Ä–µ–∂–µ.
        """
        # –í distributed —Ä–µ–∂–∏–º–µ –ø–∏—à–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ —Å main –ø—Ä–æ—Ü–µ—Å—Å–∞, –∏–Ω–∞—á–µ jsonl –±—É–¥–µ—Ç –ø–µ—Ä–µ–º–µ—à–∞–Ω.
        if self.accelerator is not None and not self.accelerator.is_main_process:
            return
        if (not jsonl_only) and self.config.use_wandb and self.wandb_run:
            import wandb
            wandb.log(metrics, step=self.global_step)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ JSONL –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–∑ UI (–≤—Å–µ–≥–¥–∞ –Ω–∞ main process)
        metrics_file = Path(self.config.output_dir) / "metrics.jsonl"
        ui_metrics_file = None
        try:
            if getattr(self.config, "ui_run_dir", None):
                ui_metrics_file = Path(str(self.config.ui_run_dir)) / "metrics.jsonl"
        except Exception:
            ui_metrics_file = None
        try:
            import json
            from datetime import datetime
            log_entry = {
                    # optim_step: —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (—Ä–∞—Å—Ç—ë—Ç –≤–Ω—É—Ç—Ä–∏ _train_on_buffer)
                    "step": self.global_step,
                    "optim_step": self.global_step,
                    # rollout_step: —Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤ (prompts/step) —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                    "rollout_step": getattr(self, "rollout_step", 0),
                    # current_step –¥–ª—è UI: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ GRPO —Å—á–∏—Ç–∞–µ–º –ø–æ rollout_step (–ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞)
                    "current_step": int(getattr(self, "rollout_step", 0)),
                    # total_steps –¥–ª—è UI/ETA: –ø–ª–∞–Ω –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ (–Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç/–ª–∏–º–∏—Ç—ã), –∞ –Ω–µ —Ç–æ–ª—å–∫–æ max_steps.
                    "total_steps": int(getattr(self, "planned_total_steps", 0)) or None,
                    # planned_total_steps: "–ø–ª–∞–Ω –Ω–∞ —ç–ø–æ—Ö—É" –±–µ–∑ –ª–∏–º–∏—Ç–æ–≤ –ø–æ max_prompts/max_steps (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
                    "planned_total_steps": int(getattr(self, "planned_total_steps_uncapped", 0)) or None,
                    "reward": metrics.get("batch_reward_mean", metrics.get("reward", 0)),
                    "loss": metrics.get("loss", 0),
                    "kl": metrics.get("kl", 0),
                    "grad_norm": metrics.get("grad_norm", 0),
                    "epoch": metrics.get("epoch", 0),
                    "learning_rate": self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else self.config.learning_rate,
                    "timestamp": datetime.now().isoformat(),
                    # –î–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É/—Å–∫–æ—Ä–æ—Å—Ç–∏:
                    # batch_size –∑–¥–µ—Å—å = prompts/step –Ω–∞ –û–î–ò–ù –ø—Ä–æ—Ü–µ—Å—Å; –≤ UI —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ num_gpus (–∏–∑ config.json)
                    "prompt_batch_size": int(self.config.batch_size),
                    "group_size": int(self.config.group_size),
                    "train_batch_size": int(self.config.train_batch_size),
                    "epochs_per_step": int(self.config.epochs_per_step),
            }
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (—á–∏—Å–ª–∞ –∏ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ status)
            for k, v in metrics.items():
                if k not in log_entry:
                    if isinstance(v, (int, float, str)):
                        log_entry[k] = v
            
            with open(metrics_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry) + "\n")
            # –î—É–±–ª–∏—Ä—É–µ–º –≤ run_dir UI (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω), —á—Ç–æ–±—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–ª –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø—É—Ç–µ–π output_dir.
            if ui_metrics_file is not None:
                ui_metrics_file.parent.mkdir(parents=True, exist_ok=True)
                with open(ui_metrics_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(log_entry) + "\n")
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ JSONL: {e}")

        # –¢–∞–∫–∂–µ –ª–æ–≥–∏—Ä—É–µ–º –≤ –∫–æ–Ω—Å–æ–ª—å (–æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏) ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        if (not jsonl_only) and self.is_main_process:
            log_str = " | ".join([
                f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}"
                for k, v in metrics.items()
                if k in ["step", "loss", "kl", "batch_reward_mean", "buffer_size", "rollouts_count"]
            ])
            if log_str:
                logger.info(f"Step {self.global_step}: {log_str}")
    
    def _log_sample(self, rollout):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å–µ–º–ø–ª (–ø—Ä–æ–º–ø—Ç –∏ –æ—Ç–≤–µ—Ç—ã) –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ UI."""
        if self.accelerator is not None and not self.accelerator.is_main_process:
            return
        try:
            import json
            from pathlib import Path
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ output_dir/samples.jsonl (UI –±—É–¥–µ—Ç —á–∏—Ç–∞—Ç—å –∏–∑ run_dir)
            samples_file = Path(self.config.output_dir) / "samples.jsonl"
            ui_samples_file = None
            try:
                if getattr(self.config, "ui_run_dir", None):
                    ui_samples_file = Path(str(self.config.ui_run_dir)) / "samples.jsonl"
            except Exception:
                ui_samples_file = None
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (–ø—Ä–æ–º–ø—Ç + completion) –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            full_texts = []
            for completion in rollout.completions:
                full_text = rollout.prompt + completion
                full_texts.append(full_text)
            
            sample_entry = {
                "step": self.global_step,
                "prompt": rollout.prompt,
                "reference_answer": rollout.metadata.get("reference_answer", ""),
                "completions": rollout.completions,
                "full_texts": full_texts,  # –ü—Ä–æ–º–ø—Ç + completion –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                "rewards": rollout.rewards.tolist(),
                "timestamp": datetime.now().isoformat(),
            }
            
            with open(samples_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(sample_entry, ensure_ascii=False) + "\n")
            # –î—É–±–ª–∏—Ä—É–µ–º –≤ run_dir UI (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)
            if ui_samples_file is not None:
                ui_samples_file.parent.mkdir(parents=True, exist_ok=True)
                with open(ui_samples_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(sample_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å —Å–µ–º–ø–ª: {e}")
    
    def _save_checkpoint(self, path: Path, is_final: bool = False):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç."""
        # DDP-safe —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ:
        # 1) –≤—Å–µ —Ä–∞–Ω–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –¥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        # 2) —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ main process
        # 3) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ç–æ–º–∞—Ä–Ω–æ–µ: –ø–∏—à–µ–º –≤ tmp-dir –∏ –¥–µ–ª–∞–µ–º rename
        # 4) –≤—Å–µ —Ä–∞–Ω–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        if self.accelerator is not None:
            self.accelerator.wait_for_everyone()

        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            tmp_path = path.with_name(path.name + "_tmp")

            # —á–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–π tmp (–µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –æ—Ç –ø–∞–¥–µ–Ω–∏—è) ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞ main
            if self.accelerator is None or self.is_main_process:
                if tmp_path.exists():
                    import shutil
                    shutil.rmtree(tmp_path, ignore_errors=True)
            # —Å–æ–∑–¥–∞—ë–º tmp-dir –Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö
            tmp_path.mkdir(parents=True, exist_ok=True)

            if self.accelerator is None:
                # Single-process: —Å–æ—Ö—Ä–∞–Ω—è–µ–º state –º–æ–¥–µ–ª–∏ –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ.
                self.model.save_pretrained(tmp_path)
            else:
                # Distributed (DDP/FSDP/DeepSpeed): —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è resume.
                self.accelerator.save_state(tmp_path)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ—Å—à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–ª—å–∫–æ –Ω–∞ main
            if self.accelerator is None or self.is_main_process:
                self.tokenizer.save_pretrained(tmp_path)
                import json
                with open(tmp_path / "grpo_config.json", "w", encoding="utf-8") as f:
                    json.dump(self.config.to_dict(), f, indent=2, ensure_ascii=False)

            # –í—Å–µ –¥–æ–∂–¥–∞–ª–∏—Å—å –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–æ–≤, –∑–∞—Ç–µ–º main –¥–µ–ª–∞–µ—Ç —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—é
            if self.accelerator is not None:
                self.accelerator.wait_for_everyone()

            if self.accelerator is None or self.is_main_process:
                if path.exists():
                    import shutil
                    shutil.rmtree(path, ignore_errors=True)
                tmp_path.rename(path)
                logger.info(f"–ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")

            # –û–±–Ω–æ–≤–ª—è–µ–º "usable" –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º final_model), –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ.
            if bool(getattr(self.config, "export_on_checkpoint", False)):
                final_dir = Path(self.config.output_dir) / "final_model"
                final_tmp = final_dir.with_name(final_dir.name + "_tmp")

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –º–µ—Ä–¥–∂–∏—Ç—å LoRA (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ inference)
                merge_lora = bool(getattr(self.config, "merge_lora", True))
                use_lora = bool(getattr(self.config, "use_lora", False))
                
                # –í–ê–ñ–ù–û: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –î–û —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è, —á—Ç–æ–±—ã –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –±—ã–ª–∏ –Ω–∞ –æ–¥–Ω–æ–π —Ç–æ—á–∫–µ
                if self.accelerator is not None:
                    self.accelerator.wait_for_everyone()

                # —á–∏—Å—Ç–∏–º tmp –Ω–∞ main
                if self.accelerator is None or self.is_main_process:
                    if final_tmp.exists():
                        import shutil
                        shutil.rmtree(final_tmp, ignore_errors=True)
                    final_tmp.mkdir(parents=True, exist_ok=True)
                
                if self.accelerator is None:
                    # === Single-process ===
                    save_model = self.model
                    
                    if merge_lora:
                        try:
                            from peft import PeftModel
                            if isinstance(save_model, PeftModel):
                                logger.info("üîÑ Merging LoRA adapters into base model for final_model...")
                                save_model = save_model.merge_and_unload()
                                logger.info("‚úÖ LoRA adapters merged successfully")
                        except ImportError:
                            pass
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Could not merge LoRA: {e}. Saving as-is.")
                    
                    save_model.save_pretrained(final_tmp, safe_serialization=True)
                
                elif merge_lora and use_lora:
                    # === Distributed + LoRA + merge ===
                    # –¢–æ–ª—å–∫–æ main process —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç (–±–µ–∑ NCCL –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π)
                    # –î—Ä—É–≥–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –ø—Ä–æ—Å—Ç–æ –∂–¥—É—Ç
                    if self.is_main_process:
                        try:
                            from peft import PeftModel
                            
                            # Unwrap –º–æ–¥–µ–ª—å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ PEFT
                            unwrapped = self.model
                            while hasattr(unwrapped, "module"):
                                unwrapped = unwrapped.module
                            
                            if isinstance(unwrapped, PeftModel):
                                logger.info("üîÑ Merging LoRA adapters for distributed final_model...")
                                merged_model = unwrapped.merge_and_unload()
                                merged_model.save_pretrained(final_tmp, safe_serialization=True)
                                logger.info("‚úÖ LoRA adapters merged and saved")
                            else:
                                # –ù–µ PEFT –º–æ–¥–µ–ª—å - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                                unwrapped.save_pretrained(final_tmp, safe_serialization=True)
                        except ImportError:
                            logger.warning("‚ö†Ô∏è PEFT not available, saving model as-is")
                            unwrapped = self.model
                            while hasattr(unwrapped, "module"):
                                unwrapped = unwrapped.module
                            unwrapped.save_pretrained(final_tmp, safe_serialization=True)
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Could not merge LoRA: {e}. Saving as-is.")
                            unwrapped = self.model
                            while hasattr(unwrapped, "module"):
                                unwrapped = unwrapped.module
                            unwrapped.save_pretrained(final_tmp, safe_serialization=True)
                else:
                    # === Distributed –±–µ–∑ merge (–∏–ª–∏ –±–µ–∑ LoRA) ===
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerate.save_model –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å FSDP/ZeRO
                    self.accelerator.save_model(self.model, final_tmp, safe_serialization=True)

                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
                if self.accelerator is not None:
                    self.accelerator.wait_for_everyone()

                if self.accelerator is None or self.is_main_process:
                    self.tokenizer.save_pretrained(final_tmp)
                
                # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º
                if self.accelerator is not None:
                    self.accelerator.wait_for_everyone()

                if self.accelerator is None or self.is_main_process:
                    if final_dir.exists():
                        import shutil
                        shutil.rmtree(final_dir, ignore_errors=True)
                    final_tmp.rename(final_dir)
                    logger.info(f"final_model –æ–±–Ω–æ–≤–ª—ë–Ω: {final_dir}")
        finally:
            if self.accelerator is not None:
                self.accelerator.wait_for_everyone()
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: float = 0.7,
        do_sample: bool = True,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt: –í–æ–ø—Ä–æ—Å/–∑–∞–¥–∞—á–∞
            max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            do_sample: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        self.model.eval()
        
        formatted_prompt = build_reasoning_prompt(
            prompt,
            self.tokenizer,
            self.config.reasoning_format,
            system_prompt=getattr(self.config, 'user_system_prompt', None),
        )
        
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.config.max_prompt_length,
        ).to(self.device)
        
        # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä–Ω—É—Ç–∞ –≤ DDP, –∏—Å–ø–æ–ª—å–∑—É–µ–º unwrapped –º–æ–¥–µ–ª—å –¥–ª—è generate()
        if self.accelerator is not None:
            unwrapped_model = self.accelerator.unwrap_model(self.model)
        elif hasattr(self.model, 'module'):
            unwrapped_model = self.model.module
        else:
            unwrapped_model = self.model
        
        with torch.no_grad():
            outputs = unwrapped_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens or self.config.max_new_tokens,
                temperature=temperature if do_sample else 1.0,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        completion = self.tokenizer.decode(
            outputs[0, inputs["input_ids"].size(1):],
            skip_special_tokens=True,
        )
        
        return completion
