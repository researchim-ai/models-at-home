#!/usr/bin/env python
"""
homellm.training.pretrain
=========================
–°–∫—Ä–∏–ø—Ç —è–∑—ã–∫–æ–≤–æ–≥–æ –ø—Ä–µ—Ç—Ä–µ–π–Ω–∞ ¬´—Å –Ω—É–ª—è¬ª –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –∏–∑ JSONL/—Ç–µ–∫—Å—Ç-—Ñ–∞–π–ª–∞.

–ì–ª–∞–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
1. –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∑–∞–≤–∏—Å–∏–º –æ—Ç minimind: —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏, –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞, –º–æ–¥–µ–ª—å.
2. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç HuggingFace `transformers` + `accelerate` ‚Äì –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ CPU / –æ–¥–Ω—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU.
3. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –∞—Ä–≥—É–º–µ–Ω—Ç—ã CLI –¥–ª—è:
   ‚Ä¢ –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É (`--data_path`),
   ‚Ä¢ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–ª—è —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤ (`--output_dir`),
   ‚Ä¢ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ (`--hidden_size`, `--num_layers`, `--n_heads`),
   ‚Ä¢ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (`--seq_len`),
   ‚Ä¢ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–∞, lr, warmup.
4. –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äì UTF-8 —Ç–µ–∫—Å—Ç) –ª–∏–±–æ JSONL c –∫–ª—é—á–æ–º "text". –ü–æ—Ç–æ–∫–æ–≤–æ–µ —á—Ç–µ–Ω–∏–µ ‚Äì –Ω–µ –¥–µ—Ä–∂–∏—Ç –≤–µ—Å—å –∫–æ—Ä–ø—É—Å –≤ RAM.
5. –†–æ—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ (RoPE) –æ–ø—Ü–∏–æ–Ω–Ω–æ, gradient checkpointing, fp16/bf16, cosine scheduler —Å warmup.
6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ `accelerate` ‚Üí —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å HF Hub.
"""
from __future__ import annotations

import argparse
import json
import logging
import math
import os
import random
from pathlib import Path
from typing import Iterable, List, Dict

import torch
from torch.utils.data import IterableDataset, DataLoader
from tqdm import tqdm
from accelerate import Accelerator
from transformers import (
    AutoTokenizer,
    GPT2Config,
    GPT2LMHeadModel,
    get_cosine_schedule_with_warmup,
    DataCollatorForLanguageModeling,
)

# –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
from homellm.models.home_model import HomeConfig, HomeForCausalLM
from homellm.models.gpt2_model import GPT2HomeConfig, GPT2HomeForCausalLM
from homellm.models.home_model_moe import HomeMoEConfig, HomeMoEForCausalLM

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] [%(levelname)s] %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)


# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------

class StreamingTextDataset(IterableDataset):
    """–ü–æ—Ç–æ—á–Ω–æ —á–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç/JSONL —Ñ–∞–π–ª –ø–æ—Å—Ç—Ä–æ—á–Ω–æ, –Ω–µ –∑–∞–≥—Ä—É–∂–∞—è –≤—Å—ë –≤ –ø–∞–º—è—Ç—å."""

    def __init__(
        self,
        file_path: str,
        tokenizer,
        seq_len: int,
        num_replicas: int = 1,
        rank: int = 0,
        split: str = "train",      # "train" | "val"
        val_ratio: float = 0.0,    # –µ—Å–ª–∏ >0, —á–∞—Å—Ç—å —Å—Ç—Ä–æ–∫ —É—Ö–æ–¥–∏—Ç –≤ val
        shard: bool = True,        # –¥–ª—è val –ø–æ—Å—Ç–∞–≤–∏–º False
        # Strict resume: –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ, –º–æ–∂–µ–º –º–≥–Ω–æ–≤–µ–Ω–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–∑–∏—Ü–∏—é –≤ .jsonl –ø–æ byte offset.
        # –í–ê–ñ–ù–û: –¥–ª—è .gz —Å—Ç—Ä–æ–≥–∏–π seek –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.
        resume_byte_offset: int = 0,
        resume_global_idx: int = 0,
        strict_resume: bool = True,
    ):
        super().__init__()
        self.file_path = Path(file_path)
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.num_replicas = num_replicas
        self.rank = rank
        self.split = split
        self.val_ratio = float(val_ratio or 0.0)
        self.shard = bool(shard)
        self.resume_byte_offset = int(resume_byte_offset or 0)
        self.resume_global_idx = int(resume_global_idx or 0)
        self.strict_resume = bool(strict_resume)

        # –ë—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å –ø—Ä–∏ –∏—Ç–µ—Ä–∞—Ü–∏–∏: –ø–æ–∑–∏—Ü–∏—è (—Å–ª–µ–¥—É—é—â–∏–π offset/idx) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ resume
        self._resume_state = {
            "byte_offset": self.resume_byte_offset,
            "global_idx": self.resume_global_idx,
        }

        if not self.file_path.exists():
            raise FileNotFoundError(f"Dataset {self.file_path} not found")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç (–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º .jsonl.gz)
        suffixes = self.file_path.suffixes  # –Ω–∞–ø—Ä–∏–º–µ—Ä [".jsonl", ".gz"]
        self._is_gz = bool(suffixes) and suffixes[-1] == ".gz"
        
        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º "–±–∞–∑–æ–≤–æ–µ" —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–µ–∑ .gz
        base_suffix = suffixes[-2].lower() if self._is_gz and len(suffixes) >= 2 else self.file_path.suffix.lower()
        self._is_jsonl = (base_suffix == ".jsonl")  # –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º ".json" –¥–ª—è pretrain (—ç—Ç–æ –º–∞—Å—Å–∏–≤, –Ω–µ JSONL)

    def _read_lines(self) -> Iterable[str]:
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ .gz —Ñ–∞–π–ª–æ–≤ (–≤–∫–ª—é—á–∞—è .jsonl.gz)
        if self._is_gz:
            import gzip
            f = gzip.open(self.file_path, "rt", encoding="utf-8")
        else:
            f = open(self.file_path, "r", encoding="utf-8")
        
        try:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                if self._is_jsonl:
                    try:
                        obj = json.loads(line)
                        line = obj.get("text", "")
                    except json.JSONDecodeError:
                        continue
                yield line
        finally:
            f.close()

    def get_resume_state(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ resume.
        –î–ª—è .gz resume –≤–æ–∑–º–æ–∂–µ–Ω —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ slow-skip (–∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç).
        """
        return {
            "file_path": str(self.file_path),
            "is_gz": bool(self._is_gz),
            "is_jsonl": bool(self._is_jsonl),
            "split": self.split,
            "val_ratio": self.val_ratio,
            "shard": self.shard,
            "num_replicas": int(self.num_replicas),
            "rank": int(self.rank),
            "byte_offset": int(self._resume_state.get("byte_offset", 0) or 0),
            "global_idx": int(self._resume_state.get("global_idx", 0) or 0),
        }

    def __iter__(self):
        """
        –ò—Ç–µ—Ä–∞—Ç–æ—Ä —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        
        –í–ê–ñ–ù–û: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–ª–∞–µ—Ç accelerate.prepare() —á–µ—Ä–µ–∑ DataLoaderShard/Dispatcher.
        –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ shard=True.
        
        –õ–æ–≥–∏–∫–∞:
        1. –ß–∏—Ç–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
        2. –ü—Ä–∏–º–µ–Ω—è–µ–º train/val split –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É
        3. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —à–∞—Ä–¥–∏—Ä—É–µ–º –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ –∏ workers (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ shard=True)
        """
        from torch.utils.data import get_worker_info
        
        # Train/val split –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        scale = 10000
        threshold = int(self.val_ratio * scale)

        # –ë–∞–∑–æ–≤—ã–π –∏—Ç–µ—Ä–∞—Ç–æ—Ä –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫ —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º.
        # –î–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ resume –Ω–∞ .jsonl –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–Ω–∞—Ä–Ω–æ–µ —á—Ç–µ–Ω–∏–µ + seek –ø–æ byte_offset.
        # –≠—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ü–û–õ–ù–£–Æ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ resume (–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –º–µ–Ω—è–ª—Å—è).
        if self.strict_resume and not self._is_gz and self._is_jsonl:
            start_off = int(self.resume_byte_offset or 0)
            start_idx = int(self.resume_global_idx or 0)

            def iter_with_offsets():
                idx = start_idx
                with open(self.file_path, "rb") as f:
                    if start_off > 0:
                        f.seek(start_off)
                    # –û–±–Ω–æ–≤–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ "–º—ã –Ω–∞ —ç—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏"
                    self._resume_state = {"byte_offset": int(f.tell()), "global_idx": int(idx)}

                    while True:
                        off = int(f.tell())
                        raw = f.readline()
                        if not raw:
                            break

                        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏ —á–∏—Å—Ç–∏–º
                        line = raw.decode("utf-8", errors="ignore").strip()
                        if not line:
                            continue

                        # –ü–∞—Ä—Å–∏–º JSONL (text –ø–æ–ª–µ), –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–≤—Ç–æ—Ä—è—è —Å—Ç–∞—Ä—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É _read_lines()
                        try:
                            obj = json.loads(line)
                            line = obj.get("text", "")
                        except json.JSONDecodeError:
                            continue

                        line = (line or "").strip()
                        if not line:
                            continue

                        # –í–ê–ñ–ù–û: idx ‚Äî —ç—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ü–û–°–õ–ï —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—É—Å—Ç—ã—Ö/–±–∏—Ç—ã—Ö —Å—Ç—Ä–æ–∫,
                        # —Ç.–µ. —Ä–æ–≤–Ω–æ —Ç–æ, —á—Ç–æ —Ä–∞–Ω—å—à–µ –¥–∞–≤–∞–ª enumerate(self._read_lines()).
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º next-state –Ω–∞ "—Å–ª–µ–¥—É—é—â—É—é –≤–∞–ª–∏–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É"
                        next_off = int(f.tell())
                        self._resume_state = {"byte_offset": next_off, "global_idx": int(idx + 1)}
                        yield idx, line
                        idx += 1

            base_iter = iter_with_offsets()
        else:
            # –§–æ–ª–±—ç–∫: —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å (–¥–ª—è .txt –∏–ª–∏ .gz). –°—Ç—Ä–æ–≥–∏–π seek —Ç—É—Ç –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç—Å—è.
            base_iter = enumerate(self._read_lines())
        
        # 1. –ü—Ä–∏–º–µ–Ω—è–µ–º train/val split –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É
        def apply_split(idx_text_pair):
            idx, text = idx_text_pair
            is_val = (threshold > 0) and ((idx % scale) < threshold)
            if self.split == "val" and not is_val:
                return None
            if self.split == "train" and is_val:
                return None
            return idx_text_pair
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ split
        filtered_iter = (pair for pair in base_iter if apply_split(pair) is not None)
        
        # 2. –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É DDP –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ shard=True)
        # –í–ê–ñ–ù–û: –ï—Å–ª–∏ shard=False, —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–ª–∞–µ—Ç accelerate.prepare()
        if self.shard and self.num_replicas > 1:
            filtered_iter = ((idx, text) for idx, text in filtered_iter if idx % self.num_replicas == self.rank)
        
        # 3. –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É DataLoader workers –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ shard=True)
        worker_info = get_worker_info()
        if self.shard and worker_info is not None and worker_info.num_workers > 1:
            total_workers = self.num_replicas * worker_info.num_workers
            worker_rank = self.rank * worker_info.num_workers + worker_info.id
            filtered_iter = ((idx, text) for idx, text in filtered_iter if idx % total_workers == worker_rank)
        
        # –ò—Ç–µ—Ä–∏—Ä—É–µ–º –ø–æ –¥–∞–Ω–Ω—ã–º (–∑–∞—à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–ª–∏ –Ω–µ—Ç, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç shard)
        for idx, text in filtered_iter:
            
            tokens = self.tokenizer(
                text,
                truncation=True,
                max_length=self.seq_len,
                padding="max_length",  # –í—Å–µ–≥–¥–∞ –¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ seq_len
                add_special_tokens=True,
                return_attention_mask=True,  # ‚úÖ –ù—É–∂–Ω–æ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ masking labels
            )
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º dict —Å attention_mask –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ masking
            # –í–ê–ñ–ù–û: labels –±—É–¥—É—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ attention_mask, –∞ –Ω–µ –ø–æ pad_token_id,
            # —á—Ç–æ–±—ã –Ω–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å "–Ω–∞—Å—Ç–æ—è—â–∏–π EOS" –µ—Å–ª–∏ pad_token = eos_token
            yield {
                "input_ids": torch.tensor(tokens["input_ids"], dtype=torch.long),
                "attention_mask": torch.tensor(tokens["attention_mask"], dtype=torch.long),
            }

    def get_sample_prompt(self, max_samples: int = 10) -> str | None:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç reservoir sampling –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤.
        """
        try:
            samples = []
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ .gz —Ñ–∞–π–ª–æ–≤
            if self._is_gz:
                import gzip
                f = gzip.open(self.file_path, "rt", encoding="utf-8")
            else:
                f = open(self.file_path, "r", encoding="utf-8")
            
            try:
                for idx, line in enumerate(f):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # –ü–∞—Ä—Å–∏–º JSONL –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if self._is_jsonl:
                        try:
                            obj = json.loads(line)
                            line = obj.get("text", "")
                        except json.JSONDecodeError:
                            continue
                    
                    if not line:
                        continue
                    
                    # Reservoir sampling –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
                    if len(samples) < max_samples:
                        samples.append(line)
                    else:
                        # –° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é max_samples/(idx+1) –∑–∞–º–µ–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
                        r = random.randint(0, idx)
                        if r < max_samples:
                            samples[r] = line
                    
                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
                    if idx >= max_samples * 10:
                        break
            finally:
                f.close()
            
            if samples:
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π –Ω–µ–ø—É—Å—Ç–æ–π —Å–µ–º–ø–ª
                for sample in samples:
                    if sample.strip():
                        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if len(sample) > 500:
                            return sample[:500] + "..."
                        return sample
        except Exception as e:
            logger.warning(f"Failed to get sample prompt: {e}")
            return None
        
        return None

    def __len__(self):
        """–ü—Ä–∏ –ø–µ—Ä–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏ –±—ã—Å—Ç—Ä–æ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ.
        
        –£—á–∏—Ç—ã–≤–∞–µ—Ç val_ratio –∏ split –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –¥–ª–∏–Ω—ã –Ω–∞ –æ–¥–∏–Ω rank.
        """
        if not hasattr(self, "_length"):
            logger.info("–ü–æ–¥—Å—á—ë—Ç —Å—Ç—Ä–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ %s ‚Äî –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è...", self.file_path)
            cnt = 0
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ .gz —Ñ–∞–π–ª–æ–≤
            if self._is_gz:
                import gzip
                f = gzip.open(self.file_path, "rt", encoding="utf-8")
            else:
                f = open(self.file_path, "r", encoding="utf-8")
            
            try:
                for _ in f:
                    cnt += 1
            finally:
                f.close()
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º val_ratio –∏ split
            if self.val_ratio > 0:
                val_cnt = int(cnt * self.val_ratio)
                train_cnt = cnt - val_cnt
            else:
                train_cnt = cnt
                val_cnt = 0
            
            # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–π split
            if self.split == "train":
                effective = train_cnt
            else:  # val
                effective = val_cnt if self.val_ratio > 0 else 0
            
            # –í–ê–ñ–ù–û: –ù–µ –¥–µ–ª–∏–º –Ω–∞ num_replicas, —Ç.–∫. —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–ª–∞–µ—Ç accelerate.prepare()
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ split
            self._length = effective
        return self._length


# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------

def parse_args():
    parser = argparse.ArgumentParser(description="HomeLLM pretraining script")
    # Files & dirs
    parser.add_argument("--data_path", type=str, required=True, help="–ü—É—Ç—å –∫ .txt/.jsonl –∫–æ—Ä–ø—É—Å—É")
    parser.add_argument("--output_dir", type=str, default="./checkpoints", help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª—å")
    parser.add_argument("--tokenizer_path", type=str, default="gpt2", help="–•—É–≥–≥–∏–Ω–≥—Ñ–µ–π—Å-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–ª–∏ –ø—É—Ç—å")

    # Model size
    parser.add_argument("--hidden_size", type=int, default=512, help="–†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞/—Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è")
    parser.add_argument("--num_layers", type=int, default=8, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ Transformer")
    parser.add_argument("--n_heads", type=int, default=8, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è")
    parser.add_argument("--seq_len", type=int, default=512, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–∞")

    parser.add_argument("--arch", type=str, default="home", choices=["gpt2", "gpt2_home", "home", "home_moe"], 
                        help="–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: home (LLaMA-style), gpt2_home (GPT-2 Classic), home_moe (MoE), gpt2 (HF GPT-2)")
    
    # MoE –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--num_experts", type=int, default=8, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã")
    parser.add_argument("--num_experts_per_tok", type=int, default=2, help="Top-K: —Å–∫–æ–ª—å–∫–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω")
    parser.add_argument("--expert_type", type=str, default="swiglu", choices=["swiglu", "mlp"], help="–¢–∏–ø —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: swiglu –∏–ª–∏ mlp")

    # Training params
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--gradient_accumulation", type=int, default=1)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=5e-4)
    parser.add_argument("--warmup_steps", type=int, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.1, help="Weight decay –¥–ª—è AdamW")
    parser.add_argument("--fp16", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fp16")
    parser.add_argument("--bf16", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bf16 (Ampere+)")
    parser.add_argument("--grad_checkpoint", action="store_true", help="–í–∫–ª—é—á–∏—Ç—å torch.gradient_checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM")
    parser.add_argument(
        "--flash_attention",
        action="store_true",
        help="–í–∫–ª—é—á–∏—Ç—å SDPA (scaled_dot_product_attention), —á—Ç–æ–±—ã PyTorch –º–æ–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å flash-attention kernels –ø—Ä–∏ fp16/bf16",
    )
    parser.add_argument(
        "--no_flash_attention",
        action="store_true",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å SDPA/flash attention (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—ã—á–Ω—ã–π attention)",
    )
    parser.add_argument("--save_every", type=int, default=10000, help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–π–Ω—Ç –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤")
    parser.add_argument("--log_every", type=int, default=500, help="–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N —à–∞–≥")

    return parser.parse_args()


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

def main():
    args = parse_args()

    if args.fp16 and args.bf16:
        raise ValueError("–ù–µ–ª—å–∑—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–¥–∞–≤–∞—Ç—å --fp16 –∏ --bf16")
    if args.flash_attention and args.no_flash_attention:
        raise ValueError("–ù–µ–ª—å–∑—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–¥–∞–≤–∞—Ç—å --flash_attention –∏ --no_flash_attention")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É–≤–µ–ª–∏—á–∏—Ç—å mixed_precision —Å–æ–≥–ª–∞—Å–Ω–æ accelerate>=0.20
    mixed_precision = (
        "fp16" if args.fp16 else "bf16" if args.bf16 else "no"
    )
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation,
        mixed_precision=mixed_precision,
    )
    is_main = accelerator.is_main_process

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA SDPA kernels (–≤–ª–∏—è–µ—Ç –Ω–∞ HomeModel, —Ç.–∫. –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç scaled_dot_product_attention)
    use_flash_attention = True
    if args.no_flash_attention:
        use_flash_attention = False
    elif args.flash_attention:
        use_flash_attention = True
    if torch.cuda.is_available():
        try:
            if use_flash_attention:
                torch.backends.cuda.enable_flash_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(True)
                torch.backends.cuda.enable_math_sdp(True)
            else:
                torch.backends.cuda.enable_flash_sdp(False)
                torch.backends.cuda.enable_mem_efficient_sdp(False)
                torch.backends.cuda.enable_math_sdp(True)
            if is_main:
                logger.info(
                    "SDPA kernels: flash=%s mem_efficient=%s math=%s (use_flash_attention=%s)",
                    getattr(torch.backends.cuda, "flash_sdp_enabled", lambda: "N/A")(),
                    getattr(torch.backends.cuda, "mem_efficient_sdp_enabled", lambda: "N/A")(),
                    getattr(torch.backends.cuda, "math_sdp_enabled", lambda: "N/A")(),
                    use_flash_attention,
                )
        except Exception as e:
            if is_main:
                logger.warning(f"Could not configure CUDA SDPA kernels: {e}")

    if is_main:
        logger.info("Arguments: %s", vars(args))

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "<|pad|>"})

    # Dataset / Dataloader
    train_dataset = StreamingTextDataset(args.data_path, tokenizer, seq_len=args.seq_len)
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
    # –í–ê–ñ–ù–û: num_workers=0 –¥–ª—è IterableDataset, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        collate_fn=data_collator,
        num_workers=0,  # IterableDataset + num_workers>0 = –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    )

    # –í—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    arch = getattr(args, "arch", "home")
    
    if arch == "home":
        # HomeModel (LLaMA-style): RMSNorm, RoPE, SwiGLU
        config = HomeConfig(
            vocab_size=len(tokenizer),
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_layers,
            num_attention_heads=args.n_heads,
            max_position_embeddings=args.seq_len,
            use_sdpa=use_flash_attention,
        )
        model = HomeForCausalLM(config)
        if is_main:
            logger.info("üè† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: HomeModel (LLaMA-style) ‚Äî RMSNorm, RoPE, SwiGLU")
    
    elif arch == "gpt2_home":
        # GPT-2 Classic: LayerNorm, Learned Pos, GELU
        config = GPT2HomeConfig(
            vocab_size=len(tokenizer),
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_layers,
            num_attention_heads=args.n_heads,
            max_position_embeddings=args.seq_len,
        )
        model = GPT2HomeForCausalLM(config)
        if is_main:
            logger.info("ü§ñ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: GPT-2 Classic ‚Äî LayerNorm, Learned Pos, GELU")
    
    elif arch == "home_moe":
        # HomeModel MoE: RMSNorm, RoPE, SwiGLU experts
        config = HomeMoEConfig(
            vocab_size=len(tokenizer),
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_layers,
            num_attention_heads=args.n_heads,
            max_position_embeddings=args.seq_len,
            use_sdpa=use_flash_attention,
            num_experts=args.num_experts,
            num_experts_per_tok=args.num_experts_per_tok,
            expert_type=args.expert_type,
        )
        model = HomeMoEForCausalLM(config)
        if is_main:
            logger.info(f"üîÄ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: HomeModel MoE ‚Äî {args.num_experts} —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, Top-{args.num_experts_per_tok}")
    
    else:
        # Fallback: HuggingFace GPT-2
        config = GPT2Config(
            vocab_size=len(tokenizer),
            n_embd=args.hidden_size,
            n_layer=args.num_layers,
            n_head=args.n_heads,
            n_positions=args.seq_len,
        )
        model = GPT2LMHeadModel(config)
        if is_main:
            logger.info("ü§ó –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: HuggingFace GPT-2 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è)")

    if args.grad_checkpoint:
        model.gradient_checkpointing_enable()

    # LR scheduler
    try:
        dataset_len = len(train_loader)
    except TypeError:
        # –î–ª—è IterableDataset –±–µ–∑ __len__ ‚Äî –æ—Ü–µ–Ω–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç epochs=1 => steps = save_every*??
        dataset_len = args.save_every * args.epochs
    num_update_steps_per_epoch = math.ceil(dataset_len / args.gradient_accumulation)
    max_train_steps = args.epochs * num_update_steps_per_epoch

    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.learning_rate, 
        betas=(0.9, 0.95), 
        eps=1e-8,
        weight_decay=args.weight_decay
    )
    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=max_train_steps,
    )

    model, optimizer, train_loader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_loader, lr_scheduler
    )

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    global_step = 0
    for epoch in range(args.epochs):
        model.train()
        if is_main:
            pbar = tqdm(total=len(train_loader), desc=f"Epoch {epoch+1}/{args.epochs}")
        for step, batch in enumerate(train_loader):
            with accelerator.accumulate(model):
                with accelerator.autocast():
                    outputs = model(**batch)
                    loss = outputs.loss
                accelerator.backward(loss)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            global_step += 1
            if is_main and ((step + 1) % args.log_every == 0 or step == 0):
                pbar.set_postfix({"loss": f"{loss.detach().float().item():.6f}", "lr": f"{lr_scheduler.get_last_lr()[0]:.2e}"})
                # –æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π
                pbar.update(args.log_every if (step + 1) % args.log_every == 0 else 1)

            # Save
            if global_step % args.save_every == 0:
                if is_main:
                    ckpt_path = output_dir / f"checkpoint_step{global_step}.pt"
                    accelerator.save_state(ckpt_path)
                    logger.info(f"Saved checkpoint to {ckpt_path}")

        if is_main:
            pbar.close()

    # Final save (merged weights -> HF format)
    if is_main:
        final_dir = output_dir / "final_model"
        final_dir.mkdir(parents=True, exist_ok=True)
        unwrapped_model = accelerator.unwrap_model(model)
        unwrapped_model.save_pretrained(final_dir, save_function=accelerator.save)
        tokenizer.save_pretrained(final_dir)
        logger.info(f"Training finished, model saved to {final_dir}")


if __name__ == "__main__":
    main() 