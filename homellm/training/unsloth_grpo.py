"""
Unsloth backend –¥–ª—è GRPO —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Unsloth –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- 2x –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ
- –î–æ 70% –º–µ–Ω—å—à–µ VRAM
- Triton —è–¥—Ä–∞ (RMSNorm, RoPE, SwiGLU)
- Smart Gradient Checkpointing
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ RL —Ñ—É–Ω–∫—Ü–∏–∏
"""
from __future__ import annotations

import json
import logging
import time
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Callable

# –ü–æ–¥–∞–≤–ª—è–µ–º warning –æ—Ç Unsloth –ø—Ä–æ –ø–æ—Ä—è–¥–æ–∫ –∏–º–ø–æ—Ä—Ç–æ–≤
# (–º—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –µ–≥–æ –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ env variables)
warnings.filterwarnings("ignore", message=".*Unsloth should be imported before.*")

import torch

logger = logging.getLogger(__name__)


def is_unsloth_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Unsloth."""
    try:
        import unsloth
        return True
    except ImportError:
        return False


def _patch_unsloth_left_pack_padding():
    """
    Monkey-patch –¥–ª—è —Ñ–∏–∫—Å–∞ –±–∞–≥–∞ –≤ Unsloth:
    torch.argsort –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç bool dtype –Ω–∞ CUDA —Å stable=True.
    
    Unsloth –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –∫–æ–¥ –≤ –∫—ç—à, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ –ø–∞—Ç—á–∏—Ç—å torch.argsort –≥–ª–æ–±–∞–ª—å–Ω–æ.
    """
    try:
        import torch
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π argsort
        _original_argsort = torch.argsort
        
        def _patched_argsort(input, dim=-1, descending=False, stable=False):
            """Patched argsort: converts bool to int for CUDA compatibility."""
            # –ï—Å–ª–∏ bool –Ω–∞ CUDA —Å stable=True ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int
            if input.dtype == torch.bool and input.is_cuda and stable:
                input = input.int()
            return _original_argsort(input, dim=dim, descending=descending, stable=stable)
        
        # –ó–∞–º–µ–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ
        torch.argsort = _patched_argsort
        
        logger.info("ü¶• Applied global fix for torch.argsort (bool on CUDA)")
        return True
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not patch torch.argsort: {e}")
        return False


def run_unsloth_grpo(
    config: Dict[str, Any],
    metrics_logger: Any,
    dataset: Any = None,
    reward_fn: Optional[Callable] = None,
) -> None:
    """
    –ó–∞–ø—É—Å–∫ GRPO —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Unsloth backend.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        metrics_logger: –õ–æ–≥–≥–µ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è UI
        dataset: –î–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∏–Ω–∞—á–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø–æ config)
        reward_fn: –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    import os
    
    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è multi-GPU ===
    # Unsloth –∏–º–µ–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É multi-GPU
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    rank = int(os.environ.get("RANK", 0))
    
    if world_size > 1:
        logger.warning("=" * 60)
        logger.warning("‚ö†Ô∏è Unsloth multi-GPU (DDP) ‚Äî —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞!")
        logger.warning(f"   WORLD_SIZE={world_size}, LOCAL_RANK={local_rank}")
        logger.warning("   –ü—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'models-at-home backend'")
        logger.warning("=" * 60)
        
        # –í–ê–ñ–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º torch.distributed –†–ê–ù–¨–®–ï –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        # —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞—Ä—å–µ—Ä—ã –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        if not torch.distributed.is_initialized():
            logger.info(f"ü¶• Rank {rank}: Initializing torch.distributed for sequential model loading...")
            torch.distributed.init_process_group(
                backend="nccl",
                init_method="env://",
                world_size=world_size,
                rank=rank,
            )
            torch.cuda.set_device(local_rank)
            logger.info(f"ü¶• Rank {rank}: torch.distributed initialized!")
        
        # –í–ê–ñ–ù–û: –ë–∞—Ä—å–µ—Ä —á—Ç–æ–±—ã –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–æ–∂–¥–∞–ª–∏—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥—Ä—É–≥ –¥—Ä—É–≥–∞
        logger.info(f"ü¶• Rank {rank}: Waiting for all processes to initialize...")
        torch.distributed.barrier()
        logger.info(f"ü¶• Rank {rank}: All processes ready!")
    
    # === –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º —Ç—é–Ω–∏–Ω–≥–∞ –î–û –∏–º–ø–æ—Ä—Ç–∞ Unsloth ===
    tuning_method = config.get("tuning_method", "full")
    
    # –í–ê–ñ–ù–û: Unsloth GRPO –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç full fine-tuning (–±–∞–≥ –≤ –∏—Ö –∫–æ–¥–µ)
    # –§–æ—Ä—Å–∏—Ä—É–µ–º LoRA –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω full
    if tuning_method == "full":
        logger.warning("‚ö†Ô∏è Unsloth GRPO –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç full fine-tuning! –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ LoRA.")
        logger.warning("   –î–ª—è full fine-tuning –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ models-at-home backend.")
        tuning_method = "lora"  # –§–æ—Ä—Å–∏—Ä—É–µ–º LoRA
        config["tuning_method"] = "lora"  # –û–±–Ω–æ–≤–ª—è–µ–º config
    
    os.environ["UNSLOTH_ENABLE_FULL_FINETUNING"] = "0"
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è –±–∞–≥–∞ —Å bool argsort –Ω–∞ CUDA
    _patch_unsloth_left_pack_padding()
    
    # –í–ê–ñ–ù–û: Unsloth –¥–æ–ª–∂–µ–Ω –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –ü–û–°–õ–ï —É—Å—Ç–∞–Ω–æ–≤–∫–∏ env variables
    try:
        from unsloth import FastLanguageModel
        from unsloth import is_bfloat16_supported
        from unsloth.models.rl import PatchFastRL
    except ImportError as e:
        raise ImportError(
            "Unsloth –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–µ—Ä–µ–∑: pip install unsloth\n"
            f"–û—à–∏–±–∫–∞: {e}"
        )
    
    try:
        from trl import GRPOConfig, GRPOTrainer
    except ImportError:
        raise ImportError(
            "trl –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –≤–µ—Ä—Å–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç GRPO. "
            "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install trl>=0.9.0"
        )
    
    from datasets import load_dataset, Dataset
    
    metrics_logger.update(status="loading_model", backend="unsloth")
    
    # === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ UI ===
    base_model_path = config.get("base_model_path")
    if not base_model_path:
        raise ValueError("base_model_path required for GRPO")
    
    max_seq_length = config.get("seq_len", 2048)
    dtype = None  # Auto-detect
    
    # Precision
    mixed_precision = config.get("mixed_precision", "bf16")
    if mixed_precision == "bf16" and is_bfloat16_supported():
        dtype = torch.bfloat16
    elif mixed_precision == "fp16":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    # === –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞ –∏–∑ UI (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤—ã—à–µ ‚Äî full ‚Üí lora) ===
    # tuning_method —Ç–µ–ø–µ—Ä—å: "lora" –∏–ª–∏ "qlora"
    
    # Unsloth GRPO –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ LoRA/QLoRA
    if tuning_method == "qlora":
        use_lora = True
        load_in_4bit = True
        full_finetuning = False
        logger.info("ü¶• Mode: QLoRA (4-bit quantization + LoRA)")
    else:  # "lora" (full –±—ã–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á—ë–Ω –Ω–∞ lora –≤—ã—à–µ)
        use_lora = True
        load_in_4bit = False
        full_finetuning = False
        logger.info("ü¶• Mode: LoRA (16-bit + LoRA)")
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–µ—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–æ)
    if "use_4bit" in config and config["use_4bit"]:
        load_in_4bit = True
    if "load_in_4bit" in config and config["load_in_4bit"]:
        load_in_4bit = True
    
    # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    lora_r = config.get("lora_r", 16)
    lora_alpha = config.get("lora_alpha", 32)
    lora_dropout = config.get("lora_dropout", 0.0)
    lora_target_modules = config.get("lora_target_modules")
    
    # –í–ê–ñ–ù–û: Unsloth —Ç—Ä–µ–±—É–µ—Ç dropout = 0 –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø–∞—Ç—á–µ–π!
    # –ò–Ω–∞—á–µ –æ–Ω –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫ QKV, O, MLP —Å–ª–æ—è–º
    if lora_dropout != 0.0:
        logger.warning(f"‚ö†Ô∏è Unsloth —Ç—Ä–µ–±—É–µ—Ç dropout=0 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏!")
        logger.warning(f"   –ú–µ–Ω—è—é dropout —Å {lora_dropout} –Ω–∞ 0.0")
        lora_dropout = 0.0
    
    logger.info(f"ü¶• Final settings: use_lora={use_lora}, load_in_4bit={load_in_4bit}, full_finetuning={full_finetuning}")
    if use_lora:
        logger.info(f"ü¶• LoRA: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")
    
    # === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Unsloth ===
    logger.info(f"ü¶• Unsloth GRPO: Loading model from {base_model_path}")
    logger.info(f"   max_seq_length={max_seq_length}, dtype={dtype}, load_in_4bit={load_in_4bit}, full_finetuning={full_finetuning}")
    
    # –î–ª—è multi-GPU: —É–∫–∞–∑—ã–≤–∞–µ–º device_map –Ω–∞ —Ç–µ–∫—É—â–∏–π GPU –ø—Ä–æ—Ü–µ—Å—Å–∞
    # –ö–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å accelerate/DDP —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ —Å–≤–æ–∏–º GPU
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    device_map = {"": f"cuda:{local_rank}"}
    logger.info(f"ü¶• Device map: {device_map} (LOCAL_RANK={local_rank})")
    
    # vLLM GPU utilization –∏–∑ UI config
    gpu_memory_utilization = config.get("grpo_vllm_gpu_util", 0.4)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è FastLanguageModel.from_pretrained
    load_kwargs = dict(
        model_name=base_model_path,
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
        device_map=device_map,
    )
    
    # fast_inference=True –≤–∫–ª—é—á–∞–µ—Ç vLLM –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–í–ê–ñ–ù–û –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏!)
    # –ù–û: fast_inference –ù–ï —Å–æ–≤–º–µ—Å—Ç–∏–º —Å trust_remote_code!
    # –ù–û: fast_inference —Ç—Ä–µ–±—É–µ—Ç LoRA, –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å full_finetuning
    
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    is_multi_gpu = world_size > 1
    rank = int(os.environ.get("RANK", 0))
    
    if use_lora:
        # LoRA mode: –≤–∫–ª—é—á–∞–µ–º fast_inference –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        load_kwargs["max_lora_rank"] = lora_r
        load_kwargs["gpu_memory_utilization"] = gpu_memory_utilization
        
        if is_multi_gpu:
            # Multi-GPU: vLLM –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç –ø—Ä–∏ DDP, –æ—Ç–∫–ª—é—á–∞–µ–º fast_inference
            # Unsloth –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ: "Unsloth currently does not support multi GPU setups"
            # –ù–æ DDP –¥–ª—è training —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ—Å—Ç–æ –±–µ–∑ vLLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            load_kwargs["fast_inference"] = False
            logger.warning("=" * 60)
            logger.warning("‚ö†Ô∏è Multi-GPU + fast_inference (vLLM) –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è Unsloth!")
            logger.warning("   –û—Ç–∫–ª—é—á–∞—é fast_inference –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã.")
            logger.warning("   –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ single GPU:")
            logger.warning("   CUDA_VISIBLE_DEVICES=0 python -m homellm.training.rl.train_rl")
            logger.warning("=" * 60)
            logger.info(f"ü¶• Multi-GPU ({world_size} GPUs): Training with DDP (–±–µ–∑ vLLM)")
        else:
            load_kwargs["fast_inference"] = True  # –í–∫–ª—é—á–∞–µ–º vLLM —Ç–æ–ª—å–∫–æ –¥–ª—è single GPU!
            logger.info(f"ü¶• Single GPU: Enabling fast_inference (vLLM) with max_lora_rank={lora_r}, gpu_util={gpu_memory_utilization}")
    else:
        # Full fine-tuning: vLLM –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        load_kwargs["full_finetuning"] = full_finetuning
        load_kwargs["trust_remote_code"] = True
        logger.info("ü¶• Full fine-tuning mode: fast_inference disabled (not supported)")
    
    # === MULTI-GPU: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ===
    # –ü—Ä–∏ DDP –∫–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Å–≤–æ—é –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏.
    # vLLM + –∫–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–≥—É—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ.
    # –†–µ—à–µ–Ω–∏–µ: –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û ‚Äî –æ–¥–∏–Ω –∑–∞ –æ–¥–Ω–∏–º.
    
    if is_multi_gpu and torch.distributed.is_initialized():
        # –ö–∞–∂–¥—ã–π rank –∂–¥—ë—Ç —Å–≤–æ–µ–π –æ—á–µ—Ä–µ–¥–∏
        for loading_rank in range(world_size):
            if rank == loading_rank:
                logger.info(f"ü¶• Rank {rank}/{world_size}: Loading model NOW...")
                model, tokenizer = FastLanguageModel.from_pretrained(**load_kwargs)
                logger.info(f"ü¶• Rank {rank}/{world_size}: Model loaded successfully!")
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            torch.distributed.barrier()
            if rank != loading_rank:
                logger.info(f"ü¶• Rank {rank}: Rank {loading_rank} finished loading, continuing...")
        logger.info(f"ü¶• Rank {rank}: All {world_size} models loaded!")
    else:
        # Single GPU ‚Äî –ø—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ–º
        model, tokenizer = FastLanguageModel.from_pretrained(**load_kwargs)
    
    # === –î–æ–±–∞–≤–ª—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã ===
    if use_lora:
        # Unsloth —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç lora_alpha = lora_r * 2 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –∑–∞–¥–∞–ª alpha, –∏—Å–ø–æ–ª—å–∑—É–µ–º r*2
        if lora_alpha is None or lora_alpha == lora_r:
            lora_alpha = lora_r * 2
            logger.info(f"ü¶• Using optimized lora_alpha = lora_r * 2 = {lora_alpha} (speeds up training)")
        
        logger.info(f"ü¶• Unsloth: Adding LoRA adapters (r={lora_r}, alpha={lora_alpha})")
        
        # Target modules –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
        if lora_target_modules:
            target_modules = lora_target_modules
        else:
            target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
            ]
        
        logger.info(f"ü¶• LoRA target modules: {target_modules}")
        
        model = FastLanguageModel.get_peft_model(
            model,
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,  # Unsloth —Ç—Ä–µ–±—É–µ—Ç 0 –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
            target_modules=target_modules,
            bias="none",
            use_gradient_checkpointing="unsloth",  # Unsloth smart checkpointing
            random_state=42,
            max_seq_length=max_seq_length,
        )
    
    # –ë–∞—Ä—å–µ—Ä –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
    if is_multi_gpu and torch.distributed.is_initialized():
        logger.info(f"ü¶• Rank {rank}: LoRA adapters added, syncing all processes...")
        torch.distributed.barrier()
    
    # Pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # === –ü—Ä–∏–º–µ–Ω—è–µ–º RL –ø–∞—Ç—á–∏ Unsloth ===
    logger.info("ü¶• Applying Unsloth RL patches...")
    try:
        PatchFastRL("unsloth/Llama-3.2-1B-Instruct", FastLanguageModel)  # dummy call to patch
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not apply Unsloth RL patches: {e}")
    
    metrics_logger.update(status="loading_dataset")
    
    # === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===
    if dataset is None:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏ –∏–∑ UI config
        data_path = (
            config.get("grpo_dataset_path") or 
            config.get("data_path") or 
            config.get("dataset_path")
        )
        dataset_key = config.get("grpo_dataset_key", "")
        dataset_source = config.get("grpo_dataset_source", "")
        
        # GSM8K –∏–∑ HuggingFace
        if "GSM8K" in dataset_source or dataset_key in ("gsm8k_en", "gsm8k_ru"):
            logger.info(f"ü¶• Loading GSM8K dataset (key={dataset_key})...")
            if dataset_key == "gsm8k_ru":
                # –†—É—Å—Å–∫–∏–π GSM8K
                dataset = load_dataset("d0rj/gsm8k-ru", split="train")
            else:
                # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π GSM8K
                dataset = load_dataset("openai/gsm8k", "main", split="train")
        elif data_path:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–∞–π–ª–∞
            logger.info(f"ü¶• Loading dataset from: {data_path}")
            if data_path.endswith((".json", ".jsonl")):
                dataset = load_dataset("json", data_files=data_path, split="train")
            else:
                # HuggingFace dataset
                hf_config = config.get("hf_dataset_config")
                if hf_config:
                    dataset = load_dataset(data_path, hf_config, split="train")
                else:
                    dataset = load_dataset(data_path, split="train")
        else:
            # Default: GSM8K English
            logger.info("ü¶• No dataset specified, defaulting to GSM8K (English)")
            dataset = load_dataset("openai/gsm8k", "main", split="train")
    
    # === –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è GRPO ===
    prompt_col = config.get("grpo_prompt_column", "question")
    answer_col = config.get("grpo_answer_column", "answer")
    
    # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
    cols = dataset.column_names
    logger.info(f"ü¶• Dataset columns: {cols}")
    
    # –î–ª—è GSM8K –∫–æ–ª–æ–Ω–∫–∏: question, answer
    if "question" in cols and prompt_col not in cols:
        prompt_col = "question"
    if "answer" in cols and answer_col not in cols:
        answer_col = "answer"
    
    # ===== System prompt –Ω–∞ –æ—Å–Ω–æ–≤–µ reasoning_format (–∫–∞–∫ –≤ –æ–±—ã—á–Ω–æ–º –±—ç–∫–µ–Ω–¥–µ!) =====
    reasoning_format = config.get("grpo_reasoning_format", "deepseek")
    custom_system_prompt = config.get("grpo_system_prompt", None)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º system prompt –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ (–∫–∞–∫ –≤ rollout.py build_reasoning_prompt)
    if custom_system_prompt:
        # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≤ UI ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        system_prompt = custom_system_prompt
    elif reasoning_format == "deepseek":
        # –§–æ—Ä–º–∞—Ç DeepSeek —Å <think> —Ç–µ–≥–∞–º–∏
        system_prompt = """A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>
<answer> answer here </answer>"""
    elif reasoning_format == "simple":
        # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç —Å <reasoning> —Ç–µ–≥–∞–º–∏
        system_prompt = """–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
<reasoning>
(–®–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è)
</reasoning>
<answer>
(–ö–æ—Ä–æ—Ç–∫–∏–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç)
</answer>"""
    elif reasoning_format == "russian":
        # –†—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
        system_prompt = """–¢—ã ‚Äî —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –†–µ—à–∞–π –∑–∞–¥–∞—á–∏ –ø–æ—à–∞–≥–æ–≤–æ.
–°–Ω–∞—á–∞–ª–∞ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å—É–∂–¥–∞–π –≤ —Ç–µ–≥–µ <reasoning>...</reasoning>,
–∑–∞—Ç–µ–º –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –≤ —Ç–µ–≥–µ <answer>...</answer>.

–ü—Ä–∏–º–µ—Ä:
<reasoning>
–î–∞–Ω–æ: ...
–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏: ...
–†–µ—à–µ–Ω–∏–µ: ...
</reasoning>
<answer>
42
</answer>"""
    elif reasoning_format == "gsm8k":
        # –§–æ—Ä–º–∞—Ç GSM8K —Å ####
        system_prompt = """You are a helpful assistant that solves math problems step by step.
Show your reasoning process, then provide the final numerical answer after ####.

Example format:
Let me solve this step by step.
Step 1: ...
Step 2: ...
Therefore, the answer is X.
#### X"""
    else:
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è deepseek
        system_prompt = """A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>
<answer> answer here </answer>"""
    
    logger.info(f"ü¶• Reasoning format: {reasoning_format}")
    logger.info(f"ü¶• System prompt preview: {system_prompt[:100]}...")
    
    def format_for_grpo(example):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä –¥–ª—è GRPO —Å chat messages (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth)."""
        question = example.get(prompt_col, "")
        raw_answer = example.get(answer_col, "")
        
        # TRL –æ–∂–∏–¥–∞–µ—Ç prompt –∫–∞–∫ —Å–ø–∏—Å–æ–∫ chat messages!
        # –ò–º–µ–Ω–Ω–æ —Ç–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth qwen3_grpo.py
        prompt = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question},
        ]
        
        # –î–ª—è GSM8K –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –ø–æ—Å–ª–µ ####, –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –±–µ—Ä—ë–º as-is
        if "####" in str(raw_answer):
            answer = str(raw_answer).split("####")[-1].strip()
        else:
            answer = str(raw_answer).strip()
        
        return {
            "prompt": prompt,  # Chat messages!
            "answer": answer,  # TRL –ø–µ—Ä–µ–¥–∞—ë—Ç —ç—Ç–æ –≤ reward_funcs –∫–∞–∫ kwarg
        }
    
    dataset = dataset.map(format_for_grpo, remove_columns=dataset.column_names)
    dataset = dataset.filter(lambda x: len(x.get("prompt", [])) > 0)
    
    # DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
    logger.info(f"ü¶• Dataset prepared: {len(dataset)} examples")
    logger.info(f"ü¶• Dataset columns after formatting: {dataset.column_names}")
    if len(dataset) > 0:
        first_prompt = dataset[0]['prompt']
        prompt_preview = first_prompt[-1]["content"][:80] if isinstance(first_prompt, list) else str(first_prompt)[:80]
        logger.info(f"ü¶• First example: prompt={prompt_preview}..., answer={dataset[0].get('answer', 'MISSING!')}")
    
    metrics_logger.update(
        status="training",
        num_train_examples=len(dataset),
        backend="unsloth",
    )
    
    # === GRPO Config ===
    output_dir = Path(config.get("output_dir", "out/unsloth_grpo"))
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ UI (—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏!)
    batch_size = config.get("grpo_train_batch_size", config.get("batch_size", 1))
    gradient_accumulation = config.get("gradient_accumulation", 4)
    num_generations = config.get("grpo_group_size", config.get("grpo_num_generations", 8))
    max_new_tokens = config.get("grpo_max_new_tokens", 512)
    learning_rate = config.get("grpo_learning_rate", config.get("learning_rate", 5e-5))
    temperature = config.get("grpo_temperature", 0.7)
    kl_weight = config.get("grpo_kl_weight", config.get("grpo_beta", 0.1))
    clip_eps = config.get("grpo_clip_eps_low", 0.2)
    algorithm = config.get("grpo_algorithm", "grpo")  # grpo, dapo, dr_grpo
    
    logger.info(f"ü¶• GRPO Config from UI:")
    logger.info(f"   learning_rate={learning_rate}, batch_size={batch_size}, grad_accum={gradient_accumulation}")
    logger.info(f"   num_generations={num_generations}, temperature={temperature}, kl_weight={kl_weight}")
    logger.info(f"   algorithm={algorithm}, clip_eps={clip_eps}")
    
    # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã GRPOConfig (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ trl)
    grpo_kwargs = dict(
        output_dir=str(output_dir),
        num_train_epochs=config.get("epochs", 1),
        max_steps=config.get("max_steps", -1),
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation,
        learning_rate=learning_rate,
        weight_decay=config.get("weight_decay", 0.001),  # –ö–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth
        warmup_steps=config.get("warmup_steps", 50),
        warmup_ratio=config.get("warmup_ratio", 0.1),
        lr_scheduler_type=config.get("lr_schedule", "linear"),  # linear –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth
        logging_steps=config.get("log_every", 1),  # –ß–∞—â–µ –ª–æ–≥–∏—Ä—É–µ–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
        save_steps=config.get("save_every", 500),
        save_total_limit=3,
        bf16=mixed_precision == "bf16" and is_bfloat16_supported(),
        fp16=mixed_precision == "fp16",
        optim="adamw_8bit",
        seed=42,
        max_grad_norm=config.get("max_grad_norm", 1.0),
        report_to="none",
        
        # GRPO specific
        num_generations=num_generations,
        temperature=temperature,
        beta=kl_weight,  # KL coefficient
        
    )
    
    # Loss type: grpo, bnpo (–¥–ª—è DAPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è bnpo), dr_grpo
    if algorithm == "dapo":
        grpo_kwargs["loss_type"] = "dapo"  # –∏–ª–∏ "bnpo" –¥–ª—è DAPO
    elif algorithm == "dr_grpo":
        grpo_kwargs["loss_type"] = "dr_grpo"
    else:
        grpo_kwargs["loss_type"] = "grpo"  # –¥–µ—Ñ–æ–ª—Ç
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ GRPOConfig
    import inspect
    grpo_sig = inspect.signature(GRPOConfig.__init__)
    
    # Epsilon –¥–ª—è PPO clipping
    if "epsilon" in grpo_sig.parameters:
        grpo_kwargs["epsilon"] = clip_eps
    elif "epsilon_low" in grpo_sig.parameters:
        grpo_kwargs["epsilon_low"] = clip_eps
        grpo_kwargs["epsilon_high"] = config.get("grpo_clip_eps_high", clip_eps)
    
    # max_prompt_length –∏ max_completion_length (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth)
    # –í–∞–∂–Ω–æ: max_prompt_length + max_completion_length <= max_seq_length
    max_prompt_length = max_seq_length // 2  # –ü–æ–ª–æ–≤–∏–Ω–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    
    if "max_prompt_length" in grpo_sig.parameters:
        grpo_kwargs["max_prompt_length"] = max_prompt_length
    
    if "max_completion_length" in grpo_sig.parameters:
        grpo_kwargs["max_completion_length"] = min(max_new_tokens, max_seq_length - max_prompt_length)
    elif "max_new_tokens" in grpo_sig.parameters:
        grpo_kwargs["max_new_tokens"] = max_new_tokens
    
    # vLLM SamplingParams (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth qwen3_grpo.py)
    if "vllm_sampling_params" in grpo_sig.parameters:
        try:
            from vllm import SamplingParams
            vllm_sampling_params = SamplingParams(
                min_p=0.1,
                top_p=1.0,
                top_k=-1,
                seed=42,
                stop=[tokenizer.eos_token] if tokenizer.eos_token else None,
                include_stop_str_in_output=True,
            )
            grpo_kwargs["vllm_sampling_params"] = vllm_sampling_params
            logger.info("ü¶• Added vLLM SamplingParams to GRPOConfig")
        except ImportError:
            logger.debug("vLLM not available, skipping sampling params")
    
    grpo_config = GRPOConfig(**grpo_kwargs)
    
    # === Reward Function —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å—ç–º–ø–ª–æ–≤ ===
    # –ü–æ–ª—É—á–∞–µ–º reward rules –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ UI
    reward_rules = config.get("grpo_reward_rules", [])
    reasoning_format = config.get("grpo_reasoning_format", "reasoning_answer")
    
    import re
    import json
    
    # === –•–µ–ª–ø–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è content –∏–∑ TRL completions ===
    # TRL –ø–µ—Ä–µ–¥–∞—ë—Ç completions –∫–∞–∫ —Å–ø–∏—Å–æ–∫ chat messages: [{"role": "assistant", "content": "..."}]
    # –ê –Ω–µ –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏!
    def _get_content(item) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ completion (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ chat message)."""
        if isinstance(item, str):
            return item
        elif isinstance(item, list) and len(item) > 0:
            # [{"role": "assistant", "content": "..."}]
            if isinstance(item[0], dict) and "content" in item[0]:
                return item[0]["content"]
            elif isinstance(item[0], str):
                return item[0]
        elif isinstance(item, dict) and "content" in item:
            return item["content"]
        return str(item)
    
    def _get_question(prompt) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∏–∑ prompt (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ chat messages).
        
        TRL –ø–µ—Ä–µ–¥–∞—ë—Ç prompts –≤ —Ñ–æ—Ä–º–∞—Ç–µ chat messages:
        [{"role": "system", "content": "..."}, {"role": "user", "content": "–≤–æ–ø—Ä–æ—Å"}]
        """
        if isinstance(prompt, str):
            return prompt
        elif isinstance(prompt, list) and len(prompt) > 0:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π user message (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth: prompts[0][-1]["content"])
            for msg in reversed(prompt):
                if isinstance(msg, dict):
                    if msg.get("role") == "user" and "content" in msg:
                        return msg["content"]
            # Fallback: –±–µ—Ä—ë–º content –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            if isinstance(prompt[-1], dict) and "content" in prompt[-1]:
                return prompt[-1]["content"]
        elif isinstance(prompt, dict) and "content" in prompt:
            return prompt["content"]
        return str(prompt)
    
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º Unsloth –ø—Ä–∏–º–µ—Ä–µ)
    global UNSLOTH_PRINTED_TIMES
    UNSLOTH_PRINTED_TIMES = 0
    
    # –ò–Ω—Ç–µ—Ä–≤–∞–ª –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_completions = config.get("grpo_log_completions", True)
    completion_log_interval = config.get("grpo_completion_log_interval", 10)
    
    # UI run dir –¥–ª—è samples.jsonl
    ui_run_dir = config.get("ui_run_dir")
    samples_file = output_dir / "samples.jsonl"
    ui_samples_file = Path(ui_run_dir) / "samples.jsonl" if ui_run_dir else None
    
    def _save_sample_to_file(
        step: int, 
        prompt_messages: List,  # Chat messages —Ñ–æ—Ä–º–∞—Ç
        completion: str, 
        reward: float, 
        reference_answer: str = "", 
        extracted: str = "",
        all_completions: Optional[List[str]] = None,
        all_rewards: Optional[List[float]] = None,
    ):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—ç–º–ø–ª –≤ samples.jsonl –¥–ª—è UI (–∫–∞–∫ –≤ –æ–±—ã—á–Ω–æ–º –±—ç–∫–µ–Ω–¥–µ)."""
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if isinstance(prompt_messages, list):
                # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                try:
                    formatted_prompt = tokenizer.apply_chat_template(
                        prompt_messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                except:
                    # Fallback: –ø—Ä–æ—Å—Ç–æ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
                    formatted_prompt = "\n".join(
                        f"[{m.get('role', 'unknown')}]: {m.get('content', '')}"
                        for m in prompt_messages if isinstance(m, dict)
                    )
            else:
                formatted_prompt = str(prompt_messages)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º full_texts (–ø—Ä–æ–º–ø—Ç + completion) –∫–∞–∫ –≤ –æ–±—ã—á–Ω–æ–º –±—ç–∫–µ–Ω–¥–µ
            completions_list = all_completions if all_completions else [completion]
            rewards_list = all_rewards if all_rewards else [reward]
            
            full_texts = [formatted_prompt + comp for comp in completions_list]
            
            sample_entry = {
                "step": step,
                "prompt": formatted_prompt,
                "reference_answer": reference_answer,
                "completions": completions_list,
                "full_texts": full_texts,  # –ü—Ä–æ–º–ø—Ç + completion –¥–ª—è UI –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                "rewards": rewards_list,
                "extracted": extracted,
                "timestamp": datetime.now().isoformat(),
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ output_dir/samples.jsonl
            with open(samples_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(sample_entry, ensure_ascii=False) + "\n")
            
            # –î—É–±–ª–∏—Ä—É–µ–º –≤ UI run_dir
            if ui_samples_file:
                ui_samples_file.parent.mkdir(parents=True, exist_ok=True)
                with open(ui_samples_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(sample_entry, ensure_ascii=False) + "\n")
            
            logger.debug(f"üìù Saved sample to samples.jsonl (step={step})")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not save sample: {e}")
    
    def _print_sample(
        step: int, 
        prompt_messages: List,  # Chat messages —Ñ–æ—Ä–º–∞—Ç
        answer: str, 
        response: str, 
        extracted: str, 
        reward: float
    ):
        """–í—ã–≤–æ–¥–∏—Ç —Å—ç–º–ø–ª –≤ –∫–æ–Ω—Å–æ–ª—å —Å –ø–æ–ª–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º (–∫–∞–∫ –≤ –æ–±—ã—á–Ω–æ–º –±—ç–∫–µ–Ω–¥–µ)."""
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if isinstance(prompt_messages, list):
            try:
                formatted_prompt = tokenizer.apply_chat_template(
                    prompt_messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
            except:
                formatted_prompt = "\n".join(
                    f"[{m.get('role', 'unknown')}]: {m.get('content', '')}"
                    for m in prompt_messages if isinstance(m, dict)
                )
        else:
            formatted_prompt = str(prompt_messages)
        
        print("\n" + "=" * 80)
        print(f"üìù SAMPLE AT STEP {step}")
        print("=" * 80)
        print(f"\n{'‚îÄ'*40} PROMPT {'‚îÄ'*40}")
        print(formatted_prompt[:1000])
        if len(formatted_prompt) > 1000:
            print(f"... (truncated, total {len(formatted_prompt)} chars)")
        print(f"\n{'‚îÄ'*40} REFERENCE ANSWER {'‚îÄ'*40}")
        print(f"‚úÖ {answer}")
        print(f"\n{'‚îÄ'*40} MODEL RESPONSE {'‚îÄ'*40}")
        print(response[:1500])
        if len(response) > 1500:
            print(f"... (truncated, total {len(response)} chars)")
        print(f"\n{'‚îÄ'*40} EVALUATION {'‚îÄ'*40}")
        print(f"üéØ Extracted: {extracted}")
        print(f"‚≠ê Reward: {reward:.4f}")
        print("=" * 80 + "\n")
    
    def create_trl_reward_fn_from_rules(rules: List[Dict], reasoning_fmt: str):
        """
        –°–æ–∑–¥–∞—ë—Ç TRL-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é reward —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ UI –ø—Ä–∞–≤–∏–ª (Reward Designer).
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç UniversalRuleReward –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ –±—ç–∫–µ–Ω–¥–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏!
        
        TRL –≤—ã–∑—ã–≤–∞–µ—Ç: reward_fn(prompts, completions, answer, **kwargs)
        """
        from homellm.training.rl.rewards.base import UniversalRuleReward
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        active_rules = [r for r in rules if r.get("enabled", True)]
        if not active_rules:
            return []
        
        # –°–æ–∑–¥–∞—ë–º UniversalRuleReward –∏–∑ –ø—Ä–∞–≤–∏–ª
        universal_reward = UniversalRuleReward.from_config(active_rules)
        logger.info(f"ü¶• Created UniversalRuleReward from {len(active_rules)} UI rules")
        
        for rule in active_rules:
            logger.info(f"ü¶•   - {rule.get('name', 'unnamed')} (weight={rule.get('weight', 1.0)})")
        
        # –°–æ–∑–¥–∞—ë–º TRL-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –æ–±—ë—Ä—Ç–∫—É
        def ui_rules_reward_fn(
            prompts: List,
            completions: List,
            answer: Optional[List[str]] = None,
            **kwargs
        ) -> List[float]:
            """TRL-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è UniversalRuleReward."""
            rewards = []
            
            # –ü–æ–ª—É—á–∞–µ–º ground truth
            gt_list = answer or []
            if isinstance(gt_list, str):
                gt_list = [gt_list]
            
            for i, completion in enumerate(completions):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ chat messages
                response = _get_content(completion)
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç
                prompt_text = _get_question(prompts[i]) if i < len(prompts) else ""
                
                # –ü–æ–ª—É—á–∞–µ–º reference answer
                reference = gt_list[i] if i < len(gt_list) else ""
                
                # –í—ã–∑—ã–≤–∞–µ–º UniversalRuleReward
                reward = universal_reward(
                    completion=response,
                    reference_answer=str(reference),
                    prompt=prompt_text,
                )
                rewards.append(reward)
            
            return rewards
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Å –æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π (TRL –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫)
        return [ui_rules_reward_fn]
    
    if reward_rules and len(reward_rules) > 0:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ UI
        logger.info(f"ü¶• Creating reward functions from {len(reward_rules)} UI rules")
        reward_fn = create_trl_reward_fn_from_rules(reward_rules, reasoning_format)
        if not reward_fn:
            logger.warning("ü¶• No valid reward rules, using defaults")
            reward_fn = None
    
    if reward_fn is None:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ reward —Ñ—É–Ω–∫—Ü–∏–∏ (–∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç—Å—è –ø–æ–¥ reasoning_format)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç reasoning_format
        if reasoning_format in ("deepseek", "simple", "russian"):
            format_pattern = re.compile(r'<answer>\s*.+?\s*</answer>', re.DOTALL | re.IGNORECASE)
            format_name = "<answer>...</answer>"
            use_answer_tags = True
        else:  # gsm8k –∏–ª–∏ –¥—Ä—É–≥–∏–µ
            format_pattern = re.compile(r'####\s*-?\d+')
            format_name = "#### <number>"
            use_answer_tags = False
        
        def default_format_fn(
            prompts: List,
            completions: List,
            answer: Optional[List[str]] = None,
            **kwargs
        ) -> List[float]:
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–æ—Ä–º–∞—Ç—É –æ—Ç–≤–µ—Ç–∞."""
            rewards = []
            for completion in completions:
                response = _get_content(completion)
                if format_pattern.search(response):
                    rewards.append(1.0)  # –§–æ—Ä–º–∞—Ç —Å–æ–±–ª—é–¥—ë–Ω
                else:
                    # –ß–∞—Å—Ç–∏—á–Ω—ã–π reward –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å –∫–∞–∫–æ–π-—Ç–æ –æ—Ç–≤–µ—Ç
                    if use_answer_tags:
                        if "<answer>" in response.lower():
                            rewards.append(0.3)
                        else:
                            rewards.append(0.0)
                    else:
                        if "####" in response:
                            rewards.append(0.3)
                        else:
                            rewards.append(0.0)
            return rewards
        
        def default_correctness_fn(
            prompts: List,
            completions: List,
            answer: Optional[List[str]] = None,
            **kwargs
        ) -> List[float]:
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å ground truth)."""
            rewards = []
            gt_list = answer or []
            
            for i, completion in enumerate(completions):
                response = _get_content(completion)
                extracted = extract_answer_from_response(response)
                true_answer = gt_list[i] if i < len(gt_list) else None
                
                if extracted is None:
                    rewards.append(-1.0)  # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ—Ç–≤–µ—Ç–∞
                    continue
                
                if true_answer is None:
                    rewards.append(0.0)
                    continue
                
                try:
                    # –ü—Ä–æ–±—É–µ–º —á–∏—Å–ª–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                    true_val = float(str(true_answer).strip().replace(",", ""))
                    guess_val = float(str(extracted).strip().replace(",", ""))
                    if guess_val == true_val:
                        rewards.append(3.0)  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                    else:
                        # –ß–∞—Å—Ç–∏—á–Ω—ã–π reward –∑–∞ –±–ª–∏–∑–∫–∏–π –æ—Ç–≤–µ—Ç
                        ratio = guess_val / true_val if true_val != 0 else 0
                        if 0.9 <= ratio <= 1.1:
                            rewards.append(1.0)
                        else:
                            rewards.append(-0.5)
                except (ValueError, TypeError):
                    # –°—Ç—Ä–æ–∫–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                    if str(extracted).strip().lower() == str(true_answer).strip().lower():
                        rewards.append(3.0)
                    else:
                        rewards.append(-0.5)
            
            return rewards
        
        reward_fn = [default_format_fn, default_correctness_fn]
        logger.info(f"ü¶• Using default reward functions for format={reasoning_format} (pattern: {format_name})")
    
    # === Wrapper –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å—ç–º–ø–ª–æ–≤ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º Unsloth –ø—Ä–∏–º–µ—Ä–µ) ===
    # –°–æ–∑–¥–∞—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é reward —Ñ—É–Ω–∫—Ü–∏—é –∫–æ—Ç–æ—Ä–∞—è –ª–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç/–æ—Ç–≤–µ—Ç/reward
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
    # reasoning_format —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω –≤—ã—à–µ
    if reasoning_format in ("deepseek", "simple", "russian"):
        # –§–æ—Ä–º–∞—Ç —Å <answer>...</answer> —Ç–µ–≥–∞–º–∏
        answer_tag_pattern = re.compile(r'<answer>\s*(.*?)\s*</answer>', re.DOTALL | re.IGNORECASE)
        use_hash_format = False
        logger.info(f"ü¶• Using <answer> tag pattern for extraction (format={reasoning_format})")
    else:
        # –§–æ—Ä–º–∞—Ç —Å #### (GSM8K style)
        answer_tag_pattern = None
        use_hash_format = True
        logger.info(f"ü¶• Using #### pattern for extraction (format={reasoning_format})")
    
    answer_hash_pattern = re.compile(r'####\s*(-?\d+(?:,\d{3})*(?:\.\d+)?)')
    
    def extract_answer_from_response(response: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ response –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞."""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º <answer> —Ç–µ–≥–∏ (–µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
        if answer_tag_pattern:
            match = answer_tag_pattern.search(response)
            if match:
                return match.group(1).strip()
        
        # –ó–∞—Ç–µ–º –ø—Ä–æ–±—É–µ–º #### —Ñ–æ—Ä–º–∞—Ç
        match = answer_hash_pattern.search(response)
        if match:
            return match.group(1).replace(",", "")
        
        # Fallback: –∏—â–µ–º –ø–æ—Å–ª–µ ####
        if "####" in response:
            after = response.split("####")[-1].strip()
            numbers = re.findall(r'-?\d+(?:\.\d+)?', after)
            if numbers:
                return numbers[0]
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback: –ø–æ—Å–ª–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –≤ —Ç–µ–∫—Å—Ç–µ
        numbers = re.findall(r'-?\d+(?:\.\d+)?', response)
        return numbers[-1] if numbers else None
    
    def logging_reward_fn(
        prompts: List,
        completions: List,
        answer: Optional[List[str]] = None,  # ground truth –æ—Ç TRL (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π!)
        **kwargs
    ) -> List[float]:
        """Reward —Ñ—É–Ω–∫—Ü–∏—è —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º (–∫–∞–∫ check_numbers –≤ Unsloth –ø—Ä–∏–º–µ—Ä–µ)."""
        global UNSLOTH_PRINTED_TIMES
        
        # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç TRL (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ)
        if UNSLOTH_PRINTED_TIMES == 0:
            logger.info(f"üîç DEBUG: answer count={len(answer) if answer else 0}, completions count={len(completions)}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ completions (TRL –ø–µ—Ä–µ–¥–∞—ë—Ç –∫–∞–∫ chat messages!)
        responses = [_get_content(c) for c in completions]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç—ã –∏–∑ responses (–∏—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é)
        extracted_responses = [extract_answer_from_response(r) for r in responses]
        
        # –ü–æ–ª—É—á–∞–µ–º ground truth
        # TRL –ø–µ—Ä–µ–¥–∞—ë—Ç answer –∫–∞–∫ –ü–û–ó–ò–¶–ò–û–ù–ù–´–ô –∞—Ä–≥—É–º–µ–Ω—Ç (–Ω–µ —á–µ—Ä–µ–∑ kwargs!)
        gt_list = answer or []
        
        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
        if gt_list is None:
            gt_list = []
        elif isinstance(gt_list, str):
            gt_list = [gt_list]
        
        # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø, –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –∏—Ç–æ–≥–æ–≤—ã–π reward
        # –û—Å–Ω–æ–≤–Ω—ã–µ rewards –∏–¥—É—Ç –∏–∑ UI-–∑–∞–¥–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π (format_check, exact_match –∏ —Ç.–¥.)
        scores = [0.0] * len(completions)  # –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0 ‚Äî –Ω–µ –≤–ª–∏—è–µ–º –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ
        
        # –í—ã—á–∏—Å–ª—è–µ–º "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π" reward —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ª–æ–≥–∞—Ö
        display_rewards = []
        for i, (guess, response) in enumerate(zip(extracted_responses, responses)):
            true_answer = gt_list[i] if i < len(gt_list) else None
            
            if guess is None:
                display_rewards.append(0.0)
                continue
            
            try:
                if true_answer is not None:
                    true_val = float(str(true_answer).strip().replace(",", ""))
                    guess_val = float(guess)
                    display_rewards.append(1.0 if guess_val == true_val else 0.0)
                else:
                    display_rewards.append(0.0)
            except:
                display_rewards.append(0.0)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º Unsloth –ø—Ä–∏–º–µ—Ä–µ)
        if log_completions and UNSLOTH_PRINTED_TIMES % completion_log_interval == 0:
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–æ–º–ø—Ç (chat messages)
            first_prompt = prompts[0] if prompts else []
            
            gt_str = str(gt_list[0]) if gt_list else "N/A"
            response_text = responses[0] if responses else "N/A"
            extracted = extracted_responses[0] if extracted_responses else "N/A"
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º display_reward –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (informational only)
            display_reward = display_rewards[0] if display_rewards else 0.0
            
            # –í—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å —Å –ø–æ–ª–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
            _print_sample(
                step=UNSLOTH_PRINTED_TIMES,
                prompt_messages=first_prompt,  # –ü–µ—Ä–µ–¥–∞—ë–º chat messages
                answer=gt_str,
                response=response_text,
                extracted=str(extracted),
                reward=display_reward if isinstance(display_reward, float) else 0.0,
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª –¥–ª—è UI (—Å –ø–æ–ª–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º –∏ –≤—Å–µ–º–∏ completions)
            _save_sample_to_file(
                step=UNSLOTH_PRINTED_TIMES,
                prompt_messages=first_prompt,  # Chat messages
                completion=response_text,
                reward=display_reward if isinstance(display_reward, float) else 0.0,
                reference_answer=gt_str,
                extracted=str(extracted),
                all_completions=responses,  # –í—Å–µ completions –∏–∑ batch
                all_rewards=display_rewards,  # –í—Å–µ rewards
            )
        
        UNSLOTH_PRINTED_TIMES += 1
        return scores
    
    # –î–æ–±–∞–≤–ª—è–µ–º logging_reward_fn –∫ —Å–ø–∏—Å–∫—É
    if isinstance(reward_fn, list):
        reward_fn.append(logging_reward_fn)
    else:
        reward_fn = [reward_fn, logging_reward_fn] if reward_fn else [logging_reward_fn]
    
    logger.info(f"ü¶• Added logging reward function (log_every={completion_log_interval} steps)")
    
    # === –ü–∞—Ç—á –¥–ª—è DDP: monkeypatch –¥–ª—è unwrap model ===
    # Unsloth –∏—Å–ø–æ–ª—å–∑—É–µ—Ç model.config –Ω–∞–ø—Ä—è–º—É—é, –Ω–æ –ø—Ä–∏ DDP –º–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞
    # –°–æ–∑–¥–∞—ë–º –æ–±—ë—Ä—Ç–∫—É –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ unwrap'–∏—Ç –º–æ–¥–µ–ª—å
    if world_size > 1:
        from accelerate.utils import extract_model_from_parallel
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π getattr –¥–ª—è DDP
        original_ddp_getattr = None
        try:
            from torch.nn.parallel import DistributedDataParallel as DDP
            original_ddp_getattr = DDP.__getattr__
            
            def patched_getattr(self, name):
                if name == 'config' and hasattr(self, 'module'):
                    return getattr(self.module, 'config', None)
                return original_ddp_getattr(self, name)
            
            DDP.__getattr__ = patched_getattr
            logger.info("ü¶• Patched DDP.__getattr__ for .config access")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not patch DDP: {e}")
    
    # === Trainer ===
    # GRPOTrainer ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º processing_class –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ Unsloth
    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,  # –ù–æ–≤—ã–π API TRL (–Ω–µ tokenizer!)
        train_dataset=dataset,
        args=grpo_config,
        reward_funcs=reward_fn,
    )
    
    # === Callback –¥–ª—è –º–µ—Ç—Ä–∏–∫ ===
    from transformers import TrainerCallback
    
    class MetricsCallback(TrainerCallback):
        def __init__(self, metrics_logger, start_time, total_steps, tokenizer):
            self.metrics_logger = metrics_logger
            self.start_time = start_time
            self.total_steps = total_steps
            self.tokenizer = tokenizer
            self.last_log_step = -1
            self.sample_log_interval = 50  # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Å—ç–º–ø–ª—ã –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
        
        def on_train_begin(self, args, state, control, **kwargs):
            self.metrics_logger.update(
                status="training",
                total_steps=self.total_steps,
                backend="unsloth",
            )
        
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs is None:
                return
            
            step = state.global_step
            
            # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ª–æ–≥–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞
            if step == self.last_log_step:
                return
            self.last_log_step = step
            
            loss = logs.get("loss", 0.0)
            lr = logs.get("learning_rate", 0.0)
            
            # GRPO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            reward = logs.get("reward", logs.get("rewards/mean", 0.0))
            kl = logs.get("kl", logs.get("kl_divergence", 0.0))
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ TRL
            policy_loss = logs.get("loss/policy", logs.get("policy_loss", None))
            value_loss = logs.get("loss/value", logs.get("value_loss", None))
            entropy = logs.get("loss/entropy", logs.get("entropy", None))
            
            elapsed = time.time() - self.start_time
            samples_per_sec = step / elapsed if elapsed > 0 else 0
            
            # –õ–æ–≥–∏—Ä—É–µ–º —à–∞–≥ —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            self.metrics_logger.log_step(
                step=step,
                loss=loss,
                lr=lr,
                samples_per_sec=samples_per_sec,
                reward=reward,
                kl=kl,
            )
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è UI
            eta_seconds = (self.total_steps - step) / samples_per_sec if samples_per_sec > 0 else 0
            self.metrics_logger.update(
                elapsed_time=elapsed,
                eta_seconds=eta_seconds,
                samples_per_second=samples_per_sec,
            )
            
            # –ö—Ä–∞—Å–∏–≤—ã–π –ª–æ–≥ –≤ –∫–æ–Ω—Å–æ–ª—å
            log_msg = f"ü¶• Step {step}/{self.total_steps} | Loss: {loss:.4f} | Reward: {reward:.4f} | KL: {kl:.4f} | LR: {lr:.2e}"
            if policy_loss is not None:
                log_msg += f" | Policy: {policy_loss:.4f}"
            logger.info(log_msg)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º completions –∏–∑ –ª–æ–≥–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
            completions = logs.get("completions", None)
            if completions and step % self.sample_log_interval == 0:
                self._log_sample_completions(step, completions)
        
        def _log_sample_completions(self, step, completions):
            """–õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤."""
            if not completions:
                return
            
            logger.info("=" * 80)
            logger.info(f"üìù Sample completions at step {step}:")
            logger.info("=" * 80)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 3 –ø—Ä–∏–º–µ—Ä–æ–≤
            samples_to_show = completions[:3] if isinstance(completions, list) else [completions]
            
            for i, completion in enumerate(samples_to_show):
                if isinstance(completion, dict):
                    prompt = completion.get("prompt", "N/A")[:200]
                    response = completion.get("response", completion.get("completion", "N/A"))[:500]
                    reward = completion.get("reward", "N/A")
                else:
                    prompt = "N/A"
                    response = str(completion)[:500]
                    reward = "N/A"
                
                logger.info(f"\n--- Sample {i+1} ---")
                logger.info(f"Prompt: {prompt}...")
                logger.info(f"Response: {response}...")
                logger.info(f"Reward: {reward}")
            
            logger.info("=" * 80)
        
        def on_save(self, args, state, control, **kwargs):
            ckpt_path = str(output_dir / f"checkpoint-{state.global_step}")
            self.metrics_logger.log_checkpoint(ckpt_path)
        
        def on_train_end(self, args, state, control, **kwargs):
            total_time = time.time() - self.start_time
            self.metrics_logger.update(
                status="completed",
                total_training_time=total_time,
                final_step=state.global_step,
            )
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º total_steps
    total_steps = grpo_config.max_steps if grpo_config.max_steps > 0 else (
        len(dataset) // (grpo_config.per_device_train_batch_size * grpo_config.gradient_accumulation_steps * max(1, world_size))
        * grpo_config.num_train_epochs
    )
    
    trainer.add_callback(MetricsCallback(metrics_logger, time.time(), total_steps, tokenizer))
    
    # === –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ ===
    logger.info("ü¶• Unsloth: Starting GRPO training...")
    start_time = time.time()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º unsloth train –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    try:
        from unsloth import unsloth_train
        unsloth_train(trainer)
    except ImportError:
        trainer.train()
    
    total_time = time.time() - start_time
    
    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    metrics_logger.update(status="saving_model")
    
    final_dir = output_dir / "final_model"
    final_dir.mkdir(parents=True, exist_ok=True)
    
    # merge_lora –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True ‚Äî –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å models-at-home backend
    # final_model/ —Å–æ–¥–µ—Ä–∂–∏—Ç merged –º–æ–¥–µ–ª—å –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ inference
    merge_lora = config.get("merge_lora", True)
    
    if use_lora:
        if merge_lora:
            # –°–Ω–∞—á–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –æ—Ç–¥–µ–ª—å–Ω–æ (–¥–æ merge)
            lora_dir = output_dir / "lora_adapters"
            lora_dir.mkdir(parents=True, exist_ok=True)
            model.save_pretrained(lora_dir)
            tokenizer.save_pretrained(lora_dir)
            logger.info(f"ü¶• Saved LoRA adapters to {lora_dir}")
            
            # Merge LoRA –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ final_model/
            logger.info("ü¶• Merging LoRA adapters into base model...")
            merged_model = model.merge_and_unload()
            merged_model.save_pretrained(final_dir)
            tokenizer.save_pretrained(final_dir)
            logger.info(f"ü¶• Saved merged model to {final_dir}")
        else:
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
            model.save_pretrained(final_dir)
            tokenizer.save_pretrained(final_dir)
            logger.info(f"ü¶• Saved LoRA adapters to {final_dir}")
    else:
        model.save_pretrained(final_dir)
        tokenizer.save_pretrained(final_dir)
        logger.info(f"ü¶• Saved full model to {final_dir}")
    
    # === –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===
    hours, rem = divmod(total_time, 3600)
    minutes, seconds = divmod(rem, 60)
    duration_str = f"{int(hours):02}:{int(minutes):02}:{seconds:05.2f}"
    
    metrics_logger.update(
        status="completed",
        total_time_seconds=total_time,
        training_duration=duration_str,
        final_model_path=str(final_dir),
        backend="unsloth",
    )
    
    logger.info(f"ü¶• Unsloth GRPO completed in {duration_str}")
