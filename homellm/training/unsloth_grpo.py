"""
Unsloth backend –¥–ª—è GRPO —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Unsloth –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- 2x –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ
- –î–æ 70% –º–µ–Ω—å—à–µ VRAM
- Triton —è–¥—Ä–∞ (RMSNorm, RoPE, SwiGLU)
- Smart Gradient Checkpointing
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ RL —Ñ—É–Ω–∫—Ü–∏–∏
"""
from __future__ import annotations

import json
import logging
import time
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Callable

# –ü–æ–¥–∞–≤–ª—è–µ–º warning –æ—Ç Unsloth –ø—Ä–æ –ø–æ—Ä—è–¥–æ–∫ –∏–º–ø–æ—Ä—Ç–æ–≤
# (–º—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –µ–≥–æ –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ env variables)
warnings.filterwarnings("ignore", message=".*Unsloth should be imported before.*")

import torch

logger = logging.getLogger(__name__)


def is_unsloth_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Unsloth."""
    try:
        import unsloth
        return True
    except ImportError:
        return False


def _patch_unsloth_left_pack_padding():
    """
    Monkey-patch –¥–ª—è —Ñ–∏–∫—Å–∞ –±–∞–≥–∞ –≤ Unsloth:
    torch.argsort –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç bool dtype –Ω–∞ CUDA —Å stable=True.
    
    Unsloth –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –∫–æ–¥ –≤ –∫—ç—à, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ –ø–∞—Ç—á–∏—Ç—å torch.argsort –≥–ª–æ–±–∞–ª—å–Ω–æ.
    """
    try:
        import torch
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π argsort
        _original_argsort = torch.argsort
        
        def _patched_argsort(input, dim=-1, descending=False, stable=False):
            """Patched argsort: converts bool to int for CUDA compatibility."""
            # –ï—Å–ª–∏ bool –Ω–∞ CUDA —Å stable=True ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int
            if input.dtype == torch.bool and input.is_cuda and stable:
                input = input.int()
            return _original_argsort(input, dim=dim, descending=descending, stable=stable)
        
        # –ó–∞–º–µ–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ
        torch.argsort = _patched_argsort
        
        logger.info("ü¶• Applied global fix for torch.argsort (bool on CUDA)")
        return True
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not patch torch.argsort: {e}")
        return False


def run_unsloth_grpo(
    config: Dict[str, Any],
    metrics_logger: Any,
    dataset: Any = None,
    reward_fn: Optional[Callable] = None,
) -> None:
    """
    –ó–∞–ø—É—Å–∫ GRPO —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Unsloth backend.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        metrics_logger: –õ–æ–≥–≥–µ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è UI
        dataset: –î–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∏–Ω–∞—á–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø–æ config)
        reward_fn: –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    import os
    
    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ multi-GPU ===
    # Unsloth –∏–º–µ–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É multi-GPU
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    
    if world_size > 1:
        logger.warning("=" * 60)
        logger.warning("‚ö†Ô∏è Unsloth multi-GPU (DDP) ‚Äî —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞!")
        logger.warning(f"   WORLD_SIZE={world_size}, LOCAL_RANK={local_rank}")
        logger.warning("   –ü—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'models-at-home backend'")
        logger.warning("=" * 60)
    
    # === –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º —Ç—é–Ω–∏–Ω–≥–∞ –î–û –∏–º–ø–æ—Ä—Ç–∞ Unsloth ===
    tuning_method = config.get("tuning_method", "full")
    
    # –í–ê–ñ–ù–û: Unsloth GRPO –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç full fine-tuning (–±–∞–≥ –≤ –∏—Ö –∫–æ–¥–µ)
    # –§–æ—Ä—Å–∏—Ä—É–µ–º LoRA –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω full
    if tuning_method == "full":
        logger.warning("‚ö†Ô∏è Unsloth GRPO –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç full fine-tuning! –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ LoRA.")
        logger.warning("   –î–ª—è full fine-tuning –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ models-at-home backend.")
        tuning_method = "lora"  # –§–æ—Ä—Å–∏—Ä—É–µ–º LoRA
        config["tuning_method"] = "lora"  # –û–±–Ω–æ–≤–ª—è–µ–º config
    
    os.environ["UNSLOTH_ENABLE_FULL_FINETUNING"] = "0"
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è –±–∞–≥–∞ —Å bool argsort –Ω–∞ CUDA
    _patch_unsloth_left_pack_padding()
    
    # –í–ê–ñ–ù–û: Unsloth –¥–æ–ª–∂–µ–Ω –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –ü–û–°–õ–ï —É—Å—Ç–∞–Ω–æ–≤–∫–∏ env variables
    try:
        from unsloth import FastLanguageModel
        from unsloth import is_bfloat16_supported
        from unsloth.models.rl import PatchFastRL
    except ImportError as e:
        raise ImportError(
            "Unsloth –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–µ—Ä–µ–∑: pip install unsloth\n"
            f"–û—à–∏–±–∫–∞: {e}"
        )
    
    try:
        from trl import GRPOConfig, GRPOTrainer
    except ImportError:
        raise ImportError(
            "trl –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –≤–µ—Ä—Å–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç GRPO. "
            "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install trl>=0.9.0"
        )
    
    from datasets import load_dataset, Dataset
    
    metrics_logger.update(status="loading_model", backend="unsloth")
    
    # === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ UI ===
    base_model_path = config.get("base_model_path")
    if not base_model_path:
        raise ValueError("base_model_path required for GRPO")
    
    max_seq_length = config.get("seq_len", 2048)
    dtype = None  # Auto-detect
    
    # Precision
    mixed_precision = config.get("mixed_precision", "bf16")
    if mixed_precision == "bf16" and is_bfloat16_supported():
        dtype = torch.bfloat16
    elif mixed_precision == "fp16":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    # === –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞ –∏–∑ UI (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤—ã—à–µ ‚Äî full ‚Üí lora) ===
    # tuning_method —Ç–µ–ø–µ—Ä—å: "lora" –∏–ª–∏ "qlora"
    
    # Unsloth GRPO –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ LoRA/QLoRA
    if tuning_method == "qlora":
        use_lora = True
        load_in_4bit = True
        full_finetuning = False
        logger.info("ü¶• Mode: QLoRA (4-bit quantization + LoRA)")
    else:  # "lora" (full –±—ã–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á—ë–Ω –Ω–∞ lora –≤—ã—à–µ)
        use_lora = True
        load_in_4bit = False
        full_finetuning = False
        logger.info("ü¶• Mode: LoRA (16-bit + LoRA)")
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–µ—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–æ)
    if "use_4bit" in config and config["use_4bit"]:
        load_in_4bit = True
    if "load_in_4bit" in config and config["load_in_4bit"]:
        load_in_4bit = True
    
    # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    lora_r = config.get("lora_r", 16)
    lora_alpha = config.get("lora_alpha", 32)
    lora_dropout = config.get("lora_dropout", 0.0)
    lora_target_modules = config.get("lora_target_modules")
    
    # –í–ê–ñ–ù–û: Unsloth —Ç—Ä–µ–±—É–µ—Ç dropout = 0 –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø–∞—Ç—á–µ–π!
    # –ò–Ω–∞—á–µ –æ–Ω –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫ QKV, O, MLP —Å–ª–æ—è–º
    if lora_dropout != 0.0:
        logger.warning(f"‚ö†Ô∏è Unsloth —Ç—Ä–µ–±—É–µ—Ç dropout=0 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏!")
        logger.warning(f"   –ú–µ–Ω—è—é dropout —Å {lora_dropout} –Ω–∞ 0.0")
        lora_dropout = 0.0
    
    logger.info(f"ü¶• Final settings: use_lora={use_lora}, load_in_4bit={load_in_4bit}, full_finetuning={full_finetuning}")
    if use_lora:
        logger.info(f"ü¶• LoRA: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")
    
    # === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Unsloth ===
    logger.info(f"ü¶• Unsloth GRPO: Loading model from {base_model_path}")
    logger.info(f"   max_seq_length={max_seq_length}, dtype={dtype}, load_in_4bit={load_in_4bit}, full_finetuning={full_finetuning}")
    
    # –î–ª—è multi-GPU: —É–∫–∞–∑—ã–≤–∞–µ–º device_map –Ω–∞ —Ç–µ–∫—É—â–∏–π GPU –ø—Ä–æ—Ü–µ—Å—Å–∞
    # –ö–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å accelerate/DDP —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ —Å–≤–æ–∏–º GPU
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    device_map = {"": f"cuda:{local_rank}"}
    logger.info(f"ü¶• Device map: {device_map} (LOCAL_RANK={local_rank})")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=base_model_path,
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
        full_finetuning=full_finetuning,  # ‚Üê NEW: –¥–ª—è full fine-tuning
        trust_remote_code=True,
        device_map=device_map,
    )
    
    # === –î–æ–±–∞–≤–ª—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã ===
    if use_lora:
        logger.info(f"ü¶• Unsloth: Adding LoRA adapters (r={lora_r}, alpha={lora_alpha})")
        
        # Target modules –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
        if lora_target_modules:
            target_modules = lora_target_modules
        else:
            target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
            ]
        
        logger.info(f"ü¶• LoRA target modules: {target_modules}")
        
        model = FastLanguageModel.get_peft_model(
            model,
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=target_modules,
            bias="none",
            use_gradient_checkpointing="unsloth",  # Unsloth smart checkpointing
            random_state=42,
            max_seq_length=max_seq_length,
        )
    
    # Pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # === –ü—Ä–∏–º–µ–Ω—è–µ–º RL –ø–∞—Ç—á–∏ Unsloth ===
    logger.info("ü¶• Applying Unsloth RL patches...")
    try:
        PatchFastRL("unsloth/Llama-3.2-1B-Instruct", FastLanguageModel)  # dummy call to patch
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not apply Unsloth RL patches: {e}")
    
    metrics_logger.update(status="loading_dataset")
    
    # === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===
    if dataset is None:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏ –∏–∑ UI config
        data_path = (
            config.get("grpo_dataset_path") or 
            config.get("data_path") or 
            config.get("dataset_path")
        )
        dataset_key = config.get("grpo_dataset_key", "")
        dataset_source = config.get("grpo_dataset_source", "")
        
        # GSM8K –∏–∑ HuggingFace
        if "GSM8K" in dataset_source or dataset_key in ("gsm8k_en", "gsm8k_ru"):
            logger.info(f"ü¶• Loading GSM8K dataset (key={dataset_key})...")
            if dataset_key == "gsm8k_ru":
                # –†—É—Å—Å–∫–∏–π GSM8K
                dataset = load_dataset("d0rj/gsm8k-ru", split="train")
            else:
                # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π GSM8K
                dataset = load_dataset("openai/gsm8k", "main", split="train")
        elif data_path:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–∞–π–ª–∞
            logger.info(f"ü¶• Loading dataset from: {data_path}")
            if data_path.endswith((".json", ".jsonl")):
                dataset = load_dataset("json", data_files=data_path, split="train")
            else:
                # HuggingFace dataset
                hf_config = config.get("hf_dataset_config")
                if hf_config:
                    dataset = load_dataset(data_path, hf_config, split="train")
                else:
                    dataset = load_dataset(data_path, split="train")
        else:
            # Default: GSM8K English
            logger.info("ü¶• No dataset specified, defaulting to GSM8K (English)")
            dataset = load_dataset("openai/gsm8k", "main", split="train")
    
    # === –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è GRPO ===
    prompt_col = config.get("grpo_prompt_column", "question")
    answer_col = config.get("grpo_answer_column", "answer")
    
    # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
    cols = dataset.column_names
    logger.info(f"ü¶• Dataset columns: {cols}")
    
    # –î–ª—è GSM8K –∫–æ–ª–æ–Ω–∫–∏: question, answer
    if "question" in cols and prompt_col not in cols:
        prompt_col = "question"
    if "answer" in cols and answer_col not in cols:
        answer_col = "answer"
    
    def format_for_grpo(example):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä –¥–ª—è GRPO."""
        question = example.get(prompt_col, "")
        answer = example.get(answer_col, "")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = f"Question: {question}\n\nLet's think step by step.\n\n"
        
        return {
            "prompt": prompt,
            "ground_truth": answer,
        }
    
    dataset = dataset.map(format_for_grpo, remove_columns=dataset.column_names)
    dataset = dataset.filter(lambda x: len(x.get("prompt", "")) > 0)
    
    logger.info(f"ü¶• Dataset prepared: {len(dataset)} examples")
    
    metrics_logger.update(
        status="training",
        num_train_examples=len(dataset),
        backend="unsloth",
    )
    
    # === GRPO Config ===
    output_dir = Path(config.get("output_dir", "out/unsloth_grpo"))
    output_dir.mkdir(parents=True, exist_ok=True)
    
    batch_size = config.get("batch_size", 1)
    gradient_accumulation = config.get("gradient_accumulation", 8)
    num_generations = config.get("grpo_num_generations", 4)
    max_new_tokens = config.get("grpo_max_new_tokens", 512)
    
    # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã GRPOConfig (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ trl)
    grpo_kwargs = dict(
        output_dir=str(output_dir),
        num_train_epochs=config.get("epochs", 1),
        max_steps=config.get("max_steps", -1),
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation,
        learning_rate=config.get("learning_rate", 5e-6),
        weight_decay=config.get("weight_decay", 0.01),
        warmup_steps=config.get("warmup_steps", 50),
        lr_scheduler_type=config.get("lr_schedule", "cosine"),
        logging_steps=config.get("log_every", 10),
        save_steps=config.get("save_every", 500),
        save_total_limit=3,
        bf16=mixed_precision == "bf16" and is_bfloat16_supported(),
        fp16=mixed_precision == "fp16",
        optim="adamw_8bit",
        seed=42,
        max_grad_norm=config.get("max_grad_norm", 1.0),
        report_to="none",
        
        # GRPO specific
        num_generations=num_generations,
        temperature=config.get("grpo_temperature", 0.7),
        beta=config.get("grpo_beta", 0.1),  # KL coefficient
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º max_completion_length –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (–Ω–æ–≤—ã–π API trl)
    # –∏–ª–∏ max_new_tokens (—Å—Ç–∞—Ä—ã–π API)
    import inspect
    grpo_sig = inspect.signature(GRPOConfig.__init__)
    if "max_completion_length" in grpo_sig.parameters:
        grpo_kwargs["max_completion_length"] = max_new_tokens
    elif "max_new_tokens" in grpo_sig.parameters:
        grpo_kwargs["max_new_tokens"] = max_new_tokens
    
    grpo_config = GRPOConfig(**grpo_kwargs)
    
    # === Reward Function ===
    # –ü–æ–ª—É—á–∞–µ–º reward rules –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ UI
    reward_rules = config.get("grpo_reward_rules", [])
    reasoning_format = config.get("grpo_reasoning_format", "reasoning_answer")
    
    import re
    
    def create_trl_reward_fn_from_rules(rules: List[Dict], reasoning_fmt: str):
        """
        –°–æ–∑–¥–∞—ë—Ç TRL-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ reward —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –ø—Ä–∞–≤–∏–ª UI.
        
        TRL –≤—ã–∑—ã–≤–∞–µ—Ç: reward_fn(completions=..., prompts=..., **kwargs)
        """
        reward_fns = []
        
        for rule in rules:
            if not rule.get("enabled", True):
                continue
                
            rule_type = rule.get("type", "format_check")
            rule_name = rule.get("name", "unknown")
            rule_weight = rule.get("weight", 1.0)
            params = rule.get("params", {})
            
            if rule_type == "format_check":
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞
                fmt = params.get("format", reasoning_fmt)
                
                def format_check_fn(
                    completions: List[str],
                    prompts: Optional[List[str]] = None,
                    fmt=fmt,
                    **kwargs
                ) -> List[float]:
                    rewards = []
                    for completion in completions:
                        if fmt == "reasoning_answer":
                            # <think>...</think> –∏–ª–∏ #### —Ñ–æ—Ä–º–∞—Ç
                            if "<think>" in completion and "</think>" in completion:
                                rewards.append(1.0)
                            elif "####" in completion:
                                after = completion.split("####")[-1].strip()
                                if re.search(r'-?\d+', after):
                                    rewards.append(1.0)
                                else:
                                    rewards.append(0.3)
                            else:
                                rewards.append(0.0)
                        elif fmt == "deepseek":
                            if "<think>" in completion and "</think>" in completion:
                                rewards.append(1.0)
                            else:
                                rewards.append(0.0)
                        else:  # gsm8k –∏–ª–∏ –¥—Ä—É–≥–æ–π
                            if "####" in completion:
                                rewards.append(1.0)
                            else:
                                rewards.append(0.0)
                    return rewards
                
                reward_fns.append(format_check_fn)
                logger.info(f"ü¶• Added reward: {rule_name} (format_check, weight={rule_weight})")
                
            elif rule_type == "exact_match":
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å ground_truth
                # TRL –Ω–µ –ø–µ—Ä–µ–¥–∞—ë—Ç ground_truth –Ω–∞–ø—Ä—è–º—É—é, –Ω–æ –æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ inputs
                
                def exact_match_fn(
                    completions: List[str],
                    prompts: Optional[List[str]] = None,
                    ground_truth: Optional[List[str]] = None,
                    **kwargs
                ) -> List[float]:
                    rewards = []
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å ground_truth –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                    gt_list = ground_truth or kwargs.get("ground_truths", []) or []
                    
                    for i, completion in enumerate(completions):
                        if i < len(gt_list):
                            gt = str(gt_list[i]).strip()
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –∏–∑ completion
                            if "####" in completion:
                                pred = completion.split("####")[-1].strip()
                            else:
                                # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ
                                numbers = re.findall(r'-?\d+(?:,\d{3})*(?:\.\d+)?', completion)
                                pred = numbers[-1] if numbers else ""
                            
                            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                            pred_clean = re.sub(r'[,\s]', '', pred)
                            gt_clean = re.sub(r'[,\s]', '', gt)
                            
                            if pred_clean == gt_clean:
                                rewards.append(1.0)
                            elif gt_clean in pred_clean or pred_clean in gt_clean:
                                rewards.append(0.5)
                            else:
                                rewards.append(0.0)
                        else:
                            # –ù–µ—Ç ground_truth ‚Äî –¥–∞—ë–º 0
                            rewards.append(0.0)
                    return rewards
                
                reward_fns.append(exact_match_fn)
                logger.info(f"ü¶• Added reward: {rule_name} (exact_match, weight={rule_weight})")
                
            elif rule_type == "reasoning_quality":
                # –ö–∞—á–µ—Å—Ç–≤–æ reasoning ‚Äî –¥–ª–∏–Ω–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
                
                def reasoning_quality_fn(
                    completions: List[str],
                    prompts: Optional[List[str]] = None,
                    **kwargs
                ) -> List[float]:
                    rewards = []
                    for completion in completions:
                        score = 0.0
                        
                        # –î–ª–∏–Ω–∞ (–Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è, –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è)
                        length = len(completion)
                        if 100 < length < 1500:
                            score += 0.3
                        elif 50 < length <= 100:
                            score += 0.1
                        
                        # –ï—Å—Ç—å —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
                        step_markers = ["Step", "step", "First", "Then", "Next", "Finally", "Therefore"]
                        if any(marker in completion for marker in step_markers):
                            score += 0.3
                        
                        # –ï—Å—Ç—å —á–∏—Å–ª–∞/–≤—ã—á–∏—Å–ª–µ–Ω–∏—è
                        if re.search(r'\d+\s*[+\-*/=]\s*\d+', completion):
                            score += 0.2
                        
                        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                        if "####" in completion or "</think>" in completion:
                            score += 0.2
                        
                        rewards.append(min(score, 1.0))
                    return rewards
                
                reward_fns.append(reasoning_quality_fn)
                logger.info(f"ü¶• Added reward: {rule_name} (reasoning_quality, weight={rule_weight})")
            
            else:
                logger.warning(f"ü¶• Unknown reward type: {rule_type}, skipping {rule_name}")
        
        return reward_fns
    
    if reward_rules and len(reward_rules) > 0:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ UI
        logger.info(f"ü¶• Creating reward functions from {len(reward_rules)} UI rules")
        reward_fn = create_trl_reward_fn_from_rules(reward_rules, reasoning_format)
        if not reward_fn:
            logger.warning("ü¶• No valid reward rules, using defaults")
            reward_fn = None
    
    if reward_fn is None:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ reward —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è GSM8K
        def default_format_fn(
            completions: List[str],
            prompts: Optional[List[str]] = None,
            **kwargs
        ) -> List[float]:
            rewards = []
            for completion in completions:
                if "####" in completion:
                    after = completion.split("####")[-1].strip()
                    if re.search(r'-?\d+', after):
                        rewards.append(1.0)
                    else:
                        rewards.append(0.3)
                else:
                    rewards.append(0.0)
            return rewards
        
        def default_length_fn(
            completions: List[str],
            prompts: Optional[List[str]] = None,
            **kwargs
        ) -> List[float]:
            rewards = []
            for completion in completions:
                length = len(completion)
                if 100 < length < 800:
                    rewards.append(1.0)
                elif 50 < length <= 100:
                    rewards.append(0.5)
                elif length >= 800:
                    rewards.append(0.3)
                else:
                    rewards.append(0.0)
            return rewards
        
        reward_fn = [default_format_fn, default_length_fn]
        logger.info("ü¶• Using default GSM8K reward functions: format + length")
    
    # === –ü–∞—Ç—á –¥–ª—è DDP: monkeypatch –¥–ª—è unwrap model ===
    # Unsloth –∏—Å–ø–æ–ª—å–∑—É–µ—Ç model.config –Ω–∞–ø—Ä—è–º—É—é, –Ω–æ –ø—Ä–∏ DDP –º–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞
    # –°–æ–∑–¥–∞—ë–º –æ–±—ë—Ä—Ç–∫—É –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ unwrap'–∏—Ç –º–æ–¥–µ–ª—å
    if world_size > 1:
        from accelerate.utils import extract_model_from_parallel
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π getattr –¥–ª—è DDP
        original_ddp_getattr = None
        try:
            from torch.nn.parallel import DistributedDataParallel as DDP
            original_ddp_getattr = DDP.__getattr__
            
            def patched_getattr(self, name):
                if name == 'config' and hasattr(self, 'module'):
                    return getattr(self.module, 'config', None)
                return original_ddp_getattr(self, name)
            
            DDP.__getattr__ = patched_getattr
            logger.info("ü¶• Patched DDP.__getattr__ for .config access")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not patch DDP: {e}")
    
    # === Trainer ===
    trainer = GRPOTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        args=grpo_config,
        reward_funcs=reward_fn,
    )
    
    # === Callback –¥–ª—è –º–µ—Ç—Ä–∏–∫ ===
    from transformers import TrainerCallback
    
    class MetricsCallback(TrainerCallback):
        def __init__(self, metrics_logger, start_time):
            self.metrics_logger = metrics_logger
            self.start_time = start_time
        
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs is None:
                return
            
            step = state.global_step
            loss = logs.get("loss", 0.0)
            lr = logs.get("learning_rate", 0.0)
            reward = logs.get("reward", 0.0)
            
            elapsed = time.time() - self.start_time
            
            self.metrics_logger.log_step(
                step=step,
                loss=loss,
                lr=lr,
            )
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ GRPO –º–µ—Ç—Ä–∏–∫–∏
            self.metrics_logger.update(
                current_reward=reward,
                kl_divergence=logs.get("kl", 0.0),
            )
        
        def on_save(self, args, state, control, **kwargs):
            ckpt_path = str(output_dir / f"checkpoint-{state.global_step}")
            self.metrics_logger.log_checkpoint(ckpt_path)
    
    trainer.add_callback(MetricsCallback(metrics_logger, time.time()))
    
    # === –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ ===
    logger.info("ü¶• Unsloth: Starting GRPO training...")
    start_time = time.time()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º unsloth train –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    try:
        from unsloth import unsloth_train
        unsloth_train(trainer)
    except ImportError:
        trainer.train()
    
    total_time = time.time() - start_time
    
    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    metrics_logger.update(status="saving_model")
    
    final_dir = output_dir / "final_model"
    final_dir.mkdir(parents=True, exist_ok=True)
    
    if use_lora:
        model.save_pretrained(final_dir)
        tokenizer.save_pretrained(final_dir)
        logger.info(f"ü¶• Saved LoRA adapters to {final_dir}")
        
        if config.get("merge_lora", False):
            merged_dir = output_dir / "merged_model"
            merged_dir.mkdir(parents=True, exist_ok=True)
            
            model = model.merge_and_unload()
            model.save_pretrained(merged_dir)
            tokenizer.save_pretrained(merged_dir)
            logger.info(f"ü¶• Saved merged model to {merged_dir}")
    else:
        model.save_pretrained(final_dir)
        tokenizer.save_pretrained(final_dir)
        logger.info(f"ü¶• Saved full model to {final_dir}")
    
    # === –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===
    hours, rem = divmod(total_time, 3600)
    minutes, seconds = divmod(rem, 60)
    duration_str = f"{int(hours):02}:{int(minutes):02}:{seconds:05.2f}"
    
    metrics_logger.update(
        status="completed",
        total_time_seconds=total_time,
        training_duration=duration_str,
        final_model_path=str(final_dir),
        backend="unsloth",
    )
    
    logger.info(f"ü¶• Unsloth GRPO completed in {duration_str}")
