"""
Motels at Home Training Studio ‚Äî –í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π
======================================================================

–ó–∞–ø—É—Å–∫:
    streamlit run homellm/app/main.py
    
–∏–ª–∏:
    ./scripts/run_studio.sh
"""

import streamlit as st
import logging
import subprocess
import json
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

import os
import signal
from pathlib import Path
from datetime import datetime
from typing import Tuple
from contextlib import suppress
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import torch
from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names, load_dataset_builder  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
try:
    from .docs import render_docs
except ImportError:
    from docs import render_docs

# –ü—É—Ç–∏
PROJECT_ROOT = Path(__file__).parent.parent.parent
CONFIGS_DIR = PROJECT_ROOT / "configs"
DATASET_DIR = PROJECT_ROOT / "datasets"  # datasets —Å "s"!
MODELS_DIR = PROJECT_ROOT / "models"  # –°–∫–∞—á–∞–Ω–Ω—ã–µ HF –º–æ–¥–µ–ª–∏
OUTPUT_DIR = PROJECT_ROOT / "out"
RUNS_DIR = PROJECT_ROOT / ".runs"
RUNS_DIR.mkdir(exist_ok=True)
MODELS_DIR.mkdir(exist_ok=True)

# ============================================================================
# Page Config
# ============================================================================

st.set_page_config(
    page_title="HomeLLM Training Studio",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS ‚Äî —á–∏—Å—Ç–∞—è —Ç—ë–º–Ω–∞—è —Ç–µ–º–∞ —Å —Ö–æ—Ä–æ—à–∏–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–º
st.markdown("""
<style>
    /* –ó–∞–≥–æ–ª–æ–≤–æ–∫ */
    .main-header {
        color: #ff6b6b;
        font-size: 2.5rem;
        font-weight: 800;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .sub-header {
        color: #888888;
        text-align: center;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    
    /* –°—Ç–∞—Ç—É—Å */
    .status-running {
        color: #22c55e !important;
        font-weight: bold;
    }
    
    .status-completed {
        color: #3b82f6 !important;
        font-weight: bold;
    }
    
    .status-error {
        color: #ef4444 !important;
        font-weight: bold;
    }
    
    /* ASCII art –±–ª–æ–∫ */
    .model-ascii {
        background: #1e1e1e;
        border: 1px solid #333;
        border-radius: 8px;
        padding: 1rem;
        font-family: 'Courier New', monospace;
        color: #00ff88;
        white-space: pre;
        overflow-x: auto;
    }
    
    /* –ö–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ç—Ä–∏–∫ */
    div[data-testid="metric-container"] {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 0.5rem;
    }
    
    /* –ö–Ω–æ–ø–∫–∏ –∑–∞–ø—É—Å–∫–∞ */
    .stButton > button[kind="primary"] {
        background: linear-gradient(90deg, #e94560, #ff6b6b);
        color: white !important;
        border: none;
        font-weight: 600;
    }
    
    /* Code –±–ª–æ–∫–∏ */
    pre {
        background: #0d1117 !important;
        color: #c9d1d9 !important;
        border: 1px solid #30363d !important;
    }
    
    /* Inline code - –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–π —Å—Ç–∏–ª—å —Å —á—ë—Ç–∫–æ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç—å—é */
    code {
        color: #1a1a2e !important;
        background-color: #e8f4f8 !important;
        padding: 2px 8px !important;
        border-radius: 4px !important;
        font-weight: 600 !important;
        border: 1px solid #b8d4e3 !important;
        font-size: 0.9em !important;
    }
    
    /* Code –≤–Ω—É—Ç—Ä–∏ pre –±–ª–æ–∫–æ–≤ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å—Ç–∏–ª—å –¥–ª—è code blocks */
    pre code {
        color: #c9d1d9 !important;
        background-color: transparent !important;
        padding: 0 !important;
        font-weight: normal !important;
        border: none !important;
        font-size: inherit !important;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# Persistence ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞–º–∏
# ============================================================================

ACTIVE_RUN_FILE = RUNS_DIR / "active_run.json"


def save_active_run(run_id: str, config: dict = None):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –≤ —Ñ–∞–π–ª."""
    data = {
        "run_id": run_id,
        "started_at": datetime.now().isoformat(),
        "config": config or {}
    }
    with open(ACTIVE_RUN_FILE, "w") as f:
        json.dump(data, f, indent=2)


def load_active_run() -> dict | None:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –∏–∑ —Ñ–∞–π–ª–∞."""
    if not ACTIVE_RUN_FILE.exists():
        return None
    try:
        with open(ACTIVE_RUN_FILE) as f:
            return json.load(f)
    except:
        return None


def clear_active_run():
    """–û—á–∏—Å—Ç–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run."""
    ACTIVE_RUN_FILE.unlink(missing_ok=True)


def restore_session_state():
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π run
    active = load_active_run()
    if active and active.get("run_id"):
        run_id = active["run_id"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ run –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        run_dir = RUNS_DIR / run_id
        if run_dir.exists():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ metrics.json)
            pid_path = run_dir / "pid"
            metrics_path = run_dir / "metrics.json"
            process_alive = False
            metrics = None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∏–∑ metrics.json (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
            if metrics_path.exists():
                try:
                    import time
                    metrics_mtime = metrics_path.stat().st_mtime
                    metrics_age_minutes = (time.time() - metrics_mtime) / 60
                    
                    with open(metrics_path) as f:
                        metrics = json.load(f)
                    
                    status = metrics.get("status", "")
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–π - —Ç–æ—á–Ω–æ –Ω–µ –∂–∏–≤
                    if status in ("completed", "error", "stopped"):
                        process_alive = False
                    else:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å —á–µ—Ä–µ–∑ PID
                        if pid_path.exists():
                            try:
                                with open(pid_path) as f:
                                    pid = int(f.read().strip())
                                os.kill(pid, 0)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
                                process_alive = True
                            except ProcessLookupError:
                                process_alive = False
                            except (ValueError, PermissionError):
                                # PermissionError –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤
                                # –ù–æ –µ—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ —Å–≤–µ–∂–∏–µ (< 5 –º–∏–Ω—É—Ç) - —Å—á–∏—Ç–∞–µ–º –∂–∏–≤—ã–º
                                process_alive = metrics_age_minutes < 5
                    
                    # –ï—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å –¥–∞–≤–Ω–æ ‚Äî –ù–ï —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –º—ë—Ä—Ç–≤—ã–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
                    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å PID; PermissionError —Ç—Ä–∞–∫—Ç—É–µ–º –∫–∞–∫ "–∂–∏–≤".
                    if status not in ("completed", "error", "stopped") and metrics_age_minutes > 5:
                        pid_alive = False
                        if pid_path.exists():
                            try:
                                with open(pid_path) as f:
                                    pid = int(f.read().strip())
                                os.kill(pid, 0)
                                pid_alive = True
                            except PermissionError:
                                pid_alive = True
                            except (ProcessLookupError, ValueError, FileNotFoundError):
                                pid_alive = False
                        if pid_alive:
                            process_alive = True
                            logger.warning(
                                f"Metrics not updated for {metrics_age_minutes:.1f} minutes, but PID looks alive. "
                                f"Treating process as running (metrics may be stalled)."
                            )
                        else:
                            process_alive = False
                            logger.warning(f"Metrics not updated for {metrics_age_minutes:.1f} minutes and PID not alive, assuming process dead")
                            clear_active_run()
                except Exception as e:
                    logger.warning(f"Failed to check metrics: {e}")
                    # Fallback –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É PID
                    if pid_path.exists():
                        try:
                            with open(pid_path) as f:
                                pid = int(f.read().strip())
                            os.kill(pid, 0)
                            process_alive = True
                        except (ProcessLookupError, ValueError, PermissionError):
                            process_alive = False
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            st.session_state.current_run_id = run_id
            st.session_state.training_active = process_alive
            
            # –ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à—ë–Ω, –æ—á–∏—â–∞–µ–º active_run
            if not process_alive:
                if metrics and metrics.get("status") in ("completed", "error", "stopped"):
                    clear_active_run()


# ============================================================================
# Session State
# ============================================================================

if "training_process" not in st.session_state:
    st.session_state.training_process = None
if "current_run_id" not in st.session_state:
    st.session_state.current_run_id = None
if "training_active" not in st.session_state:
    st.session_state.training_active = False
if "selected_chat_model" not in st.session_state:
    st.session_state.selected_chat_model = None

# –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
if "session_restored" not in st.session_state:
    restore_session_state()
    st.session_state.session_restored = True


# ============================================================================
# Helper Functions
# ============================================================================

def get_available_datasets():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."""
    datasets = []
    if DATASET_DIR.exists():
        for f in DATASET_DIR.glob("*.jsonl"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        # –í–ê–ñ–ù–û: –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º .json, —Ç.–∫. —ç—Ç–æ –æ–±—ã—á–Ω–æ –º–∞—Å—Å–∏–≤, –∞ –Ω–µ JSONL (–ø–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        # –¥–ª—è f in DATASET_DIR.glob("*.json"):
        #     size_mb = f.stat().st_size / (1024 * 1024)
        #     datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt.gz"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.jsonl.gz"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
    return datasets


def get_gpu_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU."""
    gpus = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            gpus.append({
                "id": i,
                "name": props.name,
                "memory_gb": round(memory_gb, 1),
                "compute_capability": f"{props.major}.{props.minor}",
            })
    return gpus


def get_available_configs():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö accelerate –∫–æ–Ω—Ñ–∏–≥–æ–≤."""
    configs = []
    if CONFIGS_DIR.exists():
        for f in CONFIGS_DIR.glob("*.yaml"):
            name = f.stem.replace("accelerate_", "").replace("_", " ").title()
            configs.append({
                "file": f.name,
                "name": name,
                "path": str(f),
            })
    return configs


# –û–ø–∏—Å–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
PARALLEL_TYPES = {
    "default": {
        "name": "Single GPU / CPU",
        "type": "None",
        "description": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞",
        "icon": "üñ•Ô∏è",
    },
    "multi_gpu": {
        "name": "Multi-GPU (DDP)",
        "type": "Data Parallel",
        "description": "Distributed Data Parallel ‚Äî –∫–∞–∂–¥–∞—è GPU –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏ –∏ —á–∞—Å—Ç—å –±–∞—Ç—á–∞",
        "icon": "üîÑ",
    },
    "fsdp": {
        "name": "FSDP",
        "type": "Model Parallel",
        "description": "Fully Sharded Data Parallel (PyTorch native). Liger fused CE —Ä–∞–±–æ—Ç–∞–µ—Ç!",
        "icon": "‚ö°",
    },
    "fsdp_offload": {
        "name": "FSDP + CPU Offload",
        "type": "Model Parallel + CPU Offload",
        "description": "FSDP + –≤—ã–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ CPU. –≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ Liger fused CE –æ—Ç–∫–ª—é—á—ë–Ω.",
        "icon": "üíæ",
    },
    "fsdp2": {
        "name": "FSDP2 + CPU Offload",
        "type": "Model Parallel + CPU Offload",
        "description": "FSDP v2 —Å DTensor + CPU. –≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ Liger fused CE –æ—Ç–∫–ª—é—á—ë–Ω.",
        "icon": "üî•",
    },
    "deepspeed_zero2": {
        "name": "DeepSpeed ZeRO-2",
        "type": "Data Parallel + Optimizer Parallel",
        "description": "–®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É GPU",
        "icon": "üöÄ",
    },
    "deepspeed_zero3": {
        "name": "DeepSpeed ZeRO-3",
        "type": "Full Model Parallel",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å + –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä + –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã",
        "icon": "üí™",
    },
    "deepspeed_zero3_offload": {
        "name": "ZeRO-3 + CPU Offload",
        "type": "Model Parallel + CPU Offload",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ + –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM",
        "icon": "üßä",
    },
}


def estimate_parameters(
    hidden_size: int,
    num_layers: int,
    vocab_size: int = 50257,
    intermediate_size: int | None = None,
) -> int:
    """
    –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞—à–µ–π `HomeModel` (—Å–º. `homellm/models/home_model.py`).

    –í–ê–ñ–ù–û:
    - –£ –Ω–∞—Å RoPE (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ù–ï –æ–±—É—á–∞–µ–º—ã–µ) => `seq_len` –Ω–∞ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –≤–ª–∏—è–µ—Ç.
    - lm_head weight tied –∫ embed_tokens => –Ω–µ —É–¥–≤–∞–∏–≤–∞–µ–º vocab*hidden.
    """
    h = int(hidden_size)
    l = int(num_layers)
    v = int(vocab_size)
    i = int(intermediate_size) if intermediate_size is not None else int(h * 4)

    # Embedding
    embed = v * h

    # Per-layer:
    # - Attention: q/k/v/out, bias=False => 4 * (H*H)
    attn = 4 * h * h
    # - SwiGLU MLP: w1(H->I), w2(I->H), w3(H->I), bias=False => 3 * (H*I)
    mlp = 3 * h * i
    # - RMSNorm weights: 2 * H
    norms = 2 * h

    # Final norm
    final_norm = h

    return int(embed + l * (attn + mlp + norms) + final_norm)


def format_params(n: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    if n >= 1e9:
        return f"{n/1e9:.2f}B"
    elif n >= 1e6:
        return f"{n/1e6:.1f}M"
    elif n >= 1e3:
        return f"{n/1e3:.1f}K"
    return str(n)


def format_time(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏."""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        return f"{seconds/3600:.1f}h"


def load_metrics(run_id: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    run_dir = RUNS_DIR / run_id
    run_config = {}
    
    # –î–ª—è GRPO —á–∏—Ç–∞–µ–º –∏–∑ metrics.jsonl (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ output_dir –∏–ª–∏ run_dir)
    metrics_jsonl_paths = []
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ metrics.jsonl –≤ output_dir
    config_path = run_dir / "config.json"
    if config_path.exists():
        try:
            with open(config_path) as f:
                run_config = json.load(f) or {}
                output_dir = run_config.get("output_dir", "")
                if output_dir:
                    metrics_jsonl_paths.append(Path(output_dir) / "metrics.jsonl")
        except:
            pass
    
    # Fallback: –∏—â–µ–º –≤ run_dir
    metrics_jsonl_paths.append(run_dir / "metrics.jsonl")
    
    for metrics_jsonl_path in metrics_jsonl_paths:
        if metrics_jsonl_path.exists():
            try:
                import pandas as pd
                # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ JSONL
                lines = []
                with open(metrics_jsonl_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            try:
                                lines.append(json.loads(line))
                            except json.JSONDecodeError:
                                continue
                
                if lines:
                    df = pd.DataFrame(lines)
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –∫–∞–∫ —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                    latest = df.iloc[-1].to_dict()
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
                    latest["reward_history"] = df["reward"].tolist() if "reward" in df.columns else []
                    latest["loss_history"] = df["loss"].tolist() if "loss" in df.columns else []
                    latest["kl_history"] = df["kl"].tolist() if "kl" in df.columns else []
                    latest["steps_history"] = df["step"].tolist() if "step" in df.columns else list(range(len(df)))
                    latest["lr_history"] = df["learning_rate"].tolist() if "learning_rate" in df.columns else (df["lr"].tolist() if "lr" in df.columns else [])
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫
                    # –î–ª—è GRPO –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ UI —Å—á–∏—Ç–∞–µ–º –ø–æ rollout_step (–ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞), –∞ optim_step –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ.
                    latest["current_step"] = latest.get("current_step", latest.get("rollout_step", latest.get("step", len(lines) - 1)))
                    latest["current_loss"] = latest.get("loss", 0)
                    latest["current_lr"] = latest.get("learning_rate", latest.get("lr", 0))
                    latest["reward"] = latest.get("reward", latest.get("batch_reward_mean", 0))
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–ø–ª—ã –µ—Å–ª–∏ –µ—Å—Ç—å
                    if "samples" in df.columns:
                        latest["samples_history"] = df["samples"].tolist()
                    
                    # –°—Ç–∞—Ç—É—Å –º–æ–∂–µ—Ç –±—ã—Ç—å NaN –∏–∑ pandas, –Ω—É–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Å—Ç—Ä–æ–∫–µ
                    raw_status = latest.get("status")
                    import math
                    if raw_status is None or (isinstance(raw_status, float) and math.isnan(raw_status)):
                        latest["status"] = "training"
                    else:
                        latest["status"] = str(raw_status)
                    latest["stage"] = "grpo"

                    # –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å—á—ë—Ç—á–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ jsonl)
                    try:
                        if "prompts_generated_total" in df.columns:
                            latest["prompts_generated_total"] = int(df["prompts_generated_total"].fillna(0).iloc[-1])
                        elif "prompts_generated" in df.columns:
                            latest["prompts_generated_total"] = int(df["prompts_generated"].fillna(0).sum())
                        if "prompts_used_total" in df.columns:
                            latest["prompts_used_total"] = int(df["prompts_used_total"].fillna(0).iloc[-1])
                        elif "prompts_used" in df.columns:
                            latest["prompts_used_total"] = int(df["prompts_used"].fillna(0).sum())
                        if "completions_generated_total" in df.columns:
                            latest["completions_generated_total"] = int(df["completions_generated_total"].fillna(0).iloc[-1])
                        elif "completions_generated" in df.columns:
                            latest["completions_generated_total"] = int(df["completions_generated"].fillna(0).sum())
                        if "experiences_tuned_total" in df.columns:
                            latest["experiences_tuned_total"] = int(df["experiences_tuned_total"].fillna(0).iloc[-1])
                        elif "experiences_tuned" in df.columns:
                            latest["experiences_tuned_total"] = int(df["experiences_tuned"].fillna(0).sum())
                    except Exception:
                        pass

                    # --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞/ETA –¥–ª—è GRPO ---
                    # total_steps: –±–µ—Ä—ë–º –∏–∑ –º–µ—Ç—Ä–∏–∫ (–µ—Å–ª–∏ –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è) –∏–ª–∏ –∏–∑ run_config (config.json)
                    total_steps = latest.get("total_steps", None)
                    if total_steps in (None, "", 0):
                        # legacy: optim-step –ª–∏–º–∏—Ç
                        total_steps = run_config.get("grpo_max_optim_steps", run_config.get("grpo_max_steps", run_config.get("max_steps", None)))
                    try:
                        total_steps_int = int(total_steps) if total_steps is not None else None
                    except Exception:
                        total_steps_int = None
                    # –ï—Å–ª–∏ –ª–∏–º–∏—Ç–∞ –Ω–µ—Ç ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º None (UI –ø–æ–∫–∞–∂–µ—Ç "–±–µ–∑ –ª–∏–º–∏—Ç–∞"), –∞ –Ω–µ "1".
                    latest["total_steps"] = total_steps_int if (total_steps_int is not None and total_steps_int > 0) else None

                    # elapsed/eta: –ø–æ timestamp, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    elapsed_seconds = 0.0
                    eta_seconds = 0.0
                    try:
                        if "timestamp" in df.columns:
                            t0 = pd.to_datetime(df["timestamp"].iloc[0], errors="coerce")
                            t1 = pd.to_datetime(df["timestamp"].iloc[-1], errors="coerce")
                            if pd.notna(t0) and pd.notna(t1):
                                # –ï—Å–ª–∏ –ø–æ–∫–∞ –≤—Å–µ–≥–æ 1 –∑–∞–ø–∏—Å—å, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º elapsed –∫–∞–∫ now - t0, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ "0s" –≤ UI.
                                if len(df) == 1:
                                    elapsed_seconds = max(0.0, (pd.Timestamp.now(tz=t0.tz) - t0).total_seconds())
                                else:
                                    elapsed_seconds = max(0.0, (t1 - t0).total_seconds())

                        # ETA —Å—á–∏—Ç–∞–µ–º –ø–æ current_step (rollout_step), –∞ –Ω–µ –ø–æ optim_step,
                        # –∏–Ω–∞—á–µ –ø—Ä–æ–≥—Ä–µ—Å—Å/ETA –±—É–¥—É—Ç "—É–±–µ–≥–∞—Ç—å" –∏–∑-–∑–∞ multiple optimizer updates per rollout.
                        if "timestamp" in df.columns and "current_step" in df.columns and len(df) >= 2:
                            s0 = float(df["current_step"].iloc[0])
                            s1 = float(df["current_step"].iloc[-1])
                            # —Å—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–º step (—É—á–∏—Ç—ã–≤–∞–µ–º —á—Ç–æ –ª–æ–≥ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ)
                            ds = max(0.0, s1 - s0)
                            if ds > 0 and elapsed_seconds > 0 and latest["total_steps"] is not None:
                                sec_per_step = elapsed_seconds / ds
                                remaining = max(0.0, float(latest["total_steps"]) - float(latest.get("current_step", s1)))
                                eta_seconds = sec_per_step * remaining
                    except Exception:
                        pass
                    latest["elapsed_seconds"] = float(elapsed_seconds)
                    latest["eta_seconds"] = float(eta_seconds)

                    # rollout_step —Ç–æ–∂–µ –ø—Ä–æ–∫–∏–Ω–µ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    if "rollout_step" in latest:
                        latest["current_rollout_step"] = latest.get("rollout_step", 0)

                    return latest
            except Exception as e:
                # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—É—Ç—å
                if metrics_jsonl_path == metrics_jsonl_paths[-1]:
                    pass  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—É—Ç–∏
    
    # –û–±—ã—á–Ω—ã–π –ø—É—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç–∞–¥–∏–π
    metrics_path = run_dir / "metrics.json"
    if metrics_path.exists():
        try:
            with open(metrics_path) as f:
                return json.load(f)
        except:
            pass
    return None


def _close_run_log_files(run_id: str):
    """–ó–∞–∫—Ä—ã—Ç—å —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã stdout/stderr, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ session_state."""
    for k in (f"stdout_file_{run_id}", f"stderr_file_{run_id}"):
        f = st.session_state.get(k)
        if f:
            with suppress(Exception):
                f.close()
            with suppress(Exception):
                del st.session_state[k]


def start_training(config: dict) -> tuple[str, subprocess.Popen]:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤ —Ñ–æ–Ω–µ."""
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –Ø–≤–Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ UI-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ ‚Äú—Ç–∏—Ö–∏—Ö‚Äù –ø–µ—Ä–µ—Ç–∏—Ä–∞–Ω–∏–π –ø—Ä–µ—Å–µ—Ç–∞–º–∏
    try:
        logger.info(
            "Start training with: stage=%s mixed_precision=%s fp16_pure=%s grad_checkpoint=%s use_flash_attention=%s",
            config.get("stage"),
            config.get("mixed_precision"),
            config.get("fp16_pure"),
            config.get("grad_checkpoint"),
            config.get("use_flash_attention"),
        )
    except Exception:
        pass
    
    # –õ–û–ì–ò–ö–ê –ü–£–¢–ï–ô
    # config["output_dir"] - —ç—Ç–æ –∫–æ—Ä–µ–Ω—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä out/my_model)
    # –ú—ã —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ out/my_model/run_2023.../checkpoint_...
    experiment_root = Path(PROJECT_ROOT) / config.get("output_dir", "out/default")
    run_output_dir = experiment_root / run_id
    
    # –û–±–Ω–æ–≤–ª—è–µ–º output_dir –≤ –∫–æ–Ω—Ñ–∏–≥–µ, —á—Ç–æ–±—ã worker —Å–æ—Ö—Ä–∞–Ω—è–ª —Ç—É–¥–∞
    config["output_dir"] = str(run_output_dir)
    
    # –ü–∞–ø–∫–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞ (–ª–æ–≥–∏, –º–µ—Ç—Ä–∏–∫–∏)
    # –ú–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö —Ç–∞–º –∂–µ, –≥–¥–µ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã, –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    run_dir = RUNS_DIR / run_id # –û—Å—Ç–∞–≤–ª—è–µ–º .runs –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ª–æ–≥–æ–≤ —Å—Ç—Ä–∏–º–ª–∏—Ç–∞
    run_dir.mkdir(parents=True, exist_ok=True)
    run_output_dir.mkdir(parents=True, exist_ok=True) # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤–µ—Å–æ–≤
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    # –°–æ–∑–¥–∞—ë–º –Ω–∞—á–∞–ª—å–Ω—ã–π metrics —Ñ–∞–π–ª
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0}, f)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ distributed
    distributed_mode = config.get("distributed_mode", "default")
    config_file = config.get("config_file")
    num_gpus = config.get("num_gpus", 1)
    
    if distributed_mode != "default" and config_file:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerate launch —Å –∫–æ–Ω—Ñ–∏–≥–æ–º
        # –í–ê–ñ–ù–û: gradient_accumulation_steps –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ config.json, –Ω–µ —á–µ—Ä–µ–∑ CLI —Ñ–ª–∞–≥–∏
        cmd = [
            "accelerate", "launch",
            "--config_file", config_file,
            "--num_processes", str(num_gpus),
            "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫
        cmd = [
            "python", "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        f.write(" ".join(cmd))
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    # –í–ê–ñ–ù–û: –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä GPU –∏–∑ UI
    env = os.environ.copy()
    gpu_ids = config.get("gpu_ids") or []
    if gpu_ids:
        env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,  # –û—Ç–¥–µ–ª—è–µ–º –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        env=env,
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
    st.session_state[f"stdout_file_{run_id}"] = stdout_file
    st.session_state[f"stderr_file_{run_id}"] = stderr_file
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PID –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def stop_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É."""
    stopped = False
    
    # –ü—Ä–æ–±—É–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ PID –∏–∑ —Ñ–∞–π–ª–∞ (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
    if st.session_state.current_run_id:
        pid_path = RUNS_DIR / st.session_state.current_run_id / "pid"
        if pid_path.exists():
            try:
                with open(pid_path) as f:
                    pid = int(f.read().strip())
                # –£–±–∏–≤–∞–µ–º process group (–≤–∞–∂–Ω–æ –¥–ª—è accelerate/DDP)
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è —É–±–∏—Ç—å process group
                    os.killpg(os.getpgid(pid), signal.SIGTERM)
                    stopped = True
                except (ProcessLookupError, OSError):
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å (–ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –≤ –≥—Ä—É–ø–ø–µ –∏–ª–∏ —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è), –ø—Ä–æ–±—É–µ–º –ø–æ PID
                    try:
                        os.kill(pid, signal.SIGTERM)
                        stopped = True
                    except ProcessLookupError:
                        pass
                
                # –ñ–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
                time.sleep(0.5)
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
                    os.kill(pid, 0)
                    # –ï—Å–ª–∏ –∂–∏–≤, —É–±–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ (process group)
                    try:
                        os.killpg(os.getpgid(pid), signal.SIGKILL)
                    except (ProcessLookupError, OSError):
                        os.kill(pid, signal.SIGKILL)
                except ProcessLookupError:
                    pass  # –ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
            except Exception as e:
                pass
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = RUNS_DIR / st.session_state.current_run_id / "metrics.json"
        if metrics_path.exists():
            try:
                with open(metrics_path) as f:
                    metrics = json.load(f)
                metrics["status"] = "stopped"
                with open(metrics_path, "w") as f:
                    json.dump(metrics, f, indent=2)
            except:
                pass
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ subprocess
    if st.session_state.training_process:
        try:
            st.session_state.training_process.terminate()
            st.session_state.training_process.wait(timeout=2)
        except:
            try:
                st.session_state.training_process.kill()
            except:
                pass
        st.session_state.training_process = None
    
    st.session_state.training_active = False
    
    # –û—á–∏—â–∞–µ–º active_run
    clear_active_run()
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –ª–æ–≥–æ–≤
    if st.session_state.current_run_id:
        _close_run_log_files(st.session_state.current_run_id)
    
    return stopped


def start_grpo_training(config: dict) -> tuple[str, subprocess.Popen]:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å GRPO –æ–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–µ."""
    training_backend = config.get("training_backend", "models-at-home")
    run_id = datetime.now().strftime("grpo_%Y%m%d_%H%M%S")
    
    # –õ–æ–≥–∏—Ä—É–µ–º backend –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    logger.info(f"üß† GRPO Training backend: {training_backend}")
    
    # –ü–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    experiment_root = Path(PROJECT_ROOT) / config.get("output_dir", "out/grpo")
    run_output_dir = experiment_root / run_id
    
    config["output_dir"] = str(run_output_dir)
    
    run_dir = RUNS_DIR / run_id
    run_dir.mkdir(parents=True, exist_ok=True)
    run_output_dir.mkdir(parents=True, exist_ok=True)

    # –î–ª—è "–∂–µ–ª–µ–∑–Ω–æ–≥–æ" –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º –ø—É—Ç—å –¥–æ run_dir –≤ worker.
    # Worker –±—É–¥–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å metrics.jsonl/samples.jsonl –≤ —ç—Ç—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.
    config["ui_run_dir"] = str(run_dir)
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2, default=str)
    
    # –ù–∞—á–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0, "stage": "grpo"}, f)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ GRPO
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º --config_json –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤—Å–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—è reward_rules
    import sys
    
    # –ü–µ—Ä–µ–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥ –∫–∞–∫ JSON —Å—Ç—Ä–æ–∫—É
    config_json = json.dumps(config, default=str)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º distributed (–∫–∞–∫ –≤ start_training)
    distributed_mode = config.get("distributed_mode", "default")
    config_file = config.get("config_file")
    num_gpus = config.get("num_gpus", 1)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ distributed
    if distributed_mode != "default" and config_file:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerate launch —Å –∫–æ–Ω—Ñ–∏–≥–æ–º (–∫–∞–∫ –≤ pretrain/SFT)
        cmd = [
            "accelerate", "launch",
            "--config_file", config_file,
            "--num_processes", str(num_gpus),
            "-m", "homellm.training.rl.train_gsm8k",
            "--config_json", config_json,
        ]
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ (single GPU –∏–ª–∏ CPU)
        cmd = [
            sys.executable, "-m", "homellm.training.rl.train_gsm8k",
            "--config_json", config_json,
        ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–±–µ–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ JSON)
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        if distributed_mode != "default" and config_file:
            f.write(f"accelerate launch --config_file {config_file} --num_processes {num_gpus} -m homellm.training.rl.train_gsm8k --config_json <config from {config_path}>")
        else:
            f.write(f"{sys.executable} -m homellm.training.rl.train_gsm8k --config_json <config from {config_path}>")
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    # –í–ê–ñ–ù–û: –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä GPU –∏–∑ UI (–∫–∞–∫ –≤ start_training)
    env = os.environ.copy()
    env["PYTHONUNBUFFERED"] = "1"
    gpu_ids = list(config.get("gpu_ids") or [])
    
    # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è vLLM –Ω–∞ –¥—Ä—É–≥–æ–π GPU ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –≤ CUDA_VISIBLE_DEVICES
    vllm_device = config.get("grpo_vllm_device", "")
    use_rollout_engine = config.get("grpo_use_rollout_engine", False)
    rollout_backend = config.get("grpo_rollout_backend", "hf")
    
    if use_rollout_engine and rollout_backend == "vllm" and vllm_device.startswith("cuda:"):
        vllm_gpu_id = int(vllm_device.split(":")[1])
        if vllm_gpu_id not in gpu_ids:
            gpu_ids.append(vllm_gpu_id)
            logger.info(f"üß© –î–æ–±–∞–≤–ª–µ–Ω–∞ GPU {vllm_gpu_id} –¥–ª—è vLLM rollout engine")
    
    if gpu_ids:
        # –ù–ï —Å–æ—Ä—Ç–∏—Ä—É–µ–º! –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω –¥–ª—è —Ä–µ–º–∞–ø–ø–∏–Ω–≥–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
        # –ü–µ—Ä–≤—ã–µ GPU ‚Äî –¥–ª—è training, –ø–æ—Å–ª–µ–¥–Ω—è—è (–µ—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∞) ‚Äî –¥–ª—è vLLM
        gpu_ids = list(dict.fromkeys(gpu_ids))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
        logger.info(f"üéØ CUDA_VISIBLE_DEVICES={env['CUDA_VISIBLE_DEVICES']}")
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,
        env=env,
    )
    
    st.session_state[f"stdout_file_{run_id}"] = stdout_file
    st.session_state[f"stderr_file_{run_id}"] = stderr_file
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PID
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def is_process_running(run_id: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–ø—É—â–µ–Ω –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å."""
    pid_path = RUNS_DIR / run_id / "pid"
    if not pid_path.exists():
        return False
    
    try:
        with open(pid_path) as f:
            pid = int(f.read().strip())
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
        os.kill(pid, 0)
        return True
    except PermissionError:
        # –ü—Ä–æ—Ü–µ—Å—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—É—â–µ–Ω –æ—Ç –¥—Ä—É–≥–æ–≥–æ —é–∑–µ—Ä–∞)
        # –°—á–∏—Ç–∞–µ–º, —á—Ç–æ –æ–Ω –∂–∏–≤
        return True
    except (ProcessLookupError, ValueError, FileNotFoundError):
        return False


# ============================================================================
# UI Components
# ============================================================================

def render_header():
    st.markdown("# üè† Models at Home Training Studio")
    st.caption("–í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–º–∞")


def get_nested_value(data: dict, path: str):
    """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - 'key1.key2' - –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏
    - 'messages [—Å–ø–∏—Å–æ–∫ –∏–∑ N —ç–ª.]' - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å—å —Å–ø–∏—Å–æ–∫
    - 'messages[].content' - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª—è –∏–∑ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    - 'messages[0]' - –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞
    """
    if not path: return None
    
    # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã —Ç–∏–ø–∞ " [—Å–ø–∏—Å–æ–∫]" –∏–ª–∏ " [—Å–ø–∏—Å–æ–∫ –∏–∑ 3 —ç–ª.]"
    import re
    path = re.sub(r' \[—Å–ø–∏—Å–æ–∫.*?\]$', '', path)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π —Ç–∏–ø–∞ 'messages[].content' (–≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞)
    if "[]." in path:
        parts = path.split("[].", 1)
        list_path = parts[0]
        remaining_path = parts[1]
        
        list_val = get_nested_value(data, list_path)
        if isinstance(list_val, list):
            results = []
            for item in list_val:
                if isinstance(item, dict):
                    val = get_nested_value(item, remaining_path)
                    results.append(val)
            return results
        return None
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π —Ç–∏–ø–∞ 'messages[0]' (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å)
    if "[" in path and "]" in path:
        # –ü–∞—Ä—Å–∏–º –∏–Ω–¥–µ–∫—Å
        match = re.search(r'\[(\d+)\]', path)
        if match:
            idx = int(match.group(1))
            base_path = path[:match.start()]
            after_path = path[match.end():]
            if after_path.startswith('.'):
                after_path = after_path[1:]
            
            base_val = get_nested_value(data, base_path)
            if isinstance(base_val, list) and 0 <= idx < len(base_val):
                if after_path:
                    return get_nested_value(base_val[idx], after_path)
                return base_val[idx]
            return None
    
    # –û–±—ã—á–Ω—ã–π –ø—É—Ç—å —á–µ—Ä–µ–∑ —Ç–æ—á–∫–∏
    keys = path.split('.')
    curr = data
    try:
        for k in keys:
            if isinstance(curr, dict):
                curr = curr.get(k)
            elif isinstance(curr, list) and k.isdigit():
                curr = curr[int(k)]
            else:
                return None
            if curr is None: return None
        return curr
    except:
        return None


def get_dataset_columns(file_path: str):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ (–≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ) –∏ –ø—Ä–∏–º–µ—Ä."""
    path = Path(file_path)
    if not path.exists():
        return [], {}
        
    try:
        sample_data = {}
        if path.suffix == ".jsonl":
            with open(path, "r", encoding="utf-8") as f:
                line = f.readline()
                if line:
                    sample_data = json.loads(line)
        elif path.suffix == ".json":
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list) and len(data) > 0:
                    sample_data = data[0]
                elif isinstance(data, dict):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º –∫–ª—é—á–∏
                    return list(data.keys()), {k: v[0] if isinstance(v, list) else v for k, v in data.items()}
        elif path.suffix == ".csv":
            import csv
            with open(path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                row = next(reader, None)
                if row:
                    sample_data = row
        
        if sample_data:
            # –î–ª—è get_dataset_columns –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–ª–æ—Å–∫–∏–µ –∫–ª—é—á–∏, 
            # —Ç–∞–∫ –∫–∞–∫ render_sft_main_config —Ç–µ–ø–µ—Ä—å —Å–∞–º —Å—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤–æ.
            # –ù–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º –≤–æ–∑–≤—Ä–∞—Ç sample_data
            return [], sample_data
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return [], {}
    
    return [], {}


def flatten_json_structure(d: dict, parent_path: str = '', depth: int = 0) -> list:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç JSON –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ –¥–µ—Ä–µ–≤–∞."""
    items = []
    for k, v in d.items():
        current_path = f"{parent_path}.{k}" if parent_path else k
        
        if isinstance(v, dict):
            # –ü–∞–ø–∫–∞
            items.append({"type": "folder", "key": k, "path": current_path, "depth": depth, "val": ""})
            items.extend(flatten_json_structure(v, current_path, depth + 1))
        elif isinstance(v, list):
            # –°–ø–∏—Å–æ–∫
            items.append({"type": "list", "key": k, "path": current_path, "depth": depth, "val": f"List[{len(v)}]"})
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            if v:
                if isinstance(v[0], dict):
                    items.extend(flatten_json_structure(v[0], current_path, depth + 1))
                else:
                    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ (—Å—Ç—Ä–æ–∫ –∏ —Ç.–¥.) - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ –ª–∏—Å—Ç
                    # –ù–æ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç, –ø—Ä–æ—Å—Ç–æ –¥–∞–µ–º –ø–æ–Ω—è—Ç—å —á—Ç–æ –≤–Ω—É—Ç—Ä–∏
                    # –î–ª—è SFT –º—ã –æ–±—ã—á–Ω–æ –Ω–µ –≤—ã–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ –≤–µ—Å—å —Å–ø–∏—Å–æ–∫
                    pass
        else:
            # –õ–∏—Å—Ç (–∑–Ω–∞—á–µ–Ω–∏–µ)
            items.append({"type": "leaf", "key": k, "path": current_path, "depth": depth, "val": str(v)})
    return items


def get_all_leaf_paths(data, parent_path: str = '', depth: int = 0, max_depth: int = 10) -> list:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –í–°–ï –ø—É—Ç–∏ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º, –≤–∫–ª—é—á–∞—è –≥–ª—É–±–æ–∫—É—é –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å."""
    if depth > max_depth:
        return []
    
    paths = []
    
    if isinstance(data, dict):
        for k, v in data.items():
            current_path = f"{parent_path}.{k}" if parent_path else k
            
            if isinstance(v, dict):
                # –í–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å - —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
                paths.extend(get_all_leaf_paths(v, current_path, depth + 1, max_depth))
            elif isinstance(v, list):
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º —Å–ø–∏—Å–æ–∫ –∫–∞–∫ –æ–ø—Ü–∏—é (–¥–ª—è chat-—Ñ–æ—Ä–º–∞—Ç–∞)
                paths.append(f"{current_path} [—Å–ø–∏—Å–æ–∫ –∏–∑ {len(v)} —ç–ª.]")
                # –†–∞—Å–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if v:
                    if isinstance(v[0], dict):
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª—è –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–∏—Å–∫–∞
                        for inner_k, inner_v in v[0].items():
                            inner_path = f"{current_path}[].{inner_k}"
                            if isinstance(inner_v, dict):
                                paths.extend(get_all_leaf_paths(inner_v, inner_path, depth + 2, max_depth))
                            elif isinstance(inner_v, list):
                                paths.append(f"{inner_path} [—Å–ø–∏—Å–æ–∫]")
                                if inner_v and isinstance(inner_v[0], dict):
                                    paths.extend(get_all_leaf_paths(inner_v[0], f"{inner_path}[]", depth + 3, max_depth))
                            else:
                                paths.append(inner_path)
                    else:
                        # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤
                        paths.append(f"{current_path}[0]")
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                paths.append(current_path)
    
    return paths


def render_sft_main_config(data_path: str):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä SFT ‚Äî –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç + —Ä—É—á–Ω–æ–π –≤—ã–±–æ—Ä.
    
    Args:
        data_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
    """
    st.markdown("### üõ†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SFT")
    
    columns, sample = get_dataset_columns(data_path)
    
    if not sample:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏–ª–∏ –æ–Ω –ø—É—Å—Ç.")
        return {}
    
    # ===== –ê–í–¢–û–î–ï–¢–ï–ö–¢ –§–û–†–ú–ê–¢–ê =====
    def detect_chat_field(data: dict) -> tuple:
        """–ò—â–µ—Ç –ø–æ–ª–µ —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π (chat-—Ñ–æ—Ä–º–∞—Ç)."""
        for key, value in data.items():
            if isinstance(value, list) and value and isinstance(value[0], dict):
                first_item = value[0]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –ø–æ–ª—è –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ role/content
                has_role = any(k.lower() in ['role', 'from', 'type'] for k in first_item.keys())
                has_content = any(k.lower() in ['content', 'value', 'text', 'message'] for k in first_item.keys())
                if has_role and has_content:
                    return key, value
        return None, None
    
    chat_field, chat_value = detect_chat_field(sample)
    detected_format = "chat" if chat_field else "instruct"
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—É—Ç–∏ –¥–ª—è instruct —Ä–µ–∂–∏–º–∞
    all_paths = get_all_leaf_paths(sample)
    simple_fields = [k for k in sample.keys() if not isinstance(sample[k], (dict, list))]
    
    col_json, col_config = st.columns([1, 1])
    
    # ===== –õ–ï–í–ê–Ø –ö–û–õ–û–ù–ö–ê: JSON –ø—Ä–µ–≤—å—é =====
    with col_json:
        st.markdown("#### üìÑ –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
        with st.container(height=500):
            st.json(sample, expanded=True)
    
    # ===== –ü–†–ê–í–ê–Ø –ö–û–õ–û–ù–ö–ê: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è =====
    with col_config:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç –Ω–∞—à–µ–ª
        if detected_format == "chat":
            st.success(f"üîç **–ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç:** –Ω–∞–π–¥–µ–Ω Chat-—Ñ–æ—Ä–º–∞—Ç –≤ –ø–æ–ª–µ `{chat_field}`")
        else:
            st.info("üîç **–ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç:** Instruct-—Ñ–æ—Ä–º–∞—Ç (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è)")
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Ä–µ–∂–∏–º–∞
        format_choice = st.radio(
            "–§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö:",
            ["üí¨ Chat (—Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π)", "üìù Instruct (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è)"],
            index=0 if detected_format == "chat" else 1,
            key="sft_format_choice",
            horizontal=True
        )
        
        is_chat = "Chat" in format_choice
        
        st.markdown("---")
        
        sft_columns = {}
        
        if is_chat:
            # ===== CHAT –†–ï–ñ–ò–ú =====
            st.markdown("#### üí¨ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chat-—Ñ–æ—Ä–º–∞—Ç–∞")
            
            # –í—ã–±–æ—Ä –ø–æ–ª—è —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π
            list_fields = [k for k, v in sample.items() if isinstance(v, list) and v and isinstance(v[0], dict)]
            
            if not list_fields:
                st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ–π —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π!")
                return {}
            
            messages_field = st.selectbox(
                "üìã –ü–æ–ª–µ —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏:",
                list_fields,
                index=list_fields.index(chat_field) if chat_field in list_fields else 0,
                key="sft_messages_field"
            )
            
            messages = sample[messages_field]
            first_msg = messages[0]
            inner_fields = list(first_msg.keys())
            
            st.caption(f"–ù–∞–π–¥–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π
            c1, c2 = st.columns(2)
            
            role_guess = next((f for f in inner_fields if f.lower() in ['role', 'from', 'type']), inner_fields[0])
            content_guess = next((f for f in inner_fields if f.lower() in ['content', 'value', 'text', 'message']), inner_fields[-1])
            
            role_field = c1.selectbox(
                "–ü–æ–ª–µ **—Ä–æ–ª–∏**:",
                inner_fields,
                index=inner_fields.index(role_guess) if role_guess in inner_fields else 0,
                key="sft_chat_role"
            )
            content_field = c2.selectbox(
                "–ü–æ–ª–µ **—Ç–µ–∫—Å—Ç–∞**:",
                inner_fields,
                index=inner_fields.index(content_guess) if content_guess in inner_fields else 0,
                key="sft_chat_content"
            )
            
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–æ–ª–∏
            unique_roles = sorted(set(str(m.get(role_field, "")) for m in messages))
            st.caption(f"–†–æ–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: `{', '.join(unique_roles)}`")
            
            # –ú–∞–ø–ø–∏–Ω–≥ —Ä–æ–ª–µ–π
            st.markdown("**–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–æ–ª–µ–π:**")
            c1, c2, c3 = st.columns(3)
            
            sys_guess = next((r for r in unique_roles if 'system' in r.lower()), None)
            user_guess = next((r for r in unique_roles if r.lower() in ['user', 'human']), unique_roles[0] if unique_roles else "")
            asst_guess = next((r for r in unique_roles if r.lower() in ['assistant', 'gpt', 'bot']), unique_roles[-1] if len(unique_roles) > 1 else "")
            
            role_system = c1.selectbox("‚öôÔ∏è System =", ["(–Ω–µ—Ç)"] + unique_roles,
                index=(unique_roles.index(sys_guess) + 1) if sys_guess in unique_roles else 0, key="sft_map_sys")
            role_user = c2.selectbox("üë§ User =", unique_roles,
                index=unique_roles.index(user_guess) if user_guess in unique_roles else 0, key="sft_map_user")
            role_assistant = c3.selectbox("ü§ñ Assistant =", unique_roles,
                index=unique_roles.index(asst_guess) if asst_guess in unique_roles else 0, key="sft_map_asst")
            
            sft_columns = {
                "format": "chat",
                "messages_path": messages_field,
                "role_field": role_field,
                "content_field": content_field,
                "role_system": role_system if role_system != "(–Ω–µ—Ç)" else "",
                "role_user": role_user,
                "role_assistant": role_assistant
            }
            
        else:
            # ===== INSTRUCT –†–ï–ñ–ò–ú =====
            st.markdown("#### üìù –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Instruct-—Ñ–æ—Ä–º–∞—Ç–∞")
            st.caption("–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–ª—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–æ–ª–∏:")
            
            # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—É—Ç–∏
            field_options = ["(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)"] + all_paths
            
            system_path = st.selectbox("‚öôÔ∏è **System** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):", field_options, index=0, key="sft_inst_sys")
            user_path = st.selectbox("üë§ **User** (–≤–æ–ø—Ä–æ—Å/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è):", field_options, index=0, key="sft_inst_user")
            assistant_path = st.selectbox("ü§ñ **Assistant** (–æ—Ç–≤–µ—Ç):", field_options, index=0, key="sft_inst_asst")
            
            if user_path == "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" or assistant_path == "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)":
                st.warning("üëÜ –í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–ª—è **User** –∏ **Assistant**")
                return {}
            
            sft_columns = {
                "format": "instruct",
                "instruction": user_path,
                "output": assistant_path,
                "system_field": system_path if system_path != "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" else ""
            }
        
        # ===== –ù–ê–°–¢–†–û–ô–ö–ò –®–ê–ë–õ–û–ù–ê =====
        st.markdown("---")
        with st.expander("üè∑Ô∏è –¢–µ–≥–∏ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", expanded=False):
            default_system = st.text_input("System prompt (–ø–æ —É–º–æ–ª—á.):", "You are a helpful assistant.", key="sft_def_sys")
            tc1, tc2 = st.columns(2)
            user_tag = tc1.text_input("User tag:", "### User:", key="sft_tag_user")
            assistant_tag = tc2.text_input("Assistant tag:", "### Assistant:", key="sft_tag_asst")
        
        if 'default_system' not in dir():
            default_system, user_tag, assistant_tag = "You are a helpful assistant.", "### User:", "### Assistant:"
        
        sft_template = {
            "system": default_system,
            "separator": "\n\n",
            "user_tag": user_tag,
            "bot_tag": assistant_tag
        }
        
        # ===== –ü–†–ï–í–¨–Æ =====
        st.markdown("---")
        st.markdown("#### üëÅÔ∏è –ü—Ä–µ–≤—å—é:")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ session_state (–µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è SFT)
            sft_tokenizer = st.session_state.get("sft_tokenizer")
            use_model_chat_template = False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ chat_template
            if sft_tokenizer and hasattr(sft_tokenizer, 'chat_template') and sft_tokenizer.chat_template:
                use_model_chat_template = True
            
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π chat_template
            user_chat_template = st.session_state.get("sft_user_chat_template", "").strip()
            if user_chat_template and sft_tokenizer:
                # –í—Ä–µ–º–µ–Ω–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω
                sft_tokenizer.chat_template = user_chat_template
                use_model_chat_template = True
            
            preview = ""
            
            if sft_columns["format"] == "chat":
                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ sample
                messages = sample[sft_columns["messages_path"]]
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç messages
                std_messages = []
                for msg in messages:
                    role_val = str(msg.get(sft_columns["role_field"], ""))
                    content_val = str(msg.get(sft_columns["content_field"], ""))[:300]
                    
                    # –ú–∞–ø–ø–∏–Ω–≥ —Ä–æ–ª–µ–π
                    if role_val == sft_columns.get("role_system"):
                        std_messages.append({"role": "system", "content": content_val})
                    elif role_val == sft_columns.get("role_user"):
                        std_messages.append({"role": "user", "content": content_val})
                    elif role_val == sft_columns.get("role_assistant"):
                        std_messages.append({"role": "assistant", "content": content_val})
                
                # –ï—Å–ª–∏ –Ω–µ—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ
                if not std_messages or std_messages[0]["role"] != "system":
                    std_messages.insert(0, {"role": "system", "content": default_system})
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º chat_template –µ—Å–ª–∏ –µ—Å—Ç—å
                if use_model_chat_template and sft_tokenizer:
                    try:
                        preview = sft_tokenizer.apply_chat_template(
                            std_messages,
                            tokenize=False,
                            add_generation_prompt=False
                        )
                        st.caption("‚ú® –ü—Ä–µ–≤—å—é —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ **chat_template –º–æ–¥–µ–ª–∏**")
                    except Exception as e:
                        st.warning(f"–û—à–∏–±–∫–∞ apply_chat_template: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback.")
                        use_model_chat_template = False
                
                # Fallback: –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç —Å —Ç–µ–≥–∞–º–∏
                if not use_model_chat_template or not preview:
                    sep = "\n\n"
                    sys_text = default_system
                    preview = ""
                    
                    for msg in messages:
                        role = str(msg.get(sft_columns["role_field"], ""))
                        content = str(msg.get(sft_columns["content_field"], ""))
                        
                        if role == sft_columns["role_system"]:
                            sys_text = content
                        elif role == sft_columns["role_user"]:
                            preview += f"{user_tag}\n{content[:200]}{'...' if len(content) > 200 else ''}{sep}"
                        elif role == sft_columns["role_assistant"]:
                            preview += f"{assistant_tag}\n{content[:200]}{'...' if len(content) > 200 else ''}{sep}"
                    
                    preview = f"{sys_text}{sep}" + preview + "<|endoftext|>"
                    st.caption("‚ÑπÔ∏è –ü—Ä–µ–≤—å—é —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ **—Ç–µ–≥–∏** (chat_template –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)")
            else:
                # Instruct —Ñ–æ—Ä–º–∞—Ç
                user_val = str(get_nested_value(sample, sft_columns["instruction"]) or "")[:300]
                asst_val = str(get_nested_value(sample, sft_columns["output"]) or "")[:300]
                
                # System prompt
                sys_val = default_system
                system_field = sft_columns.get("system_field")
                if system_field and system_field != "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" and system_field.strip():
                    field_sys = get_nested_value(sample, system_field)
                    if field_sys is not None:
                        field_sys_str = str(field_sys).strip()
                        if field_sys_str:
                            sys_val = field_sys_str[:200]
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º messages –¥–ª—è chat_template
                std_messages = [
                    {"role": "system", "content": sys_val},
                    {"role": "user", "content": user_val},
                    {"role": "assistant", "content": asst_val}
                ]
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º chat_template –µ—Å–ª–∏ –µ—Å—Ç—å
                if use_model_chat_template and sft_tokenizer:
                    try:
                        preview = sft_tokenizer.apply_chat_template(
                            std_messages,
                            tokenize=False,
                            add_generation_prompt=False
                        )
                        st.caption("‚ú® –ü—Ä–µ–≤—å—é —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ **chat_template –º–æ–¥–µ–ª–∏**")
                    except Exception as e:
                        st.warning(f"–û—à–∏–±–∫–∞ apply_chat_template: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback.")
                        use_model_chat_template = False
                
                # Fallback
                if not use_model_chat_template or not preview:
                    sep = "\n\n"
                    preview = f"{sys_val}{sep}{user_tag}\n{user_val}{sep}{assistant_tag}\n{asst_val}<|endoftext|>"
                    st.caption("‚ÑπÔ∏è –ü—Ä–µ–≤—å—é —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ **—Ç–µ–≥–∏** (chat_template –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)")
            
            with st.container(height=400):
                st.code(preview, language=None)
            
            st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞: {e}")

    return {"sft_columns": sft_columns, "sft_template": sft_template}


# ============================================================================
# GRPO Configuration
# ============================================================================

def render_grpo_sidebar_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO –≤ —Å–∞–π–¥–±–∞—Ä–µ.
    
    Training Backend —Ç–µ–ø–µ—Ä—å –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤ render_model_config() (–¥–æ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏),
    –≤–º–µ—Å—Ç–µ —Å –º–µ—Ç–æ–¥–æ–º —Ç—é–Ω–∏–Ω–≥–∞ (lora/qlora/full).
    """
    st.sidebar.subheader("üß† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã GRPO")
    
    # –ê–ª–≥–æ—Ä–∏—Ç–º
    algorithm = st.sidebar.selectbox(
        "–ê–ª–≥–æ—Ä–∏—Ç–º",
        ["grpo", "dapo", "dr_grpo"],
        format_func=lambda x: {
            "grpo": "‚≠ê GRPO (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)",
            "dapo": "DAPO (Dynamic Advantage)",
            "dr_grpo": "Dr.GRPO (—É–ª—É—á—à–µ–Ω–Ω—ã–π)",
        }[x],
        help="""
        **GRPO** ‚≠ê: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è! –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Group Relative Policy Optimization
        **DAPO**: Token-level loss + –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ + dynamic sampling
        **Dr.GRPO**: –ë–µ–∑ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ std, —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        """
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    group_size = st.sidebar.slider(
        "Group size (G)",
        min_value=8,
        max_value=32,
        value=8,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç. –í–∞–∂–Ω–æ: –¥–ª—è GRPO –æ–±—ã—á–Ω–æ –Ω—É–∂–Ω–æ G>=8 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."
    )

    prompt_batch_size = st.sidebar.slider(
        "Prompt batch size (prompts/step)",
        min_value=1,
        max_value=64,
        value=8,
        step=1,
        help="–°–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ (–∑–∞–¥–∞—á) –±—Ä–∞—Ç—å –Ω–∞ –æ–¥–∏–Ω RL-—à–∞–≥ (rollouts_per_step –≤ re-grpo)."
    )
    
    max_new_tokens = st.sidebar.slider(
        "Max new tokens",
        min_value=128,
        max_value=16384,
        value=1024,
        step=128,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"
    )
    
    temperature = st.sidebar.slider(
        "Temperature",
        min_value=0.1,
        max_value=2.0,
        value=0.7,
        step=0.1,
        help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    grpo_learning_rate = st.sidebar.select_slider(
        "Learning Rate (GRPO)",
        options=[1e-7, 5e-7, 1e-6, 3e-6, 5e-6, 1e-5, 3e-5, 5e-5, 1e-4],
        value=5e-5,
        format_func=lambda x: f"{x:.0e}",
        help="""**–î–ª—è LoRA:** —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è **5e-5** ‚Äî —Å—Ä–∞–∑—É –Ω–∞—á–∏–Ω–∞–µ—Ç —É—á–∏—Ç—å—Å—è.
**–î–ª—è full fine-tuning:** 1e-6 ‚Äî 5e-6.
–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π LR —Å LoRA = –º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å!"""
    )
    
    train_batch_size = st.sidebar.slider(
        "Train Batch Size",
        min_value=1,
        max_value=128,
        value=2,
        step=1,
        help="–†–∞–∑–º–µ—Ä –º–∏–∫—Ä–æ-–±–∞—Ç—á–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –æ–ø—ã—Ç–µ. –£–º–µ–Ω—å—à–∏—Ç–µ –¥–æ 1-2, –µ—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç OOM"
    )

    grpo_grad_accum = st.sidebar.slider(
        "Gradient accumulation steps",
        min_value=1,
        max_value=32,
        value=4,
        step=1,
        help="–ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–∫–∞–∫ –≤ PPO/GRPO). –ù–µ –º–µ–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –¥–∞–Ω–Ω—ã—Ö, —Ç–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch –Ω–∞ —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."
    )
    
    # –õ–∏–º–∏—Ç –ø–æ –¥–∞–Ω–Ω—ã–º (–ø–æ–Ω—è—Ç–Ω–µ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —á–µ–º optim-steps).
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç (—Å —É—á—ë—Ç–æ–º "–ú–∞–∫—Å. –ø—Ä–∏–º–µ—Ä–æ–≤" –≤ main-config).
    effective_ds = st.session_state.get("grpo_effective_dataset_size", None)
    if isinstance(effective_ds, int) and effective_ds > 0:
        grpo_max_prompts = st.sidebar.number_input(
            "Max prompts (–ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É)",
            min_value=1,
            max_value=int(effective_ds),
            value=int(effective_ds),
            step=max(1, int(effective_ds) // 50),
            help="–°–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á (prompts) –ø—Ä–æ–π—Ç–∏ –≤—Å–µ–≥–æ. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é = –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç (—Å —É—á—ë—Ç–æ–º max_samples).",
        )
    else:
        st.sidebar.info("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –≤ GRPO (–≤–∫–ª–∞–¥–∫–∞ –ó–∞–ø—É—Å–∫), —á—Ç–æ–±—ã –ª–∏–º–∏—Ç —Å—á–∏—Ç–∞–ª—Å—è –æ—Ç –µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.")
        grpo_max_prompts = None
    
    epochs_per_step = st.sidebar.slider(
        "Epochs per step",
        min_value=1,
        max_value=5,
        value=1,
        help="–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ rollout'–æ–≤"
    )
    
    # KL
    kl_weight = st.sidebar.slider(
        "KL weight",
        min_value=0.0,
        max_value=0.1,
        value=0.0,
        step=0.001,
        format="%.3f",
        help="""**KL penalty** –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏.

- **0.0** ‚Äî –±–µ–∑ KL (–æ–±—ã—á–Ω–æ –¥–ª—è full fine-tuning)
- **0.001** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–µ—Å–ª–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ)
- **0.01+** ‚Äî —Å–∏–ª—å–Ω—ã–π constraint"""
    )
    
    # –ö–ª–∏–ø–ø–∏–Ω–≥ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    with st.sidebar.expander("‚öôÔ∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"):
        clip_eps_low = st.slider("Clip Œµ (low)", 0.1, 0.3, 0.2, 0.01)
        clip_eps_high = st.slider(
            "Clip Œµ (high)", 
            0.1, 0.4, 
            0.28 if algorithm == "dapo" else 0.2, 
            0.01,
            help="DAPO —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç 0.28 –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã"
        )

        min_lr_ratio = st.slider(
            "Min LR ratio (floor)",
            0.0, 0.5,
            0.1,
            0.01,
            help="–ù–∏–∂–Ω–∏–π –ø—Ä–µ–¥–µ–ª LR: lr = base_lr * ratio –≤ –∫–æ–Ω—Ü–µ cosine. 0.0 = –¥–æ –Ω—É–ª—è."
        )
        
        # ============================================================
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        # ============================================================
        
        if algorithm == "dapo":
            st.markdown("---")
            st.markdown("**üéØ DAPO-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏**")
            
            dynamic_sampling = st.checkbox(
                "Dynamic sampling",
                value=True,
                help=(
                    "–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –≥—Ä—É–ø–ø—ã –≥–¥–µ –≤—Å–µ rewards –æ–¥–∏–Ω–∞–∫–æ–≤—ã (zero-gradient).\n\n"
                    "**‚ö†Ô∏è –ó–∞–º–µ–¥–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ** ‚Äî –¥–µ–ª–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏!\n"
                    "–û—Ç–∫–ª—é—á–∏—Ç–µ –µ—Å–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–∞."
                )
            )
            
            # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –¥–æ–±–æ—Ä–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ dynamic_sampling –≤–∫–ª—é—á—ë–Ω)
            if dynamic_sampling:
                max_refill_rounds = st.slider(
                    "Max refill rounds",
                    min_value=1,
                    max_value=8,
                    value=3,
                    step=1,
                    help=(
                        "–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—ã—Ç–∞—Ç—å—Å—è –¥–æ–±–∏—Ä–∞—Ç—å –≥—Ä—É–ø–ø—ã.\n"
                        "**8** = –º–∞–∫—Å–∏–º—É–º (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö)\n"
                        "**2-3** = –±—ã—Å—Ç—Ä–µ–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)"
                    )
                )
            else:
                max_refill_rounds = 0
            
            token_level_loss = st.checkbox(
                "Token-level loss",
                value=True,
                help="–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å loss –ø–æ —Ç–æ–∫–µ–Ω–∞–º (DAPO), –∞ –Ω–µ –ø–æ —Å—ç–º–ø–ª–∞–º (GRPO)"
            )
        
        elif algorithm == "dr_grpo":
            st.markdown("---")
            st.markdown("**üî¨ Dr.GRPO-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏**")
            st.info(
                "Dr.GRPO –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:\n"
                "‚Ä¢ –û—Ç–∫–ª—é—á–∞–µ—Ç –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ std\n"
                "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ –¥–ª–∏–Ω–µ"
            )
            # Dr.GRPO –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç dynamic_sampling –∏ token_level_loss
            dynamic_sampling = False
            max_refill_rounds = 0
            token_level_loss = False
        
        else:  # GRPO
            st.markdown("---")
            st.markdown("**üìä GRPO-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏**")
            st.info(
                "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO:\n"
                "‚Ä¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è advantages: (r - mean) / std\n"
                "‚Ä¢ Sample-level loss –∞–≥—Ä–µ–≥–∞—Ü–∏—è"
            )
            # GRPO –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç dynamic_sampling –∏ token_level_loss
            dynamic_sampling = False
            max_refill_rounds = 0
            token_level_loss = False

        # Liger –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ –æ–±—â–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ Precision & Memory (—Å–∞–π–¥–±–∞—Ä)
        # –ó–¥–µ—Å—å —Ç–æ–ª—å–∫–æ –≤—ã—á–∏—Å–ª—è–µ–º loss_type –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        algorithm_to_loss_type = {
            "grpo": "grpo",
            "dapo": "dapo", 
            "dr_grpo": "dr_grpo",
        }
        grpo_liger_loss_type = algorithm_to_loss_type.get(algorithm, "grpo")
        
        st.markdown("---")
        st.markdown("**üöÄ Rollout engine (–æ—Ç–¥–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)**")
        grpo_use_rollout_engine = st.checkbox(
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
            value=False,
            help=(
                "–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (rollout) –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é, "
                "–∞ DDP/ZeRO-3/FSDP –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è teacher-forcing logprobs –∏ backprop. "
                "–≠—Ç–æ —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç GRPO –ø—Ä–∏ ZeRO-3/FSDP."
            ),
        )
        grpo_rollout_backend = st.selectbox(
            "Rollout backend",
            options=["hf", "vllm"],
            index=0,
            help=(
                "**hf** = HuggingFace –º–æ–¥–µ–ª—å. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π, —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–µ–∑–¥–µ.\n\n"
                "**vllm** = vLLM (PagedAttention, continuous batching). –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏!\n"
                "- –î–ª—è LoRA: –±—ã—Å—Ç—Ä–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è (~—Å–µ–∫—É–Ω–¥—ã)\n"
                "- –î–ª—è full fine-tuning: –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ vLLM (~5-15 —Å–µ–∫), —É–≤–µ–ª–∏—á—å—Ç–µ sync interval"
            ),
            disabled=not grpo_use_rollout_engine,
        )
        
        # vLLM: –Ω–∞ –∫–∞–∫–æ–π GPU –∑–∞–ø—É—Å–∫–∞—Ç—å?
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU
        try:
            import torch
            num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
        except:
            num_gpus = 0
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ multi-GPU DDP + vLLM
        selected_num_gpus = st.session_state.get("num_gpus", 1) or 1
        if grpo_rollout_backend == "vllm" and selected_num_gpus > 1:
            st.warning(
                "‚ö†Ô∏è **vLLM + Multi-GPU DDP –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è!**\n\n"
                "–ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU –¥–ª—è DDP training, vLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è.\n\n"
                "**–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è vLLM**: –≤—ã–±–µ—Ä–∏—Ç–µ 1 GPU –¥–ª—è training, "
                "–∞ vLLM —Ä–∞–∑–º–µ—Å—Ç–∏—Ç–µ –Ω–∞ –¥—Ä—É–≥–æ–π GPU."
            )
        
        # –í—ã–±–æ—Ä GPU –¥–ª—è vLLM
        gpu_options = [f"cuda:{i}" for i in range(num_gpus)] if num_gpus > 0 else ["cuda:0"]
        gpu_labels = {}
        for i in range(num_gpus):
            try:
                name = torch.cuda.get_device_name(i)
                gpu_labels[f"cuda:{i}"] = f"üéÆ GPU {i}: {name}"
            except:
                gpu_labels[f"cuda:{i}"] = f"üéÆ GPU {i}"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º GPU –¥–ª—è training (–ø–µ—Ä–≤–∞—è –≤—ã–±—Ä–∞–Ω–Ω–∞—è –≤ multi-select –∏–ª–∏ 0)
        training_gpu_id = 0
        selected_gpus = st.session_state.get("selected_gpus", [0])
        if selected_gpus:
            training_gpu_id = selected_gpus[0] if isinstance(selected_gpus[0], int) else 0
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é vLLM –Ω–∞ –¥—Ä—É–≥–æ–π GPU –µ—Å–ª–∏ –µ—Å—Ç—å
        default_vllm_gpu_idx = 0
        if num_gpus > 1:
            # –í—ã–±–∏—Ä–∞–µ–º GPU –∫–æ—Ç–æ—Ä–∞—è –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è training
            for i in range(num_gpus):
                if i != training_gpu_id:
                    default_vllm_gpu_idx = i
                    break
        
        grpo_vllm_device = st.selectbox(
            "vLLM GPU",
            options=gpu_options,
            index=min(default_vllm_gpu_idx, len(gpu_options) - 1),
            format_func=lambda x: gpu_labels.get(x, x),
            help=(
                "–í—ã–±–µ—Ä–∏—Ç–µ GPU –¥–ª—è vLLM rollout –º–æ–¥–µ–ª–∏.\n\n"
                "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –û–¢–î–ï–õ–¨–ù–£–Æ GPU –¥–ª—è vLLM!\n"
                f"- Training –Ω–∞: cuda:{training_gpu_id}\n"
                f"- vLLM –ª—É—á—à–µ –Ω–∞: cuda:{default_vllm_gpu_idx if num_gpus > 1 else 0}\n\n"
                "–ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ 1 GPU ‚Äî vLLM –∏ training –¥–µ–ª—è—Ç –ø–∞–º—è—Ç—å (—É–º–µ–Ω—å—à–∏—Ç–µ % –ø–∞–º—è—Ç–∏ –¥–ª—è vLLM)."
            ),
            disabled=not (grpo_use_rollout_engine and grpo_rollout_backend == "vllm"),
        )
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ 1 GPU
        if grpo_rollout_backend == "vllm" and num_gpus <= 1:
            st.warning(
                "‚ö†Ô∏è **–î–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ 1 GPU!**\n\n"
                "vLLM –∏ training –±—É–¥—É—Ç –Ω–∞ –æ–¥–Ω–æ–π GPU.\n"
                "- –£–º–µ–Ω—å—à–∏—Ç–µ **vLLM GPU Memory** –¥–æ 30-40%\n"
                "- –ò–ª–∏ –ø—Ä–æ–±—Ä–æ—Å—å—Ç–µ –±–æ–ª—å—à–µ GPU –≤ Docker (`--gpus all`)"
            )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω–∞ —Ç–æ–π –∂–µ GPU –∏–ª–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π
        vllm_gpu_id = int(grpo_vllm_device.split(":")[1]) if grpo_vllm_device.startswith("cuda:") else 0
        same_gpu = (vllm_gpu_id == training_gpu_id)
        
        if grpo_rollout_backend == "vllm" and same_gpu and num_gpus > 1:
            st.warning(
                f"‚ö†Ô∏è vLLM –∏ training –Ω–∞ –æ–¥–Ω–æ–π GPU (cuda:{training_gpu_id})!\n"
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥—É—é GPU –¥–ª—è vLLM."
            )
        
        # vLLM GPU memory utilization
        # –ï—Å–ª–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π GPU ‚Äî –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ, –µ—Å–ª–∏ –Ω–∞ —Ç–æ–π –∂–µ ‚Äî –º–µ–Ω—å—à–µ
        default_memory = 40 if same_gpu else 85
        max_memory = 60 if same_gpu else 95
        
        grpo_vllm_gpu_memory = st.slider(
            "vLLM GPU Memory (%)",
            min_value=10,
            max_value=max_memory,
            value=default_memory,
            step=5,
            help=(
                "–°–∫–æ–ª—å–∫–æ % GPU –ø–∞–º—è—Ç–∏ –≤—ã–¥–µ–ª–∏—Ç—å –¥–ª—è vLLM.\n\n"
                "–ï—Å–ª–∏ vLLM –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π GPU ‚Äî —Å—Ç–∞–≤—å—Ç–µ 70-90%.\n"
                "–ï—Å–ª–∏ –Ω–∞ —Ç–æ–π –∂–µ GPU —á—Ç–æ training ‚Äî —Å—Ç–∞–≤—å—Ç–µ 30-50%."
            ),
            disabled=not (grpo_use_rollout_engine and grpo_rollout_backend == "vllm"),
        )
        grpo_rollout_sync_interval = st.slider(
            "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (–∫–∞–∂–¥—ã–µ N rollout-step)",
            min_value=1,
            max_value=20,
            value=1,
            step=1,
            help=(
                "1 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ on-policy (—á–∞—â–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è = –±–æ–ª—å—à–µ overhead). "
                "2-10 = –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ rollout –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç —á—É—Ç—å 'stale'."
            ),
            disabled=not grpo_use_rollout_engine,
        )
        grpo_rollout_trainable_only = st.checkbox(
            "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (LoRA)",
            value=True,
            help=(
                "**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è LoRA** ‚Äî —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –±—ã—Å—Ç—Ä–∞—è (~MB –∞–¥–∞–ø—Ç–µ—Ä–∞).\n\n"
                "**–î–ª—è full fine-tuning** ‚Äî —Å–Ω–∏–º–∏—Ç–µ –≥–∞–ª–æ—á–∫—É, –±—É–¥—É—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤—Å–µ –≤–µ—Å–∞ (~GB). "
                "–ü—Ä–∏ ZeRO-3 —ç—Ç–æ –¥–æ—Ä–æ–≥–æ, –ø–æ—ç—Ç–æ–º—É —É–≤–µ–ª–∏—á—å—Ç–µ sync_interval."
            ),
            disabled=not grpo_use_rollout_engine,
        )
        grpo_rollout_offload_to_cpu = st.checkbox(
            "Offload rollout –º–æ–¥–µ–ª—å –Ω–∞ CPU –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏—è–º–∏ (—ç–∫–æ–Ω–æ–º–∏—Ç VRAM)",
            value=False,
            help="–ü–æ–ª–µ–∑–Ω–æ –µ—Å–ª–∏ VRAM –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç (–æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ ZeRO-3 + full finetune). –ú–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å rollout.",
            disabled=not grpo_use_rollout_engine,
        )
    
    # –í–ê–ñ–ù–û: LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –±–µ—Ä—É—Ç—Å—è –∏–∑ render_model_config() (—Å–µ–∫—Ü–∏—è "üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞")
    # –ó–¥–µ—Å—å –º—ã –ù–ï –¥—É–±–ª–∏—Ä—É–µ–º –∏—Ö, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –≤ UI
    # –í—Å–µ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (use_lora, lora_r, lora_alpha, lora_dropout, lora_target_modules)
    # –∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (use_4bit, use_8bit) –±—É–¥—É—Ç –≤–∑—è—Ç—ã –∏–∑ model_config
    
    # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ç–º–ø–ª–æ–≤ ===
    st.sidebar.markdown("---")
    st.sidebar.markdown("**üìù –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ç–º–ø–ª–æ–≤**")
    
    grpo_log_completions = st.sidebar.checkbox(
        "–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã",
        value=True,
        help="–í—ã–≤–æ–¥–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å"
    )
    
    grpo_completion_log_interval = st.sidebar.slider(
        "–ò–Ω—Ç–µ—Ä–≤–∞–ª –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—à–∞–≥–∏)",
        min_value=1,
        max_value=100,
        value=10,
        step=1,
        help="–ö–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
    )
    
    return {
        # Training Backend —Ç–µ–ø–µ—Ä—å –≤ render_model_config()
        
        # GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ)
        "grpo_algorithm": algorithm,
        "grpo_group_size": group_size,
        "grpo_prompt_batch_size": prompt_batch_size,
        "grpo_max_new_tokens": max_new_tokens,
        "grpo_temperature": temperature,
        "grpo_learning_rate": grpo_learning_rate,
        "grpo_train_batch_size": train_batch_size,
        "gradient_accumulation": grpo_grad_accum,
        "grpo_max_prompts": grpo_max_prompts,
        "grpo_epochs_per_step": epochs_per_step,
        "grpo_kl_weight": kl_weight,
        "grpo_clip_eps_low": clip_eps_low,
        "grpo_clip_eps_high": clip_eps_high,
        "grpo_dynamic_sampling": dynamic_sampling,
        "grpo_max_refill_rounds": max_refill_rounds,
        "grpo_token_level_loss": token_level_loss,
        "grpo_min_lr_ratio": min_lr_ratio,

        # Liger loss_type ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ Liger –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ distributed_config (Precision & Memory)
        "grpo_liger_loss_type": grpo_liger_loss_type,

        # Rollout engine
        "grpo_use_rollout_engine": grpo_use_rollout_engine,
        "grpo_rollout_backend": grpo_rollout_backend,
        "grpo_rollout_sync_interval": grpo_rollout_sync_interval,
        "grpo_rollout_trainable_only": grpo_rollout_trainable_only,
        "grpo_rollout_offload_to_cpu": grpo_rollout_offload_to_cpu,
        "grpo_vllm_gpu_memory": grpo_vllm_gpu_memory / 100.0,  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º % –≤ 0.0-1.0
        "grpo_vllm_device": grpo_vllm_device,  # "main_gpu" –∏–ª–∏ "cpu"
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        "grpo_log_completions": grpo_log_completions,
        "grpo_completion_log_interval": grpo_completion_log_interval,
    }


def render_grpo_main_config(data_path: str = None):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä Reward —Ñ—É–Ω–∫—Ü–∏–π —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ä–µ–¥–∞–∫—Ç–æ—Ä–æ–º –ø—Ä–∞–≤–∏–ª."""
    import re
    import json as json_lib
    
    # =========================================================================
    # 1. –î–ê–¢–ê–°–ï–¢ –î–õ–Ø REASONING (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
    # =========================================================================
    st.markdown("### üìö –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è Reasoning")
    
    grpo_dataset_path = None
    grpo_max_samples = None
    grpo_dataset_language = "en"
    dataset_source = "custom"
    dataset_key = "custom"
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if "grpo_field_mapping" not in st.session_state:
        st.session_state.grpo_field_mapping = {
            "prompt_field": "question",
            "reference_field": "answer",
            "metadata_fields": [],
        }
    if "grpo_prompt_template" not in st.session_state:
        st.session_state.grpo_prompt_template = "{{prompt}}"
    if "grpo_system_prompt" not in st.session_state:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π reasoning –ø—Ä–æ–º–ø—Ç —Å —Ç–µ–≥–∞–º–∏
        st.session_state.grpo_system_prompt = """You are a helpful assistant that solves problems step by step.
Think through the problem carefully inside <reasoning>...</reasoning> tags.
Then provide your final answer inside <answer>...</answer> tags.

Example format:
<reasoning>
Let me analyze this step by step...
Step 1: ...
Step 2: ...
Therefore, the answer is X.
</reasoning>
<answer>X</answer>"""
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    local_datasets = []
    if DATASET_DIR.exists():
        for f in sorted(DATASET_DIR.iterdir(), key=lambda x: x.name.lower()):
            if f.suffix in (".jsonl", ".json"):
                local_datasets.append(f)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞
    dataset_options = ["-- –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç --"]
    for f in local_datasets:
        dataset_options.append(str(f))
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    default_idx = 0
    saved_selection = st.session_state.get("grpo_dataset_selectbox")
    if saved_selection and saved_selection in dataset_options:
        default_idx = dataset_options.index(saved_selection)
    elif data_path and data_path in dataset_options:
        default_idx = dataset_options.index(data_path)
    elif data_path:
        for i, opt in enumerate(dataset_options):
            if data_path in opt:
                default_idx = i
                break
    
    selected_dataset = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç",
        options=dataset_options,
        index=default_idx,
        key="grpo_dataset_selectbox",
        help="–°–∫–∞—á–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–î–∞–Ω–Ω—ã–µ' ‚Üí üß† Reasoning"
    )
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_samples = []
    dataset_fields = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    if selected_dataset and not selected_dataset.startswith("--"):
        grpo_dataset_path = selected_dataset
        st.session_state.grpo_dataset_path = grpo_dataset_path
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if "ru" in selected_dataset.lower() or "russian" in selected_dataset.lower():
            grpo_dataset_language = "ru"
        else:
            grpo_dataset_language = "en"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–µ–º–ø–ª—ã –¥–ª—è –ø—Ä–µ–≤—å—é –∏ –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç–∞ –ø–æ–ª–µ–π
        try:
            p = Path(grpo_dataset_path)
            if p.exists():
                if p.suffix == ".jsonl":
                    with open(p, "r", encoding="utf-8", errors="ignore") as f:
                        for i, line in enumerate(f):
                            if i >= 100:  # –ß–∏—Ç–∞–µ–º –¥–æ 100 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                                break
                            line = line.strip()
                            if line:
                                try:
                                    dataset_samples.append(json_lib.loads(line))
                                except:
                                    pass
                elif p.suffix == ".json":
                    with open(p, "r", encoding="utf-8") as f:
                        obj = json_lib.load(f)
                    if isinstance(obj, list):
                        dataset_samples = obj[:100]
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ–ª—è –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
                if dataset_samples:
                    all_fields = set()
                    for sample in dataset_samples[:20]:
                        if isinstance(sample, dict):
                            all_fields.update(sample.keys())
                    dataset_fields = sorted(list(all_fields))
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        
        st.success(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç: `{Path(selected_dataset).name}` ({len(dataset_samples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–ª—è –ø—Ä–µ–≤—å—é)")
        
        # =====================================================================
        # –ü–†–ï–í–¨–Æ –î–ê–ù–ù–´–•
        # =====================================================================
        with st.expander("üëÄ –ü—Ä–µ–≤—å—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞", expanded=True):
            if dataset_samples:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–µ–º–ø–ª–æ–≤
                preview_count = min(5, len(dataset_samples))
                
                # –¢–∞–±–ª–∏—á–Ω—ã–π –≤–∏–¥
                if dataset_fields:
                    st.caption(f"**–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø–æ–ª—è:** {', '.join(dataset_fields)}")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–µ–º–ø–ª—ã
                for i, sample in enumerate(dataset_samples[:preview_count]):
                    with st.container():
                        st.markdown(f"**–ü—Ä–∏–º–µ—Ä {i+1}:**")
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ
                        cols = st.columns(2)
                        field_list = list(sample.items()) if isinstance(sample, dict) else []
                        for j, (key, value) in enumerate(field_list):
                            col_idx = j % 2
                            with cols[col_idx]:
                                val_str = str(value)[:300]
                                if len(str(value)) > 300:
                                    val_str += "..."
                                st.text_area(f"`{key}`", value=val_str, height=80, disabled=True, key=f"preview_{i}_{key}")
                        st.markdown("---")
            else:
                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–≤—å—é")
        
        # =====================================================================
        # –ú–ê–ü–ü–ò–ù–ì –ü–û–õ–ï–ô
        # =====================================================================
        st.markdown("#### üîó –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π –¥–∞—Ç–∞—Å–µ—Ç–∞")
        st.caption("–£–∫–∞–∂–∏—Ç–µ –∫–∞–∫–∏–µ –ø–æ–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞")
        
        # –ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        auto_prompt_field = None
        auto_reference_field = None
        
        prompt_candidates = ["question", "prompt", "input", "instruction", "problem", "query", "text"]
        reference_candidates = ["answer", "response", "output", "solution", "target", "completion", "label"]
        
        for candidate in prompt_candidates:
            if candidate in dataset_fields:
                auto_prompt_field = candidate
                break
        
        for candidate in reference_candidates:
            if candidate in dataset_fields:
                auto_reference_field = candidate
                break
        
        # –ï—Å–ª–∏ –≤ session_state –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –æ–Ω–∏ –≤–∞–ª–∏–¥–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        saved_prompt_field = st.session_state.grpo_field_mapping.get("prompt_field")
        saved_reference_field = st.session_state.grpo_field_mapping.get("reference_field")
        
        if saved_prompt_field in dataset_fields:
            auto_prompt_field = saved_prompt_field
        if saved_reference_field in dataset_fields:
            auto_reference_field = saved_reference_field
        
        mapping_cols = st.columns(2)
        
        with mapping_cols[0]:
            # –ü–æ–ª–µ –ø—Ä–æ–º–ø—Ç–∞
            prompt_options = ["-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --"] + dataset_fields
            prompt_idx = 0
            if auto_prompt_field and auto_prompt_field in prompt_options:
                prompt_idx = prompt_options.index(auto_prompt_field)
            
            prompt_field = st.selectbox(
                "üìù –ü–æ–ª–µ –ø—Ä–æ–º–ø—Ç–∞ (–≤–æ–ø—Ä–æ—Å/–∑–∞–¥–∞—á–∞)",
                options=prompt_options,
                index=prompt_idx,
                key="grpo_prompt_field_select",
                help="–ü–æ–ª–µ —Å –≤–æ–ø—Ä–æ—Å–æ–º/–∑–∞–¥–∞—á–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏"
            )
            if prompt_field and prompt_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --":
                st.session_state.grpo_field_mapping["prompt_field"] = prompt_field
        
        with mapping_cols[1]:
            # –ü–æ–ª–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            reference_options = ["-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --"] + dataset_fields
            reference_idx = 0
            if auto_reference_field and auto_reference_field in reference_options:
                reference_idx = reference_options.index(auto_reference_field)
            
            reference_field = st.selectbox(
                "‚úÖ –ü–æ–ª–µ –æ—Ç–≤–µ—Ç–∞ (—ç—Ç–∞–ª–æ–Ω)",
                options=reference_options,
                index=reference_idx,
                key="grpo_reference_field_select",
                help="–ü–æ–ª–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–∏"
            )
            if reference_field and reference_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --":
                st.session_state.grpo_field_mapping["reference_field"] = reference_field
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è metadata (–º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ reward)
        other_fields = [f for f in dataset_fields if f not in [prompt_field, reference_field]]
        if other_fields:
            metadata_fields = st.multiselect(
                "üìã –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (–¥–ª—è reward —Ñ—É–Ω–∫—Ü–∏–π)",
                options=other_fields,
                default=st.session_state.grpo_field_mapping.get("metadata_fields", []),
                key="grpo_metadata_fields_select",
                help="–≠—Ç–∏ –ø–æ–ª—è –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –∫–∞–∫ {{metadata.–∏–º—è_–ø–æ–ª—è}} –≤ reward –ø—Ä–∞–≤–∏–ª–∞—Ö"
            )
            st.session_state.grpo_field_mapping["metadata_fields"] = metadata_fields
        
        st.markdown("---")
        
        # =====================================================================
        # –®–ê–ë–õ–û–ù –ü–†–û–ú–ü–¢–ê
        # =====================================================================
        st.markdown("#### üìù –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞")
        st.caption("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∫–∞–∫ –±—É–¥–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏")
        
        # –ü—Ä–µ—Å–µ—Ç—ã —à–∞–±–ª–æ–Ω–æ–≤ (Reasoning –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        template_presets = {
            "ü§î Reasoning (—Ç–µ–≥–∏ <reasoning>/<answer>)": {
                "system": """You are a helpful assistant that solves problems step by step.
Think through the problem carefully inside <reasoning>...</reasoning> tags.
Then provide your final answer inside <answer>...</answer> tags.

Example format:
<reasoning>
Let me analyze this step by step...
Step 1: ...
Step 2: ...
Therefore, the answer is X.
</reasoning>
<answer>X</answer>""",
                "template": "{{prompt}}"
            },
            "üßÆ Math (GSM8K —Å—Ç–∏–ª—å)": {
                "system": "You are a helpful assistant that solves math problems step by step. Show your reasoning, then provide the final numerical answer after ####.",
                "template": "{{prompt}}"
            },
            "üßÆ Math RU (—Ä—É—Å—Å–∫–∏–π)": {
                "system": """–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ—à–∞–≥–æ–≤–æ.
–†–∞–∑–º—ã—à–ª—è–π –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–æ–≤ <reasoning>...</reasoning>.
–ó–∞—Ç–µ–º –¥–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–æ–≤ <answer>...</answer>.""",
                "template": "{{prompt}}"
            },
            "ü§î DeepSeek R1 —Å—Ç–∏–ª—å": {
                "system": """A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.
The reasoning process and answer are enclosed within <think>...</think> and <answer>...</answer> tags respectively.""",
                "template": "{{prompt}}"
            },
            "üìö QA (–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)": {
                "system": "Answer the question accurately and concisely.",
                "template": "Question: {{prompt}}\n\nAnswer:"
            },
            "üî¢ –ü—Ä–æ—Å—Ç–æ–π (–±–µ–∑ system prompt)": {
                "system": "",
                "template": "{{prompt}}"
            },
            "üéØ Custom (—Å–≤–æ–π —à–∞–±–ª–æ–Ω)": {
                "system": "",
                "template": "{{prompt}}"
            },
        }
        
        selected_preset = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–µ—Å–µ—Ç",
            options=list(template_presets.keys()),
            key="grpo_template_preset",
        )
        
        preset_data = template_presets[selected_preset]
        
        # System prompt
        system_prompt = st.text_area(
            "System prompt (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)",
            value=st.session_state.grpo_system_prompt or preset_data["system"],
            height=80,
            key="grpo_system_prompt_input",
            help="–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏. –ë—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω —á–µ—Ä–µ–∑ chat_template –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω."
        )
        st.session_state.grpo_system_prompt = system_prompt
        
        # –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞
        st.markdown("**–®–∞–±–ª–æ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞:**")
        st.caption("–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: `{{prompt}}` (–ø–æ–ª–µ –ø—Ä–æ–º–ø—Ç–∞), `{{reference}}` (–ø–æ–ª–µ –æ—Ç–≤–µ—Ç–∞), `{{metadata.–∏–º—è}}` (–¥–æ–ø. –ø–æ–ª—è)")
        
        prompt_template = st.text_area(
            "–®–∞–±–ª–æ–Ω",
            value=st.session_state.grpo_prompt_template if "Custom" in selected_preset else preset_data["template"],
            height=100,
            key="grpo_prompt_template_input",
            label_visibility="collapsed",
        )
        st.session_state.grpo_prompt_template = prompt_template
        
        # –ü—Ä–µ–≤—å—é –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        if dataset_samples and prompt_field and prompt_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --":
            st.markdown("**–ü—Ä–µ–≤—å—é –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞:**")
            sample = dataset_samples[0]
            
            # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
            preview_prompt = prompt_template
            if isinstance(sample, dict):
                # {{prompt}}
                if prompt_field in sample:
                    preview_prompt = preview_prompt.replace("{{prompt}}", str(sample[prompt_field]))
                # {{reference}}
                if reference_field and reference_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --" and reference_field in sample:
                    preview_prompt = preview_prompt.replace("{{reference}}", str(sample[reference_field]))
                # {{metadata.xxx}}
                for key, value in sample.items():
                    preview_prompt = preview_prompt.replace(f"{{{{metadata.{key}}}}}", str(value))
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é
            preview_full = ""
            if system_prompt:
                preview_full = f"[System]: {system_prompt}\n\n[User]: {preview_prompt}"
            else:
                preview_full = f"[User]: {preview_prompt}"
            
            st.code(preview_full, language=None)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π –æ—Ç–≤–µ—Ç
            if reference_field and reference_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --" and reference_field in sample:
                ref_val = str(sample[reference_field])[:500]
                if len(str(sample[reference_field])) > 500:
                    ref_val += "..."
                st.caption(f"**–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:** {ref_val}")
        
        st.markdown("---")
        
    else:
        # –û—á–∏—â–∞–µ–º session_state –µ—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –≤—ã–±—Ä–∞–Ω
        if "grpo_dataset_path" in st.session_state:
            del st.session_state.grpo_dataset_path
        st.warning("‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –µ–≥–æ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ **üíæ –î–∞–Ω–Ω—ã–µ** ‚Üí üß† Reasoning")
        prompt_field = None
        reference_field = None
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
    grpo_max_samples = st.number_input(
        "–ú–∞–∫—Å. –ø—Ä–∏–º–µ—Ä–æ–≤ (0 = –≤—Å–µ)",
        min_value=0,
        max_value=50000,
        value=0,
        step=100,
        help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
    )

    # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ª–∏–º–∏—Ç–∞ "Max prompts" –≤ sidebar
    effective_size = None
    try:
        if grpo_max_samples and int(grpo_max_samples) > 0:
            effective_size = int(grpo_max_samples)
        elif grpo_dataset_path:
            p = Path(grpo_dataset_path)
            if p.exists() and p.suffix == ".jsonl":
                with open(p, "r", encoding="utf-8", errors="ignore") as f:
                    effective_size = sum(1 for _ in f if _.strip())
            elif p.exists() and p.suffix == ".json":
                with open(p, "r", encoding="utf-8") as f:
                    obj = json_lib.load(f)
                if isinstance(obj, list):
                    effective_size = len(obj)
        if isinstance(effective_size, int) and effective_size > 0:
            st.session_state["grpo_effective_dataset_size"] = int(effective_size)
            st.caption(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: **{effective_size} –ø—Ä–∏–º–µ—Ä–æ–≤**")
    except Exception:
        pass
    
    # =========================================================================
    # 2. REWARD DESIGNER
    # =========================================================================
    st.markdown("### üéØ Reward Designer")
    st.caption("–°–æ–∑–¥–∞–≤–∞–π—Ç–µ –≥–∏–±–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å —É—Å–ª–æ–≤–∏—è–º–∏, –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –∏ —Ñ–æ—Ä–º—É–ª–∞–º–∏")
    
    # =========================================================================
    # –ü–µ—Å–æ—á–Ω–∏—Ü–∞ ‚Äî –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞)
    # =========================================================================
    st.markdown("#### üß™ –ü–µ—Å–æ—á–Ω–∏—Ü–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ø–µ—Å–æ—á–Ω–∏—Ü—ã
    # –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –∑–Ω–∞—á–µ–Ω–∏—è - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    default_prompt = ""
    default_reference = ""
    reference_is_empty = False
    
    # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω - –±–µ—Ä—ë–º –¥–∞–Ω–Ω—ã–µ –æ—Ç—Ç—É–¥–∞
    if dataset_samples and prompt_field and prompt_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --":
        sample = dataset_samples[0]
        if isinstance(sample, dict) and prompt_field in sample:
            default_prompt = str(sample[prompt_field]) or ""
        if isinstance(sample, dict) and reference_field and reference_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --" and reference_field in sample:
            ref_val = sample[reference_field]
            # –î–ª—è GSM8K-—Å—Ç–∏–ª—è –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –ø–æ—Å–ª–µ ####
            if isinstance(ref_val, str) and "####" in ref_val:
                parts = ref_val.split("####")
                if len(parts) > 1:
                    default_reference = parts[-1].strip().replace(",", "").split()[0] if parts[-1].strip() else ""
                else:
                    default_reference = str(ref_val) if ref_val else ""
            else:
                default_reference = str(ref_val) if ref_val else ""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç–æ–π –ª–∏ –æ—Ç–≤–µ—Ç
            reference_is_empty = not default_reference.strip()
    
    with st.expander("üìù –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ", expanded=True):
        # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä –ø—Ä–∏–º–µ—Ä–∞
        selected_sample_idx = 0
        if dataset_samples:
            sample_options = [f"–ü—Ä–∏–º–µ—Ä {i+1}" for i in range(min(10, len(dataset_samples)))]
            selected_sample_idx = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–º–µ—Ä –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞",
                options=range(len(sample_options)),
                format_func=lambda x: sample_options[x],
                key="grpo_sandbox_sample_idx",
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º default_prompt/reference –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            if selected_sample_idx < len(dataset_samples):
                sample = dataset_samples[selected_sample_idx]
                if isinstance(sample, dict) and prompt_field and prompt_field in sample:
                    default_prompt = str(sample[prompt_field]) or ""
                if isinstance(sample, dict) and reference_field and reference_field != "-- –Ω–µ –≤—ã–±—Ä–∞–Ω–æ --" and reference_field in sample:
                    ref_val = sample[reference_field]
                    if isinstance(ref_val, str) and "####" in ref_val:
                        parts = ref_val.split("####")
                        if len(parts) > 1:
                            default_reference = parts[-1].strip().replace(",", "").split()[0] if parts[-1].strip() else ""
                        else:
                            default_reference = str(ref_val) if ref_val else ""
                    else:
                        default_reference = str(ref_val) if ref_val else ""
                    
                    reference_is_empty = not default_reference.strip()
            
            st.caption("üí° –î–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        else:
            st.info("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –≤—ã—à–µ —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã")
        
        # –°–æ–∑–¥–∞—ë–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π key –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
        dataset_key_hash = hash(grpo_dataset_path or "none") % 10000
        
        sample_prompt = st.text_area(
            "**–ü—Ä–æ–º–ø—Ç** (–≤–æ–ø—Ä–æ—Å/–∑–∞–¥–∞—á–∞)",
            value=default_prompt,
            height=100,
            key=f"sample_prompt_{dataset_key_hash}_{selected_sample_idx}",
        )
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π
        if reference_is_empty:
            st.warning("‚ö†Ô∏è –ü–æ–ª–µ –æ—Ç–≤–µ—Ç–∞ –ø—É—Å—Ç–æ–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ! –í—ã–±–µ—Ä–∏—Ç–µ –¥—Ä—É–≥–æ–µ –ø–æ–ª–µ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ –≤—Ä—É—á–Ω—É—é.")
        
        sample_reference = st.text_input(
            "**–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç**",
            value=default_reference,
            key=f"sample_reference_{dataset_key_hash}_{selected_sample_idx}",
            help="–ï—Å–ª–∏ –ø—É—Å—Ç–æ - –∑–Ω–∞—á–∏—Ç –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –ø–æ–ª–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞"
        )
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if dataset_samples and selected_sample_idx < len(dataset_samples):
            sample = dataset_samples[selected_sample_idx]
            metadata_fields_list = st.session_state.grpo_field_mapping.get("metadata_fields", [])
            if metadata_fields_list and isinstance(sample, dict):
                st.markdown("**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:**")
                for mf in metadata_fields_list:
                    if mf in sample:
                        st.caption(f"`{{{{metadata.{mf}}}}}` = {str(sample[mf])[:100]}")
    
    st.markdown("---")
    
    # =========================================================================
    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª
    # =========================================================================
    st.markdown("#### üèóÔ∏è –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä Reward-–ø—Ä–∞–≤–∏–ª")
    
    # –°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
    with st.expander("üìñ –°–ø—Ä–∞–≤–∫–∞: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å", expanded=False):
        st.markdown("""
**–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:**
- `{{response}}` ‚Äî –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (completion)
- `{{reference}}` ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–∏–∑ –ø–æ–ª—è –æ—Ç–≤–µ—Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞)
- `{{prompt}}` ‚Äî –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–∞)
- `{{extracted.–∏–º—è}}` ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ, –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–µ regex-–≥—Ä—É–ø–ø–æ–π
- `{{metadata.–∏–º—è_–ø–æ–ª—è}}` ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞

**–î–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞:**
–í—Å–µ –ø–æ–ª—è –∏–∑ –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ `{{metadata.–∏–º—è_–ø–æ–ª—è}}`.
–ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–µ `difficulty`, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `{{metadata.difficulty}}`.

**–û–ø–µ—Ä–∞—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:**
- `contains` ‚Äî —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Å—Ç—Ä–æ–∫—É
- `not_contains` ‚Äî –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç
- `matches` ‚Äî —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex
- `not_matches` ‚Äî –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex
- `equals` ‚Äî —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
- `==`, `!=`, `>`, `<`, `>=`, `<=` ‚Äî –¥–ª—è —á–∏—Å–µ–ª

**–ü—Ä–∏–º–µ—Ä regex —Å –≥—Ä—É–ø–ø–∞–º–∏:**
```
<answer>(?P<model_answer>\\d+)</answer>
```
–ò–∑–≤–ª–µ—á—ë—Ç —á–∏—Å–ª–æ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `{{extracted.model_answer}}`

**–§–æ—Ä–º—É–ª–∞ reward:**
```
1.0 if {{extracted.model_answer}} == {{reference}} else 0.0
```

**–ü—Ä–∏–º–µ—Ä —Å metadata:**
```
weight = float({{metadata.difficulty}}) / 10.0
```
        """)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª
    if "reward_rules" not in st.session_state:
        st.session_state.reward_rules = [
            {
                "id": 0,
                "name": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞",
                "enabled": True,
                "weight": 1.0,
                "conditions": [
                    {"type": "contains", "target": "{{response}}", "value": "<reasoning>"},
                    {"type": "contains", "target": "{{response}}", "value": "</reasoning>"},
                    {"type": "contains", "target": "{{response}}", "value": "<answer>"},
                    {"type": "contains", "target": "{{response}}", "value": "</answer>"},
                ],
                "condition_logic": "all",  # all / any / custom
                "reward_formula": "1.0",
                "else_reward": "0.0",
            },
            {
                "id": 1,
                "name": "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç",
                "enabled": True,
                "weight": 2.0,
                "extractors": [
                    {"name": "model_answer", "pattern": r"<answer>\s*(\d+)\s*</answer>", "source": "{{response}}"},
                ],
                "conditions": [
                    {"type": "equals_numeric", "left": "{{extracted.model_answer}}", "right": "{{reference}}", "tolerance": 0.01},
                ],
                "condition_logic": "all",
                "reward_formula": "1.0",
                "else_reward": "0.0",
            },
            {
                "id": 2,
                "name": "–ö–∞—á–µ—Å—Ç–≤–æ reasoning",
                "enabled": True,
                "weight": 0.5,
                "extractors": [
                    {"name": "reasoning_text", "pattern": r"<reasoning>(.*?)</reasoning>", "source": "{{response}}", "flags": "DOTALL"},
                ],
                "conditions": [
                    {"type": "length_between", "target": "{{extracted.reasoning_text}}", "min": 50, "max": 2000},
                ],
                "condition_logic": "all",
                "reward_formula": "min(len({{extracted.reasoning_text}}) / 200.0, 1.0)",
                "else_reward": "0.0",
            },
        ]
        st.session_state.next_rule_id = 3
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞
    def render_rule(rule, idx):
        with st.expander(
            f"{'‚úÖ' if rule['enabled'] else '‚è∏Ô∏è'} **{rule['name']}** (–≤–µ—Å: {rule['weight']})",
            expanded=False
        ):
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∞–≤–∏–ª–∞
            c1, c2, c3, c4 = st.columns([3, 1, 1, 1])
            with c1:
                rule["name"] = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ", value=rule["name"], key=f"rule_name_{rule['id']}")
            with c2:
                rule["weight"] = st.number_input("–í–µ—Å", 0.0, 10.0, float(rule["weight"]), 0.1, key=f"rule_weight_{rule['id']}")
            with c3:
                rule["enabled"] = st.checkbox("–í–∫–ª", value=rule["enabled"], key=f"rule_enabled_{rule['id']}")
            with c4:
                if st.button("üóëÔ∏è", key=f"rule_del_{rule['id']}"):
                    st.session_state.reward_rules.pop(idx)
                    st.rerun()
            
            # === EXTRACTORS (regex –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π) ===
            st.markdown("##### üîç –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã (regex)")
            st.caption("–ò–∑–≤–ª–µ–∫–∞—é—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ `{{extracted.–∏–º—è}}`")
            
            if "extractors" not in rule:
                rule["extractors"] = []
            
            for ei, ext in enumerate(rule["extractors"]):
                ec1, ec2, ec3, ec4 = st.columns([2, 4, 2, 1])
                with ec1:
                    ext["name"] = st.text_input("–ò–º—è", value=ext.get("name", f"var{ei}"), key=f"ext_name_{rule['id']}_{ei}")
                with ec2:
                    ext["pattern"] = st.text_input("Regex", value=ext.get("pattern", ""), key=f"ext_pattern_{rule['id']}_{ei}")
                with ec3:
                    ext["source"] = st.selectbox(
                        "–ò—Å—Ç–æ—á–Ω–∏–∫", 
                        ["{{response}}", "{{reference}}", "{{prompt}}"],
                        index=["{{response}}", "{{reference}}", "{{prompt}}"].index(ext.get("source", "{{response}}")),
                        key=f"ext_source_{rule['id']}_{ei}"
                    )
                with ec4:
                    if st.button("‚úñ", key=f"ext_del_{rule['id']}_{ei}"):
                        rule["extractors"].pop(ei)
                        st.rerun()
            
            if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä", key=f"add_ext_{rule['id']}"):
                rule["extractors"].append({"name": f"var{len(rule['extractors'])}", "pattern": r"(.*)", "source": "{{response}}"})
                st.rerun()
            
            st.markdown("---")
            
            # === CONDITIONS ===
            st.markdown("##### ‚ö° –£—Å–ª–æ–≤–∏—è")
            
            condition_types = {
                "contains": "—Å–æ–¥–µ—Ä–∂–∏—Ç",
                "not_contains": "–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç",
                "matches": "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex",
                "not_matches": "–Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex",
                "equals": "—Ä–∞–≤–Ω–æ (—Å—Ç—Ä–æ–∫–∞)",
                "equals_numeric": "—Ä–∞–≤–Ω–æ (—á–∏—Å–ª–æ)",
                "greater": "> –±–æ–ª—å—à–µ",
                "less": "< –º–µ–Ω—å—à–µ",
                "length_between": "–¥–ª–∏–Ω–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ",
                "length_min": "–¥–ª–∏–Ω–∞ >= –º–∏–Ω",
                "length_max": "–¥–ª–∏–Ω–∞ <= –º–∞–∫—Å",
            }
            
            if "conditions" not in rule:
                rule["conditions"] = []
            
            for ci, cond in enumerate(rule["conditions"]):
                ctype = cond.get("type", "contains")
                
                cc1, cc2 = st.columns([1, 4])
                with cc1:
                    if ci > 0:
                        st.write("**AND**" if rule.get("condition_logic") == "all" else "**OR**")
                    else:
                        st.write("**IF**")
                
                with cc2:
                    ccc1, ccc2, ccc3, ccc4 = st.columns([3, 2, 3, 1])
                    
                    with ccc1:
                        # –õ–µ–≤—ã–π –æ–ø–µ—Ä–∞–Ω–¥
                        target_options = ["{{response}}", "{{reference}}", "{{prompt}}"]
                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                        for ext in rule.get("extractors", []):
                            target_options.append(f"{{{{extracted.{ext['name']}}}}}")
                        
                        left_val = cond.get("target") or cond.get("left", "{{response}}")
                        if left_val not in target_options:
                            target_options.append(left_val)
                        
                        new_left = st.selectbox(
                            "–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å",
                            target_options,
                            index=target_options.index(left_val) if left_val in target_options else 0,
                            key=f"cond_left_{rule['id']}_{ci}",
                            label_visibility="collapsed"
                        )
                        cond["target"] = new_left
                        cond["left"] = new_left
                    
                    with ccc2:
                        new_type = st.selectbox(
                            "–û–ø–µ—Ä–∞—Ç–æ—Ä",
                            list(condition_types.keys()),
                            format_func=lambda x: condition_types.get(x, x),
                            index=list(condition_types.keys()).index(ctype) if ctype in condition_types else 0,
                            key=f"cond_type_{rule['id']}_{ci}",
                            label_visibility="collapsed"
                        )
                        cond["type"] = new_type
                    
                    with ccc3:
                        # –ü—Ä–∞–≤—ã–π –æ–ø–µ—Ä–∞–Ω–¥ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∏–ø–∞
                        if new_type in ["contains", "not_contains", "equals"]:
                            cond["value"] = st.text_input("–ó–Ω–∞—á–µ–Ω–∏–µ", value=cond.get("value", ""), key=f"cond_val_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type in ["matches", "not_matches"]:
                            cond["pattern"] = st.text_input("Regex", value=cond.get("pattern", ""), key=f"cond_pat_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type == "equals_numeric":
                            right_opts = ["{{reference}}"] + [f"{{{{extracted.{e['name']}}}}}" for e in rule.get("extractors", [])]
                            right_val = cond.get("right", "{{reference}}")
                            if right_val not in right_opts:
                                right_opts.append(right_val)
                            cond["right"] = st.selectbox("–°—Ä–∞–≤–Ω–∏—Ç—å —Å", right_opts, index=right_opts.index(right_val) if right_val in right_opts else 0, key=f"cond_right_{rule['id']}_{ci}", label_visibility="collapsed")
                            cond["tolerance"] = st.number_input("¬±", 0.0, 100.0, float(cond.get("tolerance", 0.01)), 0.01, key=f"cond_tol_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type in ["greater", "less"]:
                            cond["value"] = st.number_input("–ß–∏—Å–ª–æ", value=float(cond.get("value", 0)), key=f"cond_num_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type == "length_between":
                            lc1, lc2 = st.columns(2)
                            cond["min"] = lc1.number_input("–ú–∏–Ω", 0, 100000, int(cond.get("min", 10)), key=f"cond_min_{rule['id']}_{ci}")
                            cond["max"] = lc2.number_input("–ú–∞–∫—Å", 0, 100000, int(cond.get("max", 5000)), key=f"cond_max_{rule['id']}_{ci}")
                        elif new_type == "length_min":
                            cond["min"] = st.number_input("–ú–∏–Ω –¥–ª–∏–Ω–∞", 0, 100000, int(cond.get("min", 10)), key=f"cond_minl_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type == "length_max":
                            cond["max"] = st.number_input("–ú–∞–∫—Å –¥–ª–∏–Ω–∞", 0, 100000, int(cond.get("max", 5000)), key=f"cond_maxl_{rule['id']}_{ci}", label_visibility="collapsed")
                    
                    with ccc4:
                        if st.button("‚úñ", key=f"cond_del_{rule['id']}_{ci}"):
                            rule["conditions"].pop(ci)
                            st.rerun()
            
            # –õ–æ–≥–∏–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π
            lc1, lc2 = st.columns([2, 3])
            with lc1:
                rule["condition_logic"] = st.radio(
                    "–õ–æ–≥–∏–∫–∞",
                    ["all", "any"],
                    format_func=lambda x: "–í–°–ï —É—Å–ª–æ–≤–∏—è (AND)" if x == "all" else "–õ–Æ–ë–û–ï —É—Å–ª–æ–≤–∏–µ (OR)",
                    index=0 if rule.get("condition_logic", "all") == "all" else 1,
                    key=f"cond_logic_{rule['id']}",
                    horizontal=True
                )
            with lc2:
                if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å —É—Å–ª–æ–≤–∏–µ", key=f"add_cond_{rule['id']}"):
                    rule["conditions"].append({"type": "contains", "target": "{{response}}", "value": ""})
                    st.rerun()
            
            st.markdown("---")
            
            # === REWARD FORMULA ===
            st.markdown("##### üéØ –§–æ—Ä–º—É–ª–∞ Reward")
            
            rc1, rc2 = st.columns(2)
            with rc1:
                rule["reward_formula"] = st.text_input(
                    "–ï—Å–ª–∏ —É—Å–ª–æ–≤–∏—è TRUE",
                    value=rule.get("reward_formula", "1.0"),
                    key=f"reward_form_{rule['id']}",
                    help="–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ Python-–≤—ã—Ä–∞–∂–µ–Ω–∏—è: `min(len({{extracted.text}}) / 100, 1.0)`"
                )
            with rc2:
                rule["else_reward"] = st.text_input(
                    "–ï—Å–ª–∏ —É—Å–ª–æ–≤–∏—è FALSE",
                    value=rule.get("else_reward", "0.0"),
                    key=f"else_form_{rule['id']}"
                )
    
    # –†–µ–Ω–¥–µ—Ä–∏–º –≤—Å–µ –ø—Ä–∞–≤–∏–ª–∞
    for idx, rule in enumerate(st.session_state.reward_rules):
        render_rule(rule, idx)
    
    # –ö–Ω–æ–ø–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    st.markdown("---")
    
    col_add1, col_add2, col_add3 = st.columns(3)
    
    with col_add1:
        if st.button("‚ûï –ü—É—Å—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ", type="secondary"):
            new_id = st.session_state.next_rule_id
            st.session_state.next_rule_id += 1
            st.session_state.reward_rules.append({
                "id": new_id,
                "name": f"–ü—Ä–∞–≤–∏–ª–æ {new_id + 1}",
                "enabled": True,
                "weight": 1.0,
                "extractors": [],
                "conditions": [],
                "condition_logic": "all",
                "reward_formula": "1.0",
                "else_reward": "0.0",
            })
            st.rerun()
    
    with col_add2:
        preset_rules = st.selectbox(
            "–î–æ–±–∞–≤–∏—Ç—å —à–∞–±–ª–æ–Ω",
            [
                "-- –≤—ã–±–µ—Ä–∏—Ç–µ --",
                "üîç Regex + —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ",
                "üè∑Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–≥–æ–≤",
                "üìè –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã",
                "üî§ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞",
                "üîÑ –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã",
                "üêç Python —Ñ–æ—Ä–º—É–ª–∞",
            ],
            key="preset_select"
        )
    
    with col_add3:
        if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å —à–∞–±–ª–æ–Ω", type="primary"):
            new_id = st.session_state.next_rule_id
            st.session_state.next_rule_id += 1
            
            if preset_rules == "üîç Regex + —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "Regex –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ", "enabled": True, "weight": 1.0,
                    "extractors": [{"name": "answer", "pattern": r"<answer>\s*(\d+)\s*</answer>", "source": "{{response}}"}],
                    "conditions": [{"type": "equals_numeric", "left": "{{extracted.answer}}", "right": "{{reference}}", "tolerance": 0.01}],
                    "condition_logic": "all", "reward_formula": "1.0", "else_reward": "0.0",
                })
            elif preset_rules == "üè∑Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–≥–æ–≤":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–§–æ—Ä–º–∞—Ç —Ç–µ–≥–æ–≤", "enabled": True, "weight": 1.0,
                    "extractors": [],
                    "conditions": [
                        {"type": "contains", "target": "{{response}}", "value": "<reasoning>"},
                        {"type": "contains", "target": "{{response}}", "value": "</reasoning>"},
                        {"type": "contains", "target": "{{response}}", "value": "<answer>"},
                        {"type": "contains", "target": "{{response}}", "value": "</answer>"},
                    ],
                    "condition_logic": "all", "reward_formula": "1.0", "else_reward": "0.0",
                })
            elif preset_rules == "üìè –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞", "enabled": True, "weight": 0.5,
                    "extractors": [],
                    "conditions": [{"type": "length_between", "target": "{{response}}", "min": 100, "max": 3000}],
                    "condition_logic": "all", "reward_formula": "1.0", "else_reward": "-0.5",
                })
            elif preset_rules == "üî§ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "enabled": True, "weight": 0.3,
                    "extractors": [],
                    "conditions": [
                        {"type": "contains", "target": "{{response}}", "value": "therefore"},
                    ],
                    "condition_logic": "any", "reward_formula": "0.5", "else_reward": "0.0",
                })
            elif preset_rules == "üîÑ –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤", "enabled": True, "weight": 0.5,
                    "extractors": [],
                    "conditions": [{"type": "not_matches", "target": "{{response}}", "pattern": r"(.{20,})\1"}],
                    "condition_logic": "all", "reward_formula": "0.5", "else_reward": "-0.5",
                })
            elif preset_rules == "üêç Python —Ñ–æ—Ä–º—É–ª–∞":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "Python —Ñ–æ—Ä–º—É–ª–∞", "enabled": True, "weight": 1.0,
                    "extractors": [{"name": "reasoning", "pattern": r"<reasoning>(.*?)</reasoning>", "source": "{{response}}", "flags": "DOTALL"}],
                    "conditions": [],
                    "condition_logic": "all", 
                    "reward_formula": "min(len({{extracted.reasoning}}) / 500.0, 1.0) if {{extracted.reasoning}} else 0.0",
                    "else_reward": "0.0",
                })
            else:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —à–∞–±–ª–æ–Ω")
            st.rerun()
    
    # =========================================================================
    # –§–æ—Ä–º–∞—Ç reasoning
    # =========================================================================
    st.markdown("---")
    st.markdown("#### üìù –§–æ—Ä–º–∞—Ç Reasoning")
    
    reasoning_format = st.selectbox(
        "–§–æ—Ä–º–∞—Ç —Ç–µ–≥–æ–≤",
        ["deepseek", "simple", "russian", "gsm8k"],
        format_func=lambda x: {
            "deepseek": "DeepSeek (<think>...</think>, <answer>...</answer>)",
            "simple": "Simple (<reasoning>...</reasoning>, <answer>...</answer>)",
            "russian": "Russian (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ)",
            "gsm8k": "GSM8K (#### <number>)",
        }[x],
    )
    
    # –ü—Ä–µ–≤—å—é —Ñ–æ—Ä–º–∞—Ç–∞
    format_examples = {
        "deepseek": """<think>
–î–∞–Ω–æ: ... 
–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏: ...
–†–µ—à–µ–Ω–∏–µ: ...
</think>
<answer>
42
</answer>""",
        "simple": """<reasoning>
–®–∞–≥ 1: ...
–®–∞–≥ 2: ...
</reasoning>
<answer>
42
</answer>""",
        "russian": """<reasoning>
–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: ...
–í—ã—á–∏—Å–ª–µ–Ω–∏—è: ...
</reasoning>
<answer>
42
</answer>""",
        "gsm8k": """Let me solve this step by step.
Step 1: First, I need to...
Step 2: Then, I calculate...
Therefore, the answer is 42.
#### 42""",
    }
    
    with st.expander("üìã –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞"):
        st.code(format_examples[reasoning_format], language=None)
    
    # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é reward –ø—Ä–∞–≤–∏–ª (–Ω–æ–≤—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
    reward_rules = [
        {
            "name": rule["name"],
            "weight": rule["weight"],
            "enabled": rule.get("enabled", True),
            "extractors": rule.get("extractors", []),
            "conditions": rule.get("conditions", []),
            "condition_logic": rule.get("condition_logic", "all"),
            "reward_formula": rule.get("reward_formula", "1.0"),
            "else_reward": rule.get("else_reward", "0.0"),
        }
        for rule in st.session_state.get("reward_rules", [])
        if rule.get("enabled", True)
    ]
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∞–ø–ø–∏–Ω–≥–∞ –ø–æ–ª–µ–π –∏–∑ session_state
    field_mapping = st.session_state.get("grpo_field_mapping", {
        "prompt_field": "question",
        "reference_field": "answer",
        "metadata_fields": [],
    })
    prompt_template_value = st.session_state.get("grpo_prompt_template", "{{prompt}}")
    system_prompt_value = st.session_state.get("grpo_system_prompt", "")
    
    return {
        "grpo_dataset_source": dataset_source,
        "grpo_dataset_key": dataset_key,
        "grpo_dataset_path": grpo_dataset_path,
        "grpo_dataset_language": grpo_dataset_language,
        "grpo_max_samples": grpo_max_samples if grpo_max_samples > 0 else None,
        "grpo_reward_rules": reward_rules,
        "grpo_reasoning_format": reasoning_format,
        "grpo_system_prompt": system_prompt_value,  # System prompt –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∞–ø–ø–∏–Ω–≥–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        "grpo_field_mapping": field_mapping,
        "grpo_prompt_template": prompt_template_value,
    }


def render_model_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.header("üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –†–µ–∂–∏–º")
    
    # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    stage_options = {
        "pretrain": "Pretraining (—Å –Ω—É–ª—è)",
        "continual_pretrain": "Continual Pretraining (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)",
        "sft": "SFT (Fine-Tuning)",
        "grpo": "üß† GRPO (RL –¥–ª—è Reasoning)"
    }
    selected_stage = st.sidebar.selectbox(
        "–≠—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è",
        options=list(stage_options.keys()),
        format_func=lambda x: stage_options[x],
        help="–í—ã–±–µ—Ä–∏—Ç–µ —ç—Ç–∞–ø: –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è, –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ pretrain, –¥–æ–æ–±—É—á–µ–Ω–∏–µ (SFT) –∏–ª–∏ RL –æ–±—É—á–µ–Ω–∏–µ (GRPO)"
    )
    
    # –ò–º—è –º–æ–¥–µ–ª–∏ (–¥–ª—è –ø–∞–ø–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
    if selected_stage == "pretrain":
        model_name_default = "home_pretrain"
    elif selected_stage == "continual_pretrain":
        model_name_default = "home_continual_pretrain"
    elif selected_stage == "grpo":
        model_name_default = "home_grpo"
    else:
        model_name_default = "home_sft"
    model_name = st.sidebar.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", value=model_name_default, help="–ò–º—è –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    base_model_path = None
    
    if selected_stage in ("sft", "continual_pretrain", "grpo"):
        stage_label = {"sft": "SFT", "continual_pretrain": "Continual Pretraining", "grpo": "GRPO"}.get(selected_stage, selected_stage)
        st.sidebar.subheader("üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        available = get_available_models()
        
        if selected_stage == "continual_pretrain":
            # –î–ª—è continual_pretrain —Ñ–∏–ª—å—Ç—Ä—É–µ–º: –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º final/export –º–æ–¥–µ–ª–∏ –∏ HF –º–æ–¥–µ–ª–∏
            # Checkpoint'—ã —Ç–æ–∂–µ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã (–¥–ª—è resume), –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
            hf_models = [m for m in available if m["type"] == "hf"]
            final_models = [m for m in available if m["type"] == "final"]
            checkpoint_models = [m for m in available if m["type"] == "checkpoint"]
            
            if hf_models or final_models:
                st.sidebar.info("üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ü§ó HF –º–æ–¥–µ–ª—å –∏–ª–∏ final_model –¥–ª—è continual pretraining")
                available_filtered = hf_models + final_models + checkpoint_models
            else:
                if checkpoint_models:
                    st.sidebar.warning(
                        "‚ö†Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ checkpoint'—ã. –î–ª—è resume —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, "
                        "–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ continual pretraining –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å final_model."
                    )
                available_filtered = checkpoint_models if checkpoint_models else available
        else:
            # –î–ª—è SFT –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ (HF –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–º–∏)
            hf_models = [m for m in available if m["type"] == "hf"]
            other_models = [m for m in available if m["type"] != "hf"]
            available_filtered = hf_models + other_models
        
        if not available_filtered:
            st.sidebar.warning(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è {stage_label}. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ ü§ñ –ú–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–±—É—á–∏—Ç–µ Pretrain!")
            # –ú–æ–∂–Ω–æ –¥–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–≤–µ—Å—Ç–∏ –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é
            base_model_path = st.sidebar.text_input("–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –≤—Ä—É—á–Ω—É—é", placeholder="/path/to/model")
        else:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–ø—Ü–∏–π —Å –ø–æ–º–µ—Ç–∫–∞–º–∏ —Ç–∏–ø–æ–≤
            def get_model_label(m):
                if m["type"] == "hf":
                    return m["name"]  # –£–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ü§ó
                elif m["type"] == "final":
                    return f"{m['name']} (‚úÖ final)"
                else:
                    return f"{m['name']} (‚ö†Ô∏è checkpoint)"
            
            model_options = [get_model_label(m) for m in available_filtered]
            
            selected_base_name = st.sidebar.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å", 
                options=model_options,
                help="ü§ó ‚Äî –º–æ–¥–µ–ª–∏ —Å HuggingFace, ‚úÖ final ‚Äî –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, ‚ö†Ô∏è checkpoint ‚Äî –¥–ª—è resume"
            )
            
            # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å –ø–æ –∏–Ω–¥–µ–∫—Å—É (model_options –∏ available_filtered —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥—É)
            selected_idx = model_options.index(selected_base_name)
            selected_model = available_filtered[selected_idx]
            base_model_path = selected_model["path"]
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è checkpoint –≤ continual_pretrain
            if selected_stage == "continual_pretrain" and selected_model["type"] == "checkpoint":
                st.sidebar.info(
                    "‚ÑπÔ∏è –í—ã–±—Ä–∞–Ω checkpoint. –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω resume (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ scheduler). "
                    "–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–æ–≤–æ–≥–æ continual pretraining –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å final_model –∏–ª–∏ ü§ó HF –º–æ–¥–µ–ª—å."
                )
            elif selected_model["type"] == "hf":
                st.sidebar.success("‚úÖ HuggingFace –º–æ–¥–µ–ª—å")
            
            st.sidebar.caption(f"–ü—É—Ç—å: `{base_model_path}`")
    
    # –§–ª–∞–≥, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    loaded_config = None
    
    if selected_stage in ("sft", "continual_pretrain", "grpo") and base_model_path:
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥
        # –í–ê–ñ–ù–û: —Ä–∞–∑–ª–∏—á–∞–µ–º run_config.json (training params) –∏ config.json (model params)
        try:
            base_path = Path(base_model_path)
            cfg_path = None
            cfg_type = None  # "run" –∏–ª–∏ "model"
            
            # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º run_config.json (–ø–æ–ª–Ω—ã–π training config)
            # –î–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: checkpoint_step6800 -> parent/run_config.json
            if (base_path.parent / "run_config.json").exists():
                cfg_path = base_path.parent / "run_config.json"
                cfg_type = "run"
            elif (base_path / "run_config.json").exists():
                cfg_path = base_path / "run_config.json"
                cfg_type = "run"
            # 2. –ï—Å–ª–∏ run_config –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º config.json (—Ç–æ–ª—å–∫–æ model params)
            elif (base_path / "config.json").exists():
                cfg_path = base_path / "config.json"
                cfg_type = "model"
            
            if cfg_path and cfg_path.exists():
                with open(cfg_path) as f:
                    loaded_config = json.load(f)
                if cfg_type == "run":
                    st.sidebar.success("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ run_config.json")
                else:
                    # config.json —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–∏
                    # transformers config -> –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
                    if "num_hidden_layers" in loaded_config and "num_layers" not in loaded_config:
                        loaded_config["num_layers"] = loaded_config["num_hidden_layers"]
                    if "num_attention_heads" in loaded_config and "n_heads" not in loaded_config:
                        loaded_config["n_heads"] = loaded_config["num_attention_heads"]
                    if "max_position_embeddings" in loaded_config and "seq_len" not in loaded_config:
                        loaded_config["seq_len"] = loaded_config["max_position_embeddings"]
                    st.sidebar.info("‚ÑπÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ config.json (training params –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)")
            else:
                st.sidebar.warning("‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ä—É—á–Ω—É—é")
        except Exception as e:
             st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è config: {e}")

    st.sidebar.subheader("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")

    blueprint_path = ""
    model_type = "HomeModel (GPT-2 style)"  # default
    # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    default_h, default_l, default_n, default_seq = 512, 8, 8, 2048

    # –í—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢–û–õ–¨–ö–û –¥–ª—è pretrain (—Å –Ω—É–ª—è)
    # –î–ª—è SFT/Continual Pretrain –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
    if not loaded_config:
        # === –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° VISUAL MODEL BUILDER ===
        blueprints_dir = PROJECT_ROOT / "blueprints"
        blueprints_dir.mkdir(exist_ok=True)
        blueprints = list(blueprints_dir.glob("*.json"))
            
        arch_options = ["HomeModel (GPT-2 style)", "Llama (Custom)", "Mistral (Custom)", "Custom Blueprint (Visual Builder)"]
        
        model_type = st.sidebar.selectbox(
            "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏",
            options=arch_options,
            index=0,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É. Custom Blueprint –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑ Visual Builder."
        )

    if not loaded_config and model_type == "Custom Blueprint (Visual Builder)":
        blueprints_dir = PROJECT_ROOT / "blueprints"
        blueprints = list(blueprints_dir.glob("*.json"))
        # –õ–æ–≥–∏–∫–∞ –¥–ª—è Blueprint –ø—Ä–æ–µ–∫—Ç–æ–≤
        if blueprints:
            bp_names = [b.name for b in blueprints]
            selected_bp = st.sidebar.selectbox("–ü—Ä–æ–µ–∫—Ç (Visual Builder)", bp_names)
            blueprint_path = str(blueprints_dir / selected_bp)
        else:
            st.sidebar.warning("Blueprint –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–ø–∫–µ `blueprints/`. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç –≤ Visual Model Builder –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é.")
            blueprint_path = st.sidebar.text_input("–ü—É—Ç—å –∫ blueprint.json", value=str(blueprints_dir / "model.json"))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ –∏–∑ blueprint
        try:
            with open(blueprint_path) as f:
                bp_data = json.load(f)
            st.sidebar.info(f"Blocks: {len(bp_data.get('blocks', []))} | Hidden: {bp_data.get('hidden_size')} | Vocab: {bp_data.get('vocab_size')}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç—ã –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–æ–≤ –Ω–∏–∂–µ (—á—Ç–æ–±—ã –æ–Ω–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏)
            default_h = bp_data.get("hidden_size", 512)
            default_seq = bp_data.get("max_position_embeddings", 2048)
            # –°–ª–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–∏ –∏–∑ Repeater, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –≥—Ä—É–±–æ
            default_l = len(bp_data.get("blocks", []))
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            st.sidebar.markdown("**–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä**")
            tokenizer_mode = st.sidebar.radio("Tokenizer Source", ["Standard (GPT-2)", "HF Repo", "Local Path"], horizontal=True)
            if tokenizer_mode == "Standard (GPT-2)":
                tokenizer_path = "gpt2"
            elif tokenizer_mode == "HF Repo":
                tokenizer_path = st.sidebar.text_input("HF ID", "meta-llama/Llama-2-7b-hf")
            else:
                tokenizer_path = st.sidebar.text_input("Local Path", str(PROJECT_ROOT / "tokenizers/my_tok"))
                
        except Exception as e:
            st.sidebar.error(f"Invalid Blueprint: {e}")
            tokenizer_path = "gpt2"
    elif not loaded_config:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è pretrain
        tokenizer_path = None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ
    else:
        # –î–ª—è SFT/Continual Pretrain —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±–µ—Ä–µ—Ç—Å—è –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        tokenizer_path = None

    if loaded_config:
        # –†–µ–∂–∏–º –¥–ª—è SFT/Continual Pretrain —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º
        # –°–ª–∞–π–¥–µ—Ä—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã
        disabled_sliders = True
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö –∏–º–µ–Ω –∫–ª—é—á–µ–π)
        hidden_size = loaded_config.get("hidden_size", 512)
        # num_hidden_layers - HF, num_layers - –Ω–∞—à –∫–æ–Ω—Ñ–∏–≥
        num_layers = loaded_config.get("num_hidden_layers", loaded_config.get("num_layers", 8))
        num_attention_heads = loaded_config.get("num_attention_heads", loaded_config.get("n_heads", 8))
        max_position_embeddings = loaded_config.get("max_position_embeddings", loaded_config.get("seq_len", 2048))
        
        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç—Å—è
        n_heads = num_attention_heads
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        base_model_type = loaded_config.get("model_type", loaded_config.get("architectures", ["Unknown"])[0] if "architectures" in loaded_config else "HomeModel")
        if isinstance(base_model_type, list):
            base_model_type = base_model_type[0] if base_model_type else "Unknown"
        st.sidebar.markdown(f"**–¢–∏–ø –º–æ–¥–µ–ª–∏:** `{base_model_type}`")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        c1, c2 = st.sidebar.columns(2)
        c1.metric("Hidden Size", hidden_size)
        c2.metric("Layers", num_layers)
        c1.metric("Heads", n_heads)
        c2.metric("Max Context", f"{max_position_embeddings:,}")
        
        st.sidebar.info("üîí –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞ (–æ—Ç –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏)")
        
        # seq_len –ú–û–ñ–ù–û –º–µ–Ω—è—Ç—å - —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –º–µ–Ω—å—à–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        st.sidebar.markdown("---")
        st.sidebar.markdown("**‚öôÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è**")
        
        # –û–ø—Ü–∏–∏ seq_len: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ + –º–∞–∫—Å–∏–º—É–º –º–æ–¥–µ–ª–∏
        seq_len_opts = [512, 1024, 2048, 4096, 8192]
        if max_position_embeddings not in seq_len_opts:
            seq_len_opts.append(max_position_embeddings)
        seq_len_opts = sorted([s for s in seq_len_opts if s <= max_position_embeddings])
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048 –∏–ª–∏ –º–∞–∫—Å–∏–º—É–º –µ—Å–ª–∏ –º–µ–Ω—å—à–µ
        default_seq = min(2048, max_position_embeddings)
        default_idx = seq_len_opts.index(default_seq) if default_seq in seq_len_opts else len(seq_len_opts) - 1
        
        seq_len = st.sidebar.selectbox(
            "Seq Length (–æ–±—É—á–µ–Ω–∏–µ)",
            seq_len_opts,
            index=default_idx,
            help=f"–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–∞–∫—Å. –º–æ–¥–µ–ª–∏: {max_position_embeddings:,}. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏."
        )
        
        if seq_len < max_position_embeddings:
            st.sidebar.caption(f"üí° –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {seq_len}, –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ {max_position_embeddings:,}")
        
    else:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        d_hid, d_layers = 512, 8
        
        if selected_stage == "sft":
            st.sidebar.caption("‚ö†Ô∏è –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é!")

        # –ü—Ä–µ—Å–µ—Ç—ã
        preset = st.sidebar.selectbox(
            "–ü—Ä–µ—Å–µ—Ç",
            ["Tiny (25M)", "Small (80M)", "Medium (200M)", "Large (400M)", "Custom"],
            index=0
        )
        
        presets = {
            "Tiny (25M)": (512, 8, 8),
            "Small (80M)": (768, 12, 12),
            "Medium (200M)": (1024, 16, 16),
            "Large (400M)": (1280, 20, 20),
        }
        
        if preset != "Custom" and preset in presets:
            default_h, default_l, default_n = presets[preset]
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ blueprint mode, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ blueprint –∫–∞–∫ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–æ–≤
            # –ù–æ –Ω–µ –º–µ–Ω—è–µ–º default_h/l/n –≥–ª–æ–±–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å –ª–æ–≥–∏–∫—É –ø—Ä–µ—Å–µ—Ç–æ–≤ –µ—Å–ª–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ
            if model_type != "Custom Blueprint (Visual Builder)":
                 default_h, default_l, default_n = 512, 8, 8
        
        # –°–ª–∞–π–¥–µ—Ä—ã (—Ç–µ–ø–µ—Ä—å –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª—é–ø—Ä–∏–Ω—Ç–∞ –∏–ª–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –º–µ–Ω—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã HomeModel)
        disabled_sliders = (model_type == "Custom Blueprint (Visual Builder)")
        
        hidden_size = st.sidebar.slider(
            "Hidden Size", 
            min_value=128, 
            max_value=2048, 
            value=default_h, 
            step=64,
            help="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è",
            disabled=disabled_sliders
        )
        
        num_layers = st.sidebar.slider(
            "Num Layers", 
            min_value=2, 
            max_value=32, 
            value=default_l,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞",
            disabled=disabled_sliders
        )
        
        n_heads = st.sidebar.slider(
            "Attention Heads", 
            min_value=2, 
            max_value=32, 
            value=default_n,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è",
            disabled=disabled_sliders
        )
        
        seq_len_opts = [512, 1024, 2048, 4096, 6144, 8192]
        if default_seq not in seq_len_opts: seq_len_opts.append(default_seq)
        seq_len_opts = sorted(seq_len_opts)
        
        seq_len = st.sidebar.selectbox(
            "Seq Length",
            seq_len_opts,
            index=seq_len_opts.index(default_seq) if default_seq in seq_len_opts else 0,
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
            disabled=disabled_sliders
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ hidden_size –∏ n_heads
    if not disabled_sliders and hidden_size % n_heads != 0:
        st.sidebar.error(f"‚ö†Ô∏è hidden_size ({hidden_size}) –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ n_heads ({n_heads}) –±–µ–∑ –æ—Å—Ç–∞—Ç–∫–∞!")
        valid_heads = [str(i) for i in range(1, min(33, hidden_size+1)) if hidden_size % i == 0][:10]
        if valid_heads:
            st.sidebar.info(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è n_heads: {', '.join(valid_heads)}")
    
    # –û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (HomeModel RoPE/SwiGLU)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º loaded_config –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –¥–µ—Ñ–æ–ª—Ç—ã
    vocab_size = int(loaded_config.get("vocab_size", 50257)) if loaded_config else 50257
    intermediate_size = int(loaded_config.get("intermediate_size") or (int(hidden_size) * 4)) if loaded_config else (int(hidden_size) * 4)
    est_params = estimate_parameters(
        hidden_size,
        num_layers,
        vocab_size=vocab_size,
        intermediate_size=intermediate_size,
    )
    st.sidebar.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (‚âà)", format_params(est_params))
    
    # Model ID –¥–ª—è pretrain from scratch (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è HF –º–æ–¥–µ–ª–µ–π)
    model_id = None
    if selected_stage == "pretrain":
        st.sidebar.subheader("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
        use_hf_model = st.sidebar.checkbox(
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å HuggingFace –º–æ–¥–µ–ª—å",
            value=False,
            help="–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å HF model_id –¥–ª—è pretrain from scratch"
        )
        if use_hf_model:
            model_id = st.sidebar.text_input(
                "HF Model ID",
                placeholder="gpt2, microsoft/DialoGPT-small, etc.",
                help="HuggingFace model ID –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –Ω—É–ª—è"
            )
            if model_id:
                st.sidebar.info(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {model_id}")
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                st.sidebar.warning(
                    "‚ö†Ô∏è **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å `trust_remote_code=True` –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å "
                    "—á—É–∂–æ–π –∫–æ–¥. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏."
                )
    
    # === Training Backend (—Ç–æ–ª—å–∫–æ –¥–ª—è GRPO!) ===
    # –î–ª—è pretrain/sft –∏—Å–ø–æ–ª—å–∑—É–µ–º models-at-home –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if selected_stage == "grpo":
        st.sidebar.subheader("‚öôÔ∏è Training Backend")
        
        backend_options = ["üè† models-at-home", "ü¶• Unsloth"]
        selected_backend_display = st.sidebar.radio(
            "Backend",
            backend_options,
            index=0,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é models-at-home
            help=(
                "**üè† models-at-home** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):\n"
                "‚Ä¢ Multi-GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞ (DDP)\n"
                "‚Ä¢ Full fine-tuning + LoRA/QLoRA\n"
                "‚Ä¢ FlashAttention + Liger Kernels\n\n"
                "**ü¶• Unsloth** (–±—ã—Å—Ç—Ä–µ–µ –Ω–∞ 1 GPU):\n"
                "‚Ä¢ ‚ö° 2x –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ **–æ–¥–Ω–æ–π GPU**\n"
                "‚Ä¢ üíæ –î–æ 70% –º–µ–Ω—å—à–µ VRAM\n"
                "‚Ä¢ ‚ö†Ô∏è **–¢–æ–ª—å–∫–æ LoRA/QLoRA** (–Ω–µ full)\n"
                "‚Ä¢ ‚ö†Ô∏è Multi-GPU: —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –±–µ–∑ fast_inference"
            ),
            key="training_backend_radio"
        )
        training_backend = "unsloth" if "Unsloth" in selected_backend_display else "models-at-home"
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ backend
        if training_backend == "unsloth":
            try:
                import unsloth
                st.sidebar.success("ü¶• **Unsloth**: 2x –±—ã—Å—Ç—Ä–µ–µ, 70% –º–µ–Ω—å—à–µ VRAM")
                st.sidebar.caption("‚ö†Ô∏è –¢–æ–ª—å–∫–æ LoRA/QLoRA ‚Ä¢ 1 GPU –¥–ª—è fast_inference")
            except ImportError:
                st.sidebar.error("ü¶• **Unsloth –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!**")
        else:
            st.sidebar.info("üè† **models-at-home**: Multi-GPU, Full/LoRA/QLoRA")
        
        st.sidebar.markdown("---")
    else:
        # –î–ª—è pretrain/sft ‚Äî –≤—Å–µ–≥–¥–∞ models-at-home
        training_backend = "models-at-home"
    
    # –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞ (full/LoRA/QLoRA)
    st.sidebar.subheader("üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞")
    
    # –ü—Ä–∏ Unsloth (—Ç–æ–ª—å–∫–æ GRPO) –¥–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ lora/qlora
    if training_backend == "unsloth":
        tuning_options = ["lora", "qlora"]
        tuning_index = 0  # lora –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        tuning_help = "Unsloth –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ LoRA –∏ QLoRA (–Ω–µ full fine-tuning)"
    else:
        tuning_options = ["full", "lora", "qlora"]
        tuning_index = 0  # full –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        tuning_help = "full: –ø–æ–ª–Ω—ã–π fine-tuning, lora: LoRA, qlora: QLoRA (4-bit + LoRA)"
    
    tuning_method = st.sidebar.selectbox(
        "–ú–µ—Ç–æ–¥",
        tuning_options,
        index=tuning_index,
        help=tuning_help
    )
    
    lora_r = None
    lora_alpha = None
    lora_dropout = None
    lora_target_modules = None
    
    if tuning_method in ("lora", "qlora"):
        st.sidebar.markdown("**LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**")
        lora_r = st.sidebar.slider(
            "LoRA r (rank)", 
            min_value=8, max_value=128, value=32, step=8,
            help="–†–∞–Ω–≥ LoRA –º–∞—Ç—Ä–∏—Ü. **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è ‚â•32** –¥–ª—è —Ö–æ—Ä–æ—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. "
                 "–ü—Ä–∏ rank=32 –¥–ª—è 0.5B-3B –º–æ–¥–µ–ª–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ—á—Ç–∏ –∫–∞–∫ full fine-tuning."
        )
        lora_alpha = st.sidebar.slider(
            "LoRA alpha", 
            min_value=8, max_value=256, value=32, step=8,
            help="Scaling factor. –û–±—ã—á–Ω–æ = lora_r –∏–ª–∏ 2√ólora_r"
        )
        lora_dropout = st.sidebar.slider("LoRA dropout", min_value=0.0, max_value=0.5, value=0.05, step=0.05)
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ target_modules –¥–ª—è LLaMA/Qwen/Mistral-–ø–æ–¥–æ–±–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
        LORA_TARGET_MODULES = [
            "q_proj",      # Query projection (Attention)
            "k_proj",      # Key projection (Attention)
            "v_proj",      # Value projection (Attention)
            "o_proj",      # Output projection (Attention)
            "gate_proj",   # Gate projection (MLP/SwiGLU)
            "up_proj",     # Up projection (MLP)
            "down_proj",   # Down projection (MLP)
            "lm_head",     # Output head (logits) ‚Äî –û–ì–†–û–ú–ù–´–ô!
            "embed_tokens", # Input embeddings ‚Äî –û–ì–†–û–ú–ù–´–ô!
        ]
        
        # –î–µ—Ñ–æ–ª—Ç: all-linear (attention + MLP) ‚Äî –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç verl
        # –≠—Ç–æ –¥–∞—ë—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ—á—Ç–∏ –∫–∞–∫ full fine-tuning
        default_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        
        lora_target_modules = st.sidebar.multiselect(
            "üéØ Target modules",
            options=LORA_TARGET_MODULES,
            default=default_modules,
            help="""**–ö–∞–∫–∏–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å —á–µ—Ä–µ–∑ LoRA:**

**Attention:**
‚Ä¢ `q_proj` ‚Äî Query projection  
‚Ä¢ `k_proj` ‚Äî Key projection  
‚Ä¢ `v_proj` ‚Äî Value projection  
‚Ä¢ `o_proj` ‚Äî Output projection  

**MLP/FFN:**
‚Ä¢ `gate_proj` ‚Äî Gate (SwiGLU)
‚Ä¢ `up_proj` ‚Äî Up projection
‚Ä¢ `down_proj` ‚Äî Down projection

**Output/Input (‚ö†Ô∏è –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ!):**
‚Ä¢ `lm_head` ‚Äî Output head. **–û–ì–†–û–ú–ù–´–ô!**
‚Ä¢ `embed_tokens` ‚Äî Input embeddings. **–û–ì–†–û–ú–ù–´–ô!**

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–∏–∑ verl):**
- **all-linear** (–≤—Å–µ 7 —Å–ª–æ—ë–≤) ‚Äî —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å ‚âà full fine-tuning!
- –¢–æ–ª—å–∫–æ attention (4 —Å–ª–æ—è) ‚Äî –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ
- rank‚â•32 + all-linear = –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å

‚ö†Ô∏è **lm_head/embed_tokens:**
- –¢–æ–ª—å–∫–æ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –Ω–æ–≤–æ–º—É –¥–æ–º–µ–Ω—É/—è–∑—ã–∫—É
- –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –≤ Multi-GPU (find_unused_parameters)"""
        )
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–±—Ä–∞–Ω–æ ‚Äî –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç
        if not lora_target_modules:
            lora_target_modules = None
    
    # –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
    config = {
        "stage": selected_stage,
        "training_backend": training_backend,  # "models-at-home" –∏–ª–∏ "unsloth"
        "base_model_path": base_model_path,
        "model_name_input": model_name,
        "model_id": model_id if model_id else None,
        "tuning_method": tuning_method,
        "lora_r": lora_r,
        "lora_alpha": lora_alpha,
        "lora_dropout": lora_dropout,
        "lora_target_modules": lora_target_modules,
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–∞–π–¥–µ—Ä–æ–≤ (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "n_heads": n_heads,
        "seq_len": seq_len,
    }

    if model_type == "Custom Blueprint (Visual Builder)":
        config["model_type"] = "blueprint"
        config["blueprint_path"] = blueprint_path
        if tokenizer_path:
             config["tokenizer_path"] = tokenizer_path
    else:
        config["model_type"] = "home"
        if "Llama" in model_type: config["arch_preset"] = "llama"
        if "Mistral" in model_type: config["arch_preset"] = "mistral"
        
    return config


def render_training_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è."""
    st.sidebar.header("üìà –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    
    batch_size = st.sidebar.slider(
        "Batch Size",
        min_value=1,
        max_value=256,
        value=4,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞"
    )
    
    grad_accum = st.sidebar.slider(
        "Gradient Accumulation",
        min_value=1,
        max_value=32,
        value=8,
        help="–®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞"
    )
    
    st.sidebar.caption(f"Effective batch: {batch_size * grad_accum}")
    
    learning_rate = st.sidebar.select_slider(
        "Learning Rate",
        options=[1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3],
        value=5e-4,
        format_func=lambda x: f"{x:.0e}"
    )

    lr_schedule_label = st.sidebar.selectbox(
        "LR scheduler",
        options=[
            "Cosine (with warmup)",
            "Linear (with warmup)",
            "Constant (with warmup)",
            "Cosine with Restarts (with warmup)",
        ],
        index=0,
        help=(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –º–µ–Ω—è—Ç—å learning rate –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è. "
            "–í–∞–∂–Ω–æ: scheduler —à–∞–≥–∞–µ—Ç –Ω–∞ update-step (–ø–æ—Å–ª–µ grad_accum), –Ω–µ –Ω–∞ –∫–∞–∂–¥–æ–º micro-batch."
        ),
    )
    lr_schedule_map = {
        "Cosine (with warmup)": "cosine",
        "Linear (with warmup)": "linear",
        "Constant (with warmup)": "constant_with_warmup",
        "Cosine with Restarts (with warmup)": "cosine_with_restarts",
    }
    lr_schedule = lr_schedule_map[lr_schedule_label]

    min_lr_ratio = st.sidebar.slider(
        "Min LR Ratio (Cosine floor)",
        min_value=0.0,
        max_value=0.2,
        value=0.0,
        step=0.01,
        help="0.0 = cosine –º–æ–∂–µ—Ç —É–π—Ç–∏ –ø–æ—á—Ç–∏ –≤ 0 –∫ –∫–æ–Ω—Ü—É. –ù–∞–ø—Ä–∏–º–µ—Ä 0.05 = –Ω–µ –Ω–∏–∂–µ 5% –æ—Ç base LR."
    )
    
    warmup_steps = st.sidebar.number_input(
        "Warmup Steps",
        min_value=0,
        max_value=10000,
        value=1000
    )

    scheduler_resync_on_resume = st.sidebar.checkbox(
        "Resync LR scheduler –ø—Ä–∏ resume (—Ñ–∏–∫—Å –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤)",
        value=True,
        help=(
            "–ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –±—ã–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å–æ scheduler, –∫–æ—Ç–æ—Ä—ã–π —à–∞–≥–∞–ª –ø–æ micro-batch (–∞ –Ω–µ –ø–æ update-step), "
            "LR –ø—Ä–∏ resume –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –ø–æ—á—Ç–∏ 0. –≠—Ç–æ—Ç —Ñ–ª–∞–≥ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç scheduler –Ω–∞ global_step."
        ),
    )
    
    # –í—ã–±–æ—Ä: epochs –∏–ª–∏ max_steps
    training_mode = st.sidebar.radio(
        "–†–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏",
        ["–ü–æ —ç–ø–æ—Ö–∞–º", "–ü–æ —à–∞–≥–∞–º"],
        help="–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏"
    )
    
    if training_mode == "–ü–æ —ç–ø–æ—Ö–∞–º":
        epochs = st.sidebar.number_input(
            "Epochs",
            min_value=1,
            max_value=10,
            value=1
        )
        max_steps = None
    else:
        epochs = 1
        max_steps = st.sidebar.number_input(
            "Max Steps",
            min_value=1,
            max_value=1000000,
            value=10000,
            step=1000,
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
        )
    
    max_grad_norm = st.sidebar.number_input(
        "Max Gradient Norm",
        min_value=0.0,
        max_value=10.0,
        value=1.0,
        step=0.1,
        help="Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (0 = –æ—Ç–∫–ª—é—á–∏—Ç—å)"
    )
    
    return {
        "batch_size": batch_size,
        "gradient_accumulation": grad_accum,
        "learning_rate": learning_rate,
        "lr_schedule": lr_schedule,
        "min_lr_ratio": min_lr_ratio,
        "warmup_steps": warmup_steps,
        "scheduler_resync_on_resume": scheduler_resync_on_resume,
        "epochs": epochs,
        "max_steps": max_steps,
        "max_grad_norm": max_grad_norm,
    }


def render_dataset_config(stage="pretrain"):
    """–í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    st.sidebar.header("üìÅ –î–∞–Ω–Ω—ã–µ")
    
    datasets = get_available_datasets()
    
    if datasets:
        dataset_options = [f"{name} ({size})" for name, size in datasets]
        selected = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç", dataset_options)
        selected_name = selected.split(" (")[0]
        data_path = str(DATASET_DIR / selected_name)
    else:
        st.sidebar.warning("–î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ datasets/")
        data_path = st.sidebar.text_input("–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É", "datasets/data.jsonl")

    # Sharding mode: –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥–≤–æ–π–Ω–æ–≥–æ —à–∞—Ä–¥–∏–Ω–≥–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É resume
    st.sidebar.divider()
    st.sidebar.subheader("üß© –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ")
    sharding_mode = st.sidebar.selectbox(
        "Sharding mode",
        options=["auto", "dataset", "accelerate"],
        index=0,
        help=(
            "auto: –¥–ª—è streaming (IterableDataset) –≤—ã–±–∏—Ä–∞–µ–º dataset-level —à–∞—Ä–¥–∏–Ω–≥ (—Å—Ç—Ä–æ–≥–æ –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å strict resume).\n"
            "dataset: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç —Å–∞–º –¥–∞—Ç–∞—Å–µ—Ç (shard=True), DataLoader –ù–ï –≥–æ—Ç–æ–≤–∏–º —á–µ—Ä–µ–∑ accelerate.\n"
            "accelerate: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç accelerate.prepare(DataLoader); —Å—Ç—Ä–æ–≥–∏–π resume –¥–ª—è streaming –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è."
        ),
    )
    
    # Validation / Eval
    st.sidebar.divider()
    st.sidebar.subheader("üìä –í–∞–ª–∏–¥–∞—Ü–∏—è")
    
    val_ratio = st.sidebar.slider(
        "Validation fraction",
        min_value=0.0,
        max_value=0.2,
        value=0.01,
        step=0.005,
        help="–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ validation, –µ—Å–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π val-—Ñ–∞–π–ª –Ω–µ –∑–∞–¥–∞–Ω"
    )
    
    eval_every = st.sidebar.number_input(
        "Eval Every N Steps",
        min_value=0,
        max_value=50000,
        value=200,
        step=10,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é (0 = –æ—Ç–∫–ª—é—á–∏—Ç—å)"
    )
    
    eval_batches = st.sidebar.number_input(
        "Eval Batches",
        min_value=1,
        max_value=500,
        value=20,
        step=1,
        help="–°–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –ø—Ä–æ–≥–æ–Ω—è—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ)"
    )
    
    return {
        "data_path": data_path,
        "sharding_mode": sharding_mode,
        "val_ratio": val_ratio,
        "eval_every": eval_every,
        "eval_batches": eval_batches,
    }


def render_output_config(model_name="training_run"):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞."""
    st.sidebar.header("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å: out/{model_name}
    default_dir = f"out/{model_name}"
    
    output_dir = st.sidebar.text_input(
        "Output Directory (Experiment Root)",
        value=default_dir,
        help="–ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ —ç—Ç–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
    )
    
    save_every = st.sidebar.number_input(
        "Save Checkpoint Every N Steps",
        min_value=100,
        max_value=50000,
        value=200,
        step=100,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã"
    )

    export_on_checkpoint = st.sidebar.checkbox(
        "–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å final_model –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞",
        value=True,
        help=(
            "–ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å `final_model/` –Ω–∞ –∫–∞–∂–¥–æ–º checkpoint, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –±—ã–ª–æ —Å—Ä–∞–∑—É –≥—Ä—É–∑–∏—Ç—å –≤ —á–∞—Ç. "
            "–ú–∏–Ω—É—Å: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ."
        ),
    )
    
    merge_lora = st.sidebar.checkbox(
        "Merge LoRA –≤ final_model",
        value=True,
        help=(
            "–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –±—É–¥—É—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ final_model. "
            "–≠—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç inference (–º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∫–∞–∫ –æ–±—ã—á–Ω–∞—è HF –º–æ–¥–µ–ª—å), –Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞. "
            "–ï—Å–ª–∏ –≤—ã–∫–ª—é—á–µ–Ω–æ, —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ LoRA –≤–µ—Å–∞ (—Ç—Ä–µ–±—É–µ—Ç PEFT –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏)."
        ),
    )
    
    log_every = st.sidebar.number_input(
        "Log Every N Steps",
        min_value=1,
        max_value=1000,
        value=10,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –º–µ—Ç—Ä–∏–∫–∏"
    )
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞—Ö
    output_path = PROJECT_ROOT / output_dir
    if output_path.exists():
        checkpoints = list(output_path.rglob("checkpoint_step*"))
        final_models = list(output_path.rglob("final_model"))
        final_model = final_models[0] if final_models else None
        
        if checkpoints or final_model:
            st.sidebar.caption(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {len(checkpoints)}")
            if final_model and final_model.exists():
                st.sidebar.caption("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    return {
        "output_dir": output_dir,
        "save_every": save_every,
        "export_on_checkpoint": export_on_checkpoint,
        "merge_lora": merge_lora,
        "log_every": log_every,
        "tokenizer_path": "gpt2"
    }


def get_available_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫).
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã:
    - Pretrain –º–æ–¥–µ–ª–∏ (home_pretrain/)
    - SFT –º–æ–¥–µ–ª–∏ (home_sft/)  
    - GRPO/RL –º–æ–¥–µ–ª–∏ (home_grpo/, home_rl/)
    - LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (adapter_config.json)
    - HuggingFace –º–æ–¥–µ–ª–∏ (models/)
    """
    models = []
    
    def detect_training_type(model_dir: Path) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –ø–æ –ø—É—Ç–∏."""
        path_str = str(model_dir).lower()
        if "grpo" in path_str or "_rl" in path_str:
            return "grpo"
        elif "sft" in path_str:
            return "sft"
        elif "pretrain" in path_str:
            return "pretrain"
        return "unknown"
    
    def is_lora_model(model_dir: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–º."""
        return (model_dir / "adapter_config.json").exists()
    
    def get_model_info(model_dir: Path) -> dict:
        """–ß–∏—Ç–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤."""
        info = {"max_context": None, "vocab_size": None, "hidden_size": None, "num_params": None}
        
        # –ß–∏—Ç–∞–µ–º config.json
        config_path = model_dir / "config.json"
        if config_path.exists():
            try:
                with open(config_path) as f:
                    cfg = json.load(f)
                info["max_context"] = cfg.get("max_position_embeddings") or cfg.get("n_positions") or cfg.get("max_seq_len")
                info["vocab_size"] = cfg.get("vocab_size")
                info["hidden_size"] = cfg.get("hidden_size") or cfg.get("n_embd")
                info["model_type"] = cfg.get("model_type", "unknown")
            except:
                pass
        
        # –î–ª—è LoRA —á–∏—Ç–∞–µ–º adapter_config.json
        adapter_config_path = model_dir / "adapter_config.json"
        if adapter_config_path.exists():
            try:
                with open(adapter_config_path) as f:
                    adapter_cfg = json.load(f)
                info["base_model"] = adapter_cfg.get("base_model_name_or_path")
                info["lora_r"] = adapter_cfg.get("r")
                info["lora_alpha"] = adapter_cfg.get("lora_alpha")
            except:
                pass
        
        return info
    
    def has_model_weights(model_dir: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏."""
        return (
            (model_dir / "pytorch_model.bin").exists() or 
            (model_dir / "model.safetensors").exists() or
            (model_dir / "adapter_model.bin").exists() or
            (model_dir / "adapter_model.safetensors").exists() or
            any(model_dir.glob("model-*.safetensors")) or  # Sharded –º–æ–¥–µ–ª–∏
            any(model_dir.glob("pytorch_model-*.bin"))
        )
    
    # 1. –ò—â–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤ out/ (–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)
    if OUTPUT_DIR.exists():
        # –ò—â–µ–º config.json –∏ adapter_config.json
        config_files = list(OUTPUT_DIR.rglob("config.json")) + list(OUTPUT_DIR.rglob("adapter_config.json"))
        seen_dirs = set()
        
        for config_file in config_files:
            model_dir = config_file.parent
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if str(model_dir) in seen_dirs:
                continue
            seen_dirs.add(str(model_dir))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤
            if not has_model_weights(model_dir):
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø
            is_lora = is_lora_model(model_dir)
            training_type = detect_training_type(model_dir)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ (final/checkpoint)
            m_type = "checkpoint" if "checkpoint" in model_dir.name.lower() else "final"
            if model_dir.name == "final_model":
                m_type = "final"
            elif model_dir.name == "lora_adapters":
                m_type = "lora"
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_info = get_model_info(model_dir)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è
            rel_path = model_dir.relative_to(OUTPUT_DIR)
            
            # –≠–º–æ–¥–∑–∏ –ø–æ —Ç–∏–ø—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
            type_emoji = {
                "pretrain": "üìö",
                "sft": "üí¨", 
                "grpo": "üß†",
                "unknown": "üì¶"
            }.get(training_type, "üì¶")
            
            lora_badge = " [LoRA]" if is_lora else ""
            
            models.append({
                "name": f"{type_emoji} {rel_path}{lora_badge}",
                "path": str(model_dir),
                "type": m_type,
                "training_type": training_type,
                "is_lora": is_lora,
                "model_info": model_info,
                "time": model_dir.stat().st_mtime
            })
    
    # 2. –ò—â–µ–º –≤ models/ (—Å–∫–∞—á–∞–Ω–Ω—ã–µ —Å HuggingFace)
    if MODELS_DIR.exists():
        for model_dir in MODELS_DIR.iterdir():
            if model_dir.is_dir():
                if not has_model_weights(model_dir):
                    continue
                    
                # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å config.json
                if not (model_dir / "config.json").exists():
                    continue
                
                model_info = get_model_info(model_dir)
                
                models.append({
                    "name": f"ü§ó {model_dir.name}",
                    "path": str(model_dir),
                    "type": "hf",
                    "training_type": "base",
                    "is_lora": False,
                    "model_info": model_info,
                    "time": model_dir.stat().st_mtime
                })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
    models.sort(key=lambda x: x["time"], reverse=True)
    return models


def render_distributed_config(training_config: dict | None = None, is_grpo: bool = False, grpo_backend: str | None = None):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GPU, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∏ –ø–∞–º—è—Ç–∏."""
    st.sidebar.header("üñ•Ô∏è GPU –∏ –ü–∞–º—è—Ç—å")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    gpus = get_gpu_info()
    
    if gpus:
        st.sidebar.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ GPU: {len(gpus)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ GPU
        for gpu in gpus:
            st.sidebar.markdown(f"""
            **GPU {gpu['id']}**: {gpu['name']}  
            üìä VRAM: {gpu['memory_gb']} GB | CC: {gpu['compute_capability']}
            """)
        
        # –í—ã–±–æ—Ä GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        gpu_options = [f"GPU {g['id']}: {g['name']}" for g in gpus]
        if len(gpus) > 1:
            selected_gpus = st.sidebar.multiselect(
                "–í—ã–±–µ—Ä–∏—Ç–µ GPU",
                options=gpu_options,
                default=gpu_options,
                help="–í—ã–±–µ—Ä–∏—Ç–µ GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è",
                key="gpu_select_multiselect"
            )
            num_gpus = len(selected_gpus)
            gpu_ids = [gpu_options.index(g) for g in selected_gpus]
        else:
            num_gpus = 1
            gpu_ids = [0]
            st.sidebar.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è GPU")
    else:
        st.sidebar.warning("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU")
        num_gpus = 0
        gpu_ids = []
    
    st.sidebar.markdown("---")
    
    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
    st.sidebar.subheader("‚ö° –¢–∏–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–µ–∂–∏–º
    if num_gpus == 0:
        available_modes = ["default"]
        recommended_idx = 0
    elif num_gpus == 1:
        available_modes = ["default", "deepspeed_zero3_offload"]
        recommended_idx = 0
    else:
        # –ü—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º multi_gpu –∏–ª–∏ fsdp
        available_modes = ["multi_gpu", "deepspeed_zero3", "deepspeed_zero3_offload", "deepspeed_zero2", "fsdp", "fsdp_offload", "fsdp2", "default"]
        recommended_idx = 0  # multi_gpu –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–ø—Ü–∏–∏ –¥–ª—è selectbox
    mode_options = []
    for i, mode in enumerate(available_modes):
        info = PARALLEL_TYPES[mode]
        label = f"{info['icon']} {info['name']}"
        if i == recommended_idx and num_gpus > 1:
            label += " ‚≠ê"  # –û—Ç–º–µ—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π
        mode_options.append(label)
    
    selected_mode_display = st.sidebar.selectbox(
        "–†–µ–∂–∏–º",
        options=mode_options,
        index=recommended_idx,
        help="–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
    )
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
    selected_idx = mode_options.index(selected_mode_display)
    selected_mode = available_modes[selected_idx]
    mode_info = PARALLEL_TYPES[selected_mode]
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ
    st.sidebar.markdown(f"""
    <div style="background: rgba(255,255,255,0.05); padding: 10px; border-radius: 8px; margin: 10px 0;">
    <b>–¢–∏–ø:</b> {mode_info['type']}<br>
    <small>{mode_info['description']}</small>
    </div>
    """, unsafe_allow_html=True)
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω single GPU –ø—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
    if num_gpus > 1 and selected_mode == "default":
        st.sidebar.warning(f"‚ö†Ô∏è –í—ã–±—Ä–∞–Ω Single GPU, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ {num_gpus} GPU. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º Multi-GPU!")
    
    # –ö–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª
    config_file = None
    if selected_mode != "default":
        config_path = CONFIGS_DIR / f"accelerate_{selected_mode}.yaml"
        if config_path.exists():
            config_file = str(config_path)
            st.sidebar.caption(f"üìÑ –ö–æ–Ω—Ñ–∏–≥: `{config_path.name}`")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∑–∞–ø—É—Å–∫–∞
    st.sidebar.markdown("---")
    st.sidebar.subheader("üöÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–ø—É—Å–∫–∞")
    
    if num_gpus == 0:
        launch_info = "**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** CPU"
    elif selected_mode == "default":
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** GPU {gpu_ids[0] if gpu_ids else 0}"
    else:
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:** {num_gpus} √ó GPU\n**–†–µ–∂–∏–º:** {mode_info['type']}"
    
    st.sidebar.info(launch_info)

    # Compute / precision (–Ω—É–∂–Ω–æ –∏ –¥–ª—è GRPO, –ø–æ—Ç–æ–º—É —á—Ç–æ training_config –¥–ª—è GRPO –ø—É—Å—Ç–æ–π)
    st.sidebar.markdown("---")
    st.sidebar.subheader("üß† Precision & Memory")
    
    # === Backend selector ===
    # –î–ª—è GRPO backend –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤ render_grpo_sidebar_config(), –∑–¥–µ—Å—å –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º
    if not is_grpo:
        backend_options = ["üè† models-at-home", "ü¶• Unsloth (2x faster)"]
        default_backend = training_config.get("training_backend", "models-at-home") if training_config else "models-at-home"
        default_idx = 1 if default_backend == "unsloth" else 0
        
        selected_backend_display = st.sidebar.radio(
            "Training Backend",
            backend_options,
            index=default_idx,
            help=(
                "**üè† models-at-home**: –ù–∞—à backend —Å FlashAttention + Liger Kernels\n\n"
                "**ü¶• Unsloth**: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π backend –æ—Ç Unsloth AI:\n"
                "‚Ä¢ 2x –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ\n"
                "‚Ä¢ –î–æ 70% –º–µ–Ω—å—à–µ VRAM\n"
                "‚Ä¢ Triton —è–¥—Ä–∞ (RMSNorm, RoPE, MLP)\n"
                "‚Ä¢ –£–º–Ω—ã–π gradient checkpointing\n\n"
                "‚ö†Ô∏è Unsloth –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multi-GPU"
            ),
        )
        training_backend = "unsloth" if "Unsloth" in selected_backend_display else "models-at-home"
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º backend
        if training_backend == "unsloth":
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Unsloth
            try:
                import unsloth
                unsloth_available = True
            except ImportError:
                unsloth_available = False
            
            if unsloth_available:
                st.sidebar.success("ü¶• **Unsloth —Ä–µ–∂–∏–º**: —É—Å–∫–æ—Ä–µ–Ω–∏–µ + —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏")
            else:
                st.sidebar.error("ü¶• **Unsloth –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!**")
                st.sidebar.caption("–ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ Docker –æ–±—Ä–∞–∑: `docker compose build`")
            
            if num_gpus > 1:
                st.sidebar.warning("‚ö†Ô∏è Unsloth –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multi-GPU. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ 1 GPU.")
        else:
            st.sidebar.info("üè† **models-at-home —Ä–µ–∂–∏–º**: FlashAttn + Liger")
        
        st.sidebar.markdown("---")
    else:
        # –î–ª—è GRPO backend –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∏–∑ model_config (–≤—ã–±—Ä–∞–Ω –≤ render_model_config)
        training_backend = grpo_backend if grpo_backend else "models-at-home"

    # –ï—Å–ª–∏ training_config –ø–µ—Ä–µ–¥–∞–Ω (SFT/Pretrain) ‚Äî –±–µ—Ä—ë–º –¥–µ—Ñ–æ–ª—Ç –∏–∑ –Ω–µ–≥–æ, –∏–Ω–∞—á–µ bf16 (GRPO –¥–µ—Ñ–æ–ª—Ç)
    default_mp = (training_config.get("mixed_precision") if training_config else None) or "bf16"
    mixed_precision = st.sidebar.selectbox(
        "Mixed Precision",
        ["no", "fp16", "bf16"],
        index=["no", "fp16", "bf16"].index(default_mp) if default_mp in ("no", "fp16", "bf16") else 2,
        help=(
            "bf16 —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Ampere+ GPU (–æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ VRAM –∏ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ). "
            "fp16 –≤ AMP —Ä–µ–∂–∏–º–µ —á–∞—Å—Ç–æ –¥–µ—Ä–∂–∏—Ç fp32-\"master\" –≤–µ—Å–∞ + GradScaler –∏ –º–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –±–æ–ª—å—à–µ VRAM, —á–µ–º bf16. "
            "–î–ª—è FlashAttention –Ω—É–∂–µ–Ω fp16/bf16."
        ),
    )

    # FP16: –¥–≤–∞ —Ä–µ–∂–∏–º–∞
    # - AMP fp16 (–¥–µ—Ñ–æ–ª—Ç): fp32 master-–≤–µ—Å–∞ + GradScaler => –º–æ–∂–µ—Ç —Å—Ç–∞—Ä—Ç–æ–≤–∞—Ç—å —Å –±–æ–ª—å—à–∏–º VRAM, —á–µ–º bf16
    # - Pure fp16: –≤–µ—Å–∞ fp16, –±–µ–∑ GradScaler => VRAM –±–ª–∏–∂–µ –∫ bf16, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º
    fp16_pure = False
    if mixed_precision == "fp16":
        fp16_pure = st.sidebar.checkbox(
            "FP16 Pure (–≤–µ—Å–∞ fp16, –±–µ–∑ GradScaler)",
            value=False,
            help=(
                "–°–Ω–∏–∂–∞–µ—Ç VRAM –Ω–∞ —Å—Ç–∞—Ä—Ç–µ (–≤–µ—Å–∞ fp16, –∫–∞–∫ —É bf16), –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º, —á–µ–º AMP fp16. "
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ, –µ—Å–ª–∏ fp16 OOM'–∏—Ç—Å—è –∏–∑-–∑–∞ fp32 master-–≤–µ—Å–æ–≤."
            ),
        )

    default_gc = bool(training_config.get("grad_checkpoint", False)) if training_config else False
    grad_checkpoint = st.sidebar.checkbox(
        "Gradient Checkpointing",
        value=default_gc,
        help="–≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ. –î–ª—è GRPO (–æ—Å–æ–±–µ–Ω–Ω–æ full+–¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã) —á–∞—Å—Ç–æ must-have.",
    )

    # === Backend-specific optimizations ===
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è models-at-home backend
    if training_backend == "models-at-home":
        # FlashAttention toggle (–¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞–¥–∏–π).
        # - HF –º–æ–¥–µ–ª–∏: attn_implementation=flash_attention_2 (—Ç—Ä–µ–±—É–µ—Ç flash_attn + fp16/bf16)
        # - Home –º–æ–¥–µ–ª–∏: SDPA (scaled_dot_product_attention) –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å flash kernel –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ fp16/bf16
        default_fa = bool(training_config.get("use_flash_attention", True)) if training_config else True
        flash_attention = st.sidebar.checkbox(
            "FlashAttention (—É—Å–∫–æ—Ä–µ–Ω–∏–µ attention)",
            value=default_fa,
            help=(
                "–í–∫–ª—é—á–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π attention –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ. "
                "–î–ª—è HF-–º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç FlashAttention2 (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω flash-attn –∏ –≤–∫–ª—é—á–µ–Ω fp16/bf16). "
                "–î–ª—è Home-–º–æ–¥–µ–ª–µ–π —É–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SDPA."
            ),
        )
        
        # Liger Kernel ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Triton kernels –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        # –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ –í–°–ï–ú —Ä–µ–∂–∏–º–∞–º: Pretrain, SFT, GRPO
        default_liger = bool(training_config.get("use_liger", True)) if training_config else True
        use_liger = st.sidebar.checkbox(
            "ü¶Å Liger Kernel –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
            value=default_liger,
            help=(
                "**–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ –í–°–ï–ú —Ä–µ–∂–∏–º–∞–º** (Pretrain, SFT, GRPO).\n\n"
                "–í–∫–ª—é—á–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Triton-–∫–µ—Ä–Ω–µ–ª—ã:\n"
                "‚Ä¢ RMSNorm, RoPE, MLP ‚Äî –ø–∞—Ç—á–∏–Ω–≥ –º–æ–¥–µ–ª–∏\n"
                "‚Ä¢ üî• Fused Loss ‚Äî –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits (–¥–æ 80% —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏!)\n\n"
                "**–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏**: Qwen, Llama, Mistral, Gemma, Phi."
            ),
        )
        
        # Fused Loss ‚Äî –ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç logits!
        # –î–ª—è Pretrain/SFT —ç—Ç–æ Fused CE, –¥–ª—è GRPO ‚Äî Fused GRPO Loss (–≤–∫–ª—é—á–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        default_liger_fused_ce = bool(training_config.get("liger_fused_ce", True)) if training_config else True
        liger_fused_ce = st.sidebar.checkbox(
            "üî• Fused Loss (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)",
            value=default_liger_fused_ce,
            disabled=not use_liger,
            help=(
                "**Pretrain/SFT**: LigerFusedLinearCrossEntropyLoss\n"
                "**GRPO**: LigerFusedLinearGRPOLoss (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)\n\n"
                "–ù–ï –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π logits —Ç–µ–Ω–∑–æ—Ä [batch, seq, vocab] ‚Äî "
                "—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –¥–æ 80%! **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞—Ç—å.**"
            ),
        )
    else:
        # Unsloth backend ‚Äî —ç—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è Unsloth –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        flash_attention = True  # Unsloth –≤–∫–ª—é—á–∞–µ—Ç —Å–≤–æ–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        use_liger = False  # Unsloth –∏–º–µ–µ—Ç —Å–≤–æ–∏ Triton kernels
        liger_fused_ce = False
        
        st.sidebar.caption(
            "ü¶• **Unsloth –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n"
            "‚Ä¢ Triton RMSNorm, RoPE, SwiGLU\n"
            "‚Ä¢ Smart Gradient Checkpointing\n"
            "‚Ä¢ Fused Cross-Entropy Loss"
        )

    # –ü–æ—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ batch semantics (—á–∞—Å—Ç–∞—è –ø—Ä–∏—á–∏–Ω–∞ "–ø–æ—á–µ–º—É —Ç–∞–∫ –º–Ω–æ–≥–æ VRAM –≤ DDP")
    if training_config:
        try:
            micro_bsz = int(training_config.get("batch_size", 1))
            grad_accum = int(training_config.get("gradient_accumulation", 1))
            eff_per_gpu = micro_bsz * grad_accum
            global_batch = eff_per_gpu * max(1, int(num_gpus or 1))
            st.sidebar.caption(
                f"Batch semantics: **per‚ÄëGPU microbatch** = {micro_bsz}, "
                f"accum = {grad_accum} ‚Üí **effective per‚ÄëGPU** = {eff_per_gpu}, "
                f"**global** = {global_batch} (√ó{max(1, int(num_gpus or 1))} GPU)"
            )
            st.sidebar.caption("–í–∞–∂–Ω–æ: –≤ DDP `batch_size` –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ, —Ç.–µ. —ç—Ç–æ –∏–º–µ–Ω–Ω–æ per‚ÄëGPU.")
        except Exception:
            pass
    
    return {
        "distributed_mode": selected_mode,
        "num_gpus": num_gpus,
        "gpu_ids": gpu_ids,
        "config_file": config_file,
        "parallel_type": mode_info['type'],
        "mixed_precision": mixed_precision,
        "fp16_pure": fp16_pure,
        "grad_checkpoint": grad_checkpoint,
        "use_flash_attention": flash_attention,
        "use_liger": use_liger,
        "liger_fused_ce": liger_fused_ce,
        "training_backend": training_backend,  # "models-at-home" –∏–ª–∏ "unsloth"
    }


# Graceful fallback –¥–ª—è @st.fragment (—Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö Streamlit)
try:
    fragment = st.fragment
except (AttributeError, Exception):
    # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π Streamlit
    fragment = lambda *args, **kwargs: lambda fn: fn

@fragment(run_every=3)  # –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã
def live_metrics_fragment():
    """Fragment –¥–ª—è –∂–∏–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    if not st.session_state.current_run_id:
        st.info("–í—ã–±–µ—Ä–∏—Ç–µ run –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫")
        return
    
    run_id = st.session_state.current_run_id
    metrics = load_metrics(run_id)
    process_alive = is_process_running(run_id)
    
    # –°—Ç–∞—Ç—É—Å
    if process_alive:
        st.success(f"üü¢ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–ø—É—â–µ–Ω (Run: {run_id})")
    else:
        if metrics and metrics.get("status") == "completed":
            duration = metrics.get("training_duration", "unknown")
            st.success(f"‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration} (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        elif metrics and metrics.get("status") == "error":
            st.error(f"‚ùå –û—à–∏–±–∫–∞ (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –æ—à–∏–±–∫–µ
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        elif metrics and metrics.get("status") == "stopped":
            st.warning(f"‚èπÔ∏è –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        else:
            st.info(f"üìã –ü—Ä–æ—Å–º–æ—Ç—Ä –º–µ—Ç—Ä–∏–∫ (Run: {run_id})")
    
    if metrics:
        render_metrics_dashboard(metrics)
    else:
        st.info("–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")


def render_metrics_dashboard(metrics: dict):
    """–î–∞—à–±–æ—Ä–¥ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è."""
    
    status = metrics.get("status", "unknown")
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É (–º–æ–∂–µ—Ç –±—ã—Ç—å NaN/float –∏–∑ pandas)
    import math
    if status is None or (isinstance(status, float) and math.isnan(status)):
        status = "training"
    status = str(status)
    
    # Status indicator
    status_emoji = {
        "training": "üü¢", 
        "completed": "‚úÖ", 
        "error": "‚ùå",
        "initializing": "‚è≥",
        "loading_tokenizer": "‚è≥",
        "loading_dataset": "‚è≥",
        "building_model": "‚è≥",
        "saving_model": "üíæ",
    }.get(status, "‚è≥")
    
    st.subheader(f"{status_emoji} –°—Ç–∞—Ç—É—Å: {status.upper()}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ metrics.json (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –∏–∑ config
    model_params = None
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ metrics.json (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ)
        if "num_parameters" in metrics:
            model_params = metrics["num_parameters"]
        else:
            # Fallback: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑ config
            run_id = st.session_state.get("current_run_id", "active")
            if run_id and run_id != "active":
                run_dir = RUNS_DIR / run_id
                config_path = run_dir / "config.json"
                if config_path.exists():
                    with open(config_path) as f:
                        rc = json.load(f)
                        if "hidden_size" in rc and "num_layers" in rc:
                            vocab_size = rc.get("vocab_size", 50257)
                            intermediate_size = rc.get("intermediate_size")
                            model_params = estimate_parameters(
                                rc["hidden_size"],
                                rc["num_layers"],
                                vocab_size=vocab_size,
                                intermediate_size=intermediate_size,
                            )
    except Exception:
        pass
    
    # Metrics cards
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    
    with col1:
        current_step = metrics.get("current_step", 0)
        total_steps = metrics.get("total_steps", None)
        progress = (current_step / total_steps * 100) if isinstance(total_steps, (int, float)) and total_steps > 0 else 0
        # –∑–∞—â–∏—Ç–∞ –æ—Ç "48000%" –∏ –ø—Ä–æ—á–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        progress = max(0.0, min(float(progress), 100.0))
        planned_total = metrics.get("planned_total_steps", None)
        suffix = f"Step {current_step}/{total_steps}" if isinstance(total_steps, (int, float)) and total_steps else f"Step {current_step} (–±–µ–∑ –ª–∏–º–∏—Ç–∞)"
        if planned_total is not None and int(planned_total) != int(total_steps):
            suffix = f"{suffix} (–ø–ª–∞–Ω: {planned_total})"
        st.metric("–ü—Ä–æ–≥—Ä–µ—Å—Å", f"{progress:.1f}%", suffix)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è GRPO: –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É (—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—Ä–æ—à–ª–æ)
    if metrics.get("stage") == "grpo":
        run_id = st.session_state.get("current_run_id", None)
        dataset_total = None
        prompt_bsz = metrics.get("prompt_batch_size", None)
        group_size = metrics.get("group_size", None)
        rollout_step = metrics.get("rollout_step", metrics.get("current_rollout_step", 0))
        num_gpus = 1
        try:
            if run_id:
                run_dir = RUNS_DIR / run_id
                cfg_path = run_dir / "config.json"
                if cfg_path.exists():
                    with open(cfg_path) as f:
                        rc = json.load(f) or {}
                    # num_gpus —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ config.json –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ GRPO
                    num_gpus = int(rc.get("num_gpus", 1) or 1)
                    # dataset_size: –±–µ—Ä—ë–º –∏–∑ run-config –µ—Å–ª–∏ –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ, –∏–Ω–∞—á–µ –ø–æ–ø—Ä–æ–±—É–µ–º dataset_info.json –∏–∑ output_dir
                    dataset_total = rc.get("dataset_size", None)
                    if dataset_total is None:
                        out_dir = rc.get("output_dir")
                        if out_dir:
                            info_path = Path(out_dir) / "dataset_info.json"
                            if info_path.exists():
                                with open(info_path) as inf:
                                    dataset_total = (json.load(inf) or {}).get("dataset_size", None)
        except Exception:
            pass
        try:
            dataset_total = int(dataset_total) if dataset_total is not None else None
        except Exception:
            dataset_total = None
        try:
            prompt_bsz = int(prompt_bsz) if prompt_bsz is not None else None
        except Exception:
            prompt_bsz = None
        try:
            group_size = int(group_size) if group_size is not None else None
        except Exception:
            group_size = None
        try:
            rollout_step = int(rollout_step)
        except Exception:
            rollout_step = 0

        # prompts/completions: –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –§–ê–ö–¢ –∏–∑ –º–µ—Ç—Ä–∏–∫, –∏–Ω–∞—á–µ fallback –Ω–∞ –æ—Ü–µ–Ω–∫—É
        prompts_seen = metrics.get("prompts_generated_total", None)
        prompts_used = metrics.get("prompts_used_total", None)
        completions_seen = metrics.get("completions_generated_total", None)
        experiences_tuned = metrics.get("experiences_tuned_total", None)

        # –û—Ü–µ–Ω–∫–∞ "—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—Ä–æ—à–ª–æ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É" –∏–∑ rollout_step (–≥–ª–æ–±–∞–ª—å–Ω–æ, —Å —É—á—ë—Ç–æ–º num_gpus).
        # –≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Å–µ–º–∞–Ω—Ç–∏–∫–µ "prompts/step".
        prompts_seen_est = None
        if prompt_bsz is not None:
            prompts_seen_est = rollout_step * prompt_bsz * max(1, num_gpus)
            if prompts_seen is None or (isinstance(prompts_seen, (int, float)) and float(prompts_seen) < float(prompts_seen_est)):
                prompts_seen = prompts_seen_est
        if completions_seen is None and prompts_seen is not None and group_size is not None:
            completions_seen = prompts_seen * group_size

        if prompts_seen is not None:
            if dataset_total is not None and dataset_total > 0:
                ds_progress = max(0.0, min(100.0, prompts_seen / dataset_total * 100.0))
                st.caption(f"–ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è): **{int(prompts_seen):,}/{dataset_total:,} –ø—Ä–æ–º–ø—Ç–æ–≤** ({ds_progress:.1f}%)")
                st.progress(ds_progress / 100.0)
            else:
                st.caption(f"–ü—Ä–æ–º–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è): **{int(prompts_seen):,}**")

            if prompts_used is not None:
                st.caption(f"–ü—Ä–æ–º–ø—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ –æ–±—É—á–µ–Ω–∏–∏ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏): **{int(prompts_used):,}**")
            if completions_seen is not None:
                st.caption(f"Completion'–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: **{int(completions_seen):,}**")
            if experiences_tuned is not None:
                st.caption(f"Experience'–æ–≤ –ø—Ä–æ—Ç—é–Ω–µ–Ω–æ (–±–∞—Ç—á–µ–π –≤ train): **{int(experiences_tuned):,}**")

        # –°–∫–æ—Ä–æ—Å—Ç–∏ (prompts/s, completions/s) –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ steps/timestamps
        try:
            elapsed = float(metrics.get("elapsed_seconds", 0.0))
            if elapsed > 0 and prompts_seen is not None:
                st.caption(f"–°–∫–æ—Ä–æ—Å—Ç—å: **{(float(prompts_seen)/elapsed):.2f} –ø—Ä–æ–º–ø—Ç–æ–≤/—Å**")
            if elapsed > 0 and completions_seen is not None:
                st.caption(f"–°–∫–æ—Ä–æ—Å—Ç—å: **{(float(completions_seen)/elapsed):.2f} completion/—Å**")
            if elapsed > 0 and experiences_tuned is not None:
                st.caption(f"–°–∫–æ—Ä–æ—Å—Ç—å: **{(float(experiences_tuned)/elapsed):.2f} tuned exp/—Å**")
        except Exception:
            pass
    
    with col2:
        if metrics.get("stage") == "grpo":
            reward = metrics.get("reward", metrics.get("batch_reward_mean", 0))
            st.metric("Reward", f"{reward:.4f}")
        else:
            loss = metrics.get("current_loss", 0)
            st.metric("Train Loss", f"{loss:.4f}")
    
    with col3:
        if metrics.get("stage") == "grpo":
            kl = metrics.get("kl", 0)
            st.metric("KL Divergence", f"{kl:.4f}")
        else:
            vloss = metrics.get("current_val_loss", None)
            if vloss is None:
                st.metric("Val Loss", "‚Äî")
            else:
                st.metric("Val Loss", f"{vloss:.4f}")
    
    with col4:
        lr = metrics.get("current_lr", 0)
        st.metric("Learning Rate", f"{lr:.2e}")
    
    with col5:
        if model_params:
            st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(model_params))
        else:
            st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", "‚Äî")

    # –î–æ–ø. –ø–æ—è—Å–Ω–µ–Ω–∏—è: –ø–ª–∞–Ω vs —Ñ–∞–∫—Ç, –ø—Ä–∏—á–∏–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, LR floor
    planned_total = metrics.get("planned_total_steps", None)
    total_steps = metrics.get("total_steps", None)
    stop_reason = metrics.get("stop_reason", None)
    min_lr_ratio = metrics.get("min_lr_ratio", None)
    if planned_total is not None and total_steps is not None and int(planned_total) != int(total_steps):
        st.caption(f"–ü–ª–∞–Ω —à–∞–≥–æ–≤: {planned_total} ‚Ä¢ –§–∞–∫—Ç (–¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞/ETA): {total_steps}")
    if stop_reason:
        st.caption(f"–ü—Ä–∏—á–∏–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: `{stop_reason}`")
    if min_lr_ratio is not None and float(min_lr_ratio) > 0:
        st.caption(f"Cosine LR floor –≤–∫–ª—é—á—ë–Ω: min_lr_ratio={float(min_lr_ratio):.2f}")
    
    with col6:
        eta = metrics.get("eta_seconds", 0)
        elapsed = metrics.get("elapsed_seconds", 0)
        st.metric("–í—Ä–µ–º—è", f"{format_time(elapsed)}", delta=f"–û—Å—Ç: {format_time(eta)}", delta_color="normal")
    
    # Progress bar
    st.progress(min(progress / 100, 1.0))
    
    # Charts
    # –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–ª—é—á –ø–æ run_id, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏
    rid = st.session_state.get("current_run_id", "active") or "active"
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è –¥–ª—è GRPO
    if metrics.get("stage") == "grpo":
        st.markdown("---")
        st.subheader("üß† GRPO –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ GRPO
        col_grpo1, col_grpo2, col_grpo3, col_grpo4 = st.columns(4)
        with col_grpo1:
            reward = metrics.get("reward", metrics.get("batch_reward_mean", 0))
            st.metric("Reward", f"{reward:.4f}")
        with col_grpo2:
            kl = metrics.get("kl", 0)
            st.metric("KL Divergence", f"{kl:.4f}")
        with col_grpo3:
            grad_norm = metrics.get("grad_norm", 0)
            st.metric("Grad Norm", f"{grad_norm:.4f}")
        with col_grpo4:
            buffer_size = metrics.get("buffer_size", 0)
            st.metric("Buffer Size", f"{buffer_size}")
        
        # –ì—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è GRPO
        if metrics.get("reward_history") and len(metrics["reward_history"]) > 0:
            col1, col2 = st.columns(2)
            
            with col1:
                # Reward chart
                fig_reward = go.Figure()
                steps = metrics.get("steps_history", list(range(len(metrics["reward_history"]))))
                fig_reward.add_trace(go.Scatter(
                    x=steps,
                    y=metrics["reward_history"],
                    mode='lines',
                    name='Reward',
                    line=dict(color='#10b981', width=2)
                ))
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
                if len(metrics["reward_history"]) > 10:
                    import pandas as pd
                    df_reward = pd.DataFrame({"reward": metrics["reward_history"]})
                    df_reward["reward_smooth"] = df_reward["reward"].rolling(window=min(10, len(df_reward)//4), min_periods=1).mean()
                    fig_reward.add_trace(go.Scatter(
                        x=steps,
                        y=df_reward["reward_smooth"].tolist(),
                        mode='lines',
                        name='Reward (smooth)',
                        line=dict(color='#34d399', width=2, dash='dash')
                    ))
                fig_reward.update_layout(
                    title="Reward Curve",
                    xaxis_title="Step",
                    yaxis_title="Reward",
                    template="plotly_dark",
                    height=300,
                    margin=dict(l=0, r=0, t=40, b=0)
                )
                st.plotly_chart(fig_reward, key=f"reward_chart_{rid}")
            
            with col2:
                # Loss chart –¥–ª—è GRPO
                if metrics.get("loss_history") and len(metrics["loss_history"]) > 0:
                    fig_loss = go.Figure()
                    steps = metrics.get("steps_history", list(range(len(metrics["loss_history"]))))
                    fig_loss.add_trace(go.Scatter(
                        x=steps,
                        y=metrics["loss_history"],
                        mode='lines',
                        name='GRPO Loss',
                        line=dict(color='#e94560', width=2)
                    ))
                    if metrics.get("kl_history") and len(metrics["kl_history"]) > 0:
                        fig_loss.add_trace(go.Scatter(
                            x=steps,
                            y=metrics["kl_history"],
                            mode='lines',
                            name='KL Divergence',
                            line=dict(color='#f59e0b', width=2, dash='dash')
                        ))
                    fig_loss.update_layout(
                        title="Loss & KL Divergence",
                        xaxis_title="Step",
                        yaxis_title="Loss / KL",
                        template="plotly_dark",
                        height=300,
                        margin=dict(l=0, r=0, t=40, b=0)
                    )
                    st.plotly_chart(fig_loss, key=f"grpo_loss_chart_{rid}")
                else:
                    st.info("–û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ loss...")
        
        # –û–∫–æ—à–∫–æ —Å —Å–µ–º–ø–ª–∞–º–∏
        st.markdown("---")
        st.subheader("üìù –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–π")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–µ–º–ø–ª—ã –∏–∑ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        run_id = st.session_state.get("current_run_id")
        if run_id and run_id != "active":
            run_dir = RUNS_DIR / run_id
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ samples.jsonl –≤ run_dir –∏–ª–∏ –≤ output_dir
            samples_file = None
            config_path = run_dir / "config.json"
            if config_path.exists():
                try:
                    with open(config_path) as f:
                        config = json.load(f)
                        output_dir = config.get("output_dir", "")
                        if output_dir:
                            samples_file = Path(output_dir) / "samples.jsonl"
                except:
                    pass
            
            # Fallback: –∏—â–µ–º –≤ run_dir
            if not samples_file or not samples_file.exists():
                samples_file = run_dir / "samples.jsonl"
            
            samples_data = []
            if samples_file.exists():
                try:
                    with open(samples_file, "r", encoding="utf-8") as f:
                        for line in f:
                            if line.strip():
                                samples_data.append(json.loads(line))
                except Exception as e:
                    st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–µ–º–ø–ª—ã: {e}")
            
            if samples_data:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–µ–º–ø–ª–æ–≤
                num_samples = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–º–ø–ª–æ–≤", 1, min(10, len(samples_data)), 3)
                recent_samples = samples_data[-num_samples:]
                
                for idx, sample in enumerate(reversed(recent_samples)):
                    with st.expander(f"–°–µ–º–ø–ª {len(samples_data) - idx} (Step {sample.get('step', '?')})", expanded=(idx == 0)):
                        prompt = sample.get("prompt", "")
                        reference = sample.get("reference_answer", "")
                        completions = sample.get("completions", [])
                        rewards = sample.get("rewards", [])
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç+–æ—Ç–≤–µ—Ç –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–µ–º–ø–ª–∞ (—á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —á—Ç–æ –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç)
                        if idx == 0 and completions:
                            st.markdown("**üîç –ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç + –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (—á—Ç–æ –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å):**")
                            st.caption("–≠—Ç–æ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ—Ç–æ—Ä—ã–π –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –æ —Ç–µ–≥–∞—Ö")
                            
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º full_texts –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º
                            full_texts = sample.get("full_texts", [])
                            if not full_texts:
                                full_texts = [prompt + comp for comp in completions]
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç (—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º reward)
                            if rewards:
                                best_completion_idx = max(range(len(rewards)), key=lambda i: rewards[i])
                                best_reward = rewards[best_completion_idx]
                                best_full_text = full_texts[best_completion_idx] if best_completion_idx < len(full_texts) else prompt + completions[best_completion_idx]
                                
                                # –í—ã–¥–µ–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                                system_prompt_start = best_full_text.find("system") if "system" in best_full_text.lower() else -1
                                if system_prompt_start == -1:
                                    # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
                                    for marker in ["<|im_start|>", "A conversation", "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ"]:
                                        if marker in best_full_text:
                                            system_prompt_start = best_full_text.find(marker)
                                            break
                                
                                st.code(best_full_text, language=None)
                                st.caption(f"‚úÖ –õ—É—á—à–∏–π –æ—Ç–≤–µ—Ç (reward={best_reward:.4f})")
                                
                                # –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ö—É–¥—à–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                                worst_completion_idx = min(range(len(rewards)), key=lambda i: rewards[i])
                                worst_reward = rewards[worst_completion_idx]
                                if worst_reward < best_reward:
                                    with st.expander(f"üìâ –•—É–¥—à–∏–π –æ—Ç–≤–µ—Ç (reward={worst_reward:.4f})", expanded=False):
                                        worst_full_text = full_texts[worst_completion_idx] if worst_completion_idx < len(full_texts) else prompt + completions[worst_completion_idx]
                                        st.code(worst_full_text, language=None)
                                        st.caption("–°—Ä–∞–≤–Ω–∏—Ç–µ —Å –ª—É—á—à–∏–º –æ—Ç–≤–µ—Ç–æ–º –≤—ã—à–µ - –≤–∏–¥–Ω–æ –ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ —Ç–µ–≥–∞—Ö –≤ –ø—Ä–æ–º–ø—Ç–µ?")
                            else:
                                # –ï—Å–ª–∏ –Ω–µ—Ç rewards, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π
                                st.code(full_texts[0] if full_texts else prompt + completions[0], language=None)
                            
                            st.markdown("---")
                        
                        # –¢–µ–∫—É—â–µ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –ø—Ä–æ–º–ø—Ç –∏ –æ—Ç–≤–µ—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                        col_s1, col_s2 = st.columns([1, 1])
                        
                        with col_s1:
                            st.markdown("**üì• –ü—Ä–æ–º–ø—Ç:**")
                            st.code(prompt, language=None)
                        
                        with col_s2:
                            st.markdown("**‚úÖ –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:**")
                            st.code(reference, language=None)
                        
                        st.markdown("**ü§ñ –û—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏:**")
                        
                        if completions:
                            for i, (completion, reward) in enumerate(zip(completions, rewards)):
                                reward_color = "üü¢" if reward > 0.5 else "üü°" if reward > 0 else "üî¥"
                                with st.container():
                                    st.markdown(f"{reward_color} **–û—Ç–≤–µ—Ç {i+1}** (Reward: {reward:.4f})")
                                    st.code(completion[:500] + ("..." if len(completion) > 500 else ""), language=None)
                        else:
                            st.info("–û–∂–∏–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π...")
            else:
                st.info("–°–µ–º–ø–ª—ã –±—É–¥—É—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –∑–¥–µ—Å—å –ø–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª `samples.jsonl` –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ run.")
        
        st.markdown("---")
    
    # –û–±—ã—á–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç–∞–¥–∏–π
    elif metrics.get("loss_history"):
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º steps_history –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        steps_history = metrics.get("steps_history", list(range(1, len(metrics["loss_history"]) + 1)))
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Loss chart
            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                x=steps_history,
                y=metrics["loss_history"],
                mode='lines',
                name='Train Loss',
                line=dict(color='#e94560', width=2)
            ))
            if metrics.get("val_loss_history"):
                fig_loss.add_trace(go.Scatter(
                    x=metrics["val_steps_history"],
                    y=metrics["val_loss_history"],
                    mode='lines',
                    name='Val Loss',
                    line=dict(width=2, dash="dash", color='#60a5fa')
                ))
            fig_loss.update_layout(
                title="Training & Validation Loss",
                xaxis_title="Step",
                yaxis_title="Loss",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_loss, key=f"loss_chart_{rid}")
        
        with col2:
            # LR chart
            lr_history = metrics.get("lr_history", [])
            if lr_history:
                fig_lr = go.Figure()
                fig_lr.add_trace(go.Scatter(
                    x=steps_history[:len(lr_history)],
                    y=lr_history,
                    mode='lines',
                    name='LR',
                    line=dict(color='#60a5fa', width=2)
                ))
                fig_lr.update_layout(
                    title="Learning Rate Schedule",
                    xaxis_title="Step",
                    yaxis_title="LR",
                    template="plotly_dark",
                    height=300,
                    margin=dict(l=0, r=0, t=40, b=0)
                )
                st.plotly_chart(fig_lr, key=f"lr_chart_{rid}")
        
        # GRPO —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏: Reward –∏ KL
        if metrics.get("reward_history") or metrics.get("kl_history"):
            col3, col4 = st.columns(2)
            
            with col3:
                reward_history = metrics.get("reward_history", [])
                if reward_history:
                    fig_reward = go.Figure()
                    fig_reward.add_trace(go.Scatter(
                        x=steps_history[:len(reward_history)],
                        y=reward_history,
                        mode='lines',
                        name='Reward',
                        line=dict(color='#10b981', width=2)
                    ))
                    fig_reward.update_layout(
                        title="üéØ Reward (GRPO)",
                        xaxis_title="Step",
                        yaxis_title="Reward",
                        template="plotly_dark",
                        height=300,
                        margin=dict(l=0, r=0, t=40, b=0)
                    )
                    st.plotly_chart(fig_reward, key=f"reward_chart_{rid}")
            
            with col4:
                kl_history = metrics.get("kl_history", [])
                if kl_history:
                    fig_kl = go.Figure()
                    fig_kl.add_trace(go.Scatter(
                        x=steps_history[:len(kl_history)],
                        y=kl_history,
                        mode='lines',
                        name='KL Divergence',
                        line=dict(color='#f59e0b', width=2)
                    ))
                    fig_kl.update_layout(
                        title="üìä KL Divergence (GRPO)",
                        xaxis_title="Step",
                        yaxis_title="KL",
                        template="plotly_dark",
                        height=300,
                        margin=dict(l=0, r=0, t=40, b=0)
                    )
                    st.plotly_chart(fig_kl, key=f"kl_chart_{rid}")
    
    # Checkpoints
    if metrics.get("checkpoints"):
        with st.expander("üì¶ Checkpoints"):
            for ckpt in metrics["checkpoints"]:
                ckpt_loss = ckpt.get("loss")
                if ckpt_loss is not None:
                    st.text(f"Step {ckpt['step']}: Loss {ckpt_loss:.4f} | {ckpt['path']}")
                else:
                    st.text(f"Step {ckpt['step']}: {ckpt['path']}")
    
    # –ü—Ä–∏–º–µ—Ä —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–¥–ª—è SFT)
    sample_prompt = metrics.get("sample_prompt")
    if sample_prompt:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –Ω–∞–ª–∏—á–∏—é stage –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
        stage = metrics.get("stage", "pretrain")
        if stage == "sft":
            title = "üìù –ü—Ä–∏–º–µ—Ä —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (SFT)"
            caption = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:"
            tip = "üí° –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç–µ–≥–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ"
        else:
            title = "üìù –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (Pretrain)"
            caption = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:"
            tip = "üí° –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –≤ —Ç–µ–∫—Å—Ç–µ"
        
        with st.expander(title, expanded=True):
            st.caption(caption)
            st.code(sample_prompt, language=None)
            st.caption(tip)
    
    # GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    gpu_stats = metrics.get("gpu_stats", [])
    if gpu_stats:
        st.subheader("üñ•Ô∏è –ù–∞–≥—Ä—É–∑–∫–∞ GPU")
        
        cols = st.columns(len(gpu_stats))
        for i, (col, gpu) in enumerate(zip(cols, gpu_stats)):
            with col:
                st.markdown(f"**GPU {gpu['id']}**")
                
                # Memory bar
                mem_percent = gpu.get('memory_percent', 0)
                st.progress(min(mem_percent / 100, 1.0), text=f"VRAM: {gpu['memory_used_gb']:.1f} / {gpu['memory_total_gb']:.1f} GB ({mem_percent:.0f}%)")
                
                # Utilization
                util = gpu.get('utilization')
                if util is not None:
                    st.progress(min(util / 100, 1.0), text=f"–ó–∞–≥—Ä—É–∑–∫–∞: {util}%")
                else:
                    st.caption("–ó–∞–≥—Ä—É–∑–∫–∞: N/A")
    
    # Error
    if metrics.get("error"):
        st.error("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
        with st.expander("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏ (Traceback)", expanded=True):
            st.code(metrics['error'], language="python")
    
    # –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
    if st.session_state.current_run_id:
        run_dir = RUNS_DIR / st.session_state.current_run_id
        stderr_path = run_dir / "stderr.log"
        stdout_path = run_dir / "stdout.log"
        
        with st.expander("üìã –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.caption("stdout (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stdout_path.exists():
                    with open(stdout_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)
            
            with col2:
                st.caption("stderr (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stderr_path.exists():
                    with open(stderr_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)



def download_hf_dataset(repo_id, subset, split, limit_type, limit_val, limit_bytes, save_path, filters=None):
    """–§—É–Ω–∫—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    try:
        status_text = f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: {repo_id}..."
        st.toast(status_text)
        print(status_text)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã stream=True —á—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –≤—Å–µ –≤ –ø–∞–º—è—Ç—å
        # –î–ª—è –º–Ω–æ–≥–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ "default" –º–æ–∂–µ—Ç –ª–æ–º–∞—Ç—å load_dataset - –ø–µ—Ä–µ–¥–∞—ë–º None
        subset_arg = None if (not subset or subset.strip() == "" or subset.lower() == "default") else subset
        
        ds = load_dataset(repo_id, subset_arg, split=split, streaming=True)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
        save_path = DATASET_DIR / save_path
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        count = 0
        current_bytes = 0
        
        with open(save_path, "w", encoding="utf-8") as f:
            for item in ds:
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                if filters:
                    # –§–∏–ª—å—Ç—Ä –ø–æ score
                    if "score_col" in filters and "min_score" in filters:
                         col = filters["score_col"]
                         min_s = filters["min_score"]
                         # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Ç–∏–ø–∞
                         if col in item and item[col] is not None:
                             try:
                                 val = float(item[col])
                                 if val < min_s:
                                     continue
                             except ValueError:
                                 pass
                    
                    # –§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É
                    if "lang_col" in filters and "target_lang" in filters:
                         col = filters["lang_col"]
                         target = filters["target_lang"]
                         if col in item and item[col] is not None:
                             val = str(item[col])
                             if target.lower() not in val.lower():
                                 continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSONL
                line = json.dumps(item, ensure_ascii=False) + "\n"
                line_bytes = len(line.encode('utf-8'))
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)" and count >= limit_val:
                    break
                if limit_type == "–ì–ë (–†–∞–∑–º–µ—Ä)" and (current_bytes + line_bytes) > limit_bytes:
                    break
                    
                f.write(line)
                count += 1
                current_bytes += line_bytes
                
                if count % 1000 == 0:
                    print(f"Downloaded {count} lines, {current_bytes / 1024**2:.2f} MB")

        st.success(f"–ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {count} —Å—Ç—Ä–æ–∫ ({current_bytes / 1024**2:.2f} MB) –≤ {save_path}")
        return True

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        print(traceback.format_exc())
        return False


def render_data_manager(stage: str = "pretrain"):
    """–í–∫–ª–∞–¥–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏.
    
    Args:
        stage: –¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º ('pretrain', 'sft', 'grpo', 'continual_pretrain')
    """
    st.header("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
    
    col_upload, col_list = st.columns([1, 2])
    
    with col_upload:
        # –°–µ–∫—Ü–∏—è 1: –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        with st.expander("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤", expanded=False):
            uploaded_files = st.file_uploader(
                "–ü–µ—Ä–µ—Ç–∞—â–∏—Ç–µ —Ñ–∞–π–ª—ã —Å—é–¥–∞", 
                type=["jsonl", "txt"],  # –í–ê–ñ–ù–û: –Ω–µ –≤–∫–ª—é—á–∞–µ–º .json, —Ç.–∫. —ç—Ç–æ –æ–±—ã—á–Ω–æ –º–∞—Å—Å–∏–≤, –∞ –Ω–µ JSONL 
                accept_multiple_files=True
            )
            
            if uploaded_files:
                if st.button("üì• –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª—ã"):
                    for uploaded_file in uploaded_files:
                        save_path = DATASET_DIR / uploaded_file.name
                        with open(save_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        st.toast(f"–§–∞–π–ª {uploaded_file.name} —Å–æ—Ö—Ä–∞–Ω—ë–Ω!", icon="‚úÖ")
                    time.sleep(1)
                    st.rerun()

        # –°–µ–∫—Ü–∏—è 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å HuggingFace
        st.subheader("ü§ó –°–∫–∞—á–∞—Ç—å —Å HuggingFace")
        
        # –†–∞–∑–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
        if stage == "grpo":
            # GRPO ‚Äî Reasoning –¥–∞—Ç–∞—Å–µ—Ç—ã
            presets = {
                # English
                "üß† GSM8K (math, EN)": ("gsm8k", "main", "train"),
                "üß† OpenR1-Math-220k": ("open-r1/OpenR1-Math-220k", "default", "train"),
                "üß† ARC-Challenge (EN)": ("allenai/ai2_arc", "ARC-Challenge", "train"),
                "üß† CommonsenseQA (EN)": ("tau/commonsense_qa", "default", "train"),
                "üß† HellaSwag (EN)": ("Rowan/hellaswag", "default", "train"),
                "üß† TriviaQA (EN)": ("trivia_qa", "rc", "train"),
                "üß† PIQA (EN)": ("piqa", "default", "train"),
                "üß† WinoGrande (EN)": ("winogrande", "winogrande_xl", "train"),
                # Russian
                "üß† GSM8K-RU (math, RU)": ("d0rj/gsm8k-ru", "default", "train"),
                "üß† Gromov-MAX (math, RU)": ("attn-signs/gromov-max", "default", "train"),
                "üß† MGSM-RU (multilingual)": ("juletxara/mgsm", "ru", "train"),
                # –†—É—á–Ω–æ–π –≤–≤–æ–¥
                "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None, None),
            }
            st.caption("üß† **Reasoning –¥–∞—Ç–∞—Å–µ—Ç—ã** –¥–ª—è GRPO —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
        elif stage == "sft":
            # SFT ‚Äî Instruction-following –¥–∞—Ç–∞—Å–µ—Ç—ã
            presets = {
                # Russian
                "üîµ OpenOrca-ru": ("d0rj/OpenOrca-ru", "default", "train"),
                "üîµ ru-instruct": ("d0rj/ru-instruct", "default", "train"),
                "üîµ GrandMaster-PRO-MAX": ("Vikhrmodels/GrandMaster-PRO-MAX", "default", "train"),
                "üîµ Alpaca-GPT4-ru": ("IlyaGusev/ru_turbo_alpaca", "default", "train"),
                "üîµ Saiga-ru (Vikhr)": ("Vikhrmodels/Saiga-2-7b", "default", "train"),
                # English
                "üîµ OpenOrca (EN)": ("Open-Orca/OpenOrca", "default", "train"),
                "üîµ Alpaca-GPT4 (EN)": ("vicgalle/alpaca-gpt4", "default", "train"),
                "üîµ ShareGPT (EN)": ("anon8231489123/ShareGPT_Vicuna_unfiltered", "default", "train"),
                "üîµ Dolly-15k (EN)": ("databricks/databricks-dolly-15k", "default", "train"),
                "üîµ FLAN (EN)": ("Muennighoff/flan", "default", "train"),
                # –†—É—á–Ω–æ–π –≤–≤–æ–¥
                "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None, None),
            }
            st.caption("üîµ **Instruction-following –¥–∞—Ç–∞—Å–µ—Ç—ã** –¥–ª—è SFT —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
        else:
            # Pretrain / Continual Pretrain ‚Äî Large text corpora
            presets = {
                # Russian
                "üü¢ FineWeb-2 (Russian)": ("HuggingFaceFW/fineweb-2", "rus_Cyrl", "train"),
                "üü¢ MC4-ru (Russian web)": ("mc4", "ru", "train"),
                "üü¢ Wikipedia-ru": ("graelo/wikipedia", "20230601.ru", "train"),
                "üü¢ Taiga Corpus (RU)": ("IlyaGusev/taiga_ru", "default", "train"),
                # English
                "üü¢ FineWeb-Edu (Educational)": ("HuggingFaceFW/fineweb-edu", "default", "train"),
                "üü¢ Wikitext-103": ("wikitext", "wikitext-103-v1", "train"),
                "üü¢ The Pile (subset)": ("EleutherAI/pile", "default", "train"),
                "üü¢ C4 (EN)": ("allenai/c4", "en", "train"),
                "üü¢ RedPajama-v2": ("togethercomputer/RedPajama-Data-V2", "default", "train"),
                "üü¢ SlimPajama": ("cerebras/SlimPajama-627B", "default", "train"),
                # –†—É—á–Ω–æ–π –≤–≤–æ–¥
                "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None, None),
            }
            st.caption("üü¢ **Text corpora** –¥–ª—è Pretrain / Continual Pretrain")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (FineWeb-2 Russian)
        if "hf_repo_id_input" not in st.session_state:
            st.session_state.hf_repo_id_input = "HuggingFaceFW/fineweb-2"
        if "hf_subset_default" not in st.session_state:
            st.session_state.hf_subset_default = "rus_Cyrl"
        if "hf_split_default" not in st.session_state:
            st.session_state.hf_split_default = "train"
        
        # –ü—Ä–µ–¥–∑–∞–ø–æ–ª–Ω—è–µ–º –∫—ç—à –¥–ª—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –ø—Ä–µ—Å–µ—Ç–∞ (FineWeb-2)
        # —á—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–≥ —Å—Ä–∞–∑—É —Å–∫–∞—á–∏–≤–∞—Ç—å –±–µ–∑ –Ω–∞–∂–∞—Ç–∏—è "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å"
        if "ds_repo_info" not in st.session_state:
            st.session_state.ds_repo_info = {}
        
        default_repo = "HuggingFaceFW/fineweb-2"
        if default_repo not in st.session_state.ds_repo_info:
            st.session_state.ds_repo_info[default_repo] = {
                "configs": ["rus_Cyrl"],  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —è–∑—ã–∫
                "splits": ["train", "test"],  # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ splits
                "features": {},  # –ó–∞–ø–æ–ª–Ω–∏—Ç—Å—è –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ
                "selected_config": "rus_Cyrl"
            }
        
        def on_preset_change():
            """Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ–ª–µ–π –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ø—Ä–µ—Å–µ—Ç–∞."""
            sel = st.session_state.dataset_preset_selector
            preset_data = presets.get(sel)
            if preset_data and preset_data[0]:
                st.session_state.hf_repo_id_input = preset_data[0]
                st.session_state.hf_subset_default = preset_data[1]
                st.session_state.hf_split_default = preset_data[2]
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º selectbox —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã
                if "hf_split_select" in st.session_state:
                    del st.session_state.hf_split_select
                if "hf_subset_select" in st.session_state:
                    del st.session_state.hf_subset_select

        # –°–µ–ª–µ–∫—Ç–æ—Ä –ø—Ä–µ—Å–µ—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é FineWeb-2 Russian)
        st.selectbox(
            "üìö –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã",
            options=list(presets.keys()),
            index=0,  # FineWeb-2 Russian –ø–µ—Ä–≤—ã–π
            key="dataset_preset_selector",
            on_change=on_preset_change,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –≥–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî –≤—Å–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω—è—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )

        repo_id = st.text_input("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (ID)", key="hf_repo_id_input")
        
        # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π"):
            try:
                with st.spinner(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {repo_id}..."):
                    # 1. –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥–∏
                    configs = get_dataset_config_names(repo_id)
                    
                    # 2. –í—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è splits/features
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π (rus_Cyrl) > –ø–µ—Ä–≤—ã–π –≤ —Å–ø–∏—Å–∫–µ
                    default_subset = st.session_state.hf_subset_default
                    if default_subset in configs:
                        selected_config = default_subset
                    else:
                        selected_config = configs[0] if configs else None
                    
                    splits = []
                    features_info = {}
                    
                    if selected_config:
                        splits = get_dataset_split_names(repo_id, selected_config)
                        # 3. –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (features)
                        try:
                            ds_builder = load_dataset_builder(repo_id, selected_config)
                            if ds_builder.info.features:
                                features_info = ds_builder.info.features
                        except Exception as e:
                            print(f"Could not load features: {e}")

                    st.session_state.ds_repo_info[repo_id] = {
                        "configs": configs,
                        "splits": splits,
                        "features": features_info,
                        "selected_config": selected_config  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –¥–ª—è –∫–∞–∫–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞ splits
                    }
                    
                    # –í–ê–ñ–ù–û: –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—ã–±–æ—Ä –≤–∏–¥–∂–µ—Ç–æ–≤ —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if "hf_split_select" in st.session_state:
                        del st.session_state.hf_split_select
                    if "hf_subset_select" in st.session_state:
                        del st.session_state.hf_subset_select
                    
                    st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(configs)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π, splits: {splits}")
            except Exception as e:
                st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: {e}")

        # –†–∞–±–æ—Ç–∞–µ–º —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        repo_info = st.session_state.ds_repo_info.get(repo_id, {})
        available_configs = repo_info.get("configs", [])
        available_splits = repo_info.get("splits", [])
        features = repo_info.get("features", {})
        
        if available_configs:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π subset, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –≤ —Å–ø–∏—Å–∫–µ
            default_idx = 0
            if st.session_state.hf_subset_default in available_configs:
                default_idx = available_configs.index(st.session_state.hf_subset_default)
            subset = st.selectbox("Subset (–∫–æ–Ω—Ñ–∏–≥)", available_configs, index=default_idx, key="hf_subset_select")
        else:
            subset = st.text_input("Subset (–∫–æ–Ω—Ñ–∏–≥)", st.session_state.hf_subset_default, key="hf_subset_input")
        
        if available_splits:
            default_idx = 0
            if st.session_state.hf_split_default in available_splits:
                default_idx = available_splits.index(st.session_state.hf_split_default)
            split = st.selectbox("Split", available_splits, index=default_idx, key="hf_split_select")
        else:
            split = st.text_input("Split", st.session_state.hf_split_default, key="hf_split_input")

        # --- –£–ú–ù–´–ï –§–ò–õ–¨–¢–†–´ ---
        with st.expander("üõ†Ô∏è –§–∏–ª—å—Ç—Ä—ã –∏ –õ–∏–º–∏—Ç—ã", expanded=True):
            # –õ–∏–º–∏—Ç—ã (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã)
            col_lim1, col_lim2 = st.columns(2)
            with col_lim1:
                limit_type = st.radio("–¢–∏–ø –ª–∏–º–∏—Ç–∞", ["–ì–ë (–†–∞–∑–º–µ—Ä)", "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)"], key="limit_type")
            
            with col_lim2:
                limit_val = 0
                limit_bytes = 0
                
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)":
                    limit_val = st.number_input("–ö–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫", value=100000, step=10000, key="limit_val")
                else:
                    limit_gb = st.number_input("–†–∞–∑–º–µ—Ä (–ì–ë)", value=2.0, step=0.5, min_value=0.1, key="limit_gb")
                    limit_bytes = int(limit_gb * 1024**3)
            
            st.divider()
            
            # –§–∏–ª—å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (features)
            active_filters = {}
            
            if features:
                st.caption("üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏ –∏—Ö —Ç–∏–ø–æ–≤
                # features —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å {col_name: feature_info}
                # feature_info –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π 'Value(dtype='string')' –∏–ª–∏ –æ–±—ä–µ–∫—Ç–æ–º
                
                # 1. –§–∏–ª—å—Ç—Ä —á–∏—Å–ª–æ–≤–æ–π (Score/Quality)
                float_cols = []
                string_cols = []
                
                for col_name, feature_def in features.items():
                    # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø
                    dtype = getattr(feature_def, 'dtype', str(feature_def))
                    if 'float' in str(dtype):
                        float_cols.append(col_name)
                    elif 'string' in str(dtype):
                        string_cols.append(col_name)
                
                col_f1, col_f2 = st.columns(2)
                
                with col_f1:
                    if float_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é (float)**")
                        selected_float_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + float_cols, key="sel_float_col")
                        if selected_float_col != "(–Ω–µ—Ç)":
                            min_val = st.slider(f"–ú–∏–Ω. –∑–Ω–∞—á–µ–Ω–∏–µ {selected_float_col}", 0.0, 1.0, 0.0, key="val_float_col")
                            active_filters["score_col"] = selected_float_col
                            active_filters["min_score"] = min_val
                    else:
                        st.caption("–ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

                with col_f2:
                    if string_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–∫—Å—Ç—É (contains)**")
                        selected_str_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + string_cols, key="sel_str_col")
                        if selected_str_col != "(–Ω–µ—Ç)":
                            target_str = st.text_input(f"–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:", key="val_str_col")
                            if target_str:
                                active_filters["lang_col"] = selected_str_col
                                active_filters["target_lang"] = target_str
                    else:
                        st.caption("–¢–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            else:
                st.info("‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ –ª–∏–º–∏—Ç—ã –ø–æ –æ–±—ä–µ–º—É.")


        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ repo_id
        # –ù–∞–ø—Ä–∏–º–µ—Ä: "HuggingFaceFW/fineweb-2" ‚Üí "fineweb-2.jsonl"
        # "d0rj/gsm8k-ru" ‚Üí "gsm8k-ru.jsonl"
        def compute_default_filename():
            """–í—ã—á–∏—Å–ª—è–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π repo_id –∏ subset."""
            computed_name = "dataset.jsonl"
            if repo_id:
                # –ë–µ—Ä—ë–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ "/" (–∏–ª–∏ –≤—Å—ë –µ—Å–ª–∏ –Ω–µ—Ç "/")
                name_part = repo_id.split("/")[-1] if "/" in repo_id else repo_id
                # –î–æ–±–∞–≤–ª—è–µ–º subset –µ—Å–ª–∏ –æ–Ω –Ω–µ default
                current_subset = st.session_state.get('hf_subset_select') or st.session_state.get('hf_subset_input') or st.session_state.get('hf_subset_default', '')
                if current_subset and current_subset not in ('default', 'main', ''):
                    name_part = f"{name_part}-{current_subset}"
                computed_name = f"{name_part}.jsonl"
            return computed_name
        
        computed_filename = compute_default_filename()
        
        # –°–æ–∑–¥–∞—ë–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ repo_id –∏ subset
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ—Ç key –¥–ª—è –≤–∏–¥–∂–µ—Ç–∞, —á—Ç–æ–±—ã –æ–Ω –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞–ª—Å—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ repo_id/subset
        current_subset = st.session_state.get('hf_subset_select') or st.session_state.get('hf_subset_input') or st.session_state.get('hf_subset_default', '')
        repo_subset_key = f"{repo_id}::{current_subset}"
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ session_state (—É–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã)
        normalized_key = repo_subset_key.replace('/', '_').replace(':', '_').replace('-', '_')
        widget_key = f"save_filename_{normalized_key}"
        
        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π –∫–ª—é—á (repo_id/subset –∏–∑–º–µ–Ω–∏–ª–∏—Å—å), –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–µ –∏–º—è
        if widget_key not in st.session_state:
            st.session_state[widget_key] = computed_filename
        
        save_filename = st.text_input(
            "–ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", 
            value=st.session_state.get(widget_key, computed_filename), 
            key=widget_key,
            help="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏ subset. –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é."
        )
        
        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º callback
        def on_download_click(active_filters_map):
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ session_state —è–≤–Ω–æ
            r_id = st.session_state.get('hf_repo_id_input')
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º subset –∏ split
            if st.session_state.get('hf_subset_select'):
                sub = st.session_state.get('hf_subset_select')
            else:
                sub = st.session_state.get('hf_subset_input')
                
            if st.session_state.get('hf_split_select'):
                spl = st.session_state.get('hf_split_select')
            else:
                spl = st.session_state.get('hf_split_input')
                
            l_type = st.session_state.get('limit_type')
            l_val = st.session_state.get('limit_val', 0) or 0
            
            l_gb = st.session_state.get('limit_gb', 2.0)
            l_bytes = int(l_gb * 1024**3)

            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—Ç –∂–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π key
            current_subset_for_key = sub or st.session_state.get('hf_subset_default', '')
            repo_subset_key_for_download = f"{r_id}::{current_subset_for_key}"
            normalized_key_for_download = repo_subset_key_for_download.replace('/', '_').replace(':', '_').replace('-', '_')
            widget_key_for_download = f"save_filename_{normalized_key_for_download}"
            s_path = st.session_state.get(widget_key_for_download, "dataset.jsonl")
            
            # –ü–µ—Ä–µ–¥–∞—ë–º —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞–∫ –µ—Å—Ç—å (active_filters_map —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏)
            filters_to_pass = active_filters_map or None
            
            download_hf_dataset(r_id, sub, spl, l_type, l_val, l_bytes, s_path, filters=filters_to_pass)

        st.button("–°–∫–∞—á–∞—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å", on_click=on_download_click, args=(active_filters,))
    
    with col_list:
        st.subheader("–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã")
        
        datasets = []
        if DATASET_DIR.exists():
            # JSONL / JSON
            for f in list(DATASET_DIR.glob("*.jsonl")) + list(DATASET_DIR.glob("*.json")):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "JSONL/JSON",
                    "size_mb": size_mb,
                    "path": f
                })
            # TXT
            for f in DATASET_DIR.glob("*.txt"):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "Text",
                    "size_mb": size_mb,
                    "path": f
                })
        
        if not datasets:
            st.info("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã —Å–ª–µ–≤–∞.")
        else:
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–ø–∏—Å–æ–∫
            for ds in datasets:
                with st.expander(f"üìÑ {ds['name']} ({ds['size_mb']:.1f} MB)"):
                    st.caption(f"–¢–∏–ø: {ds['type']}")
                    
                    # Preview
                    try:
                        with open(ds['path'], "r", encoding="utf-8") as f:
                            head = []
                            for i, line in enumerate(f):
                                if i >= 5:
                                    break
                                head.append(line.strip())
                        st.markdown("**Preview (–ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫):**")
                        if head:
                            st.code("\n".join(head), language="json" if "JSON" in ds['type'] else "text")
                        else:
                            st.info("–§–∞–π–ª –ø—É—Å—Ç")
                        
                        col_del, col_info = st.columns([1, 4])
                        with col_del:
                            if st.button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"del_{ds['name']}"):
                                ds['path'].unlink()
                                st.toast(f"–§–∞–π–ª {ds['name']} —É–¥–∞–ª—ë–Ω", icon="üóëÔ∏è")
                                time.sleep(1)
                                st.rerun()
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")


def download_hf_model(repo_id: str, save_name: str, revision: str = "main"):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å HuggingFace –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ –≤ MODELS_DIR.
    """
    from huggingface_hub import snapshot_download
    
    save_path = MODELS_DIR / save_name
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ
    if save_path.exists():
        st.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å `{save_name}` —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        return False
    
    try:
        with st.spinner(f"‚è≥ –°–∫–∞—á–∏–≤–∞–µ–º {repo_id}..."):
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar = st.progress(0, text="–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
            status_text = st.empty()
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            status_text.text(f"–°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∏–∑ {repo_id}...")
            progress_bar.progress(10, text="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤...")
            
            # snapshot_download —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å—é –º–æ–¥–µ–ª—å
            local_path = snapshot_download(
                repo_id=repo_id,
                revision=revision,
                local_dir=str(save_path),
                local_dir_use_symlinks=False,  # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã, –Ω–µ —Å–∏–º–ª–∏–Ω–∫–∏
                ignore_patterns=["*.md", "*.txt", "*.gitattributes", ".git*"],  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–Ω—É–∂–Ω–æ–µ
            )
            
            progress_bar.progress(90, text="–ü—Ä–æ–≤–µ—Ä–∫–∞...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–∫–∞—á–∞–ª–æ—Å—å
            config_file = save_path / "config.json"
            if not config_file.exists():
                st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω config.json –≤ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
                return False
            
            # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            import json
            with open(config_file) as f:
                model_config = json.load(f)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_type = model_config.get("model_type", "unknown")
            hidden_size = model_config.get("hidden_size", "?")
            num_layers = model_config.get("num_hidden_layers", model_config.get("n_layer", "?"))
            vocab_size = model_config.get("vocab_size", "?")
            
            progress_bar.progress(100, text="–ì–æ—Ç–æ–≤–æ!")
            status_text.empty()
            
            st.success(f"""‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞!
- **–ü—É—Ç—å:** `{save_path}`
- **–¢–∏–ø:** {model_type}
- **Hidden:** {hidden_size}, **Layers:** {num_layers}, **Vocab:** {vocab_size}
""")
            return True
            
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        import traceback
        print(traceback.format_exc())
        # –£–¥–∞–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω–æ–µ
        if save_path.exists():
            import shutil
            shutil.rmtree(save_path, ignore_errors=True)
        return False


def render_model_manager():
    """–í–∫–ª–∞–¥–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ (—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å HuggingFace)."""
    st.header("ü§ñ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏")
    
    col_download, col_list = st.columns([1, 2])
    
    with col_download:
        st.subheader("ü§ó –°–∫–∞—á–∞—Ç—å —Å HuggingFace")
        
        # –ü—Ä–µ—Å–µ—Ç—ã –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è continual pretraining / SFT
        model_presets = {
            "üî• SmolLM2-135M (135M params)": ("HuggingFaceTB/SmolLM2-135M", "SmolLM2-135M"),
            "üî• SmolLM2-360M (360M params)": ("HuggingFaceTB/SmolLM2-360M", "SmolLM2-360M"),
            "üî• SmolLM2-1.7B (1.7B params)": ("HuggingFaceTB/SmolLM2-1.7B", "SmolLM2-1.7B"),
            "ü¶ô TinyLlama-1.1B (1.1B params)": ("TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T", "TinyLlama-1.1B"),
            "üêç Pythia-70M (70M params)": ("EleutherAI/pythia-70m", "Pythia-70M"),
            "üêç Pythia-160M (160M params)": ("EleutherAI/pythia-160m", "Pythia-160M"),
            "üêç Pythia-410M (410M params)": ("EleutherAI/pythia-410m", "Pythia-410M"),
            "üêç Pythia-1B (1B params)": ("EleutherAI/pythia-1b", "Pythia-1B"),
            "ü§ñ GPT-2 Small (124M params)": ("openai-community/gpt2", "GPT2-Small"),
            "ü§ñ GPT-2 Medium (355M params)": ("openai-community/gpt2-medium", "GPT2-Medium"),
            "ü¶ä Qwen2.5-0.5B (0.5B params)": ("Qwen/Qwen2.5-0.5B", "Qwen2.5-0.5B"),
            "ü¶ä Qwen2.5-1.5B (1.5B params)": ("Qwen/Qwen2.5-1.5B", "Qwen2.5-1.5B"),
            "üá∑üá∫ ruGPT3-Small (125M, Russian)": ("ai-forever/rugpt3small_based_on_gpt2", "ruGPT3-Small"),
            "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None),
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        if "model_repo_id" not in st.session_state:
            st.session_state.model_repo_id = "HuggingFaceTB/SmolLM2-135M"
        if "model_save_name" not in st.session_state:
            st.session_state.model_save_name = "SmolLM2-135M"
        
        def on_model_preset_change():
            sel = st.session_state.model_preset_selector
            preset_data = model_presets.get(sel)
            if preset_data and preset_data[0]:
                st.session_state.model_repo_id = preset_data[0]
                st.session_state.model_save_name = preset_data[1]
        
        st.selectbox(
            "üìö –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏",
            options=list(model_presets.keys()),
            index=0,
            key="model_preset_selector",
            on_change=on_model_preset_change,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å ‚Äî repo_id –∑–∞–ø–æ–ª–Ω–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )
        
        repo_id = st.text_input("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (ID)", key="model_repo_id")
        save_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", key="model_save_name", 
                                   help="–ü–∞–ø–∫–∞ –≤ models/, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        st.markdown("""
<div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); 
            padding: 12px; border-radius: 8px; margin: 10px 0;
            border: 1px solid #0f3460; color: #e8e8e8;">
<b style="color: #4fc3f7;">üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b><br>
‚Ä¢ <b style="color: #81d4fa;">SmolLM2</b> ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç HuggingFace<br>
‚Ä¢ <b style="color: #81d4fa;">Pythia</b> ‚Äî –æ—Ç–ª–∏—á–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã<br>
‚Ä¢ <b style="color: #81d4fa;">TinyLlama</b> ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω–∞—è, —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 3T —Ç–æ–∫–µ–Ω–æ–≤<br>
‚Ä¢ <b style="color: #81d4fa;">Qwen2.5</b> ‚Äî —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç Alibaba
</div>
""", unsafe_allow_html=True)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        size_estimates = {
            "70m": "~150 MB", "135m": "~300 MB", "160m": "~350 MB",
            "360m": "~800 MB", "410m": "~900 MB", "0.5b": "~1 GB",
            "1b": "~2 GB", "1.1b": "~2.5 GB", "1.5b": "~3 GB", "1.7b": "~3.5 GB",
            "124m": "~500 MB", "355m": "~1.5 GB", "125m": "~500 MB",
        }
        
        estimated_size = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        repo_lower = repo_id.lower()
        for size_key, size_val in size_estimates.items():
            if size_key in repo_lower:
                estimated_size = size_val
                break
        
        st.caption(f"üì¶ –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä: **{estimated_size}**")
        
        if st.button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
            if not repo_id or not save_name:
                st.error("–£–∫–∞–∂–∏—Ç–µ repo_id –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ!")
            else:
                success = download_hf_model(repo_id, save_name)
                if success:
                    time.sleep(1)
                    st.rerun()
    
    with col_list:
        st.subheader("üìÅ –°–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        
        models = []
        if MODELS_DIR.exists():
            for model_dir in MODELS_DIR.iterdir():
                if model_dir.is_dir():
                    config_path = model_dir / "config.json"
                    if config_path.exists():
                        try:
                            import json
                            with open(config_path) as f:
                                cfg = json.load(f)
                            
                            # –†–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏
                            total_size = sum(
                                f.stat().st_size for f in model_dir.rglob("*") if f.is_file()
                            )
                            size_gb = total_size / (1024**3)
                            
                            models.append({
                                "name": model_dir.name,
                                "path": model_dir,
                                "model_type": cfg.get("model_type", "unknown"),
                                "hidden_size": cfg.get("hidden_size", "?"),
                                "num_layers": cfg.get("num_hidden_layers", cfg.get("n_layer", "?")),
                                "vocab_size": cfg.get("vocab_size", "?"),
                                "size_gb": size_gb,
                            })
                        except Exception:
                            models.append({
                                "name": model_dir.name,
                                "path": model_dir,
                                "model_type": "?",
                                "hidden_size": "?",
                                "num_layers": "?",
                                "vocab_size": "?",
                                "size_gb": 0,
                            })
        
        if not models:
            st.info("–ù–µ—Ç —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å —Å–ª–µ–≤–∞ –¥–ª—è Continual Pretraining –∏–ª–∏ SFT.")
        else:
            for m in sorted(models, key=lambda x: x["name"]):
                with st.expander(f"ü§ñ {m['name']} ({m['size_gb']:.2f} GB)"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"""
- **–¢–∏–ø:** `{m['model_type']}`
- **Hidden Size:** {m['hidden_size']}
- **Layers:** {m['num_layers']}
- **Vocab:** {m['vocab_size']}
""")
                    with col2:
                        st.caption(f"üìÇ –ü—É—Ç—å: `{m['path']}`")
                        
                        # –ö–Ω–æ–ø–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                        if st.button("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", key=f"use_{m['name']}", 
                                     help="–í—ã–±—Ä–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –¥–ª—è Continual Pretrain / SFT"):
                            st.session_state.selected_base_model = str(m['path'])
                            st.toast(f"–ú–æ–¥–µ–ª—å {m['name']} –≤—ã–±—Ä–∞–Ω–∞! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –ó–∞–ø—É—Å–∫ –∏ –≤—ã–±–µ—Ä–∏—Ç–µ Continual Pretrain –∏–ª–∏ SFT.", icon="‚úÖ")
                        
                        # –ö–Ω–æ–ø–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è
                        if st.button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"del_model_{m['name']}"):
                            import shutil
                            shutil.rmtree(m['path'])
                            st.toast(f"–ú–æ–¥–µ–ª—å {m['name']} —É–¥–∞–ª–µ–Ω–∞", icon="üóëÔ∏è")
                            time.sleep(1)
                            st.rerun()
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞
        st.markdown("---")
        st.info("""
üí° **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∫–∞—á–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å:**
1. –ù–∞–∂–º–∏—Ç–µ **üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å** –Ω–∞ –Ω—É–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üöÄ –ó–∞–ø—É—Å–∫**
3. –í —Å–∞–π–¥–±–∞—Ä–µ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º **Continual Pretrain** –∏–ª–∏ **SFT**
4. –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—Å—è –∫–∞–∫ –±–∞–∑–æ–≤–∞—è
""")


def _bytes_to_gb(x: int) -> float:
    return float(x) / (1024**3)


def _sum_tensor_bytes(obj) -> int:
    """
    –°—á–∏—Ç–∞–µ—Ç –±–∞–π—Ç—ã —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ CUDA –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Ç–µ–Ω–∑–æ—Ä/—Å–ø–∏—Å–æ–∫/–∫–æ—Ä—Ç–µ–∂/—Å–ª–æ–≤–∞—Ä—å).
    """
    import torch

    total = 0
    if obj is None:
        return 0
    if torch.is_tensor(obj):
        if obj.is_cuda:
            return int(obj.numel() * obj.element_size())
        return 0
    if isinstance(obj, dict):
        for v in obj.values():
            total += _sum_tensor_bytes(v)
        return total
    if isinstance(obj, (list, tuple)):
        for v in obj:
            total += _sum_tensor_bytes(v)
        return total
    return 0


def _estimate_memory_footprint(config, batch_size, distributed_mode="default", num_gpus=1):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ VRAM –¥–ª—è –ª—é–±–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π.
    """
    from homellm.models.memory_estimator import estimate_memory_footprint
    return estimate_memory_footprint(config, batch_size, distributed_mode, num_gpus)


def _profile_memory_footprint_cuda(config, batch_size: int):
    """
    –¢–æ—á–Ω—ã–π (–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω–æ) –∑–∞–º–µ—Ä –ø–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º CUDA –∞–ª–ª–æ–∫–∞—Ü–∏—è–º:
    - –¥–µ–ª–∞–µ–º warmup —à–∞–≥, —á—Ç–æ–±—ã AdamW –ø—Ä–æ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª state
    - –∑–∞—Ç–µ–º –º–µ—Ä—è–µ–º peak allocated/reserved –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º breakdown: Model+Optim (steady), Act (peak - steady), Buf (reserved - peak).
    """
    import torch

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

    # –°–æ–±–∏—Ä–∞–µ–º HomeForCausalLM (–¥–ª—è Blueprint —Ä–µ–∂–∏–º–∞ –ª—É—á—à–µ –¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ; –ø–æ–∫–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é Home –º–æ–¥–µ–ª—å)
    from homellm.models.home_model import HomeConfig, HomeForCausalLM

    vocab_size = int(config.get("vocab_size", 50257))
    hidden_size = int(config["hidden_size"])
    num_layers = int(config["num_layers"])
    n_heads = int(config["n_heads"])
    seq_len = int(config["seq_len"])

    mp = (config.get("mixed_precision") or "no").lower()
    if mp == "bf16":
        dtype = torch.bfloat16
    elif mp == "fp16":
        dtype = torch.float16
    else:
        dtype = torch.float32

    model_cfg = HomeConfig(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_layers,
        num_attention_heads=n_heads,
        max_position_embeddings=seq_len,
        dropout=float(config.get("dropout", 0.0)),
    )

    device = torch.device("cuda:0")
    torch.cuda.set_device(device)

    model = HomeForCausalLM(model_cfg).to(device=device, dtype=dtype)
    model.train()
    if config.get("grad_checkpoint", False) and hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()

    opt_name = (config.get("optimizer") or "adamw").lower()
    lr = float(config.get("lr", 1e-3))
    wd = float(config.get("weight_decay", 0.01))
    if opt_name == "adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    else:
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    def run_step():
        input_ids = torch.randint(0, vocab_size, (int(batch_size), seq_len), device=device)
        labels = input_ids.clone()
        out = model(input_ids=input_ids, labels=labels, use_cache=False)
        loss = out.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

    # Warmup: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è optimizer state + –ø—Ä–æ–≥—Ä–µ–≤ allocator'–∞
    torch.cuda.synchronize()
    run_step()
    torch.cuda.synchronize()

    # –ò–∑–º–µ—Ä–µ–Ω–∏–µ
    torch.cuda.reset_peak_memory_stats(device)
    alloc_before = torch.cuda.memory_allocated(device)
    reserved_before = torch.cuda.memory_reserved(device)

    run_step()
    torch.cuda.synchronize()

    peak_alloc = torch.cuda.max_memory_allocated(device)
    peak_reserved = torch.cuda.max_memory_reserved(device)
    alloc_after = torch.cuda.memory_allocated(device)

    # Tensor-based breakdown (–ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç "—á—Ç–æ –∏–º–µ–Ω–Ω–æ –∂–∏–≤—ë—Ç", –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç caching allocator)
    weights_bytes = 0
    for p in model.parameters():
        if p.is_cuda:
            weights_bytes += int(p.numel() * p.element_size())
    for b in model.buffers():
        if torch.is_tensor(b) and b.is_cuda:
            weights_bytes += int(b.numel() * b.element_size())

    opt_state_bytes = 0
    for st in optimizer.state.values():
        opt_state_bytes += _sum_tensor_bytes(st)

    # –ü–æ—Å–ª–µ zero_grad(set_to_none=True) –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –¥–µ—Ä–∂–∞—Ç—å –ø–∞–º—è—Ç—å
    grads_bytes = 0
    for p in model.parameters():
        if p.grad is not None and p.grad.is_cuda:
            grads_bytes += int(p.grad.numel() * p.grad.element_size())

    steady_alloc = alloc_after
    act_alloc = max(0, int(peak_alloc - steady_alloc))
    buf_alloc = max(0, int(peak_reserved - peak_alloc))

    total_peak = peak_reserved  # —Å–∞–º—ã–π —á–µ—Å—Ç–Ω—ã–π "—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—Ä–æ—Å–∏–ª —É –¥—Ä–∞–π–≤–µ—Ä–∞" –Ω–∞ –ø–∏–∫–µ —à–∞–≥–∞

    return {
        "method": "profile_cuda",
        "total_gb": round(_bytes_to_gb(total_peak), 2),
        "model_gb": round(_bytes_to_gb(steady_alloc), 2),
        "act_gb": round(_bytes_to_gb(act_alloc), 2),
        "buf_gb": round(_bytes_to_gb(buf_alloc), 2),
        "params": int(sum(p.numel() for p in model.parameters())),
        "detail": {
            "alloc_before_gb": round(_bytes_to_gb(alloc_before), 3),
            "reserved_before_gb": round(_bytes_to_gb(reserved_before), 3),
            "peak_alloc_gb": round(_bytes_to_gb(peak_alloc), 3),
            "peak_reserved_gb": round(_bytes_to_gb(peak_reserved), 3),
            "alloc_after_gb": round(_bytes_to_gb(alloc_after), 3),
            "tensor_weights_gb": round(_bytes_to_gb(weights_bytes), 3),
            "tensor_opt_state_gb": round(_bytes_to_gb(opt_state_bytes), 3),
            "tensor_grads_gb": round(_bytes_to_gb(grads_bytes), 3),
        },
        "notes": "–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ CUDA: warmup + –∏–∑–º–µ—Ä–µ–Ω–∏–µ peak. 'Buf' = caching allocator (reserved - peak allocated).",
    }


def calculate_memory_footprint(config, batch_size, distributed_mode="default", num_gpus=1, *, method: str = "estimate"):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫—É/–∑–∞–º–µ—Ä VRAM –¥–ª—è –ø—Ä–µ–≤—å—é.
    method:
      - 'estimate': –±—ã—Å—Ç—Ä–∞—è —Ñ–æ—Ä–º—É–ª–∞ (fallback)
      - 'profile_cuda': —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–º–µ—Ä –Ω–∞ —Ç–µ–∫—É—â–µ–π GPU (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ/–º–æ–∂–µ—Ç OOM)
    """
    try:
        if method == "profile_cuda":
            return _profile_memory_footprint_cuda(config, batch_size=int(batch_size))
        return _estimate_memory_footprint(config, batch_size=int(batch_size), distributed_mode=distributed_mode, num_gpus=num_gpus)
    except Exception as e:
        print(f"Error calculating VRAM ({method}): {e}")
        # –§–æ–ª–±—ç–∫ –Ω–∞ –æ—Ü–µ–Ω–∫—É
        out = _estimate_memory_footprint(config, batch_size=int(batch_size), distributed_mode=distributed_mode, num_gpus=num_gpus)
        out["notes"] = f"{out.get('notes','')} | profile error: {e}"
        return out


def render_quick_summary(model_config: dict, dataset_config: dict, distributed_config: dict, full_config: dict = None) -> bool:
    """
    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –±—ã—Å—Ç—Ä—É—é —Ç–∞–±–ª–∏—á–∫—É —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–º–æ–¥–µ–ª—å, –¥–∞–Ω–Ω—ã–µ, —Ä–µ–∂–∏–º).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –≤—Å–µ 3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤—ã–±—Ä–∞–Ω—ã, –∏–Ω–∞—á–µ False.
    """
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    model_name = model_config.get("model_name_input", "–ù–µ –≤—ã–±—Ä–∞–Ω–æ")
    base_model_path = model_config.get("base_model_path")
    stage = model_config.get("stage", "pretrain")
    
    if base_model_path:
        model_display = f"{Path(base_model_path).name}"
    elif model_name and model_name != "–ù–µ –≤—ã–±—Ä–∞–Ω–æ":
        model_display = model_name
    else:
        model_display = "‚ùå –ù–µ –≤—ã–±—Ä–∞–Ω–æ"
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
    # –î–ª—è GRPO –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ full_config (grpo_dataset_path), session_state –∏–ª–∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤ main area
    data_path = dataset_config.get("data_path")
    if not data_path and stage == "grpo":
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º selectbox –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ –µ–≥–æ key (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±)
        selectbox_value = st.session_state.get("grpo_dataset_selectbox")
        if selectbox_value and not selectbox_value.startswith("--"):
            data_path = selectbox_value
        # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º session_state (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ render_grpo_main_config)
        if not data_path:
            data_path = st.session_state.get("grpo_dataset_path")
        # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º full_config (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø–æ—Å–ª–µ render_grpo_main_config)
        if not data_path and full_config:
            data_path = full_config.get("grpo_dataset_path")
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º session_state –∏–∑ full_config –µ—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
            if data_path and "grpo_dataset_path" not in st.session_state:
                st.session_state.grpo_dataset_path = data_path
    
    if data_path:
        data_display = Path(data_path).name
    elif stage == "grpo":
        data_display = "üìù –í—ã–±–µ—Ä–∏—Ç–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –Ω–∏–∂–µ"
    else:
        data_display = "‚ùå –ù–µ –≤—ã–±—Ä–∞–Ω–æ"
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∂–∏–º–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    distributed_mode = distributed_config.get("distributed_mode", "default")
    mode_info = PARALLEL_TYPES.get(distributed_mode, PARALLEL_TYPES["default"])
    training_mode_display = mode_info.get("name", "–ù–µ –≤—ã–±—Ä–∞–Ω–æ")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –≤—ã–±—Ä–∞–Ω–æ
    has_model = model_display != "‚ùå –ù–µ –≤—ã–±—Ä–∞–Ω–æ"
    # –î–ª—è GRPO –¥–∞—Ç–∞—Å–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–±—Ä–∞–Ω –ø–æ–∑–∂–µ –≤ main area
    has_data = data_display != "‚ùå –ù–µ –≤—ã–±—Ä–∞–Ω–æ" and "–í—ã–±–µ—Ä–∏—Ç–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –Ω–∏–∂–µ" not in data_display
    has_mode = distributed_mode != "default" or distributed_config.get("num_gpus", 1) > 0
    
    # –î–ª—è GRPO –¥–∞—Ç–∞—Å–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤ main area (–ø–æ—Å–ª–µ render_quick_summary)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ session_state –∏–ª–∏ full_config
    if stage == "grpo":
        # –î–ª—è GRPO –¥–∞—Ç–∞—Å–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è –∑–∞–ø—É—Å–∫–∞
        all_selected = has_model and has_mode and has_data
    else:
        all_selected = has_model and has_data and has_mode
    
    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—á–∫—É
    st.markdown("""
    <style>
    .quick-summary {
        background: linear-gradient(135deg, #1e1e1e 0%, #2a2a2a 100%);
        border: 2px solid #444;
        border-radius: 12px;
        padding: 1.5rem;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
    }
    .quick-summary-header {
        color: #ff6b6b;
        font-size: 1.3rem;
        font-weight: 700;
        margin-bottom: 1rem;
        text-align: center;
    }
    .quick-summary-grid {
        display: grid;
        grid-template-columns: repeat(3, 1fr);
        gap: 1rem;
        margin-top: 1rem;
    }
    .quick-summary-item {
        background: #1a1a1a;
        border: 1px solid #333;
        border-radius: 8px;
        padding: 1rem;
        text-align: center;
    }
    .quick-summary-number {
        font-size: 2rem;
        font-weight: 800;
        color: #ff6b6b;
        margin-bottom: 0.5rem;
    }
    .quick-summary-label {
        color: #888;
        font-size: 0.9rem;
        margin-bottom: 0.5rem;
        text-transform: uppercase;
        letter-spacing: 1px;
    }
    .quick-summary-value {
        color: #fff;
        font-size: 1.1rem;
        font-weight: 600;
        word-break: break-word;
    }
    .quick-summary-check {
        color: #22c55e;
        font-size: 1.5rem;
        margin-top: 0.5rem;
    }
    .quick-summary-warning {
        color: #f59e0b;
        font-size: 0.9rem;
        margin-top: 0.5rem;
        font-style: italic;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∫–æ–Ω–∫–∏ –∏ —Ü–≤–µ—Ç–∞
    check_icon = "‚úÖ" if all_selected else "‚ö†Ô∏è"
    status_color = "#22c55e" if all_selected else "#f59e0b"
    
    st.markdown(f"""
    <div class="quick-summary">
        <div class="quick-summary-header">
            –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        </div>
        <div class="quick-summary-grid">
            <div class="quick-summary-item">
                <div class="quick-summary-label">–ú–æ–¥–µ–ª—å</div>
                <div class="quick-summary-value">{model_display}</div>
                {"<div class='quick-summary-check'>‚úÖ</div>" if has_model else "<div class='quick-summary-warning'>‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å</div>"}
            </div>
            <div class="quick-summary-item">
                <div class="quick-summary-label">–î–∞–Ω–Ω—ã–µ</div>
                <div class="quick-summary-value">{data_display}</div>
                {"<div class='quick-summary-check'>‚úÖ</div>" if has_data else "<div class='quick-summary-warning'>‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç</div>"}
            </div>
            <div class="quick-summary-item">
                <div class="quick-summary-label">–†–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏</div>
                <div class="quick-summary-value">{training_mode_display}</div>
                {"<div class='quick-summary-check'>‚úÖ</div>" if has_mode else "<div class='quick-summary-warning'>‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º</div>"}
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    return all_selected


def render_model_preview(config: dict, distributed_config: dict = None):
    """–ü—Ä–µ–≤—å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞."""
    st.subheader("üìê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏")
    
    stage = config.get("stage", "pretrain")
    if stage == "sft":
        st.info(f"üîÑ **–†–µ–∂–∏–º SFT** (Fine-Tuning)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    elif stage == "continual_pretrain":
        st.info(f"üîÑ **–†–µ–∂–∏–º Continual Pretraining** (–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    elif stage == "grpo":
        st.info(f"üß† **–†–µ–∂–∏–º GRPO** (RL –¥–ª—è Reasoning)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    else:
        st.success("üèóÔ∏è **–†–µ–∂–∏–º Pretraining** (–° –Ω—É–ª—è)")

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å
    # –ù–∞–º –Ω—É–∂–µ–Ω batch_size –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (—ç—Ç–æ –±–∞—Ç—á –Ω–∞ –¥–µ–≤–∞–π—Å)
    batch_size = config.get("batch_size", 1)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º num_gpus –∏ distributed_mode –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    dist_mode = "default"
    n_gpus = 1
    if distributed_config:
        dist_mode = distributed_config.get("distributed_mode", "default")
        n_gpus = distributed_config.get("num_gpus", 1)
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º config –Ω–∞–ø—Ä—è–º—É—é (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ distributed_config –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)
    if config.get("num_gpus"):
        n_gpus = int(config.get("num_gpus", 1))
    if config.get("distributed_mode"):
        dist_mode = config.get("distributed_mode", "default")

    mem_method = "estimate"
    # if torch.cuda.is_available():
    #     with st.expander("üß† –ü–∞–º—è—Ç—å GPU: –æ—Ü–µ–Ω–∫–∞ vs —Ç–æ—á–Ω—ã–π –∑–∞–º–µ—Ä", expanded=False):
    #         st.caption("–û—Ü–µ–Ω–∫–∞ ‚Äî –º–≥–Ω–æ–≤–µ–Ω–Ω–æ, –Ω–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ. –¢–æ—á–Ω—ã–π –∑–∞–º–µ—Ä ‚Äî –∑–∞–ø—É—Å–∫–∞–µ—Ç 2 train-step –Ω–∞ GPU (warmup + –∏–∑–º–µ—Ä–µ–Ω–∏–µ) –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º/–º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –ø–æ OOM.")
    #         do_profile = st.checkbox("–°–¥–µ–ª–∞—Ç—å —Ç–æ—á–Ω—ã–π –∑–∞–º–µ—Ä –Ω–∞ CUDA (2 —à–∞–≥–∞)", value=False, key="profile_vram_cuda")
    #         if do_profile:
    #             mem_method = "profile_cuda"

    mem_info = calculate_memory_footprint(config, batch_size, dist_mode, n_gpus, method=mem_method)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Hidden Size", config["hidden_size"])
        st.metric("Layers", config["num_layers"])
    
    with col2:
        st.metric("Attention Heads", config["n_heads"])
        st.metric("Head Dim", config["hidden_size"] // config["n_heads"])
    
    with col3:
        st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(mem_info["params"]))
        
        # –¶–≤–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ –¥–ª—è 24GB –∫–∞—Ä—Ç—ã)
        val = mem_info["total_gb"]
        color = "normal"
        if val > 24: color = "off" # –∫—Ä–∞—Å–Ω—ã–π –æ—Ç—Ç–µ–Ω–æ–∫ –≤ –¥–µ–ª—å—Ç–µ –æ–±—ã—á–Ω–æ
        
        title = "VRAM (Profile)" if mem_info.get("method") == "profile_cuda" else "VRAM (Estimate)"
        st.metric(
            title,
            f"{val:.1f} GB",
            delta=f"M: {mem_info['model_gb']} + A: {mem_info['act_gb']} GB",
            delta_color=color,
            help=(mem_info.get("notes") or "M: Model+Optim (steady)\nA: Activations/temporaries (peak - steady)\nBuf: caching allocator")
        )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
    if mem_info["total_gb"] > 0:
        st.caption("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU:")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –±–∞—Ä —á–∞—Ä—Ç —á–µ—Ä–µ–∑ HTML/CSS –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
        total = mem_info["total_gb"]
        p_model = (mem_info["model_gb"] / total) * 100
        p_act = (mem_info["act_gb"] / total) * 100
        buf_gb = float(mem_info.get("buf_gb", 0.0))
        p_buff = (buf_gb / total) * 100 if total > 0 else 0
        
        st.markdown(f"""
        <div style="display: flex; height: 20px; width: 100%; background: #333; border-radius: 4px; overflow: hidden; margin-top: 5px;">
            <div style="width: {p_model}%; background: #3b82f6; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Model & Optim">Model</div>
            <div style="width: {p_act}%; background: #e94560; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Activations">Act</div>
            <div style="width: {p_buff}%; background: #777; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Buffer">Buf</div>
        </div>
        <div style="display: flex; justify-content: space-between; font-size: 12px; color: #888; margin-top: 2px;">
            <span>Model + Optim: {mem_info['model_gb']} GB</span>
            <span>Activations: {mem_info['act_gb']} GB</span>
        </div>
        """, unsafe_allow_html=True)

        if mem_info.get("notes"):
            st.caption(mem_info["notes"])

        if mem_info.get("method") == "profile_cuda" and isinstance(mem_info.get("detail"), dict):
            with st.expander("üîç –î–µ—Ç–∞–ª–∏ –∑–∞–º–µ—Ä–∞ (CUDA allocator / tensor sums)", expanded=False):
                st.json(mem_info["detail"])

        if mem_info["act_gb"] > mem_info["model_gb"] * 2:
            st.warning("‚ö†Ô∏è –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –∑–∞–Ω–∏–º–∞—é—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏! –í–∫–ª—é—á–∏—Ç–µ Gradient Checkpointing –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ Batch Size.")

    
    # –í–∏–∑—É–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    st.markdown(f"""
    <div class="model-ascii">
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         HomeForCausalLM             ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ  Embedding: 50257 ‚Üí {config['hidden_size']:4d}           ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇ  ‚îÇ HomeBlock √ó {config['num_layers']:2d}              ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí Attention      ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ {config['n_heads']:2d} heads √ó {config['hidden_size']//config['n_heads']:3d} dim      ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí FFN (SwiGLU)   ‚îÇ    ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
    ‚îÇ  RMSNorm ‚Üí LM Head                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    </div>
        """, unsafe_allow_html=True)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–µ

    if distributed_config:
        st.subheader("‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º")
        
        mode = distributed_config.get("distributed_mode", "default")
        mode_info = PARALLEL_TYPES.get(mode, PARALLEL_TYPES["default"])
        num_gpus = distributed_config.get("num_gpus", 0)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("–†–µ–∂–∏–º", mode_info["name"])
        
        with col2:
            st.metric("–¢–∏–ø", mode_info["type"])
        
        with col3:
            if num_gpus > 0:
                st.metric("GPU", f"{num_gpus} —à—Ç.")
            else:
                st.metric("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "CPU")
        
        # –°—Ö–µ–º–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        if mode == "default":
            parallel_diagram = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Single Device      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Full Model      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Optimizer  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Gradients  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
        elif mode == "multi_gpu":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Data Parallel ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (copy)  ‚îÇ  ‚îÇ (copy)  ‚îÇ       ‚îÇ (copy)  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Sync Gradients ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ö–∞–∂–¥–∞—è GPU: –ø–æ–ª–Ω–∞—è –∫–æ–ø–∏—è –º–æ–¥–µ–ª–∏, —á–∞—Å—Ç—å –±–∞—Ç—á–∞
"""
        elif mode == "fsdp":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FSDP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Shard 0 ‚îÇ  ‚îÇ Shard 1 ‚îÇ       ‚îÇ Shard N ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ All-Gather for Forward ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ Reduce-Scatter Backward ‚îÄ‚îò       ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ü¶Å Liger fused CE: –î–ê (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ú–æ–¥–µ–ª—å —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∞ –º–µ–∂–¥—É GPU
"""
        elif mode == "fsdp_offload":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FSDP + CPU Offload ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Shard 0 ‚îÇ  ‚îÇ Shard 1 ‚îÇ       ‚îÇ Shard N ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ All-Gather for Forward ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ           üíæ CPU RAM                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ        Offloaded Parameters          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚ö†Ô∏è Liger fused CE: –ù–ï–¢ (—É–º–µ–Ω—å—à–∏—Ç–µ batch)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ CPU ‚Äî —ç–∫–æ–Ω–æ–º–∏—è VRAM
"""
        elif mode == "fsdp2":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FSDP2 + CPU Offload ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ DTensor ‚îÇ  ‚îÇ DTensor ‚îÇ       ‚îÇ DTensor ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Shard 0 ‚îÇ  ‚îÇ Shard 1 ‚îÇ       ‚îÇ Shard N ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ Per-Parameter Sharding ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ           üíæ CPU RAM                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ        Offloaded Parameters          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚ö†Ô∏è Liger fused CE: –ù–ï–¢ (—É–º–µ–Ω—å—à–∏—Ç–µ batch)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
FSDP v2: DTensor + CPU Offload
"""
        elif "deepspeed" in mode:
            if "zero3" in mode:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Params  ‚îÇ  ‚îÇ Params  ‚îÇ       ‚îÇ Params  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim   ‚îÇ  ‚îÇ Optim   ‚îÇ       ‚îÇ Optim   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  {'+ CPU Offload (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ CPU)' if 'offload' in mode else ''}          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–í—Å—ë —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–æ: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è VRAM
"""
            else:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Full    ‚îÇ  ‚îÇ Full    ‚îÇ       ‚îÇ Full    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim/N ‚îÇ  ‚îÇ Optim/N ‚îÇ       ‚îÇ Optim/N ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω—ã
"""
        else:
            parallel_diagram = ""
        
        if parallel_diagram:
            st.markdown(f"""
<div class="model-ascii">
{parallel_diagram}
</div>
            """, unsafe_allow_html=True)


def export_model_to_hf(model, tokenizer, source_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π HF —Ñ–æ—Ä–º–∞—Ç.
    
    –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LoRA/QLoRA, –º–µ—Ä–¥–∂–∏—Ç –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É,
    —á—Ç–æ–±—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±—ã–ª–∞ "–≥–æ—Ç–æ–≤–æ–π" –∏ –∑–∞–≥—Ä—É–∂–∞–ª–∞—Å—å –∫–∞–∫ –æ–±—ã—á–Ω–∞—è.
    """
    try:
        from peft import PeftModel
        
        source = Path(source_path)
        # –°–æ–∑–¥–∞–µ–º –∏–º—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: export_TIMESTAMP
        export_name = f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –ï—Å–ª–∏ —ç—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä—è–¥–æ–º —Å –Ω–∏–º, –Ω–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
        # out/model/run/checkpoint_X -> out/model/run/export_X
        if "checkpoint" in source.name:
            export_dir = source.parent / f"export_{source.name}"
        else:
            export_dir = source.parent / export_name
            
        export_dir.mkdir(parents=True, exist_ok=True)
        
        # –ï—Å–ª–∏ —ç—Ç–æ PEFT-–º–æ–¥–µ–ª—å (LoRA/QLoRA) ‚Äî –º–µ—Ä–¥–∂–∏–º –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É
        export_model = model
        try:
            if isinstance(model, PeftModel):
                logger.info("Merging LoRA adapter into base model for export...")
                export_model = model.merge_and_unload()
                logger.info("LoRA adapter merged successfully")
        except Exception as e:
            logger.warning(f"LoRA merge failed during export, saving as-is: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        export_model.save_pretrained(export_dir, safe_serialization=True)
        tokenizer.save_pretrained(export_dir)
        
        return str(export_dir)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
        return None


# ============================================================================
# Main App
# ============================================================================

def main():
    render_header()
    
    # Sidebar configs
    model_config = render_model_config()
    st.session_state.current_model_name = model_config.get("model_name_input", "home_model")
    
    current_stage = model_config.get("stage", "pretrain")
    
    # GRPO –∏–º–µ–µ—Ç —Å–≤–æ—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è
    if current_stage == "grpo":
        grpo_sidebar_config = render_grpo_sidebar_config()
        # –î–ª—è GRPO –Ω–µ –Ω—É–∂–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        training_config = {}
        # –í–ê–ñ–ù–û: –î–ª—è GRPO —Ç–æ–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º distributed config –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ multi-GPU
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π training_config –¥–ª—è render_distributed_config
        dummy_training_config = {
            # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —á–µ—Å—Ç–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É –≤ GPU —Ç–∞–±–µ: microbatch = train_batch_size
            "batch_size": grpo_sidebar_config.get("grpo_train_batch_size", 2),
            "gradient_accumulation": grpo_sidebar_config.get("gradient_accumulation", 4),
        }
        # –ü–µ—Ä–µ–¥–∞—ë–º training_backend –∏–∑ model_config (–≤—ã–±—Ä–∞–Ω –¥–æ –º–µ—Ç–æ–¥–∞ —Ç—é–Ω–∏–Ω–≥–∞)
        grpo_backend = model_config.get("training_backend", "models-at-home")
        distributed_config = render_distributed_config(training_config=dummy_training_config, is_grpo=True, grpo_backend=grpo_backend)
    else:
        grpo_sidebar_config = {}
        training_config = render_training_config()
        distributed_config = render_distributed_config(training_config=training_config, is_grpo=False)
    
    # –ü–µ—Ä–µ–¥–∞–µ–º stage –≤ dataset_config
    # –î–ª—è GRPO –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ main area
    if current_stage != "grpo":
        dataset_config = render_dataset_config(stage=current_stage)
    else:
        dataset_config = {}
    
    output_config = render_output_config(st.session_state.current_model_name)
    
    # Merge configs
    # –í–ê–ñ–ù–û: grpo_sidebar_config –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –í–°–ï –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: grpo_sidebar_config > model_config (–¥–ª—è GRPO)
    full_config = {**model_config, **training_config, **dataset_config, **output_config, **grpo_sidebar_config}
    
    # –î–ª—è GRPO: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
    if current_stage == "grpo":
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã (–µ—Å–ª–∏ use_lora=True)
        # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ model_config (–∏–∑ render_model_config())
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º use_lora –∏–∑ tuning_method (lora/qlora = use_lora=True)
        tuning_method = model_config.get("tuning_method", "full")
        use_lora_from_model = tuning_method in ("lora", "qlora")
        
        # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥ lora/qlora, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
        if use_lora_from_model:
            if "lora_r" not in model_config or model_config.get("lora_r") is None:
                raise ValueError(
                    "‚ùå –í—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥ 'lora' –∏–ª–∏ 'qlora', –Ω–æ lora_r –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! "
                    "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤ —Å–µ–∫—Ü–∏–∏ 'üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞' —É–∫–∞–∑–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä 'LoRA r'."
                )
            if "lora_alpha" not in model_config or model_config.get("lora_alpha") is None:
                raise ValueError(
                    "‚ùå –í—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥ 'lora' –∏–ª–∏ 'qlora', –Ω–æ lora_alpha –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! "
                    "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤ —Å–µ–∫—Ü–∏–∏ 'üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞' —É–∫–∞–∑–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä 'LoRA alpha'."
                )
            # –ö–æ–ø–∏—Ä—É–µ–º LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ model_config –≤ full_config –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ train_gsm8k.py
            full_config["use_lora"] = True
            full_config["lora_r"] = model_config["lora_r"]
            full_config["lora_alpha"] = model_config["lora_alpha"]
            full_config["lora_dropout"] = model_config.get("lora_dropout")
            full_config["lora_target_modules"] = model_config.get("lora_target_modules")
            # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è qlora
            if tuning_method == "qlora":
                full_config["use_4bit"] = True  # QLoRA –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4-bit
                full_config["use_8bit"] = model_config.get("use_8bit", False)
            else:
                full_config["use_4bit"] = False
                full_config["use_8bit"] = False
        else:
            # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ full, use_lora=False
            full_config["use_lora"] = False
            full_config["use_4bit"] = False
            full_config["use_8bit"] = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
        required_grpo_params = [
            "grpo_algorithm", "grpo_group_size", "grpo_max_new_tokens",
            "grpo_temperature", "grpo_learning_rate", "grpo_kl_weight",
            "grpo_clip_eps_low"
        ]
        missing_params = [p for p in required_grpo_params if p not in full_config]
        if missing_params:
            raise ValueError(
                f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_params}. "
                f"–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã."
            )
    full_config["distributed_mode"] = distributed_config["distributed_mode"]
    full_config["num_gpus"] = distributed_config["num_gpus"]
    full_config["config_file"] = distributed_config["config_file"]
    full_config["gpu_ids"] = distributed_config.get("gpu_ids", [])
    # –í–ê–ñ–ù–û: –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ UI (sidebar) –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞–¥ –ø—Ä–µ—Å–µ—Ç–∞–º–∏ training_config.
    # –ò–Ω–∞—á–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–±–∏—Ä–∞–µ—Ç –æ–¥–Ω–æ, –∞ –≤ —Ä–∞–Ω —É–µ–∑–∂–∞–µ—Ç –¥—Ä—É–≥–æ–µ (–∫–∞–∫ –±—ã–ª–æ —Å mixed_precision=no -> bf16).
    full_config["mixed_precision"] = distributed_config.get("mixed_precision", "bf16")
    full_config["fp16_pure"] = distributed_config.get("fp16_pure", False)
    full_config["grad_checkpoint"] = distributed_config.get("grad_checkpoint", False)
    full_config["use_flash_attention"] = distributed_config.get("use_flash_attention", True)
    full_config["use_liger"] = distributed_config.get("use_liger", True)
    full_config["liger_fused_ce"] = distributed_config.get("liger_fused_ce", False)  # Fused CE –¥–ª—è pretrain/SFT
    # –î–ª—è GRPO training_backend –±–µ—Ä—ë—Ç—Å—è –∏–∑ grpo_sidebar_config (—É–∂–µ –≤ full_config)
    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ ‚Äî –∏–∑ distributed_config
    if current_stage != "grpo":
        full_config["training_backend"] = distributed_config.get("training_backend", "models-at-home")
    # –î–ª—è GRPO training_backend —É–∂–µ –µ—Å—Ç—å –≤ full_config –∏–∑ grpo_sidebar_config
    
    # –î–ª—è SFT, Continual Pretrain –∏ GRPO –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    if model_config.get("stage") in ("sft", "continual_pretrain", "grpo") and model_config.get("base_model_path"):
        full_config["tokenizer_path"] = model_config["base_model_path"]
    
    # Main content
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(["üöÄ –ó–∞–ø—É—Å–∫", "üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "üí¨ –ß–∞—Ç", "üìú –ò—Å—Ç–æ—Ä–∏—è", "üíæ –î–∞–Ω–Ω—ã–µ", "ü§ñ –ú–æ–¥–µ–ª–∏", "üìö –£—á–µ–±–Ω–∏–∫"])
    
    with tab1:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
            all_ready = render_quick_summary(model_config, dataset_config, distributed_config, full_config)
            
            # –ü–µ—Ä–µ–¥–∞–µ–º full_config, —á—Ç–æ–±—ã –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –ø–∞–º—è—Ç–∏ –≤–∏–¥–µ–ª batch_size –∏ grad_checkpoint
            render_model_preview(full_config, distributed_config)
            
            # SFT: Chat Template –º–æ–¥–µ–ª–∏ (–æ—Ç–¥–µ–ª—å–Ω—ã–π –±–ª–æ–∫ –ü–ï–†–ï–î –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –¥–∞–Ω–Ω—ã—Ö)
            if model_config.get("stage") == "sft" and model_config.get("base_model_path"):
                st.markdown("---")
                st.markdown("### üìù Chat Template –º–æ–¥–µ–ª–∏")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º chat_template –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                base_model_path = model_config.get("base_model_path")
                model_chat_template = None
                
                try:
                    from transformers import AutoTokenizer
                    with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ chat_template –∏–∑ –º–æ–¥–µ–ª–∏..."):
                        tok = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ session_state –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–µ–≤—å—é
                        st.session_state.sft_tokenizer = tok
                        if hasattr(tok, 'chat_template') and tok.chat_template:
                            model_chat_template = tok.chat_template
                except Exception as e:
                    st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {e}")
                    st.session_state.sft_tokenizer = None
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
                if model_chat_template:
                    st.success(f"‚úÖ Chat template –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –º–æ–¥–µ–ª–∏ `{Path(base_model_path).name}`")
                else:
                    st.info("‚ÑπÔ∏è –£ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ—Ç chat_template. –ë—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏.")
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º session_state –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if "sft_user_chat_template" not in st.session_state:
                    st.session_state.sft_user_chat_template = model_chat_template or ""
                
                # –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ–µ –ø–æ–ª–µ –¥–ª—è chat_template
                user_chat_template = st.text_area(
                    "Chat Template (Jinja2):",
                    value=st.session_state.sft_user_chat_template,
                    height=200,
                    key="sft_chat_template_field",
                    help="Jinja2 —à–∞–±–ª–æ–Ω –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤. –û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.",
                    placeholder="–û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–≥–æ–≤..."
                )
                
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å session_state
                st.session_state.sft_user_chat_template = user_chat_template
                
                # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                col_btn1, col_btn2, col_btn3 = st.columns(3)
                with col_btn1:
                    if model_chat_template and st.button("‚Ü©Ô∏è –í–µ—Ä–Ω—É—Ç—å –∏–∑ –º–æ–¥–µ–ª–∏", key="sft_restore_template"):
                        st.session_state.sft_user_chat_template = model_chat_template
                        st.rerun()
                with col_btn2:
                    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å", key="sft_clear_template"):
                        st.session_state.sft_user_chat_template = ""
                        st.rerun()
                with col_btn3:
                    if user_chat_template.strip():
                        st.caption(f"–î–ª–∏–Ω–∞: {len(user_chat_template)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –î–æ–±–∞–≤–ª—è–µ–º chat_template –≤ –∫–æ–Ω—Ñ–∏–≥
                full_config["chat_template"] = user_chat_template.strip() if user_chat_template.strip() else None
            
            # SFT Config (Main Area) - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            if model_config.get("stage") == "sft" and dataset_config.get("data_path"):
                st.markdown("---")
                sft_cfg = render_sft_main_config(dataset_config["data_path"])
                full_config.update(sft_cfg)
            
            # GRPO Config (Main Area) - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ reward —Ñ—É–Ω–∫—Ü–∏–π –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            if model_config.get("stage") == "grpo":
                st.markdown("---")
                grpo_main_cfg = render_grpo_main_config(dataset_config.get("data_path"))
                full_config.update(grpo_main_cfg)
            
            st.subheader("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
            st.json(full_config)
        
        with col2:
            st.subheader("üéÆ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
            
            if st.session_state.training_active:
                if st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", type="primary"):
                    with st.spinner("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É..."):
                        stopped = stop_training()
                    if stopped:
                        st.success("‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    else:
                        st.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å (–≤–æ–∑–º–æ–∂–Ω–æ —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞)")
                    time.sleep(1)
                    st.rerun()
            else:
                # –î–ª—è GRPO –æ—Ç–¥–µ–ª—å–Ω–∞—è –∫–Ω–æ–ø–∫–∞ –∏ –∑–∞–ø—É—Å–∫
                if model_config.get("stage") == "grpo":
                    button_disabled = not all_ready
                    if st.button("üß† –ù–∞—á–∞—Ç—å GRPO –æ–±—É—á–µ–Ω–∏–µ", type="primary", disabled=button_disabled):
                        with st.spinner("–ó–∞–ø—É—Å–∫ GRPO..."):
                            run_id, process = start_grpo_training(full_config)
                            st.session_state.current_run_id = run_id
                            st.session_state.training_process = process
                            st.session_state.training_active = True
                            save_active_run(run_id, full_config)
                            st.success(f"GRPO –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ! Run ID: {run_id}")
                            time.sleep(1)
                            st.rerun()
                    if button_disabled:
                        st.caption("‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å, –¥–∞–Ω–Ω—ã–µ –∏ —Ä–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞")
                else:
                    button_disabled = not all_ready
                    if st.button("‚ñ∂Ô∏è –ù–∞—á–∞—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É", type="primary", disabled=button_disabled):
                        with st.spinner("–ó–∞–ø—É—Å–∫..."):
                            run_id, process = start_training(full_config)
                            st.session_state.current_run_id = run_id
                            st.session_state.training_process = process
                            st.session_state.training_active = True
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–π run –¥–ª—è persistence
                            save_active_run(run_id, full_config)
                            st.success(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞! Run ID: {run_id}")
                            time.sleep(1)
                            st.rerun()
                    if button_disabled:
                        st.caption("‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å, –¥–∞–Ω–Ω—ã–µ –∏ —Ä–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞")
    
    with tab2:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fragment –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        live_metrics_fragment()
    
    with tab4:
        st.header("üìú –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—É—Å–∫–æ–≤")
        st.markdown("---")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —Ç–∏–ø–∞ active_run.json)
        runs = sorted([p for p in RUNS_DIR.iterdir() if p.is_dir()], reverse=True)
        
        if runs:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø—É—Å–∫–æ–≤
            for run_dir in runs[:30]: 
                run_id = run_dir.name
                metrics = load_metrics(run_id)
                
                if metrics:
                    status = metrics.get("status", "unknown")
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–∞–ø—É—Å–∫–∏ (–µ—Å–ª–∏ –æ–Ω–∏ —Å—Ç–∞—Ä—ã–µ –∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–¥–µ–ª–∞–ª–∏)
                    is_empty = metrics.get("current_step", 0) == 0 and not metrics.get("checkpoints")
                    if is_empty and status not in ("training", "running"):
                         continue

                    status_emoji = {"training": "üü¢", "completed": "‚úÖ", "error": "‚ùå", "stopped": "‚èπÔ∏è", "resumed": "‚ñ∂Ô∏è"}.get(status, "‚è≥")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    model_name_display = run_id
                    try:
                        config_path = run_dir / "config.json"
                        if config_path.exists():
                            with open(config_path) as f:
                                rc = json.load(f)
                                # –ò—â–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ output_dir
                                if "model_name_input" in rc:
                                    model_name_display = f"{run_id} | {rc['model_name_input']}"
                                elif "output_dir" in rc:
                                    out_d = Path(rc["output_dir"])
                                    # out/home_pretrain/run_id -> home_pretrain
                                    if out_d.name == run_id:
                                        model_name_display = f"{run_id} | {out_d.parent.name}"
                                    else:
                                        model_name_display = f"{run_id} | {out_d.name}"
                    except:
                        pass
                    
                    with st.expander(f"{status_emoji} {model_name_display}"):
                        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ metrics.json (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –∏–∑ config
                        model_params = None
                        try:
                            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ metrics.json (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ)
                            if "num_parameters" in metrics:
                                model_params = metrics["num_parameters"]
                            else:
                                # Fallback: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑ config
                                config_path = run_dir / "config.json"
                                if config_path.exists():
                                    with open(config_path) as f:
                                        rc = json.load(f)
                                        if "hidden_size" in rc and "num_layers" in rc:
                                            vocab_size = rc.get("vocab_size", 50257)
                                            intermediate_size = rc.get("intermediate_size")
                                            model_params = estimate_parameters(
                                                rc["hidden_size"],
                                                rc["num_layers"],
                                                vocab_size=vocab_size,
                                                intermediate_size=intermediate_size,
                                            )
                        except Exception:
                            pass
                        
                        col1, col2, col3, col4, col5 = st.columns(5)
                        with col1:
                            st.metric("Steps", metrics.get("current_step", 0))
                        with col2:
                            st.metric("Final Loss", f"{metrics.get('current_loss', 0):.4f}")
                        with col3:
                            if model_params:
                                st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(model_params))
                            else:
                                st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", "‚Äî")
                        with col4:
                            st.metric("Status", status)
                        with col5:
                            st.metric("Duration", metrics.get("training_duration", "-"))
                        
                        # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                        checkpoints = metrics.get("checkpoints", [])
                        if checkpoints:
                            st.markdown("**üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã:**")
                            # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è loss –∏–∑ history, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
                            loss_history = metrics.get("loss_history", [])
                            steps_history = metrics.get("steps_history", [])
                            
                            for ckpt in checkpoints:
                                ckpt_loss = ckpt.get("loss")
                                # –ï—Å–ª–∏ loss –Ω–µ—Ç –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ loss_history
                                if ckpt_loss is None and steps_history and loss_history:
                                    ckpt_step = ckpt.get("step")
                                    if ckpt_step in steps_history:
                                        idx = steps_history.index(ckpt_step)
                                        if idx < len(loss_history):
                                            ckpt_loss = loss_history[idx]
                                
                                if ckpt_loss is not None:
                                    st.caption(f"Step {ckpt['step']}: Loss {ckpt_loss:.4f} | {ckpt['path']}")
                                else:
                                    st.caption(f"Step {ckpt['step']}: {ckpt['path']}")
                        
                        # –ö–Ω–æ–ø–∫–∏
                        btn_col1, btn_col2, btn_col3 = st.columns(3)
                        with btn_col1:
                            if st.button(f"üìä –ú–µ—Ç—Ä–∏–∫–∏", key=f"metrics_{run_id}"):
                                st.session_state.current_run_id = run_id
                                st.toast(f"‚úÖ –í—ã–±—Ä–∞–Ω run: {run_id}. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", icon="üìä")
                        with btn_col2:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞
                            config_path = run_dir / "config.json"
                            if config_path.exists():
                                try:
                                    with open(config_path) as f:
                                        run_config = json.load(f)
                                    model_dir = PROJECT_ROOT / run_config.get("output_dir", "")
                                    final_model = model_dir / "final_model"
                                    if final_model.exists():
                                        if st.button("üí¨ –ß–∞—Ç", key=f"chat_run_{run_id}"):
                                            st.session_state.selected_chat_model = str(final_model)
                                            st.toast("‚úÖ –ú–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üí¨ –ß–∞—Ç", icon="üí¨")
                                except:
                                    pass
                        with btn_col3:
                            # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–µ—Å–ª–∏ –±—ã–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã)
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç –†–ï–ê–õ–¨–ù–û —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–∞ –¥–∏—Å–∫–µ
                            valid_ckpt = None
                            if checkpoints:
                                latest_ckpt_path = checkpoints[-1]['path']
                                # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –µ—Å–ª–∏ –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
                                abs_ckpt_path = Path(latest_ckpt_path)
                                if not abs_ckpt_path.is_absolute():
                                    abs_ckpt_path = PROJECT_ROOT / latest_ckpt_path
                                
                                if abs_ckpt_path.exists():
                                    valid_ckpt = str(abs_ckpt_path)

                            if valid_ckpt:
                                if st.button("‚ñ∂Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å", key=f"continue_{run_id}", help="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"):
                                    try:
                                        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å—Ç–∞—Ä–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                                        config_path = run_dir / "config.json"
                                        with open(config_path) as f:
                                            old_config = json.load(f)
                                        
                                        # 3. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º output_dir —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
                                        # –°—Ç–∞—Ä—ã–π output_dir —É–∫–∞–∑—ã–≤–∞–ª –Ω–∞ –ø–∞–ø–∫—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ (run_ID)
                                        # –ú—ã —Ö–æ—Ç–∏–º —á—Ç–æ–±—ã –Ω–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –±—ã–ª –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å —Å—Ç–∞—Ä—ã–º (–≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –ø–∞–ø–∫–µ)
                                        old_output_dir = Path(old_config.get("output_dir", ""))
                                        # –ï—Å–ª–∏ –ø—É—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π - –±–µ—Ä–µ–º —Ä–æ–¥–∏—Ç–µ–ª—è. –ï—Å–ª–∏ –Ω–µ—Ç - —Ç–æ–∂–µ (–Ω–∞–¥–µ–µ–º—Å—è)
                                        # start_training –æ–∂–∏–¥–∞–µ—Ç –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                                        old_config["output_dir"] = str(old_output_dir.parent)
                                        
                                        # 4. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ resume (–ê–ë–°–û–õ–Æ–¢–ù–´–ô –ü–£–¢–¨)
                                        old_config["resume_from_checkpoint"] = valid_ckpt
                                        
                                        # 5. –ó–∞–ø—É—Å–∫–∞–µ–º
                                        with st.spinner(f"–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å {valid_ckpt}..."):
                                            new_run_id, process = start_training(old_config)
                                            st.session_state.current_run_id = new_run_id
                                            st.session_state.training_process = process
                                            st.session_state.training_active = True
                                            
                                            save_active_run(new_run_id, old_config)
                                            
                                            st.success(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∞! Run ID: {new_run_id}")
                                            time.sleep(1)
                                            st.rerun()
                                            
                                    except Exception as e:
                                        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å: {e}")
                            elif checkpoints:
                                # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –±—ã–ª–∏ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö, –Ω–æ —É–¥–∞–ª–µ–Ω—ã —Å –¥–∏—Å–∫–∞
                                st.button("‚ö†Ô∏è –§–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã", key=f"gone_{run_id}", disabled=True, help=f"–ß–µ–∫–ø–æ–∏–Ω—Ç {checkpoints[-1]['path']} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –¥–∏—Å–∫–µ")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –≤—ã–±—Ä–∞–Ω–æ
                        if st.session_state.current_run_id == run_id:
                            st.info("üëÜ –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**")
        else:
            st.info("–ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤")
            
    with tab5:
        render_data_manager(stage=current_stage)
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø—Ä–æ —á–∞—Ç
        st.markdown("---")
        st.info("üí° –ß—Ç–æ–±—ã –ø–æ–æ–±—â–∞—Ç—å—Å—è —Å –º–æ–¥–µ–ª—å—é, –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üí¨ –ß–∞—Ç** (–≤ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)")

    with tab6:
        render_model_manager()
    
    with tab7:
        render_docs()
    
    with tab3:
        st.header("üí¨ –ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é")
        st.markdown("---")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        available_models = get_available_models()
        
        if available_models:
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            col1, col2 = st.columns([3, 1])
            
            with col1:
                model_options = [m["name"] for m in available_models]
                
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ - –Ω–∞—Ö–æ–¥–∏–º –µ—ë –∏–Ω–¥–µ–∫—Å
                default_idx = 0
                if st.session_state.selected_chat_model:
                    for i, m in enumerate(available_models):
                        if m["path"] == st.session_state.selected_chat_model:
                            default_idx = i
                            break
                
                selected_model_name = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç",
                    options=model_options,
                    index=default_idx,
                    help="–í—ã–±–µ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞"
                )
                selected_model = next(m for m in available_models if m["name"] == selected_model_name)
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º selected_chat_model –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                if st.session_state.selected_chat_model:
                    st.session_state.selected_chat_model = None
            
            with col2:
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
                model_type = selected_model.get("type", "unknown")
                training_type = selected_model.get("training_type", "unknown")
                is_lora = selected_model.get("is_lora", False)
                model_info = selected_model.get("model_info", {})
                
                # –ö–∞—Ä—Ç–æ—á–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
                info_cols = st.columns([2, 1])
                with info_cols[0]:
                    # –¢–∏–ø –º–æ–¥–µ–ª–∏
                    type_labels = {
                        "final": "‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å",
                        "checkpoint": "üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç",
                        "lora": "üîß LoRA –∞–¥–∞–ø—Ç–µ—Ä",
                        "hf": "ü§ó HuggingFace"
                    }
                    st.markdown(f"**{type_labels.get(model_type, 'üì¶ –ú–æ–¥–µ–ª—å')}**")
                    
                    # –¢–∏–ø —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
                    training_labels = {
                        "pretrain": "Pre-training",
                        "sft": "SFT (Supervised Fine-Tuning)",
                        "grpo": "GRPO (Reasoning)",
                        "base": "Base Model"
                    }
                    if training_type != "unknown":
                        st.caption(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞: {training_labels.get(training_type, training_type)}")
                
                with info_cols[1]:
                    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                    if model_info.get("max_context"):
                        st.metric("–ö–æ–Ω—Ç–µ–∫—Å—Ç", f"{model_info['max_context']:,}")
                    if is_lora and model_info.get("lora_r"):
                        st.caption(f"LoRA r={model_info['lora_r']}")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è LoRA
                if is_lora and model_info.get("base_model"):
                    st.info(f"üîó –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {model_info['base_model']}")
                
                st.caption(f"üìÅ {selected_model['path']}")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            with st.expander("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", expanded=True):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –º–æ–¥–µ–ª–∏
                max_context = model_info.get("max_context") or 32168
                default_max_tokens = min(256, max_context // 4)
                max_tokens_limit = min(max_context, 32168)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º –º–∞–∫—Å–∏–º—É–º–æ–º
                
                gen_col1, gen_col2 = st.columns(2)
                with gen_col1:
                    max_tokens = st.slider(
                        "Max New Tokens", 
                        min_value=16, 
                        max_value=max_tokens_limit, 
                        value=default_max_tokens,
                        step=16,
                        help=f"–ú–∞–∫—Å–∏–º—É–º –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ö–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏: {max_context:,}"
                    )
                    temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.05)
                
                with gen_col2:
                    top_p = st.slider("Top-p (nucleus)", 0.1, 1.0, 0.9, 0.05)
                    top_k = st.slider("Top-k", 0, 100, 50, help="0 = –æ—Ç–∫–ª—é—á–µ–Ω–æ")

                # Inference Backend
                from homellm.app.vllm_chat import is_vllm_available
                vllm_available = is_vllm_available()
                
                st.markdown("---")
                backend_col1, backend_col2 = st.columns(2)
                
                with backend_col1:
                    backend_options = ["Transformers"]
                    if vllm_available:
                        backend_options.append("vLLM (–±—ã—Å—Ç—Ä–µ–µ)")
                    
                    if "chat_inference_backend" not in st.session_state:
                        st.session_state.chat_inference_backend = "Transformers"
                    
                    inference_backend = st.selectbox(
                        "Inference Backend",
                        options=backend_options,
                        index=backend_options.index(st.session_state.chat_inference_backend) if st.session_state.chat_inference_backend in backend_options else 0,
                        help="vLLM: –±—ã—Å—Ç—Ä–µ–µ (PagedAttention), –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ VRAM",
                        key="chat_backend_select",
                    )
                    st.session_state.chat_inference_backend = inference_backend
                    
                    if not vllm_available:
                        st.caption("‚ÑπÔ∏è vLLM: `pip install vllm`")
                
                with backend_col2:
                    # –†–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞ - –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞–ª–∏—á–∏—è chat_template
                    if "chat_prompt_mode" not in st.session_state:
                        st.session_state.chat_prompt_mode = "completion"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ chat_template —É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                    has_template = st.session_state.get("chat_has_template", False)
                    model_loaded = st.session_state.get("chat_backend") is not None
                    
                    if model_loaded and has_template:
                        # –û–±–∞ —Ä–µ–∂–∏–º–∞ –¥–æ—Å—Ç—É–ø–Ω—ã
                        prompt_mode_label = st.selectbox(
                            "–†–µ–∂–∏–º",
                            options=["Chat (template)", "Completion"],
                            index=0 if st.session_state.chat_prompt_mode == "chat" else 1,
                            help="Chat: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç chat_template –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞",
                            key="chat_prompt_mode_select",
                        )
                        prompt_mode = "chat" if "Chat" in prompt_mode_label else "completion"
                    elif model_loaded and not has_template:
                        # –¢–æ–ª—å–∫–æ Completion, Chat –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                        st.selectbox(
                            "–†–µ–∂–∏–º",
                            options=["Completion (no chat_template)"],
                            index=0,
                            disabled=True,
                            help="–£ –º–æ–¥–µ–ª–∏ –Ω–µ—Ç chat_template - —Ç–æ–ª—å–∫–æ —Ä–µ–∂–∏–º Completion",
                            key="chat_prompt_mode_select",
                        )
                        prompt_mode = "completion"
                    else:
                        # –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º placeholder
                        st.selectbox(
                            "–†–µ–∂–∏–º",
                            options=["–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å..."],
                            index=0,
                            disabled=True,
                            key="chat_prompt_mode_select",
                        )
                        prompt_mode = st.session_state.chat_prompt_mode
                    
                    st.session_state.chat_prompt_mode = prompt_mode
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞
            if "chat_model" not in st.session_state:
                st.session_state.chat_model = None
                st.session_state.chat_tokenizer = None
                st.session_state.chat_model_path = None
                st.session_state.chat_has_template = False
                st.session_state.chat_prompt_mode = "completion"
                st.session_state.chat_backend = None  # VLLMChatBackend –∏–ª–∏ TransformersChatBackend
                st.session_state.chat_backend_type = "transformers"  # "transformers" –∏–ª–∏ "vllm"
            
            if "messages" not in st.session_state:
                st.session_state.messages = []
            
            # –ö–Ω–æ–ø–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            if st.session_state.chat_model_path != selected_model["path"]:
                if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
                    with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å..."):
                        try:
                            from transformers import AutoTokenizer, AutoModelForCausalLM
                            from homellm.models.adapters import detect_model_type
                            from homellm.models.home_model import HomeForCausalLM
                            from homellm.app.vllm_chat import VLLMChatBackend, TransformersChatBackend, is_vllm_available
                            
                            model_path = Path(selected_model["path"])
                            device = "cuda" if torch.cuda.is_available() else "cpu"
                            dtype = torch.float16 if device == "cuda" else torch.float32
                            dtype_str = "float16" if device == "cuda" else "float32"
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ config.json –∏–ª–∏ adapter_config.json
                            config_json = model_path / "config.json"
                            adapter_config_path = model_path / "adapter_config.json"
                            is_lora_adapter = adapter_config_path.exists()
                            
                            if not config_json.exists() and not is_lora_adapter:
                                raise ValueError(f"config.json –∏–ª–∏ adapter_config.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {model_path}")
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
                            model_type = detect_model_type(model_path) if config_json.exists() else "hf"
                            
                            # –í—ã–±—Ä–∞–Ω–Ω—ã–π backend
                            use_vllm = inference_backend.startswith("vLLM") and is_vllm_available()
                            
                            st.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ {'vLLM' if use_vllm else 'Transformers'}...")
                            
                            # === vLLM Backend ===
                            if use_vllm:
                                # –î–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Å vLLM: –∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + hot-swap LoRA
                                if is_lora_adapter:
                                    with open(adapter_config_path) as f:
                                        adapter_cfg = json.load(f)
                                    base_model_id = adapter_cfg.get("base_model_name_or_path")
                                    
                                    if not base_model_id:
                                        raise ValueError("base_model_name_or_path –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ adapter_config.json")
                                    
                                    st.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ vLLM: {base_model_id}")
                                    
                                    chat_backend = VLLMChatBackend(
                                        model_path=base_model_id,
                                        dtype=dtype_str,
                                        gpu_memory_utilization=0.9,
                                        enable_lora=True,
                                        max_lora_rank=adapter_cfg.get("r", 64),
                                    )
                                    chat_backend.set_lora(str(model_path))
                                    st.success("‚úÖ vLLM –∑–∞–≥—Ä—É–∂–µ–Ω —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–º")
                                else:
                                    chat_backend = VLLMChatBackend(
                                        model_path=str(model_path),
                                        dtype=dtype_str,
                                        gpu_memory_utilization=0.9,
                                    )
                                    st.success("‚úÖ vLLM –∑–∞–≥—Ä—É–∂–µ–Ω")
                                
                                st.session_state.chat_model = None  # vLLM —É–ø—Ä–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å—é –≤–Ω—É—Ç—Ä–∏
                                st.session_state.chat_tokenizer = chat_backend.tokenizer
                                st.session_state.chat_backend = chat_backend
                                st.session_state.chat_backend_type = "vllm"
                                st.session_state.chat_has_template = chat_backend.has_chat_template
                            
                            # === Transformers Backend ===
                            else:
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                                tokenizer = None
                                tokenizer_source = None
                                
                                # –î–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ - —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
                                if is_lora_adapter:
                                    try:
                                        with open(adapter_config_path) as f:
                                            adapter_cfg = json.load(f)
                                        base_model_id = adapter_cfg.get("base_model_name_or_path")
                                        if base_model_id:
                                            tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
                                            tokenizer_source = base_model_id
                                    except Exception:
                                        pass
                                
                                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –ø–∞–ø–∫–∏ –º–æ–¥–µ–ª–∏
                                if tokenizer is None:
                                    try:
                                        tokenizer = AutoTokenizer.from_pretrained(str(model_path), trust_remote_code=True)
                                        tokenizer_source = str(model_path)
                                    except Exception:
                                        pass
                                
                                # Fallback: –∏—â–µ–º –≤ run config
                                if tokenizer is None:
                                    try:
                                        run_root = model_path.parent if "checkpoint" in model_path.name else model_path
                                        run_id = run_root.name
                                        run_cfg_path = RUNS_DIR / run_id / "config.json"
                                        if run_cfg_path.exists():
                                            with open(run_cfg_path, "r", encoding="utf-8") as f:
                                                run_cfg = json.load(f)
                                            tok_src = run_cfg.get("tokenizer_path") or run_cfg.get("base_model_path")
                                            if tok_src:
                                                tokenizer = AutoTokenizer.from_pretrained(str(tok_src), trust_remote_code=True)
                                                tokenizer_source = tok_src
                                    except Exception:
                                        pass
                                
                                # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback - GPT2 (–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º)
                                if tokenizer is None:
                                    st.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPT2 (–±–µ–∑ chat_template)")
                                    tokenizer = AutoTokenizer.from_pretrained("gpt2")
                                    tokenizer_source = "gpt2"
                                else:
                                    if tokenizer_source:
                                        st.caption(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {tokenizer_source}")
                                
                                # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
                                if is_lora_adapter:
                                    st.info("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω LoRA –∞–¥–∞–ø—Ç–µ—Ä, –∑–∞–≥—Ä—É–∂–∞–µ–º —Å merge...")
                                    try:
                                        from peft import PeftModel
                                        
                                        with open(adapter_config_path) as f:
                                            adapter_cfg = json.load(f)
                                        base_model_id = adapter_cfg.get("base_model_name_or_path")
                                        
                                        if not base_model_id:
                                            raise ValueError("base_model_name_or_path –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ adapter_config.json")
                                        
                                        st.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å: {base_model_id}")
                                        
                                        base_model = AutoModelForCausalLM.from_pretrained(
                                            base_model_id, torch_dtype=dtype, trust_remote_code=True
                                        )
                                        model = PeftModel.from_pretrained(base_model, str(model_path))
                                        
                                        st.info("Merging LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã...")
                                        model = model.merge_and_unload()
                                        st.success("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã")
                                        
                                    except ImportError:
                                        st.error("‚ùå –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Ç—Ä–µ–±—É–µ—Ç—Å—è peft. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install peft")
                                        raise
                                else:
                                    if model_type == "home":
                                        model = HomeForCausalLM.from_pretrained(str(model_path), torch_dtype=dtype)
                                    else:
                                        model = AutoModelForCausalLM.from_pretrained(
                                            str(model_path), trust_remote_code=True, torch_dtype=dtype
                                        )
                                
                                model = model.to(device)
                                model.eval()
                                
                                if tokenizer.pad_token is None:
                                    if tokenizer.eos_token:
                                        tokenizer.pad_token = tokenizer.eos_token
                                
                                chat_backend = TransformersChatBackend(model, tokenizer, device)
                                
                                st.session_state.chat_model = model
                                st.session_state.chat_tokenizer = tokenizer
                                st.session_state.chat_backend = chat_backend
                                st.session_state.chat_backend_type = "transformers"
                                st.session_state.chat_has_template = bool(getattr(tokenizer, "chat_template", None))
                                st.success("‚úÖ Transformers –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                            
                            st.session_state.chat_model_path = str(model_path)
                            st.session_state.messages = []
                            st.session_state.chat_prompt_mode = "chat" if st.session_state.chat_has_template else "completion"
                            st.rerun()
                        except Exception as e:
                            import traceback
                            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                            st.code(traceback.format_exc())
                            
                            # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ (–µ—Å–ª–∏ AutoModelForCausalLM –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
                            st.warning("–ü—Ä–æ–±—É–µ–º fallback –∑–∞–≥—Ä—É–∑–∫—É –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤...")
                            try:
                                from homellm.models.home_model import HomeForCausalLM, HomeConfig
                                from safetensors.torch import load_file
                                
                                model_safetensors = model_path / "model.safetensors"
                                model_bin = model_path / "pytorch_model.bin"
                                
                                if not (model_safetensors.exists() or model_bin.exists()):
                                    raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏")
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                                if not st.session_state.get("chat_tokenizer"):
                                    st.session_state.chat_tokenizer = AutoTokenizer.from_pretrained("gpt2")
                                    if st.session_state.chat_tokenizer.pad_token is None:
                                        st.session_state.chat_tokenizer.pad_token = st.session_state.chat_tokenizer.eos_token
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
                                if config_json.exists():
                                    config = HomeConfig.from_pretrained(str(model_path))
                                else:
                                    # –ò—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                                    run_config_path = model_path.parent / "run_config.json"
                                    if run_config_path.exists():
                                        import json as json_module
                                        with open(run_config_path) as f:
                                            run_cfg = json_module.load(f)
                                        config = HomeConfig(
                                            vocab_size=len(st.session_state.chat_tokenizer),
                                            hidden_size=run_cfg.get("hidden_size", 512),
                                            num_hidden_layers=run_cfg.get("num_layers", 8),
                                            num_attention_heads=run_cfg.get("n_heads", 8),
                                            max_position_embeddings=run_cfg.get("seq_len", 512),
                                        )
                                    else:
                                        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω config.json")
                                
                                # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                                st.session_state.chat_model = HomeForCausalLM(config)
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
                                if model_safetensors.exists():
                                    state_dict = load_file(str(model_safetensors))
                                else:
                                    state_dict = torch.load(str(model_bin), map_location="cpu")
                                
                                missing, unexpected = st.session_state.chat_model.load_state_dict(state_dict, strict=False)
                                
                                if missing:
                                    real_missing = [k for k in missing if k != "lm_head.weight"]
                                    if real_missing:
                                        st.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–µ—Å–∞: {real_missing[:5]}...")
                                
                                if hasattr(st.session_state.chat_model, "tie_weights"):
                                    st.session_state.chat_model.tie_weights()
                                
                                st.session_state.chat_model = st.session_state.chat_model.to(device)
                                st.session_state.chat_model.eval()
                                st.session_state.chat_model_path = str(model_path)
                                st.session_state.messages = []
                                st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (fallback –º–µ—Ç–æ–¥)!")
                                st.rerun()
                            except Exception as e2:
                                st.error(f"Fallback –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
                                import traceback
                                st.code(traceback.format_exc())
            else:
                # === –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–∞–Ω–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ===
                backend_type = st.session_state.get("chat_backend_type", "transformers")
                backend_emoji = "‚ö°" if backend_type == "vllm" else "üîß"
                
                st.success(f"{backend_emoji} –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: **{selected_model_name}**")
                
                # –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
                info_cols = st.columns([2, 1, 1])
                with info_cols[0]:
                    st.caption(f"Backend: {backend_type.upper()}")
                with info_cols[1]:
                    if st.session_state.chat_has_template:
                        st.caption("‚úÖ Chat template")
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É –æ —à–∞–±–ª–æ–Ω–µ
                        tokenizer = st.session_state.chat_tokenizer
                        if tokenizer and hasattr(tokenizer, 'chat_template'):
                            template_preview = str(tokenizer.chat_template)[:100]
                            if len(template_preview) == 100:
                                template_preview += "..."
                            st.caption(f"```{template_preview[:50]}...```")
                    else:
                        st.caption("‚ö†Ô∏è –ù–µ—Ç chat template")
                        st.caption("–¢–æ–ª—å–∫–æ —Ä–µ–∂–∏–º Completion")
                with info_cols[2]:
                    if selected_model.get("is_lora"):
                        st.caption("üîß LoRA")
                
                # –ö–Ω–æ–ø–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π
                action_cols = st.columns([1, 1, 1])
                with action_cols[0]:
                    if st.button("üóëÔ∏è –í—ã–≥—Ä—É–∑–∏—Ç—å", help="–û—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–∞–º—è—Ç—å"):
                        st.session_state.chat_model = None
                        st.session_state.chat_backend = None
                        st.session_state.chat_tokenizer = None
                        st.session_state.chat_model_path = None
                        st.session_state.messages = []
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        st.rerun()
                
                with action_cols[1]:
                    if st.button("üîÑ –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
                        st.session_state.messages = []
                        st.rerun()
                
                with action_cols[2]:
                    # –ö–Ω–æ–ø–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ (–¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤)
                    if st.session_state.chat_model is not None:
                        if st.button("üíæ –≠–∫—Å–ø–æ—Ä—Ç HF"):
                            with st.spinner("–≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏..."):
                                export_path = export_model_to_hf(
                                    st.session_state.chat_model, 
                                    st.session_state.chat_tokenizer, 
                                    st.session_state.chat_model_path
                                )
                                if export_path:
                                    st.success(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: `{export_path}`")
                                    time.sleep(2)
                                    st.rerun()

                # --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–ü–¢–ê ---
                with st.expander("üí¨ –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", expanded=False):
                    # –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
                    preset_prompts = {
                        "–ù–µ—Ç": "",
                        "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç": "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É.",
                        "Reasoning": "–¢—ã ‚Äî –ò–ò –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –°–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å—É–∂–¥–∞–π –ø–æ—à–∞–≥–æ–≤–æ –≤ —Ç–µ–≥–µ <think>, –∑–∞—Ç–µ–º –¥–∞–π –æ—Ç–≤–µ—Ç.",
                        "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç": "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç. –ü–∏—à–∏ —á–∏—Å—Ç—ã–π, —á–∏—Ç–∞–µ–º—ã–π –∫–æ–¥ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏.",
                        "–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫": "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫. –ü–µ—Ä–µ–≤–æ–¥–∏ —Ç–µ–∫—Å—Ç —Ç–æ—á–Ω–æ, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç–∏–ª—å.",
                        "–ö–∞—Å—Ç–æ–º–Ω—ã–π": None
                    }
                    
                    preset = st.selectbox(
                        "–®–∞–±–ª–æ–Ω",
                        options=list(preset_prompts.keys()),
                        index=0,
                        key="system_prompt_preset"
                    )
                    
                    if preset != "–ö–∞—Å—Ç–æ–º–Ω—ã–π" and preset != "–ù–µ—Ç":
                        st.session_state.system_prompt = preset_prompts[preset]
                        st.code(preset_prompts[preset], language=None)
                    elif preset == "–ö–∞—Å—Ç–æ–º–Ω—ã–π":
                        system_prompt_input = st.text_area(
                            "–°–≤–æ–π –ø—Ä–æ–º–ø—Ç:",
                            value=st.session_state.get("system_prompt", ""),
                            height=100,
                            key="system_prompt_input"
                        )
                        st.session_state.system_prompt = system_prompt_input.strip()
                    else:
                        st.session_state.system_prompt = ""
                        st.caption("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –º–æ–¥–µ–ª–∏")
                
                # --- –ò–ù–¢–ï–†–§–ï–ô–° –ß–ê–¢–ê –° –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ú –°–ö–†–û–õ–õ–û–ú ---
                chat_container = st.container(height=500) # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                
                with chat_container:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
                    for message in st.session_state.messages:
                        with st.chat_message(message["role"]):
                            st.write(message["content"])
                
                # –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤—Å–µ–≥–¥–∞ –≤–Ω–∏–∑—É)
                if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."):
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —é–∑–µ—Ä–∞ —Å—Ä–∞–∑—É)
                    with chat_container:
                        with st.chat_message("user"):
                            st.write(prompt)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                    with chat_container: # –û—Ç–≤–µ—Ç —Ç–æ–∂–µ –ø–∏—à–µ–º –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                        with st.chat_message("assistant"):
                            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è..."):
                                try:
                                    chat_backend = st.session_state.chat_backend
                                    tokenizer = st.session_state.chat_tokenizer
                                    model = st.session_state.chat_model
                                    backend_type = st.session_state.get("chat_backend_type", "transformers")
                                    
                                    # –ë–µ—Ä–µ–º –∏—Å—Ç–æ—Ä–∏—é + –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                                    conversation = st.session_state.messages.copy()
                                    
                                    has_template = st.session_state.chat_has_template
                                    use_chat_template = (prompt_mode == "chat") and has_template
                                    # –ï—Å–ª–∏ –∫–∞–∫–∏–º-—Ç–æ –æ–±—Ä–∞–∑–æ–º –≤—ã–±—Ä–∞–Ω chat —Ä–µ–∂–∏–º –±–µ–∑ template - fallback
                                    if prompt_mode == "chat" and not has_template:
                                        use_chat_template = False

                                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ chat_template)
                                    if use_chat_template:
                                        system_prompt = st.session_state.get("system_prompt", "").strip()
                                        
                                        if conversation and conversation[0].get("role") == "system":
                                            conversation.pop(0)
                                        
                                        if system_prompt:
                                            conversation.insert(0, {"role": "system", "content": system_prompt})
                                    
                                    # –§–æ—Ä–º–∏—Ä—É–µ–º prompt_text
                                    if use_chat_template:
                                        prompt_text = chat_backend.apply_chat_template(
                                            conversation, 
                                            add_generation_prompt=True
                                        )
                                    else:
                                        prompt_text = ""
                                        for m in conversation:
                                            prompt_text += f"{m['role']}: {m['content']}\n"
                                        prompt_text += "assistant: "
                                    
                                    # === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ backend ===
                                    response = chat_backend.generate(
                                        prompt=prompt_text,
                                        max_tokens=max_tokens,
                                        temperature=temperature,
                                        top_p=top_p,
                                        top_k=top_k if top_k > 0 else -1,
                                    )
                                    
                                    st.write(response)
                                    st.session_state.messages.append({"role": "assistant", "content": response})
                                except Exception as e:
                                    import traceback
                                    st.session_state.last_chat_error = traceback.format_exc()
                                    st.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                                    st.code(st.session_state.last_chat_error)
        else:
            st.info("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤–æ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–ø—É—Å–∫'!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥–¥–µ –∏—Å–∫–∞—Ç—å –º–æ–¥–µ–ª–∏
            st.markdown("""
            **–ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:**
            - `out/*/final_model/` ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
            - `out/*/checkpoint_*/` ‚Äî —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            """)


if __name__ == "__main__":
    main()


