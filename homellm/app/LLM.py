"""
Motels at Home Training Studio ‚Äî –í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π
======================================================================

–ó–∞–ø—É—Å–∫:
    streamlit run homellm/app/main.py
    
–∏–ª–∏:
    ./scripts/run_studio.sh
"""

import streamlit as st
import logging
import subprocess
import json
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

import os
import signal
from pathlib import Path
from datetime import datetime
from typing import Tuple
from contextlib import suppress
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import torch
from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names, load_dataset_builder  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
try:
    from .docs import render_docs
except ImportError:
    from docs import render_docs

# –ü—É—Ç–∏
PROJECT_ROOT = Path(__file__).parent.parent.parent
CONFIGS_DIR = PROJECT_ROOT / "configs"
DATASET_DIR = PROJECT_ROOT / "datasets"  # datasets —Å "s"!
MODELS_DIR = PROJECT_ROOT / "models"  # –°–∫–∞—á–∞–Ω–Ω—ã–µ HF –º–æ–¥–µ–ª–∏
OUTPUT_DIR = PROJECT_ROOT / "out"
RUNS_DIR = PROJECT_ROOT / ".runs"
RUNS_DIR.mkdir(exist_ok=True)
MODELS_DIR.mkdir(exist_ok=True)

# ============================================================================
# Page Config
# ============================================================================

st.set_page_config(
    page_title="HomeLLM Training Studio",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS ‚Äî —á–∏—Å—Ç–∞—è —Ç—ë–º–Ω–∞—è —Ç–µ–º–∞ —Å —Ö–æ—Ä–æ—à–∏–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–º
st.markdown("""
<style>
    /* –ó–∞–≥–æ–ª–æ–≤–æ–∫ */
    .main-header {
        color: #ff6b6b;
        font-size: 2.5rem;
        font-weight: 800;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .sub-header {
        color: #888888;
        text-align: center;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    
    /* –°—Ç–∞—Ç—É—Å */
    .status-running {
        color: #22c55e !important;
        font-weight: bold;
    }
    
    .status-completed {
        color: #3b82f6 !important;
        font-weight: bold;
    }
    
    .status-error {
        color: #ef4444 !important;
        font-weight: bold;
    }
    
    /* ASCII art –±–ª–æ–∫ */
    .model-ascii {
        background: #1e1e1e;
        border: 1px solid #333;
        border-radius: 8px;
        padding: 1rem;
        font-family: 'Courier New', monospace;
        color: #00ff88;
        white-space: pre;
        overflow-x: auto;
    }
    
    /* –ö–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ç—Ä–∏–∫ */
    div[data-testid="metric-container"] {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 0.5rem;
    }
    
    /* –ö–Ω–æ–ø–∫–∏ –∑–∞–ø—É—Å–∫–∞ */
    .stButton > button[kind="primary"] {
        background: linear-gradient(90deg, #e94560, #ff6b6b);
        color: white !important;
        border: none;
        font-weight: 600;
    }
    
    /* Code –±–ª–æ–∫–∏ */
    pre {
        background: #0d1117 !important;
        color: #c9d1d9 !important;
        border: 1px solid #30363d !important;
    }
    
    code {
        color: #79c0ff !important;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# Persistence ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞–º–∏
# ============================================================================

ACTIVE_RUN_FILE = RUNS_DIR / "active_run.json"


def save_active_run(run_id: str, config: dict = None):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –≤ —Ñ–∞–π–ª."""
    data = {
        "run_id": run_id,
        "started_at": datetime.now().isoformat(),
        "config": config or {}
    }
    with open(ACTIVE_RUN_FILE, "w") as f:
        json.dump(data, f, indent=2)


def load_active_run() -> dict | None:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –∏–∑ —Ñ–∞–π–ª–∞."""
    if not ACTIVE_RUN_FILE.exists():
        return None
    try:
        with open(ACTIVE_RUN_FILE) as f:
            return json.load(f)
    except:
        return None


def clear_active_run():
    """–û—á–∏—Å—Ç–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run."""
    if ACTIVE_RUN_FILE.exists():
        ACTIVE_RUN_FILE.unlink()


def restore_session_state():
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π run
    active = load_active_run()
    if active and active.get("run_id"):
        run_id = active["run_id"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ run –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        run_dir = RUNS_DIR / run_id
        if run_dir.exists():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ metrics.json)
            pid_path = run_dir / "pid"
            metrics_path = run_dir / "metrics.json"
            process_alive = False
            metrics = None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∏–∑ metrics.json (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
            if metrics_path.exists():
                try:
                    import time
                    metrics_mtime = metrics_path.stat().st_mtime
                    metrics_age_minutes = (time.time() - metrics_mtime) / 60
                    
                    with open(metrics_path) as f:
                        metrics = json.load(f)
                    
                    status = metrics.get("status", "")
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–π - —Ç–æ—á–Ω–æ –Ω–µ –∂–∏–≤
                    if status in ("completed", "error", "stopped"):
                        process_alive = False
                    else:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å —á–µ—Ä–µ–∑ PID
                        if pid_path.exists():
                            try:
                                with open(pid_path) as f:
                                    pid = int(f.read().strip())
                                os.kill(pid, 0)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
                                process_alive = True
                            except ProcessLookupError:
                                process_alive = False
                            except (ValueError, PermissionError):
                                # PermissionError –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤
                                # –ù–æ –µ—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ —Å–≤–µ–∂–∏–µ (< 5 –º–∏–Ω—É—Ç) - —Å—á–∏—Ç–∞–µ–º –∂–∏–≤—ã–º
                                process_alive = metrics_age_minutes < 5
                    
                    # –ï—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å –¥–∞–≤–Ω–æ ‚Äî –ù–ï —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –º—ë—Ä—Ç–≤—ã–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
                    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å PID; PermissionError —Ç—Ä–∞–∫—Ç—É–µ–º –∫–∞–∫ "–∂–∏–≤".
                    if status not in ("completed", "error", "stopped") and metrics_age_minutes > 5:
                        pid_alive = False
                        if pid_path.exists():
                            try:
                                with open(pid_path) as f:
                                    pid = int(f.read().strip())
                                os.kill(pid, 0)
                                pid_alive = True
                            except PermissionError:
                                pid_alive = True
                            except (ProcessLookupError, ValueError, FileNotFoundError):
                                pid_alive = False
                        if pid_alive:
                            process_alive = True
                            logger.warning(
                                f"Metrics not updated for {metrics_age_minutes:.1f} minutes, but PID looks alive. "
                                f"Treating process as running (metrics may be stalled)."
                            )
                        else:
                            process_alive = False
                            logger.warning(f"Metrics not updated for {metrics_age_minutes:.1f} minutes and PID not alive, assuming process dead")
                            clear_active_run()
                except Exception as e:
                    logger.warning(f"Failed to check metrics: {e}")
                    # Fallback –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É PID
                    if pid_path.exists():
                        try:
                            with open(pid_path) as f:
                                pid = int(f.read().strip())
                            os.kill(pid, 0)
                            process_alive = True
                        except (ProcessLookupError, ValueError, PermissionError):
                            process_alive = False
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            st.session_state.current_run_id = run_id
            st.session_state.training_active = process_alive
            
            # –ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à—ë–Ω, –æ—á–∏—â–∞–µ–º active_run
            if not process_alive:
                if metrics and metrics.get("status") in ("completed", "error", "stopped"):
                    clear_active_run()


# ============================================================================
# Session State
# ============================================================================

if "training_process" not in st.session_state:
    st.session_state.training_process = None
if "current_run_id" not in st.session_state:
    st.session_state.current_run_id = None
if "training_active" not in st.session_state:
    st.session_state.training_active = False
if "selected_chat_model" not in st.session_state:
    st.session_state.selected_chat_model = None

# –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
if "session_restored" not in st.session_state:
    restore_session_state()
    st.session_state.session_restored = True


# ============================================================================
# Helper Functions
# ============================================================================

def get_available_datasets():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."""
    datasets = []
    if DATASET_DIR.exists():
        for f in DATASET_DIR.glob("*.jsonl"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        # –í–ê–ñ–ù–û: –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º .json, —Ç.–∫. —ç—Ç–æ –æ–±—ã—á–Ω–æ –º–∞—Å—Å–∏–≤, –∞ –Ω–µ JSONL (–ø–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        # –¥–ª—è f in DATASET_DIR.glob("*.json"):
        #     size_mb = f.stat().st_size / (1024 * 1024)
        #     datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt.gz"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.jsonl.gz"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
    return datasets


def get_gpu_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU."""
    gpus = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            gpus.append({
                "id": i,
                "name": props.name,
                "memory_gb": round(memory_gb, 1),
                "compute_capability": f"{props.major}.{props.minor}",
            })
    return gpus


def get_available_configs():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö accelerate –∫–æ–Ω—Ñ–∏–≥–æ–≤."""
    configs = []
    if CONFIGS_DIR.exists():
        for f in CONFIGS_DIR.glob("*.yaml"):
            name = f.stem.replace("accelerate_", "").replace("_", " ").title()
            configs.append({
                "file": f.name,
                "name": name,
                "path": str(f),
            })
    return configs


# –û–ø–∏—Å–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
PARALLEL_TYPES = {
    "default": {
        "name": "Single GPU / CPU",
        "type": "None",
        "description": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞",
        "icon": "üñ•Ô∏è",
    },
    "multi_gpu": {
        "name": "Multi-GPU (DDP)",
        "type": "Data Parallel",
        "description": "Distributed Data Parallel ‚Äî –∫–∞–∂–¥–∞—è GPU –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏ –∏ —á–∞—Å—Ç—å –±–∞—Ç—á–∞",
        "icon": "üîÑ",
    },
    "fsdp": {
        "name": "FSDP",
        "type": "Data Parallel + Model Parallel",
        "description": "Fully Sharded Data Parallel ‚Äî –º–æ–¥–µ–ª—å —à–∞—Ä–¥–∏—Ä—É–µ—Ç—Å—è –º–µ–∂–¥—É GPU (PyTorch native)",
        "icon": "‚ö°",
    },
    "deepspeed_zero2": {
        "name": "DeepSpeed ZeRO-2",
        "type": "Data Parallel + Optimizer Parallel",
        "description": "–®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É GPU",
        "icon": "üöÄ",
    },
    "deepspeed_zero3": {
        "name": "DeepSpeed ZeRO-3",
        "type": "Full Model Parallel",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å + –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä + –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã",
        "icon": "üí™",
    },
    "deepspeed_zero3_offload": {
        "name": "ZeRO-3 + CPU Offload",
        "type": "Model Parallel + CPU Offload",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ + –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM",
        "icon": "üßä",
    },
}


def estimate_parameters(
    hidden_size: int,
    num_layers: int,
    vocab_size: int = 50257,
    intermediate_size: int | None = None,
) -> int:
    """
    –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞—à–µ–π `HomeModel` (—Å–º. `homellm/models/home_model.py`).

    –í–ê–ñ–ù–û:
    - –£ –Ω–∞—Å RoPE (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ù–ï –æ–±—É—á–∞–µ–º—ã–µ) => `seq_len` –Ω–∞ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –≤–ª–∏—è–µ—Ç.
    - lm_head weight tied –∫ embed_tokens => –Ω–µ —É–¥–≤–∞–∏–≤–∞–µ–º vocab*hidden.
    """
    h = int(hidden_size)
    l = int(num_layers)
    v = int(vocab_size)
    i = int(intermediate_size) if intermediate_size is not None else int(h * 4)

    # Embedding
    embed = v * h

    # Per-layer:
    # - Attention: q/k/v/out, bias=False => 4 * (H*H)
    attn = 4 * h * h
    # - SwiGLU MLP: w1(H->I), w2(I->H), w3(H->I), bias=False => 3 * (H*I)
    mlp = 3 * h * i
    # - RMSNorm weights: 2 * H
    norms = 2 * h

    # Final norm
    final_norm = h

    return int(embed + l * (attn + mlp + norms) + final_norm)


def format_params(n: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    if n >= 1e9:
        return f"{n/1e9:.2f}B"
    elif n >= 1e6:
        return f"{n/1e6:.1f}M"
    elif n >= 1e3:
        return f"{n/1e3:.1f}K"
    return str(n)


def format_time(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏."""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        return f"{seconds/3600:.1f}h"


def load_metrics(run_id: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    run_dir = RUNS_DIR / run_id
    run_config = {}
    
    # –î–ª—è GRPO —á–∏—Ç–∞–µ–º –∏–∑ metrics.jsonl (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ output_dir –∏–ª–∏ run_dir)
    metrics_jsonl_paths = []
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ metrics.jsonl –≤ output_dir
    config_path = run_dir / "config.json"
    if config_path.exists():
        try:
            with open(config_path) as f:
                run_config = json.load(f) or {}
                output_dir = run_config.get("output_dir", "")
                if output_dir:
                    metrics_jsonl_paths.append(Path(output_dir) / "metrics.jsonl")
        except:
            pass
    
    # Fallback: –∏—â–µ–º –≤ run_dir
    metrics_jsonl_paths.append(run_dir / "metrics.jsonl")
    
    for metrics_jsonl_path in metrics_jsonl_paths:
        if metrics_jsonl_path.exists():
            try:
                import pandas as pd
                # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ JSONL
                lines = []
                with open(metrics_jsonl_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            try:
                                lines.append(json.loads(line))
                            except json.JSONDecodeError:
                                continue
                
                if lines:
                    df = pd.DataFrame(lines)
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –∫–∞–∫ —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                    latest = df.iloc[-1].to_dict()
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
                    latest["reward_history"] = df["reward"].tolist() if "reward" in df.columns else []
                    latest["loss_history"] = df["loss"].tolist() if "loss" in df.columns else []
                    latest["kl_history"] = df["kl"].tolist() if "kl" in df.columns else []
                    latest["steps_history"] = df["step"].tolist() if "step" in df.columns else list(range(len(df)))
                    latest["lr_history"] = df["learning_rate"].tolist() if "learning_rate" in df.columns else (df["lr"].tolist() if "lr" in df.columns else [])
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫
                    # –î–ª—è GRPO –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ UI —Å—á–∏—Ç–∞–µ–º –ø–æ rollout_step (–ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞), –∞ optim_step –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ.
                    latest["current_step"] = latest.get("current_step", latest.get("rollout_step", latest.get("step", len(lines) - 1)))
                    latest["current_loss"] = latest.get("loss", 0)
                    latest["current_lr"] = latest.get("learning_rate", latest.get("lr", 0))
                    latest["reward"] = latest.get("reward", latest.get("batch_reward_mean", 0))
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–ø–ª—ã –µ—Å–ª–∏ –µ—Å—Ç—å
                    if "samples" in df.columns:
                        latest["samples_history"] = df["samples"].tolist()
                    
                    latest["status"] = latest.get("status", "training")
                    latest["stage"] = "grpo"

                    # –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å—á—ë—Ç—á–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ jsonl)
                    try:
                        if "prompts_generated_total" in df.columns:
                            latest["prompts_generated_total"] = int(df["prompts_generated_total"].fillna(0).iloc[-1])
                        elif "prompts_generated" in df.columns:
                            latest["prompts_generated_total"] = int(df["prompts_generated"].fillna(0).sum())
                        if "prompts_used_total" in df.columns:
                            latest["prompts_used_total"] = int(df["prompts_used_total"].fillna(0).iloc[-1])
                        elif "prompts_used" in df.columns:
                            latest["prompts_used_total"] = int(df["prompts_used"].fillna(0).sum())
                        if "completions_generated_total" in df.columns:
                            latest["completions_generated_total"] = int(df["completions_generated_total"].fillna(0).iloc[-1])
                        elif "completions_generated" in df.columns:
                            latest["completions_generated_total"] = int(df["completions_generated"].fillna(0).sum())
                        if "experiences_tuned_total" in df.columns:
                            latest["experiences_tuned_total"] = int(df["experiences_tuned_total"].fillna(0).iloc[-1])
                        elif "experiences_tuned" in df.columns:
                            latest["experiences_tuned_total"] = int(df["experiences_tuned"].fillna(0).sum())
                    except Exception:
                        pass

                    # --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞/ETA –¥–ª—è GRPO ---
                    # total_steps: –±–µ—Ä—ë–º –∏–∑ –º–µ—Ç—Ä–∏–∫ (–µ—Å–ª–∏ –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è) –∏–ª–∏ –∏–∑ run_config (config.json)
                    total_steps = latest.get("total_steps", None)
                    if total_steps in (None, "", 0):
                        # legacy: optim-step –ª–∏–º–∏—Ç
                        total_steps = run_config.get("grpo_max_optim_steps", run_config.get("grpo_max_steps", run_config.get("max_steps", None)))
                    try:
                        total_steps_int = int(total_steps) if total_steps is not None else None
                    except Exception:
                        total_steps_int = None
                    # –ï—Å–ª–∏ –ª–∏–º–∏—Ç–∞ –Ω–µ—Ç ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º None (UI –ø–æ–∫–∞–∂–µ—Ç "–±–µ–∑ –ª–∏–º–∏—Ç–∞"), –∞ –Ω–µ "1".
                    latest["total_steps"] = total_steps_int if (total_steps_int is not None and total_steps_int > 0) else None

                    # elapsed/eta: –ø–æ timestamp, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    elapsed_seconds = 0.0
                    eta_seconds = 0.0
                    try:
                        if "timestamp" in df.columns:
                            t0 = pd.to_datetime(df["timestamp"].iloc[0], errors="coerce")
                            t1 = pd.to_datetime(df["timestamp"].iloc[-1], errors="coerce")
                            if pd.notna(t0) and pd.notna(t1):
                                # –ï—Å–ª–∏ –ø–æ–∫–∞ –≤—Å–µ–≥–æ 1 –∑–∞–ø–∏—Å—å, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º elapsed –∫–∞–∫ now - t0, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ "0s" –≤ UI.
                                if len(df) == 1:
                                    elapsed_seconds = max(0.0, (pd.Timestamp.now(tz=t0.tz) - t0).total_seconds())
                                else:
                                    elapsed_seconds = max(0.0, (t1 - t0).total_seconds())

                        # ETA —Å—á–∏—Ç–∞–µ–º –ø–æ current_step (rollout_step), –∞ –Ω–µ –ø–æ optim_step,
                        # –∏–Ω–∞—á–µ –ø—Ä–æ–≥—Ä–µ—Å—Å/ETA –±—É–¥—É—Ç "—É–±–µ–≥–∞—Ç—å" –∏–∑-–∑–∞ multiple optimizer updates per rollout.
                        if "timestamp" in df.columns and "current_step" in df.columns and len(df) >= 2:
                            s0 = float(df["current_step"].iloc[0])
                            s1 = float(df["current_step"].iloc[-1])
                            # —Å—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–º step (—É—á–∏—Ç—ã–≤–∞–µ–º —á—Ç–æ –ª–æ–≥ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ)
                            ds = max(0.0, s1 - s0)
                            if ds > 0 and elapsed_seconds > 0 and latest["total_steps"] is not None:
                                sec_per_step = elapsed_seconds / ds
                                remaining = max(0.0, float(latest["total_steps"]) - float(latest.get("current_step", s1)))
                                eta_seconds = sec_per_step * remaining
                    except Exception:
                        pass
                    latest["elapsed_seconds"] = float(elapsed_seconds)
                    latest["eta_seconds"] = float(eta_seconds)

                    # rollout_step —Ç–æ–∂–µ –ø—Ä–æ–∫–∏–Ω–µ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    if "rollout_step" in latest:
                        latest["current_rollout_step"] = latest.get("rollout_step", 0)

                    return latest
            except Exception as e:
                # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—É—Ç—å
                if metrics_jsonl_path == metrics_jsonl_paths[-1]:
                    pass  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—É—Ç–∏
    
    # –û–±—ã—á–Ω—ã–π –ø—É—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç–∞–¥–∏–π
    metrics_path = run_dir / "metrics.json"
    if metrics_path.exists():
        try:
            with open(metrics_path) as f:
                return json.load(f)
        except:
            pass
    return None


def _close_run_log_files(run_id: str):
    """–ó–∞–∫—Ä—ã—Ç—å —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã stdout/stderr, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ session_state."""
    for k in (f"stdout_file_{run_id}", f"stderr_file_{run_id}"):
        f = st.session_state.get(k)
        if f:
            with suppress(Exception):
                f.close()
            with suppress(Exception):
                del st.session_state[k]


def start_training(config: dict) -> tuple[str, subprocess.Popen]:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤ —Ñ–æ–Ω–µ."""
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –õ–û–ì–ò–ö–ê –ü–£–¢–ï–ô
    # config["output_dir"] - —ç—Ç–æ –∫–æ—Ä–µ–Ω—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä out/my_model)
    # –ú—ã —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ out/my_model/run_2023.../checkpoint_...
    experiment_root = Path(PROJECT_ROOT) / config.get("output_dir", "out/default")
    run_output_dir = experiment_root / run_id
    
    # –û–±–Ω–æ–≤–ª—è–µ–º output_dir –≤ –∫–æ–Ω—Ñ–∏–≥–µ, —á—Ç–æ–±—ã worker —Å–æ—Ö—Ä–∞–Ω—è–ª —Ç—É–¥–∞
    config["output_dir"] = str(run_output_dir)
    
    # –ü–∞–ø–∫–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞ (–ª–æ–≥–∏, –º–µ—Ç—Ä–∏–∫–∏)
    # –ú–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö —Ç–∞–º –∂–µ, –≥–¥–µ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã, –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    run_dir = RUNS_DIR / run_id # –û—Å—Ç–∞–≤–ª—è–µ–º .runs –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ª–æ–≥–æ–≤ —Å—Ç—Ä–∏–º–ª–∏—Ç–∞
    run_dir.mkdir(parents=True, exist_ok=True)
    run_output_dir.mkdir(parents=True, exist_ok=True) # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤–µ—Å–æ–≤
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    # –°–æ–∑–¥–∞—ë–º –Ω–∞—á–∞–ª—å–Ω—ã–π metrics —Ñ–∞–π–ª
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0}, f)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ distributed
    distributed_mode = config.get("distributed_mode", "default")
    config_file = config.get("config_file")
    num_gpus = config.get("num_gpus", 1)
    
    if distributed_mode != "default" and config_file:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerate launch —Å –∫–æ–Ω—Ñ–∏–≥–æ–º
        # –í–ê–ñ–ù–û: gradient_accumulation_steps –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ config.json, –Ω–µ —á–µ—Ä–µ–∑ CLI —Ñ–ª–∞–≥–∏
        cmd = [
            "accelerate", "launch",
            "--config_file", config_file,
            "--num_processes", str(num_gpus),
            "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫
        cmd = [
            "python", "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        f.write(" ".join(cmd))
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    # –í–ê–ñ–ù–û: –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä GPU –∏–∑ UI
    env = os.environ.copy()
    gpu_ids = config.get("gpu_ids") or []
    if gpu_ids:
        env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,  # –û—Ç–¥–µ–ª—è–µ–º –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        env=env,
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
    st.session_state[f"stdout_file_{run_id}"] = stdout_file
    st.session_state[f"stderr_file_{run_id}"] = stderr_file
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PID –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def stop_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É."""
    stopped = False
    
    # –ü—Ä–æ–±—É–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ PID –∏–∑ —Ñ–∞–π–ª–∞ (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
    if st.session_state.current_run_id:
        pid_path = RUNS_DIR / st.session_state.current_run_id / "pid"
        if pid_path.exists():
            try:
                with open(pid_path) as f:
                    pid = int(f.read().strip())
                # –£–±–∏–≤–∞–µ–º process group (–≤–∞–∂–Ω–æ –¥–ª—è accelerate/DDP)
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è —É–±–∏—Ç—å process group
                    os.killpg(os.getpgid(pid), signal.SIGTERM)
                    stopped = True
                except (ProcessLookupError, OSError):
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å (–ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –≤ –≥—Ä—É–ø–ø–µ –∏–ª–∏ —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è), –ø—Ä–æ–±—É–µ–º –ø–æ PID
                    try:
                        os.kill(pid, signal.SIGTERM)
                        stopped = True
                    except ProcessLookupError:
                        pass
                
                # –ñ–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
                time.sleep(0.5)
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
                    os.kill(pid, 0)
                    # –ï—Å–ª–∏ –∂–∏–≤, —É–±–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ (process group)
                    try:
                        os.killpg(os.getpgid(pid), signal.SIGKILL)
                    except (ProcessLookupError, OSError):
                        os.kill(pid, signal.SIGKILL)
                except ProcessLookupError:
                    pass  # –ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
            except Exception as e:
                pass
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = RUNS_DIR / st.session_state.current_run_id / "metrics.json"
        if metrics_path.exists():
            try:
                with open(metrics_path) as f:
                    metrics = json.load(f)
                metrics["status"] = "stopped"
                with open(metrics_path, "w") as f:
                    json.dump(metrics, f, indent=2)
            except:
                pass
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ subprocess
    if st.session_state.training_process:
        try:
            st.session_state.training_process.terminate()
            st.session_state.training_process.wait(timeout=2)
        except:
            try:
                st.session_state.training_process.kill()
            except:
                pass
        st.session_state.training_process = None
    
    st.session_state.training_active = False
    
    # –û—á–∏—â–∞–µ–º active_run
    clear_active_run()
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –ª–æ–≥–æ–≤
    if st.session_state.current_run_id:
        _close_run_log_files(st.session_state.current_run_id)
    
    return stopped


def start_grpo_training(config: dict) -> tuple[str, subprocess.Popen]:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å GRPO –æ–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–µ."""
    run_id = datetime.now().strftime("grpo_%Y%m%d_%H%M%S")
    
    # –ü–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    experiment_root = Path(PROJECT_ROOT) / config.get("output_dir", "out/grpo")
    run_output_dir = experiment_root / run_id
    
    config["output_dir"] = str(run_output_dir)
    
    run_dir = RUNS_DIR / run_id
    run_dir.mkdir(parents=True, exist_ok=True)
    run_output_dir.mkdir(parents=True, exist_ok=True)

    # –î–ª—è "–∂–µ–ª–µ–∑–Ω–æ–≥–æ" –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º –ø—É—Ç—å –¥–æ run_dir –≤ worker.
    # Worker –±—É–¥–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å metrics.jsonl/samples.jsonl –≤ —ç—Ç—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.
    config["ui_run_dir"] = str(run_dir)
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2, default=str)
    
    # –ù–∞—á–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0, "stage": "grpo"}, f)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ GRPO
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º --config_json –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤—Å–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—è reward_rules
    import sys
    
    # –ü–µ—Ä–µ–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥ –∫–∞–∫ JSON —Å—Ç—Ä–æ–∫—É
    config_json = json.dumps(config, default=str)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º distributed (–∫–∞–∫ –≤ start_training)
    distributed_mode = config.get("distributed_mode", "default")
    config_file = config.get("config_file")
    num_gpus = config.get("num_gpus", 1)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ distributed
    if distributed_mode != "default" and config_file:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerate launch —Å –∫–æ–Ω—Ñ–∏–≥–æ–º (–∫–∞–∫ –≤ pretrain/SFT)
        cmd = [
            "accelerate", "launch",
            "--config_file", config_file,
            "--num_processes", str(num_gpus),
            "-m", "homellm.training.rl.train_gsm8k",
            "--config_json", config_json,
        ]
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ (single GPU –∏–ª–∏ CPU)
        cmd = [
            sys.executable, "-m", "homellm.training.rl.train_gsm8k",
            "--config_json", config_json,
        ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–±–µ–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ JSON)
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        if distributed_mode != "default" and config_file:
            f.write(f"accelerate launch --config_file {config_file} --num_processes {num_gpus} -m homellm.training.rl.train_gsm8k --config_json <config from {config_path}>")
        else:
            f.write(f"{sys.executable} -m homellm.training.rl.train_gsm8k --config_json <config from {config_path}>")
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    # –í–ê–ñ–ù–û: –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä GPU –∏–∑ UI (–∫–∞–∫ –≤ start_training)
    env = os.environ.copy()
    env["PYTHONUNBUFFERED"] = "1"
    gpu_ids = config.get("gpu_ids") or []
    if gpu_ids:
        env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
        logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è GPU: {gpu_ids}")
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,
        env=env,
    )
    
    st.session_state[f"stdout_file_{run_id}"] = stdout_file
    st.session_state[f"stderr_file_{run_id}"] = stderr_file
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PID
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def is_process_running(run_id: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–ø—É—â–µ–Ω –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å."""
    pid_path = RUNS_DIR / run_id / "pid"
    if not pid_path.exists():
        return False
    
    try:
        with open(pid_path) as f:
            pid = int(f.read().strip())
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
        os.kill(pid, 0)
        return True
    except PermissionError:
        # –ü—Ä–æ—Ü–µ—Å—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—É—â–µ–Ω –æ—Ç –¥—Ä—É–≥–æ–≥–æ —é–∑–µ—Ä–∞)
        # –°—á–∏—Ç–∞–µ–º, —á—Ç–æ –æ–Ω –∂–∏–≤
        return True
    except (ProcessLookupError, ValueError, FileNotFoundError):
        return False


# ============================================================================
# UI Components
# ============================================================================

def render_header():
    st.markdown("# üè† Models at Home Training Studio")
    st.caption("–í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–º–∞")


def get_nested_value(data: dict, path: str):
    """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - 'key1.key2' - –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏
    - 'messages [—Å–ø–∏—Å–æ–∫ –∏–∑ N —ç–ª.]' - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å—å —Å–ø–∏—Å–æ–∫
    - 'messages[].content' - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª—è –∏–∑ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    - 'messages[0]' - –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞
    """
    if not path: return None
    
    # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã —Ç–∏–ø–∞ " [—Å–ø–∏—Å–æ–∫]" –∏–ª–∏ " [—Å–ø–∏—Å–æ–∫ –∏–∑ 3 —ç–ª.]"
    import re
    path = re.sub(r' \[—Å–ø–∏—Å–æ–∫.*?\]$', '', path)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π —Ç–∏–ø–∞ 'messages[].content' (–≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞)
    if "[]." in path:
        parts = path.split("[].", 1)
        list_path = parts[0]
        remaining_path = parts[1]
        
        list_val = get_nested_value(data, list_path)
        if isinstance(list_val, list):
            results = []
            for item in list_val:
                if isinstance(item, dict):
                    val = get_nested_value(item, remaining_path)
                    results.append(val)
            return results
        return None
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π —Ç–∏–ø–∞ 'messages[0]' (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å)
    if "[" in path and "]" in path:
        # –ü–∞—Ä—Å–∏–º –∏–Ω–¥–µ–∫—Å
        match = re.search(r'\[(\d+)\]', path)
        if match:
            idx = int(match.group(1))
            base_path = path[:match.start()]
            after_path = path[match.end():]
            if after_path.startswith('.'):
                after_path = after_path[1:]
            
            base_val = get_nested_value(data, base_path)
            if isinstance(base_val, list) and 0 <= idx < len(base_val):
                if after_path:
                    return get_nested_value(base_val[idx], after_path)
                return base_val[idx]
            return None
    
    # –û–±—ã—á–Ω—ã–π –ø—É—Ç—å —á–µ—Ä–µ–∑ —Ç–æ—á–∫–∏
    keys = path.split('.')
    curr = data
    try:
        for k in keys:
            if isinstance(curr, dict):
                curr = curr.get(k)
            elif isinstance(curr, list) and k.isdigit():
                curr = curr[int(k)]
            else:
                return None
            if curr is None: return None
        return curr
    except:
        return None


def get_dataset_columns(file_path: str):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ (–≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ) –∏ –ø—Ä–∏–º–µ—Ä."""
    path = Path(file_path)
    if not path.exists():
        return [], {}
        
    try:
        sample_data = {}
        if path.suffix == ".jsonl":
            with open(path, "r", encoding="utf-8") as f:
                line = f.readline()
                if line:
                    sample_data = json.loads(line)
        elif path.suffix == ".json":
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list) and len(data) > 0:
                    sample_data = data[0]
                elif isinstance(data, dict):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º –∫–ª—é—á–∏
                    return list(data.keys()), {k: v[0] if isinstance(v, list) else v for k, v in data.items()}
        elif path.suffix == ".csv":
            import csv
            with open(path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                row = next(reader, None)
                if row:
                    sample_data = row
        
        if sample_data:
            # –î–ª—è get_dataset_columns –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–ª–æ—Å–∫–∏–µ –∫–ª—é—á–∏, 
            # —Ç–∞–∫ –∫–∞–∫ render_sft_main_config —Ç–µ–ø–µ—Ä—å —Å–∞–º —Å—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤–æ.
            # –ù–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º –≤–æ–∑–≤—Ä–∞—Ç sample_data
            return [], sample_data
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return [], {}
    
    return [], {}


def flatten_json_structure(d: dict, parent_path: str = '', depth: int = 0) -> list:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç JSON –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ –¥–µ—Ä–µ–≤–∞."""
    items = []
    for k, v in d.items():
        current_path = f"{parent_path}.{k}" if parent_path else k
        
        if isinstance(v, dict):
            # –ü–∞–ø–∫–∞
            items.append({"type": "folder", "key": k, "path": current_path, "depth": depth, "val": ""})
            items.extend(flatten_json_structure(v, current_path, depth + 1))
        elif isinstance(v, list):
            # –°–ø–∏—Å–æ–∫
            items.append({"type": "list", "key": k, "path": current_path, "depth": depth, "val": f"List[{len(v)}]"})
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            if v:
                if isinstance(v[0], dict):
                    items.extend(flatten_json_structure(v[0], current_path, depth + 1))
                else:
                    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ (—Å—Ç—Ä–æ–∫ –∏ —Ç.–¥.) - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ –ª–∏—Å—Ç
                    # –ù–æ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç, –ø—Ä–æ—Å—Ç–æ –¥–∞–µ–º –ø–æ–Ω—è—Ç—å —á—Ç–æ –≤–Ω—É—Ç—Ä–∏
                    # –î–ª—è SFT –º—ã –æ–±—ã—á–Ω–æ –Ω–µ –≤—ã–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ –≤–µ—Å—å —Å–ø–∏—Å–æ–∫
                    pass
        else:
            # –õ–∏—Å—Ç (–∑–Ω–∞—á–µ–Ω–∏–µ)
            items.append({"type": "leaf", "key": k, "path": current_path, "depth": depth, "val": str(v)})
    return items


def get_all_leaf_paths(data, parent_path: str = '', depth: int = 0, max_depth: int = 10) -> list:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –í–°–ï –ø—É—Ç–∏ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º, –≤–∫–ª—é—á–∞—è –≥–ª—É–±–æ–∫—É—é –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å."""
    if depth > max_depth:
        return []
    
    paths = []
    
    if isinstance(data, dict):
        for k, v in data.items():
            current_path = f"{parent_path}.{k}" if parent_path else k
            
            if isinstance(v, dict):
                # –í–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å - —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
                paths.extend(get_all_leaf_paths(v, current_path, depth + 1, max_depth))
            elif isinstance(v, list):
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º —Å–ø–∏—Å–æ–∫ –∫–∞–∫ –æ–ø—Ü–∏—é (–¥–ª—è chat-—Ñ–æ—Ä–º–∞—Ç–∞)
                paths.append(f"{current_path} [—Å–ø–∏—Å–æ–∫ –∏–∑ {len(v)} —ç–ª.]")
                # –†–∞—Å–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if v:
                    if isinstance(v[0], dict):
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª—è –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–∏—Å–∫–∞
                        for inner_k, inner_v in v[0].items():
                            inner_path = f"{current_path}[].{inner_k}"
                            if isinstance(inner_v, dict):
                                paths.extend(get_all_leaf_paths(inner_v, inner_path, depth + 2, max_depth))
                            elif isinstance(inner_v, list):
                                paths.append(f"{inner_path} [—Å–ø–∏—Å–æ–∫]")
                                if inner_v and isinstance(inner_v[0], dict):
                                    paths.extend(get_all_leaf_paths(inner_v[0], f"{inner_path}[]", depth + 3, max_depth))
                            else:
                                paths.append(inner_path)
                    else:
                        # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤
                        paths.append(f"{current_path}[0]")
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                paths.append(current_path)
    
    return paths


def render_sft_main_config(data_path: str):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä SFT ‚Äî –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç + —Ä—É—á–Ω–æ–π –≤—ã–±–æ—Ä."""
    st.markdown("### üõ†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SFT")
    
    columns, sample = get_dataset_columns(data_path)
    
    if not sample:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏–ª–∏ –æ–Ω –ø—É—Å—Ç.")
        return {}
    
    # ===== –ê–í–¢–û–î–ï–¢–ï–ö–¢ –§–û–†–ú–ê–¢–ê =====
    def detect_chat_field(data: dict) -> tuple:
        """–ò—â–µ—Ç –ø–æ–ª–µ —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π (chat-—Ñ–æ—Ä–º–∞—Ç)."""
        for key, value in data.items():
            if isinstance(value, list) and value and isinstance(value[0], dict):
                first_item = value[0]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –ø–æ–ª—è –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ role/content
                has_role = any(k.lower() in ['role', 'from', 'type'] for k in first_item.keys())
                has_content = any(k.lower() in ['content', 'value', 'text', 'message'] for k in first_item.keys())
                if has_role and has_content:
                    return key, value
        return None, None
    
    chat_field, chat_value = detect_chat_field(sample)
    detected_format = "chat" if chat_field else "instruct"
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—É—Ç–∏ –¥–ª—è instruct —Ä–µ–∂–∏–º–∞
    all_paths = get_all_leaf_paths(sample)
    simple_fields = [k for k in sample.keys() if not isinstance(sample[k], (dict, list))]
    
    col_json, col_config = st.columns([1, 1])
    
    # ===== –õ–ï–í–ê–Ø –ö–û–õ–û–ù–ö–ê: JSON –ø—Ä–µ–≤—å—é =====
    with col_json:
        st.markdown("#### üìÑ –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
        with st.container(height=500):
            st.json(sample, expanded=True)
    
    # ===== –ü–†–ê–í–ê–Ø –ö–û–õ–û–ù–ö–ê: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è =====
    with col_config:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç –Ω–∞—à–µ–ª
        if detected_format == "chat":
            st.success(f"üîç **–ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç:** –Ω–∞–π–¥–µ–Ω Chat-—Ñ–æ—Ä–º–∞—Ç –≤ –ø–æ–ª–µ `{chat_field}`")
        else:
            st.info("üîç **–ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç:** Instruct-—Ñ–æ—Ä–º–∞—Ç (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è)")
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Ä–µ–∂–∏–º–∞
        format_choice = st.radio(
            "–§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö:",
            ["üí¨ Chat (—Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π)", "üìù Instruct (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è)"],
            index=0 if detected_format == "chat" else 1,
            key="sft_format_choice",
            horizontal=True
        )
        
        is_chat = "Chat" in format_choice
        
        st.markdown("---")
        
        sft_columns = {}
        
        if is_chat:
            # ===== CHAT –†–ï–ñ–ò–ú =====
            st.markdown("#### üí¨ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chat-—Ñ–æ—Ä–º–∞—Ç–∞")
            
            # –í—ã–±–æ—Ä –ø–æ–ª—è —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π
            list_fields = [k for k, v in sample.items() if isinstance(v, list) and v and isinstance(v[0], dict)]
            
            if not list_fields:
                st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ–π —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π!")
                return {}
            
            messages_field = st.selectbox(
                "üìã –ü–æ–ª–µ —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏:",
                list_fields,
                index=list_fields.index(chat_field) if chat_field in list_fields else 0,
                key="sft_messages_field"
            )
            
            messages = sample[messages_field]
            first_msg = messages[0]
            inner_fields = list(first_msg.keys())
            
            st.caption(f"–ù–∞–π–¥–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π
            c1, c2 = st.columns(2)
            
            role_guess = next((f for f in inner_fields if f.lower() in ['role', 'from', 'type']), inner_fields[0])
            content_guess = next((f for f in inner_fields if f.lower() in ['content', 'value', 'text', 'message']), inner_fields[-1])
            
            role_field = c1.selectbox(
                "–ü–æ–ª–µ **—Ä–æ–ª–∏**:",
                inner_fields,
                index=inner_fields.index(role_guess) if role_guess in inner_fields else 0,
                key="sft_chat_role"
            )
            content_field = c2.selectbox(
                "–ü–æ–ª–µ **—Ç–µ–∫—Å—Ç–∞**:",
                inner_fields,
                index=inner_fields.index(content_guess) if content_guess in inner_fields else 0,
                key="sft_chat_content"
            )
            
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–æ–ª–∏
            unique_roles = sorted(set(str(m.get(role_field, "")) for m in messages))
            st.caption(f"–†–æ–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: `{', '.join(unique_roles)}`")
            
            # –ú–∞–ø–ø–∏–Ω–≥ —Ä–æ–ª–µ–π
            st.markdown("**–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–æ–ª–µ–π:**")
            c1, c2, c3 = st.columns(3)
            
            sys_guess = next((r for r in unique_roles if 'system' in r.lower()), None)
            user_guess = next((r for r in unique_roles if r.lower() in ['user', 'human']), unique_roles[0] if unique_roles else "")
            asst_guess = next((r for r in unique_roles if r.lower() in ['assistant', 'gpt', 'bot']), unique_roles[-1] if len(unique_roles) > 1 else "")
            
            role_system = c1.selectbox("‚öôÔ∏è System =", ["(–Ω–µ—Ç)"] + unique_roles,
                index=(unique_roles.index(sys_guess) + 1) if sys_guess in unique_roles else 0, key="sft_map_sys")
            role_user = c2.selectbox("üë§ User =", unique_roles,
                index=unique_roles.index(user_guess) if user_guess in unique_roles else 0, key="sft_map_user")
            role_assistant = c3.selectbox("ü§ñ Assistant =", unique_roles,
                index=unique_roles.index(asst_guess) if asst_guess in unique_roles else 0, key="sft_map_asst")
            
            sft_columns = {
                "format": "chat",
                "messages_path": messages_field,
                "role_field": role_field,
                "content_field": content_field,
                "role_system": role_system if role_system != "(–Ω–µ—Ç)" else "",
                "role_user": role_user,
                "role_assistant": role_assistant
            }
            
        else:
            # ===== INSTRUCT –†–ï–ñ–ò–ú =====
            st.markdown("#### üìù –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Instruct-—Ñ–æ—Ä–º–∞—Ç–∞")
            st.caption("–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–ª—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–æ–ª–∏:")
            
            # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—É—Ç–∏
            field_options = ["(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)"] + all_paths
            
            system_path = st.selectbox("‚öôÔ∏è **System** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):", field_options, index=0, key="sft_inst_sys")
            user_path = st.selectbox("üë§ **User** (–≤–æ–ø—Ä–æ—Å/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è):", field_options, index=0, key="sft_inst_user")
            assistant_path = st.selectbox("ü§ñ **Assistant** (–æ—Ç–≤–µ—Ç):", field_options, index=0, key="sft_inst_asst")
            
            if user_path == "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" or assistant_path == "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)":
                st.warning("üëÜ –í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–ª—è **User** –∏ **Assistant**")
                return {}
            
            sft_columns = {
                "format": "instruct",
                "instruction": user_path,
                "output": assistant_path,
                "system_field": system_path if system_path != "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" else ""
            }
        
        # ===== –ù–ê–°–¢–†–û–ô–ö–ò –®–ê–ë–õ–û–ù–ê =====
        st.markdown("---")
        with st.expander("üè∑Ô∏è –¢–µ–≥–∏ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", expanded=False):
            default_system = st.text_input("System prompt (–ø–æ —É–º–æ–ª—á.):", "You are a helpful assistant.", key="sft_def_sys")
            tc1, tc2 = st.columns(2)
            user_tag = tc1.text_input("User tag:", "### User:", key="sft_tag_user")
            assistant_tag = tc2.text_input("Assistant tag:", "### Assistant:", key="sft_tag_asst")
        
        if 'default_system' not in dir():
            default_system, user_tag, assistant_tag = "You are a helpful assistant.", "### User:", "### Assistant:"
        
        sft_template = {
            "system": default_system,
            "separator": "\n\n",
            "user_tag": user_tag,
            "bot_tag": assistant_tag
        }
        
        # ===== –ü–†–ï–í–¨–Æ =====
        st.markdown("---")
        st.markdown("#### üëÅÔ∏è –ü—Ä–µ–≤—å—é:")
        
        try:
            sep = "\n\n"
            preview = ""
            
            if sft_columns["format"] == "chat":
                messages = sample[sft_columns["messages_path"]]
                sys_text = default_system
                
                for msg in messages:
                    role = str(msg.get(sft_columns["role_field"], ""))
                    content = str(msg.get(sft_columns["content_field"], ""))
                    
                    if role == sft_columns["role_system"]:
                        sys_text = content
                    elif role == sft_columns["role_user"]:
                        preview += f"{user_tag}\n{content[:200]}{'...' if len(content) > 200 else ''}{sep}"
                    elif role == sft_columns["role_assistant"]:
                        preview += f"{assistant_tag}\n{content[:200]}{'...' if len(content) > 200 else ''}{sep}"
                
                preview = f"{sys_text}{sep}" + preview + "<|endoftext|>"
            else:
                user_val = str(get_nested_value(sample, sft_columns["instruction"]) or "")[:300]
                asst_val = str(get_nested_value(sample, sft_columns["output"]) or "")[:300]
                
                # System prompt: —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Å–µ–º–ø–ª–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –ø–æ–ª–µ
                sys_val = default_system
                system_field = sft_columns.get("system_field")
                
                # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –ø–æ–ª–µ system_field, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å–µ–º–ø–ª–∞
                if system_field and system_field != "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" and system_field.strip():
                    field_sys = get_nested_value(sample, system_field)
                    # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ –∏ –Ω–µ –ø—É—Å—Ç–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –≤–º–µ—Å—Ç–æ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ
                    if field_sys is not None:
                        field_sys_str = str(field_sys).strip()
                        if field_sys_str:
                            sys_val = field_sys_str[:200]
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≤—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ –Ω–∞—á–∞–ª–µ
                # –í–ê–ñ–ù–û: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∏–¥–µ–Ω, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
                preview = f"{sys_val}{sep}{user_tag}\n{user_val}{sep}{assistant_tag}\n{asst_val}<|endoftext|>"
            
            with st.container(height=400):
                st.code(preview, language=None)
            
            st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞: {e}")

    return {"sft_columns": sft_columns, "sft_template": sft_template}


# ============================================================================
# GRPO Configuration
# ============================================================================

def render_grpo_sidebar_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.subheader("üß† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã GRPO")
    
    # –ê–ª–≥–æ—Ä–∏—Ç–º
    algorithm = st.sidebar.selectbox(
        "–ê–ª–≥–æ—Ä–∏—Ç–º",
        ["grpo", "drgrpo", "dapo"],
        format_func=lambda x: {
            "grpo": "GRPO (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)",
            "drgrpo": "Dr.GRPO (—É–ª—É—á—à–µ–Ω–Ω—ã–π)",
            "dapo": "DAPO (–ø–æ–ª–Ω—ã–π)",
        }[x],
        help="""
        **GRPO**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Group Relative Policy Optimization
        **Dr.GRPO**: –ë–µ–∑ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ std, —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        **DAPO**: + –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥, + dynamic sampling
        """
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    group_size = st.sidebar.slider(
        "Group size (G)",
        min_value=8,
        max_value=32,
        value=8,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç. –í–∞–∂–Ω–æ: –¥–ª—è GRPO –æ–±—ã—á–Ω–æ –Ω—É–∂–Ω–æ G>=8 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."
    )

    prompt_batch_size = st.sidebar.slider(
        "Prompt batch size (prompts/step)",
        min_value=1,
        max_value=64,
        value=8,
        step=1,
        help="–°–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ (–∑–∞–¥–∞—á) –±—Ä–∞—Ç—å –Ω–∞ –æ–¥–∏–Ω RL-—à–∞–≥ (rollouts_per_step –≤ re-grpo)."
    )
    
    max_new_tokens = st.sidebar.slider(
        "Max new tokens",
        min_value=128,
        max_value=4096,
        value=1024,
        step=128,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"
    )
    
    temperature = st.sidebar.slider(
        "Temperature",
        min_value=0.1,
        max_value=2.0,
        value=0.7,
        step=0.1,
        help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    grpo_learning_rate = st.sidebar.select_slider(
        "Learning Rate (GRPO)",
        options=[1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5],
        value=5e-6,
        format_func=lambda x: f"{x:.0e}",
        help="–î–ª—è RL –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–µ–Ω—å—à–∏–π LR —á–µ–º –¥–ª—è SFT"
    )
    
    train_batch_size = st.sidebar.slider(
        "Train Batch Size",
        min_value=1,
        max_value=16,
        value=2,
        step=1,
        help="–†–∞–∑–º–µ—Ä –º–∏–∫—Ä–æ-–±–∞—Ç—á–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –æ–ø—ã—Ç–µ. –£–º–µ–Ω—å—à–∏—Ç–µ –¥–æ 1-2, –µ—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç OOM (–∫–∞–∫ –≤ re-grpo)."
    )

    grpo_grad_accum = st.sidebar.slider(
        "Gradient accumulation steps",
        min_value=1,
        max_value=32,
        value=4,
        step=1,
        help="–ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–∫–∞–∫ –≤ PPO/GRPO). –ù–µ –º–µ–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –¥–∞–Ω–Ω—ã—Ö, —Ç–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch –Ω–∞ —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."
    )
    
    # –õ–∏–º–∏—Ç –ø–æ –¥–∞–Ω–Ω—ã–º (–ø–æ–Ω—è—Ç–Ω–µ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —á–µ–º optim-steps).
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç (—Å —É—á—ë—Ç–æ–º "–ú–∞–∫—Å. –ø—Ä–∏–º–µ—Ä–æ–≤" –≤ main-config).
    effective_ds = st.session_state.get("grpo_effective_dataset_size", None)
    if isinstance(effective_ds, int) and effective_ds > 0:
        grpo_max_prompts = st.sidebar.number_input(
            "Max prompts (–ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É)",
            min_value=1,
            max_value=int(effective_ds),
            value=int(effective_ds),
            step=max(1, int(effective_ds) // 50),
            help="–°–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á (prompts) –ø—Ä–æ–π—Ç–∏ –≤—Å–µ–≥–æ. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é = –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç (—Å —É—á—ë—Ç–æ–º max_samples).",
        )
    else:
        st.sidebar.info("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –≤ GRPO (–≤–∫–ª–∞–¥–∫–∞ –ó–∞–ø—É—Å–∫), —á—Ç–æ–±—ã –ª–∏–º–∏—Ç —Å—á–∏—Ç–∞–ª—Å—è –æ—Ç –µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.")
        grpo_max_prompts = None
    
    epochs_per_step = st.sidebar.slider(
        "Epochs per step",
        min_value=1,
        max_value=5,
        value=1,
        help="–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ rollout'–æ–≤"
    )
    
    # KL
    kl_weight = st.sidebar.slider(
        "KL weight",
        min_value=0.0,
        max_value=0.1,
        value=0.0,
        step=0.01,
        help="–í–µ—Å KL-—à—Ç—Ä–∞—Ñ–∞. –î–ª—è reasoning –æ–±—ã—á–Ω–æ 0"
    )
    
    # –ö–ª–∏–ø–ø–∏–Ω–≥
    with st.sidebar.expander("‚öôÔ∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"):
        clip_eps_low = st.slider("Clip Œµ (low)", 0.1, 0.3, 0.2, 0.01)
        clip_eps_high = st.slider(
            "Clip Œµ (high)", 
            0.1, 0.4, 
            0.28 if algorithm == "dapo" else 0.2, 
            0.01,
            help="DAPO —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç 0.28 –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã"
        )
        
        dynamic_sampling = st.checkbox(
            "Dynamic sampling",
            value=algorithm == "dapo",
            help="–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –≥—Ä—É–ø–ø—ã —Å –Ω—É–ª–µ–≤—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–º"
        )
        
        token_level_loss = st.checkbox(
            "Token-level loss",
            value=algorithm == "dapo",
            help="–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å loss –ø–æ —Ç–æ–∫–µ–Ω–∞–º, –∞ –Ω–µ –ø–æ —Å—ç–º–ø–ª–∞–º"
        )

        min_lr_ratio = st.slider(
            "Min LR ratio (floor)",
            0.0, 0.5,
            0.1,
            0.01,
            help="–ù–∏–∂–Ω–∏–π –ø—Ä–µ–¥–µ–ª LR: lr = base_lr * ratio –≤ –∫–æ–Ω—Ü–µ cosine. 0.0 = –¥–æ –Ω—É–ª—è."
        )
    
    # –í–ê–ñ–ù–û: LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –±–µ—Ä—É—Ç—Å—è –∏–∑ render_model_config() (—Å–µ–∫—Ü–∏—è "üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞")
    # –ó–¥–µ—Å—å –º—ã –ù–ï –¥—É–±–ª–∏—Ä—É–µ–º –∏—Ö, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –≤ UI
    # –í—Å–µ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (use_lora, lora_r, lora_alpha, lora_dropout, lora_target_modules)
    # –∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (use_4bit, use_8bit) –±—É–¥—É—Ç –≤–∑—è—Ç—ã –∏–∑ model_config
    
    return {
        # GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ)
        "grpo_algorithm": algorithm,
        "grpo_group_size": group_size,
        "grpo_prompt_batch_size": prompt_batch_size,
        "grpo_max_new_tokens": max_new_tokens,
        "grpo_temperature": temperature,
        "grpo_learning_rate": grpo_learning_rate,
        "grpo_train_batch_size": train_batch_size,
        "gradient_accumulation": grpo_grad_accum,
        "grpo_max_prompts": grpo_max_prompts,
        "grpo_epochs_per_step": epochs_per_step,
        "grpo_kl_weight": kl_weight,
        "grpo_clip_eps_low": clip_eps_low,
        "grpo_clip_eps_high": clip_eps_high,
        "grpo_dynamic_sampling": dynamic_sampling,
        "grpo_token_level_loss": token_level_loss,
        "grpo_min_lr_ratio": min_lr_ratio,
    }


def render_grpo_main_config(data_path: str = None):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä Reward —Ñ—É–Ω–∫—Ü–∏–π —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ä–µ–¥–∞–∫—Ç–æ—Ä–æ–º –ø—Ä–∞–≤–∏–ª."""
    import re
    import json as json_lib
    
    # =========================================================================
    # 1. –î–ê–¢–ê–°–ï–¢ (–ø–µ—Ä–≤–∞—è —Å–µ–∫—Ü–∏—è!)
    # =========================================================================
    st.markdown("### üìö –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è Reasoning")
    
    grpo_dataset_path = None
    grpo_max_samples = None
    grpo_dataset_language = "en"
    dataset_source = "custom"
    dataset_key = "custom"
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    local_datasets = []
    if DATASET_DIR.exists():
        for f in sorted(DATASET_DIR.iterdir(), key=lambda x: x.name.lower()):
            if f.suffix in (".jsonl", ".json"):
                local_datasets.append(f)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ (–≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã)
    dataset_options = ["-- –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç --"]
    for f in local_datasets:
        dataset_options.append(str(f))
    
    # –ï—Å–ª–∏ data_path –ø–µ—Ä–µ–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    default_idx = 0
    if data_path and data_path in dataset_options:
        default_idx = dataset_options.index(data_path)
    elif data_path:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        for i, opt in enumerate(dataset_options):
            if data_path in opt:
                default_idx = i
                break
    
    selected_dataset = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç",
        options=dataset_options,
        index=default_idx,
        help="–°–∫–∞—á–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–î–∞–Ω–Ω—ã–µ' ‚Üí üß† Reasoning: GSM8K, MATH-RU –∏ –¥—Ä."
    )
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±–æ—Ä
    if selected_dataset and not selected_dataset.startswith("--"):
        grpo_dataset_path = selected_dataset
        st.success(f"‚úÖ –í—ã–±—Ä–∞–Ω: `{Path(selected_dataset).name}`")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if "ru" in selected_dataset.lower() or "russian" in selected_dataset.lower():
            grpo_dataset_language = "ru"
        else:
            grpo_dataset_language = "en"
    else:
        st.warning("‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –µ–≥–æ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ **üíæ –î–∞–Ω–Ω—ã–µ** ‚Üí üß† Reasoning")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
    col1, col2 = st.columns(2)
    with col1:
        grpo_max_samples = st.number_input(
            "–ú–∞–∫—Å. –ø—Ä–∏–º–µ—Ä–æ–≤ (0 = –≤—Å–µ)",
            min_value=0,
            max_value=50000,
            value=0,
            step=100,
            help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
        )
    with col2:
        grpo_dataset_language = st.selectbox(
            "–Ø–∑—ã–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞",
            ["en", "ru"],
            index=1 if grpo_dataset_language == "ru" else 0,
            format_func=lambda x: "üá¨üáß English" if x == "en" else "üá∑üá∫ –†—É—Å—Å–∫–∏–π",
        )
    
    st.markdown("---")

    # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ª–∏–º–∏—Ç–∞ "Max prompts" –≤ sidebar
    try:
        effective_size = None
        if grpo_max_samples and int(grpo_max_samples) > 0:
            # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∂–µ –æ–≥—Ä–∞–Ω–∏—á–∏–ª max_samples ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ –∫–∞–∫ "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä"
            effective_size = int(grpo_max_samples)
        elif grpo_dataset_path:
            p = Path(grpo_dataset_path)
            if p.exists() and p.suffix == ".jsonl":
                # –î–ª—è reasoning –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —ç—Ç–æ –æ–±—ã—á–Ω–æ –Ω–µ –≥–∏–≥–∞–±–∞–π—Ç—ã ‚Äî —Å—á–∏—Ç–∞–µ–º —á–µ—Å—Ç–Ω–æ
                with open(p, "r", encoding="utf-8", errors="ignore") as f:
                    effective_size = sum(1 for _ in f if _.strip())
            elif p.exists() and p.suffix == ".json":
                import json as _json
                with open(p, "r", encoding="utf-8") as f:
                    obj = _json.load(f)
                if isinstance(obj, list):
                    effective_size = len(obj)
        if isinstance(effective_size, int) and effective_size > 0:
            st.session_state["grpo_effective_dataset_size"] = int(effective_size)
            st.caption(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ª–∏–º–∏—Ç–∞: **{effective_size} prompts**")
    except Exception:
        # –Ω–µ –º–µ—à–∞–µ–º –∑–∞–ø—É—Å–∫—É UI, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ—Å—á–∏—Ç–∞—Ç—å
        pass
    
    # =========================================================================
    # 2. REWARD DESIGNER
    # =========================================================================
    st.markdown("### üéØ Reward Designer")
    st.caption("–°–æ–∑–¥–∞–≤–∞–π—Ç–µ –≥–∏–±–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å —É—Å–ª–æ–≤–∏—è–º–∏, –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –∏ —Ñ–æ—Ä–º—É–ª–∞–º–∏")
    
    # =========================================================================
    # –ü–µ—Å–æ—á–Ω–∏—Ü–∞ ‚Äî –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    # =========================================================================
    st.markdown("#### üß™ –ü–µ—Å–æ—á–Ω–∏—Ü–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    with st.expander("üìù –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ", expanded=True):
        col_left, col_right = st.columns(2)
        
        with col_left:
            sample_prompt = st.text_area(
                "**–ü—Ä–æ–º–ø—Ç** (–≤–æ–ø—Ä–æ—Å/–∑–∞–¥–∞—á–∞)",
                value="Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?",
                height=100,
                key="sample_prompt",
            )
            sample_reference = st.text_input(
                "**–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç**",
                value="72",
                key="sample_reference",
            )
        
        with col_right:
            st.markdown("**–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏** (—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤):")
            # –ü—Ä–µ—Å–µ—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤
            response_presets = {
                "‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å —Ç–µ–≥–∞–º–∏": """<reasoning>
In April, Natalia sold 48 clips.
In May, she sold half as many: 48 / 2 = 24 clips.
Total: 48 + 24 = 72 clips.
</reasoning>
<answer>72</answer>""",
                "‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç": """<reasoning>
April: 48 clips
May: 48 / 2 = 24 clips  
Total: 48 + 24 = 70 clips
</reasoning>
<answer>70</answer>""",
                "‚ùå –ë–µ–∑ —Ç–µ–≥–æ–≤": "Natalia sold 48 clips in April and 24 in May, so 72 total.",
                "üîÅ –° –ø–æ–≤—Ç–æ—Ä–∞–º–∏": """<reasoning>
Let me solve this step by step.
Step by step I will solve this.
In April she sold 48 clips.
In April she sold 48 clips.
Total is 72.
</reasoning>
<answer>72</answer>""",
            }
            preset = st.selectbox("–ë—ã—Å—Ç—Ä—ã–π –≤—ã–±–æ—Ä:", list(response_presets.keys()))
            current_response = st.text_area(
                "–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏",
                value=response_presets[preset],
                height=180,
                key="current_test_response",
                label_visibility="collapsed",
            )
    
    st.markdown("---")
    
    # =========================================================================
    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª
    # =========================================================================
    st.markdown("#### üèóÔ∏è –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä Reward-–ø—Ä–∞–≤–∏–ª")
    
    # –°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
    with st.expander("üìñ –°–ø—Ä–∞–≤–∫–∞: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å", expanded=False):
        st.markdown("""
**–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:**
- `{{response}}` ‚Äî –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
- `{{reference}}` ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç  
- `{{prompt}}` ‚Äî –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
- `{{extracted.–∏–º—è}}` ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ, –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–µ regex-–≥—Ä—É–ø–ø–æ–π

**–û–ø–µ—Ä–∞—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:**
- `contains` ‚Äî —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Å—Ç—Ä–æ–∫—É
- `not_contains` ‚Äî –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç
- `matches` ‚Äî —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex
- `not_matches` ‚Äî –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex
- `equals` ‚Äî —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
- `==`, `!=`, `>`, `<`, `>=`, `<=` ‚Äî –¥–ª—è —á–∏—Å–µ–ª

**–ü—Ä–∏–º–µ—Ä regex —Å –≥—Ä—É–ø–ø–∞–º–∏:**
```
<answer>(?P<model_answer>\\d+)</answer>
```
–ò–∑–≤–ª–µ—á—ë—Ç —á–∏—Å–ª–æ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `{{extracted.model_answer}}`

**–§–æ—Ä–º—É–ª–∞ reward:**
```
1.0 if {{extracted.model_answer}} == {{reference}} else 0.0
```
        """)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª
    if "reward_rules" not in st.session_state:
        st.session_state.reward_rules = [
            {
                "id": 0,
                "name": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞",
                "enabled": True,
                "weight": 1.0,
                "conditions": [
                    {"type": "contains", "target": "{{response}}", "value": "<reasoning>"},
                    {"type": "contains", "target": "{{response}}", "value": "</reasoning>"},
                    {"type": "contains", "target": "{{response}}", "value": "<answer>"},
                    {"type": "contains", "target": "{{response}}", "value": "</answer>"},
                ],
                "condition_logic": "all",  # all / any / custom
                "reward_formula": "1.0",
                "else_reward": "0.0",
            },
            {
                "id": 1,
                "name": "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç",
                "enabled": True,
                "weight": 2.0,
                "extractors": [
                    {"name": "model_answer", "pattern": r"<answer>\s*(\d+)\s*</answer>", "source": "{{response}}"},
                ],
                "conditions": [
                    {"type": "equals_numeric", "left": "{{extracted.model_answer}}", "right": "{{reference}}", "tolerance": 0.01},
                ],
                "condition_logic": "all",
                "reward_formula": "1.0",
                "else_reward": "0.0",
            },
            {
                "id": 2,
                "name": "–ö–∞—á–µ—Å—Ç–≤–æ reasoning",
                "enabled": True,
                "weight": 0.5,
                "extractors": [
                    {"name": "reasoning_text", "pattern": r"<reasoning>(.*?)</reasoning>", "source": "{{response}}", "flags": "DOTALL"},
                ],
                "conditions": [
                    {"type": "length_between", "target": "{{extracted.reasoning_text}}", "min": 50, "max": 2000},
                ],
                "condition_logic": "all",
                "reward_formula": "min(len({{extracted.reasoning_text}}) / 200.0, 1.0)",
                "else_reward": "0.0",
            },
        ]
        st.session_state.next_rule_id = 3
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞
    def render_rule(rule, idx):
        with st.expander(
            f"{'‚úÖ' if rule['enabled'] else '‚è∏Ô∏è'} **{rule['name']}** (–≤–µ—Å: {rule['weight']})",
            expanded=False
        ):
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∞–≤–∏–ª–∞
            c1, c2, c3, c4 = st.columns([3, 1, 1, 1])
            with c1:
                rule["name"] = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ", value=rule["name"], key=f"rule_name_{rule['id']}")
            with c2:
                rule["weight"] = st.number_input("–í–µ—Å", 0.0, 10.0, float(rule["weight"]), 0.1, key=f"rule_weight_{rule['id']}")
            with c3:
                rule["enabled"] = st.checkbox("–í–∫–ª", value=rule["enabled"], key=f"rule_enabled_{rule['id']}")
            with c4:
                if st.button("üóëÔ∏è", key=f"rule_del_{rule['id']}"):
                    st.session_state.reward_rules.pop(idx)
                    st.rerun()
            
            # === EXTRACTORS (regex –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π) ===
            st.markdown("##### üîç –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã (regex)")
            st.caption("–ò–∑–≤–ª–µ–∫–∞—é—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ `{{extracted.–∏–º—è}}`")
            
            if "extractors" not in rule:
                rule["extractors"] = []
            
            for ei, ext in enumerate(rule["extractors"]):
                ec1, ec2, ec3, ec4 = st.columns([2, 4, 2, 1])
                with ec1:
                    ext["name"] = st.text_input("–ò–º—è", value=ext.get("name", f"var{ei}"), key=f"ext_name_{rule['id']}_{ei}")
                with ec2:
                    ext["pattern"] = st.text_input("Regex", value=ext.get("pattern", ""), key=f"ext_pattern_{rule['id']}_{ei}")
                with ec3:
                    ext["source"] = st.selectbox(
                        "–ò—Å—Ç–æ—á–Ω–∏–∫", 
                        ["{{response}}", "{{reference}}", "{{prompt}}"],
                        index=["{{response}}", "{{reference}}", "{{prompt}}"].index(ext.get("source", "{{response}}")),
                        key=f"ext_source_{rule['id']}_{ei}"
                    )
                with ec4:
                    if st.button("‚úñ", key=f"ext_del_{rule['id']}_{ei}"):
                        rule["extractors"].pop(ei)
                        st.rerun()
            
            if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä", key=f"add_ext_{rule['id']}"):
                rule["extractors"].append({"name": f"var{len(rule['extractors'])}", "pattern": r"(.*)", "source": "{{response}}"})
                st.rerun()
            
            st.markdown("---")
            
            # === CONDITIONS ===
            st.markdown("##### ‚ö° –£—Å–ª–æ–≤–∏—è")
            
            condition_types = {
                "contains": "—Å–æ–¥–µ—Ä–∂–∏—Ç",
                "not_contains": "–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç",
                "matches": "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex",
                "not_matches": "–Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç regex",
                "equals": "—Ä–∞–≤–Ω–æ (—Å—Ç—Ä–æ–∫–∞)",
                "equals_numeric": "—Ä–∞–≤–Ω–æ (—á–∏—Å–ª–æ)",
                "greater": "> –±–æ–ª—å—à–µ",
                "less": "< –º–µ–Ω—å—à–µ",
                "length_between": "–¥–ª–∏–Ω–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ",
                "length_min": "–¥–ª–∏–Ω–∞ >= –º–∏–Ω",
                "length_max": "–¥–ª–∏–Ω–∞ <= –º–∞–∫—Å",
            }
            
            if "conditions" not in rule:
                rule["conditions"] = []
            
            for ci, cond in enumerate(rule["conditions"]):
                ctype = cond.get("type", "contains")
                
                cc1, cc2 = st.columns([1, 4])
                with cc1:
                    if ci > 0:
                        st.write("**AND**" if rule.get("condition_logic") == "all" else "**OR**")
                    else:
                        st.write("**IF**")
                
                with cc2:
                    ccc1, ccc2, ccc3, ccc4 = st.columns([3, 2, 3, 1])
                    
                    with ccc1:
                        # –õ–µ–≤—ã–π –æ–ø–µ—Ä–∞–Ω–¥
                        target_options = ["{{response}}", "{{reference}}", "{{prompt}}"]
                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                        for ext in rule.get("extractors", []):
                            target_options.append(f"{{{{extracted.{ext['name']}}}}}")
                        
                        left_val = cond.get("target") or cond.get("left", "{{response}}")
                        if left_val not in target_options:
                            target_options.append(left_val)
                        
                        new_left = st.selectbox(
                            "–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å",
                            target_options,
                            index=target_options.index(left_val) if left_val in target_options else 0,
                            key=f"cond_left_{rule['id']}_{ci}",
                            label_visibility="collapsed"
                        )
                        cond["target"] = new_left
                        cond["left"] = new_left
                    
                    with ccc2:
                        new_type = st.selectbox(
                            "–û–ø–µ—Ä–∞—Ç–æ—Ä",
                            list(condition_types.keys()),
                            format_func=lambda x: condition_types.get(x, x),
                            index=list(condition_types.keys()).index(ctype) if ctype in condition_types else 0,
                            key=f"cond_type_{rule['id']}_{ci}",
                            label_visibility="collapsed"
                        )
                        cond["type"] = new_type
                    
                    with ccc3:
                        # –ü—Ä–∞–≤—ã–π –æ–ø–µ—Ä–∞–Ω–¥ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∏–ø–∞
                        if new_type in ["contains", "not_contains", "equals"]:
                            cond["value"] = st.text_input("–ó–Ω–∞—á–µ–Ω–∏–µ", value=cond.get("value", ""), key=f"cond_val_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type in ["matches", "not_matches"]:
                            cond["pattern"] = st.text_input("Regex", value=cond.get("pattern", ""), key=f"cond_pat_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type == "equals_numeric":
                            right_opts = ["{{reference}}"] + [f"{{{{extracted.{e['name']}}}}}" for e in rule.get("extractors", [])]
                            right_val = cond.get("right", "{{reference}}")
                            if right_val not in right_opts:
                                right_opts.append(right_val)
                            cond["right"] = st.selectbox("–°—Ä–∞–≤–Ω–∏—Ç—å —Å", right_opts, index=right_opts.index(right_val) if right_val in right_opts else 0, key=f"cond_right_{rule['id']}_{ci}", label_visibility="collapsed")
                            cond["tolerance"] = st.number_input("¬±", 0.0, 100.0, float(cond.get("tolerance", 0.01)), 0.01, key=f"cond_tol_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type in ["greater", "less"]:
                            cond["value"] = st.number_input("–ß–∏—Å–ª–æ", value=float(cond.get("value", 0)), key=f"cond_num_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type == "length_between":
                            lc1, lc2 = st.columns(2)
                            cond["min"] = lc1.number_input("–ú–∏–Ω", 0, 100000, int(cond.get("min", 10)), key=f"cond_min_{rule['id']}_{ci}")
                            cond["max"] = lc2.number_input("–ú–∞–∫—Å", 0, 100000, int(cond.get("max", 5000)), key=f"cond_max_{rule['id']}_{ci}")
                        elif new_type == "length_min":
                            cond["min"] = st.number_input("–ú–∏–Ω –¥–ª–∏–Ω–∞", 0, 100000, int(cond.get("min", 10)), key=f"cond_minl_{rule['id']}_{ci}", label_visibility="collapsed")
                        elif new_type == "length_max":
                            cond["max"] = st.number_input("–ú–∞–∫—Å –¥–ª–∏–Ω–∞", 0, 100000, int(cond.get("max", 5000)), key=f"cond_maxl_{rule['id']}_{ci}", label_visibility="collapsed")
                    
                    with ccc4:
                        if st.button("‚úñ", key=f"cond_del_{rule['id']}_{ci}"):
                            rule["conditions"].pop(ci)
                            st.rerun()
            
            # –õ–æ–≥–∏–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π
            lc1, lc2 = st.columns([2, 3])
            with lc1:
                rule["condition_logic"] = st.radio(
                    "–õ–æ–≥–∏–∫–∞",
                    ["all", "any"],
                    format_func=lambda x: "–í–°–ï —É—Å–ª–æ–≤–∏—è (AND)" if x == "all" else "–õ–Æ–ë–û–ï —É—Å–ª–æ–≤–∏–µ (OR)",
                    index=0 if rule.get("condition_logic", "all") == "all" else 1,
                    key=f"cond_logic_{rule['id']}",
                    horizontal=True
                )
            with lc2:
                if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å —É—Å–ª–æ–≤–∏–µ", key=f"add_cond_{rule['id']}"):
                    rule["conditions"].append({"type": "contains", "target": "{{response}}", "value": ""})
                    st.rerun()
            
            st.markdown("---")
            
            # === REWARD FORMULA ===
            st.markdown("##### üéØ –§–æ—Ä–º—É–ª–∞ Reward")
            
            rc1, rc2 = st.columns(2)
            with rc1:
                rule["reward_formula"] = st.text_input(
                    "–ï—Å–ª–∏ —É—Å–ª–æ–≤–∏—è TRUE",
                    value=rule.get("reward_formula", "1.0"),
                    key=f"reward_form_{rule['id']}",
                    help="–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ Python-–≤—ã—Ä–∞–∂–µ–Ω–∏—è: `min(len({{extracted.text}}) / 100, 1.0)`"
                )
            with rc2:
                rule["else_reward"] = st.text_input(
                    "–ï—Å–ª–∏ —É—Å–ª–æ–≤–∏—è FALSE",
                    value=rule.get("else_reward", "0.0"),
                    key=f"else_form_{rule['id']}"
                )
    
    # –†–µ–Ω–¥–µ—Ä–∏–º –≤—Å–µ –ø—Ä–∞–≤–∏–ª–∞
    for idx, rule in enumerate(st.session_state.reward_rules):
        render_rule(rule, idx)
    
    # –ö–Ω–æ–ø–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    st.markdown("---")
    
    col_add1, col_add2, col_add3 = st.columns(3)
    
    with col_add1:
        if st.button("‚ûï –ü—É—Å—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ", type="secondary"):
            new_id = st.session_state.next_rule_id
            st.session_state.next_rule_id += 1
            st.session_state.reward_rules.append({
                "id": new_id,
                "name": f"–ü—Ä–∞–≤–∏–ª–æ {new_id + 1}",
                "enabled": True,
                "weight": 1.0,
                "extractors": [],
                "conditions": [],
                "condition_logic": "all",
                "reward_formula": "1.0",
                "else_reward": "0.0",
            })
            st.rerun()
    
    with col_add2:
        preset_rules = st.selectbox(
            "–î–æ–±–∞–≤–∏—Ç—å —à–∞–±–ª–æ–Ω",
            [
                "-- –≤—ã–±–µ—Ä–∏—Ç–µ --",
                "üîç Regex + —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ",
                "üè∑Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–≥–æ–≤",
                "üìè –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã",
                "üî§ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞",
                "üîÑ –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã",
                "üêç Python —Ñ–æ—Ä–º—É–ª–∞",
            ],
            key="preset_select"
        )
    
    with col_add3:
        if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å —à–∞–±–ª–æ–Ω", type="primary"):
            new_id = st.session_state.next_rule_id
            st.session_state.next_rule_id += 1
            
            if preset_rules == "üîç Regex + —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "Regex –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ", "enabled": True, "weight": 1.0,
                    "extractors": [{"name": "answer", "pattern": r"<answer>\s*(\d+)\s*</answer>", "source": "{{response}}"}],
                    "conditions": [{"type": "equals_numeric", "left": "{{extracted.answer}}", "right": "{{reference}}", "tolerance": 0.01}],
                    "condition_logic": "all", "reward_formula": "1.0", "else_reward": "0.0",
                })
            elif preset_rules == "üè∑Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–≥–æ–≤":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–§–æ—Ä–º–∞—Ç —Ç–µ–≥–æ–≤", "enabled": True, "weight": 1.0,
                    "extractors": [],
                    "conditions": [
                        {"type": "contains", "target": "{{response}}", "value": "<reasoning>"},
                        {"type": "contains", "target": "{{response}}", "value": "</reasoning>"},
                        {"type": "contains", "target": "{{response}}", "value": "<answer>"},
                        {"type": "contains", "target": "{{response}}", "value": "</answer>"},
                    ],
                    "condition_logic": "all", "reward_formula": "1.0", "else_reward": "0.0",
                })
            elif preset_rules == "üìè –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞", "enabled": True, "weight": 0.5,
                    "extractors": [],
                    "conditions": [{"type": "length_between", "target": "{{response}}", "min": 100, "max": 3000}],
                    "condition_logic": "all", "reward_formula": "1.0", "else_reward": "-0.5",
                })
            elif preset_rules == "üî§ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "enabled": True, "weight": 0.3,
                    "extractors": [],
                    "conditions": [
                        {"type": "contains", "target": "{{response}}", "value": "therefore"},
                    ],
                    "condition_logic": "any", "reward_formula": "0.5", "else_reward": "0.0",
                })
            elif preset_rules == "üîÑ –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "–ë–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤", "enabled": True, "weight": 0.5,
                    "extractors": [],
                    "conditions": [{"type": "not_matches", "target": "{{response}}", "pattern": r"(.{20,})\1"}],
                    "condition_logic": "all", "reward_formula": "0.5", "else_reward": "-0.5",
                })
            elif preset_rules == "üêç Python —Ñ–æ—Ä–º—É–ª–∞":
                st.session_state.reward_rules.append({
                    "id": new_id, "name": "Python —Ñ–æ—Ä–º—É–ª–∞", "enabled": True, "weight": 1.0,
                    "extractors": [{"name": "reasoning", "pattern": r"<reasoning>(.*?)</reasoning>", "source": "{{response}}", "flags": "DOTALL"}],
                    "conditions": [],
                    "condition_logic": "all", 
                    "reward_formula": "min(len({{extracted.reasoning}}) / 500.0, 1.0) if {{extracted.reasoning}} else 0.0",
                    "else_reward": "0.0",
                })
            else:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —à–∞–±–ª–æ–Ω")
            st.rerun()
    
    # =========================================================================
    # –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
    # =========================================================================
    st.markdown("---")
    st.markdown("#### ‚ñ∂Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª")
    
    def evaluate_rules(response: str, reference: str, prompt: str, rules: list) -> list:
        """–í—ã—á–∏—Å–ª—è–µ—Ç reward –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞."""
        results = []
        
        for rule in rules:
            if not rule.get("enabled", True):
                continue
            
            result = {
                "name": rule["name"],
                "weight": rule["weight"],
                "extracted": {},
                "conditions_met": [],
                "all_conditions_true": False,
                "raw_reward": 0.0,
                "weighted_reward": 0.0,
                "details": "",
            }
            
            # 1. –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã
            for ext in rule.get("extractors", []):
                source = ext.get("source", "{{response}}")
                source_text = source.replace("{{response}}", response).replace("{{reference}}", reference).replace("{{prompt}}", prompt)
                
                pattern = ext.get("pattern", "")
                flags = 0
                if "DOTALL" in ext.get("flags", ""):
                    flags |= re.DOTALL
                if "IGNORECASE" in ext.get("flags", ""):
                    flags |= re.IGNORECASE
                
                try:
                    match = re.search(pattern, source_text, flags)
                    if match:
                        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø –∏ –æ–±—ã—á–Ω—ã—Ö
                        if match.groups():
                            result["extracted"][ext["name"]] = match.group(1)
                        elif match.groupdict():
                            result["extracted"].update(match.groupdict())
                        else:
                            result["extracted"][ext["name"]] = match.group(0)
                    else:
                        result["extracted"][ext["name"]] = ""
                except re.error as e:
                    result["extracted"][ext["name"]] = f"REGEX_ERROR: {e}"
            
            # 2. –£—Å–ª–æ–≤–∏—è
            conditions_results = []
            for cond in rule.get("conditions", []):
                cond_type = cond.get("type", "contains")
                
                # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                def substitute_vars(text):
                    if not isinstance(text, str):
                        return text
                    text = text.replace("{{response}}", response)
                    text = text.replace("{{reference}}", reference)
                    text = text.replace("{{prompt}}", prompt)
                    for k, v in result["extracted"].items():
                        text = text.replace(f"{{{{extracted.{k}}}}}", str(v) if v else "")
                    return text
                
                target = substitute_vars(cond.get("target") or cond.get("left", ""))
                
                cond_met = False
                cond_detail = ""
                
                try:
                    if cond_type == "contains":
                        value = substitute_vars(cond.get("value", ""))
                        cond_met = value in target
                        cond_detail = f"'{value[:30]}...' {'‚úì –Ω–∞–π–¥–µ–Ω–æ' if cond_met else '‚úó –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}"
                    
                    elif cond_type == "not_contains":
                        value = substitute_vars(cond.get("value", ""))
                        cond_met = value not in target
                        cond_detail = f"'{value[:30]}' {'‚úì –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è' if cond_met else '‚úó —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è'}"
                    
                    elif cond_type == "matches":
                        pattern = cond.get("pattern", "")
                        cond_met = bool(re.search(pattern, target))
                        cond_detail = f"regex {'‚úì' if cond_met else '‚úó'}"
                    
                    elif cond_type == "not_matches":
                        pattern = cond.get("pattern", "")
                        cond_met = not bool(re.search(pattern, target))
                        cond_detail = f"not regex {'‚úì' if cond_met else '‚úó'}"
                    
                    elif cond_type == "equals":
                        value = substitute_vars(cond.get("value", ""))
                        cond_met = target.strip() == value.strip()
                        cond_detail = f"{'‚úì' if cond_met else '‚úó'} '{target[:20]}' == '{value[:20]}'"
                    
                    elif cond_type == "equals_numeric":
                        right = substitute_vars(cond.get("right", ""))
                        tolerance = float(cond.get("tolerance", 0.01))
                        try:
                            left_num = float(re.sub(r"[^\d.\-]", "", str(target)))
                            right_num = float(re.sub(r"[^\d.\-]", "", str(right)))
                            cond_met = abs(left_num - right_num) <= tolerance
                            cond_detail = f"{left_num} {'=' if cond_met else '‚â†'} {right_num} (¬±{tolerance})"
                        except:
                            cond_detail = "‚úó –Ω–µ —á–∏—Å–ª–∞"
                    
                    elif cond_type == "greater":
                        value = float(cond.get("value", 0))
                        try:
                            num = float(re.sub(r"[^\d.\-]", "", str(target)))
                            cond_met = num > value
                            cond_detail = f"{num} {'>' if cond_met else '‚â§'} {value}"
                        except:
                            cond_detail = "‚úó –Ω–µ —á–∏—Å–ª–æ"
                    
                    elif cond_type == "less":
                        value = float(cond.get("value", 0))
                        try:
                            num = float(re.sub(r"[^\d.\-]", "", str(target)))
                            cond_met = num < value
                            cond_detail = f"{num} {'<' if cond_met else '‚â•'} {value}"
                        except:
                            cond_detail = "‚úó –Ω–µ —á–∏—Å–ª–æ"
                    
                    elif cond_type == "length_between":
                        length = len(target)
                        min_len = int(cond.get("min", 0))
                        max_len = int(cond.get("max", 99999))
                        cond_met = min_len <= length <= max_len
                        cond_detail = f"len={length} {'‚úì' if cond_met else '‚úó'} [{min_len}, {max_len}]"
                    
                    elif cond_type == "length_min":
                        length = len(target)
                        min_len = int(cond.get("min", 0))
                        cond_met = length >= min_len
                        cond_detail = f"len={length} >= {min_len} {'‚úì' if cond_met else '‚úó'}"
                    
                    elif cond_type == "length_max":
                        length = len(target)
                        max_len = int(cond.get("max", 99999))
                        cond_met = length <= max_len
                        cond_detail = f"len={length} <= {max_len} {'‚úì' if cond_met else '‚úó'}"
                    
                except Exception as e:
                    cond_detail = f"‚úó –æ—à–∏–±–∫–∞: {e}"
                
                conditions_results.append({"met": cond_met, "detail": cond_detail})
            
            result["conditions_met"] = conditions_results
            
            # –õ–æ–≥–∏–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            logic = rule.get("condition_logic", "all")
            if not conditions_results:
                result["all_conditions_true"] = True  # –ù–µ—Ç —É—Å–ª–æ–≤–∏–π = –≤—Å–µ–≥–¥–∞ true
            elif logic == "all":
                result["all_conditions_true"] = all(c["met"] for c in conditions_results)
            else:  # any
                result["all_conditions_true"] = any(c["met"] for c in conditions_results)
            
            # 3. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ reward
            formula = rule.get("reward_formula", "1.0") if result["all_conditions_true"] else rule.get("else_reward", "0.0")
            
            # –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º—É–ª—É
            formula = formula.replace("{{response}}", f"'''{response}'''")
            formula = formula.replace("{{reference}}", f"'''{reference}'''")
            formula = formula.replace("{{prompt}}", f"'''{prompt}'''")
            for k, v in result["extracted"].items():
                safe_v = str(v).replace("'", "\\'") if v else ""
                formula = formula.replace(f"{{{{extracted.{k}}}}}", f"'''{safe_v}'''")
            
            try:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ
                safe_globals = {"__builtins__": {"len": len, "min": min, "max": max, "abs": abs, "float": float, "int": int, "str": str, "bool": bool}}
                result["raw_reward"] = float(eval(formula, safe_globals))
            except Exception as e:
                result["raw_reward"] = 0.0
                result["details"] = f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º—É–ª—ã: {e}"
            
            result["weighted_reward"] = result["raw_reward"] * result["weight"]
            results.append(result)
        
        return results
    
    if st.button("‚ñ∂Ô∏è –†–∞—Å—Å—á–∏—Ç–∞—Ç—å Reward", type="primary", use_container_width=True):
        results = evaluate_rules(
            current_response, 
            sample_reference, 
            sample_prompt, 
            st.session_state.reward_rules
        )
        
        total_reward = sum(r["weighted_reward"] for r in results)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        st.markdown("##### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        for r in results:
            status = "‚úÖ" if r["all_conditions_true"] else "‚ùå"
            
            with st.container():
                c1, c2, c3 = st.columns([3, 2, 2])
                with c1:
                    st.markdown(f"**{status} {r['name']}**")
                with c2:
                    st.write(f"Raw: `{r['raw_reward']:.3f}`")
                with c3:
                    color = "green" if r["weighted_reward"] > 0 else ("red" if r["weighted_reward"] < 0 else "gray")
                    st.markdown(f"√ó{r['weight']} = **:{color}[{r['weighted_reward']:.3f}]**")
                
                # –î–µ—Ç–∞–ª–∏
                if r["extracted"]:
                    st.caption(f"üì¶ –ò–∑–≤–ª–µ—á–µ–Ω–æ: {r['extracted']}")
                
                for ci, cond in enumerate(r["conditions_met"]):
                    icon = "‚úì" if cond["met"] else "‚úó"
                    st.caption(f"  {icon} {cond['detail']}")
                
                if r["details"]:
                    st.warning(r["details"])
        
        st.markdown("---")
        color = "green" if total_reward > 0 else "red"
        st.markdown(f"### üéØ –ò—Ç–æ–≥–æ–≤—ã–π Reward: :{color}[**{total_reward:.3f}**]")
        
        # –ì—Ä–∞—Ñ–∏–∫
        if results:
            import plotly.graph_objects as go
            fig = go.Figure(data=[
                go.Bar(
                    x=[r["name"] for r in results],
                    y=[r["weighted_reward"] for r in results],
                    marker_color=["#22c55e" if r["weighted_reward"] > 0 else "#ef4444" for r in results],
                    text=[f"{r['weighted_reward']:.2f}" for r in results],
                    textposition="outside"
                )
            ])
            fig.update_layout(
                title="–í–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞",
                yaxis_title="Weighted Reward",
                height=350,
                showlegend=False,
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # =========================================================================
    # –§–æ—Ä–º–∞—Ç reasoning
    # =========================================================================
    st.markdown("---")
    st.markdown("#### üìù –§–æ—Ä–º–∞—Ç Reasoning")
    
    reasoning_format = st.selectbox(
        "–§–æ—Ä–º–∞—Ç —Ç–µ–≥–æ–≤",
        ["deepseek", "simple", "russian"],
        format_func=lambda x: {
            "deepseek": "DeepSeek (<think>...</think>, <answer>...</answer>)",
            "simple": "Simple (<reasoning>...</reasoning>, <answer>...</answer>)",
            "russian": "Russian (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ)",
        }[x],
    )
    
    # –ü—Ä–µ–≤—å—é —Ñ–æ—Ä–º–∞—Ç–∞
    format_examples = {
        "deepseek": """<think>
–î–∞–Ω–æ: ... 
–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏: ...
–†–µ—à–µ–Ω–∏–µ: ...
</think>
<answer>
42
</answer>""",
        "simple": """<reasoning>
–®–∞–≥ 1: ...
–®–∞–≥ 2: ...
</reasoning>
<answer>
42
</answer>""",
        "russian": """<reasoning>
–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: ...
–í—ã—á–∏—Å–ª–µ–Ω–∏—è: ...
</reasoning>
<answer>
42
</answer>""",
    }
    
    with st.expander("üìã –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞"):
        st.code(format_examples[reasoning_format], language=None)
    
    # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é reward –ø—Ä–∞–≤–∏–ª (–Ω–æ–≤—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
    reward_rules = [
        {
            "name": rule["name"],
            "weight": rule["weight"],
            "enabled": rule.get("enabled", True),
            "extractors": rule.get("extractors", []),
            "conditions": rule.get("conditions", []),
            "condition_logic": rule.get("condition_logic", "all"),
            "reward_formula": rule.get("reward_formula", "1.0"),
            "else_reward": rule.get("else_reward", "0.0"),
        }
        for rule in st.session_state.get("reward_rules", [])
        if rule.get("enabled", True)
    ]
    
    return {
        "grpo_dataset_source": dataset_source,
        "grpo_dataset_key": dataset_key,
        "grpo_dataset_path": grpo_dataset_path,
        "grpo_dataset_language": grpo_dataset_language,
        "grpo_max_samples": grpo_max_samples if grpo_max_samples > 0 else None,
        "grpo_reward_rules": reward_rules,
        "grpo_reasoning_format": reasoning_format,
    }


def render_model_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.header("üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –†–µ–∂–∏–º")
    
    # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    stage_options = {
        "pretrain": "Pretraining (—Å –Ω—É–ª—è)",
        "continual_pretrain": "Continual Pretraining (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)",
        "sft": "SFT (Fine-Tuning)",
        "grpo": "üß† GRPO (RL –¥–ª—è Reasoning)"
    }
    selected_stage = st.sidebar.selectbox(
        "–≠—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è",
        options=list(stage_options.keys()),
        format_func=lambda x: stage_options[x],
        help="–í—ã–±–µ—Ä–∏—Ç–µ —ç—Ç–∞–ø: –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è, –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ pretrain, –¥–æ–æ–±—É—á–µ–Ω–∏–µ (SFT) –∏–ª–∏ RL –æ–±—É—á–µ–Ω–∏–µ (GRPO)"
    )
    
    # –ò–º—è –º–æ–¥–µ–ª–∏ (–¥–ª—è –ø–∞–ø–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
    if selected_stage == "pretrain":
        model_name_default = "home_pretrain"
    elif selected_stage == "continual_pretrain":
        model_name_default = "home_continual_pretrain"
    elif selected_stage == "grpo":
        model_name_default = "home_grpo"
    else:
        model_name_default = "home_sft"
    model_name = st.sidebar.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", value=model_name_default, help="–ò–º—è –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    base_model_path = None
    
    if selected_stage in ("sft", "continual_pretrain", "grpo"):
        stage_label = {"sft": "SFT", "continual_pretrain": "Continual Pretraining", "grpo": "GRPO"}.get(selected_stage, selected_stage)
        st.sidebar.subheader("üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        available = get_available_models()
        
        if selected_stage == "continual_pretrain":
            # –î–ª—è continual_pretrain —Ñ–∏–ª—å—Ç—Ä—É–µ–º: –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º final/export –º–æ–¥–µ–ª–∏ –∏ HF –º–æ–¥–µ–ª–∏
            # Checkpoint'—ã —Ç–æ–∂–µ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã (–¥–ª—è resume), –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
            hf_models = [m for m in available if m["type"] == "hf"]
            final_models = [m for m in available if m["type"] == "final"]
            checkpoint_models = [m for m in available if m["type"] == "checkpoint"]
            
            if hf_models or final_models:
                st.sidebar.info("üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ü§ó HF –º–æ–¥–µ–ª—å –∏–ª–∏ final_model –¥–ª—è continual pretraining")
                available_filtered = hf_models + final_models + checkpoint_models
            else:
                if checkpoint_models:
                    st.sidebar.warning(
                        "‚ö†Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ checkpoint'—ã. –î–ª—è resume —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, "
                        "–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ continual pretraining –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å final_model."
                    )
                available_filtered = checkpoint_models if checkpoint_models else available
        else:
            # –î–ª—è SFT –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ (HF –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–º–∏)
            hf_models = [m for m in available if m["type"] == "hf"]
            other_models = [m for m in available if m["type"] != "hf"]
            available_filtered = hf_models + other_models
        
        if not available_filtered:
            st.sidebar.warning(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è {stage_label}. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ ü§ñ –ú–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–±—É—á–∏—Ç–µ Pretrain!")
            # –ú–æ–∂–Ω–æ –¥–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–≤–µ—Å—Ç–∏ –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é
            base_model_path = st.sidebar.text_input("–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –≤—Ä—É—á–Ω—É—é", placeholder="/path/to/model")
        else:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–ø—Ü–∏–π —Å –ø–æ–º–µ—Ç–∫–∞–º–∏ —Ç–∏–ø–æ–≤
            def get_model_label(m):
                if m["type"] == "hf":
                    return m["name"]  # –£–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ü§ó
                elif m["type"] == "final":
                    return f"{m['name']} (‚úÖ final)"
                else:
                    return f"{m['name']} (‚ö†Ô∏è checkpoint)"
            
            model_options = [get_model_label(m) for m in available_filtered]
            
            selected_base_name = st.sidebar.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å", 
                options=model_options,
                help="ü§ó ‚Äî –º–æ–¥–µ–ª–∏ —Å HuggingFace, ‚úÖ final ‚Äî –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, ‚ö†Ô∏è checkpoint ‚Äî –¥–ª—è resume"
            )
            
            # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å –ø–æ –∏–Ω–¥–µ–∫—Å—É (model_options –∏ available_filtered —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥—É)
            selected_idx = model_options.index(selected_base_name)
            selected_model = available_filtered[selected_idx]
            base_model_path = selected_model["path"]
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è checkpoint –≤ continual_pretrain
            if selected_stage == "continual_pretrain" and selected_model["type"] == "checkpoint":
                st.sidebar.info(
                    "‚ÑπÔ∏è –í—ã–±—Ä–∞–Ω checkpoint. –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω resume (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ scheduler). "
                    "–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–æ–≤–æ–≥–æ continual pretraining –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å final_model –∏–ª–∏ ü§ó HF –º–æ–¥–µ–ª—å."
                )
            elif selected_model["type"] == "hf":
                st.sidebar.success("‚úÖ HuggingFace –º–æ–¥–µ–ª—å")
            
            st.sidebar.caption(f"–ü—É—Ç—å: `{base_model_path}`")
    
    # –§–ª–∞–≥, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    loaded_config = None
    
    if selected_stage in ("sft", "continual_pretrain", "grpo") and base_model_path:
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥
        # –í–ê–ñ–ù–û: —Ä–∞–∑–ª–∏—á–∞–µ–º run_config.json (training params) –∏ config.json (model params)
        try:
            base_path = Path(base_model_path)
            cfg_path = None
            cfg_type = None  # "run" –∏–ª–∏ "model"
            
            # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º run_config.json (–ø–æ–ª–Ω—ã–π training config)
            # –î–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: checkpoint_step6800 -> parent/run_config.json
            if (base_path.parent / "run_config.json").exists():
                cfg_path = base_path.parent / "run_config.json"
                cfg_type = "run"
            elif (base_path / "run_config.json").exists():
                cfg_path = base_path / "run_config.json"
                cfg_type = "run"
            # 2. –ï—Å–ª–∏ run_config –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º config.json (—Ç–æ–ª—å–∫–æ model params)
            elif (base_path / "config.json").exists():
                cfg_path = base_path / "config.json"
                cfg_type = "model"
            
            if cfg_path and cfg_path.exists():
                with open(cfg_path) as f:
                    loaded_config = json.load(f)
                if cfg_type == "run":
                    st.sidebar.success("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ run_config.json")
                else:
                    # config.json —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–∏
                    # transformers config -> –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
                    if "num_hidden_layers" in loaded_config and "num_layers" not in loaded_config:
                        loaded_config["num_layers"] = loaded_config["num_hidden_layers"]
                    if "num_attention_heads" in loaded_config and "n_heads" not in loaded_config:
                        loaded_config["n_heads"] = loaded_config["num_attention_heads"]
                    if "max_position_embeddings" in loaded_config and "seq_len" not in loaded_config:
                        loaded_config["seq_len"] = loaded_config["max_position_embeddings"]
                    st.sidebar.info("‚ÑπÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ config.json (training params –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)")
            else:
                st.sidebar.warning("‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ä—É—á–Ω—É—é")
        except Exception as e:
             st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è config: {e}")

    st.sidebar.subheader("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")

    blueprint_path = ""
    model_type = "HomeModel (GPT-2 style)"  # default
    # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    default_h, default_l, default_n, default_seq = 512, 8, 8, 2048

    # –í—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢–û–õ–¨–ö–û –¥–ª—è pretrain (—Å –Ω—É–ª—è)
    # –î–ª—è SFT/Continual Pretrain –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
    if not loaded_config:
        # === –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° VISUAL MODEL BUILDER ===
        blueprints_dir = PROJECT_ROOT / "blueprints"
        blueprints_dir.mkdir(exist_ok=True)
        blueprints = list(blueprints_dir.glob("*.json"))
            
        arch_options = ["HomeModel (GPT-2 style)", "Llama (Custom)", "Mistral (Custom)", "Custom Blueprint (Visual Builder)"]
        
        model_type = st.sidebar.selectbox(
            "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏",
            options=arch_options,
            index=0,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É. Custom Blueprint –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑ Visual Builder."
        )

    if not loaded_config and model_type == "Custom Blueprint (Visual Builder)":
        blueprints_dir = PROJECT_ROOT / "blueprints"
        blueprints = list(blueprints_dir.glob("*.json"))
        # –õ–æ–≥–∏–∫–∞ –¥–ª—è Blueprint –ø—Ä–æ–µ–∫—Ç–æ–≤
        if blueprints:
            bp_names = [b.name for b in blueprints]
            selected_bp = st.sidebar.selectbox("–ü—Ä–æ–µ–∫—Ç (Visual Builder)", bp_names)
            blueprint_path = str(blueprints_dir / selected_bp)
        else:
            st.sidebar.warning("Blueprint –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–ø–∫–µ `blueprints/`. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç –≤ Visual Model Builder –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é.")
            blueprint_path = st.sidebar.text_input("–ü—É—Ç—å –∫ blueprint.json", value=str(blueprints_dir / "model.json"))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ –∏–∑ blueprint
        try:
            with open(blueprint_path) as f:
                bp_data = json.load(f)
            st.sidebar.info(f"Blocks: {len(bp_data.get('blocks', []))} | Hidden: {bp_data.get('hidden_size')} | Vocab: {bp_data.get('vocab_size')}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç—ã –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–æ–≤ –Ω–∏–∂–µ (—á—Ç–æ–±—ã –æ–Ω–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏)
            default_h = bp_data.get("hidden_size", 512)
            default_seq = bp_data.get("max_position_embeddings", 2048)
            # –°–ª–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–∏ –∏–∑ Repeater, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –≥—Ä—É–±–æ
            default_l = len(bp_data.get("blocks", []))
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            st.sidebar.markdown("**–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä**")
            tokenizer_mode = st.sidebar.radio("Tokenizer Source", ["Standard (GPT-2)", "HF Repo", "Local Path"], horizontal=True)
            if tokenizer_mode == "Standard (GPT-2)":
                tokenizer_path = "gpt2"
            elif tokenizer_mode == "HF Repo":
                tokenizer_path = st.sidebar.text_input("HF ID", "meta-llama/Llama-2-7b-hf")
            else:
                tokenizer_path = st.sidebar.text_input("Local Path", str(PROJECT_ROOT / "tokenizers/my_tok"))
                
        except Exception as e:
            st.sidebar.error(f"Invalid Blueprint: {e}")
            tokenizer_path = "gpt2"
    elif not loaded_config:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è pretrain
        tokenizer_path = None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ
    else:
        # –î–ª—è SFT/Continual Pretrain —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±–µ—Ä–µ—Ç—Å—è –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        tokenizer_path = None

    if loaded_config:
        # –†–µ–∂–∏–º –¥–ª—è SFT/Continual Pretrain —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º
        # –°–ª–∞–π–¥–µ—Ä—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã
        disabled_sliders = True
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö –∏–º–µ–Ω –∫–ª—é—á–µ–π)
        hidden_size = loaded_config.get("hidden_size", 512)
        # num_hidden_layers - HF, num_layers - –Ω–∞—à –∫–æ–Ω—Ñ–∏–≥
        num_layers = loaded_config.get("num_hidden_layers", loaded_config.get("num_layers", 8))
        num_attention_heads = loaded_config.get("num_attention_heads", loaded_config.get("n_heads", 8))
        max_position_embeddings = loaded_config.get("max_position_embeddings", loaded_config.get("seq_len", 2048))
        
        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç—Å—è
        n_heads = num_attention_heads
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        base_model_type = loaded_config.get("model_type", loaded_config.get("architectures", ["Unknown"])[0] if "architectures" in loaded_config else "HomeModel")
        if isinstance(base_model_type, list):
            base_model_type = base_model_type[0] if base_model_type else "Unknown"
        st.sidebar.markdown(f"**–¢–∏–ø –º–æ–¥–µ–ª–∏:** `{base_model_type}`")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        c1, c2 = st.sidebar.columns(2)
        c1.metric("Hidden Size", hidden_size)
        c2.metric("Layers", num_layers)
        c1.metric("Heads", n_heads)
        c2.metric("Max Context", f"{max_position_embeddings:,}")
        
        st.sidebar.info("üîí –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞ (–æ—Ç –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏)")
        
        # seq_len –ú–û–ñ–ù–û –º–µ–Ω—è—Ç—å - —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –º–µ–Ω—å—à–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        st.sidebar.markdown("---")
        st.sidebar.markdown("**‚öôÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è**")
        
        # –û–ø—Ü–∏–∏ seq_len: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ + –º–∞–∫—Å–∏–º—É–º –º–æ–¥–µ–ª–∏
        seq_len_opts = [512, 1024, 2048, 4096, 8192]
        if max_position_embeddings not in seq_len_opts:
            seq_len_opts.append(max_position_embeddings)
        seq_len_opts = sorted([s for s in seq_len_opts if s <= max_position_embeddings])
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048 –∏–ª–∏ –º–∞–∫—Å–∏–º—É–º –µ—Å–ª–∏ –º–µ–Ω—å—à–µ
        default_seq = min(2048, max_position_embeddings)
        default_idx = seq_len_opts.index(default_seq) if default_seq in seq_len_opts else len(seq_len_opts) - 1
        
        seq_len = st.sidebar.selectbox(
            "Seq Length (–æ–±—É—á–µ–Ω–∏–µ)",
            seq_len_opts,
            index=default_idx,
            help=f"–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–∞–∫—Å. –º–æ–¥–µ–ª–∏: {max_position_embeddings:,}. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏."
        )
        
        if seq_len < max_position_embeddings:
            st.sidebar.caption(f"üí° –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {seq_len}, –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ {max_position_embeddings:,}")
        
    else:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        d_hid, d_layers = 512, 8
        
        if selected_stage == "sft":
            st.sidebar.caption("‚ö†Ô∏è –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é!")

        # –ü—Ä–µ—Å–µ—Ç—ã
        preset = st.sidebar.selectbox(
            "–ü—Ä–µ—Å–µ—Ç",
            ["Tiny (25M)", "Small (80M)", "Medium (200M)", "Large (400M)", "Custom"],
            index=0
        )
        
        presets = {
            "Tiny (25M)": (512, 8, 8),
            "Small (80M)": (768, 12, 12),
            "Medium (200M)": (1024, 16, 16),
            "Large (400M)": (1280, 20, 20),
        }
        
        if preset != "Custom" and preset in presets:
            default_h, default_l, default_n = presets[preset]
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ blueprint mode, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ blueprint –∫–∞–∫ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–æ–≤
            # –ù–æ –Ω–µ –º–µ–Ω—è–µ–º default_h/l/n –≥–ª–æ–±–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å –ª–æ–≥–∏–∫—É –ø—Ä–µ—Å–µ—Ç–æ–≤ –µ—Å–ª–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ
            if model_type != "Custom Blueprint (Visual Builder)":
                 default_h, default_l, default_n = 512, 8, 8
        
        # –°–ª–∞–π–¥–µ—Ä—ã (—Ç–µ–ø–µ—Ä—å –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª—é–ø—Ä–∏–Ω—Ç–∞ –∏–ª–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –º–µ–Ω—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã HomeModel)
        disabled_sliders = (model_type == "Custom Blueprint (Visual Builder)")
        
        hidden_size = st.sidebar.slider(
            "Hidden Size", 
            min_value=128, 
            max_value=2048, 
            value=default_h, 
            step=64,
            help="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è",
            disabled=disabled_sliders
        )
        
        num_layers = st.sidebar.slider(
            "Num Layers", 
            min_value=2, 
            max_value=32, 
            value=default_l,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞",
            disabled=disabled_sliders
        )
        
        n_heads = st.sidebar.slider(
            "Attention Heads", 
            min_value=2, 
            max_value=32, 
            value=default_n,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è",
            disabled=disabled_sliders
        )
        
        seq_len_opts = [512, 1024, 2048, 4096, 6144, 8192]
        if default_seq not in seq_len_opts: seq_len_opts.append(default_seq)
        seq_len_opts = sorted(seq_len_opts)
        
        seq_len = st.sidebar.selectbox(
            "Seq Length",
            seq_len_opts,
            index=seq_len_opts.index(default_seq) if default_seq in seq_len_opts else 0,
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
            disabled=disabled_sliders
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ hidden_size –∏ n_heads
    if not disabled_sliders and hidden_size % n_heads != 0:
        st.sidebar.error(f"‚ö†Ô∏è hidden_size ({hidden_size}) –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ n_heads ({n_heads}) –±–µ–∑ –æ—Å—Ç–∞—Ç–∫–∞!")
        valid_heads = [str(i) for i in range(1, min(33, hidden_size+1)) if hidden_size % i == 0][:10]
        if valid_heads:
            st.sidebar.info(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è n_heads: {', '.join(valid_heads)}")
    
    # –û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (HomeModel RoPE/SwiGLU)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º loaded_config –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –¥–µ—Ñ–æ–ª—Ç—ã
    vocab_size = int(loaded_config.get("vocab_size", 50257)) if loaded_config else 50257
    intermediate_size = int(loaded_config.get("intermediate_size") or (int(hidden_size) * 4)) if loaded_config else (int(hidden_size) * 4)
    est_params = estimate_parameters(
        hidden_size,
        num_layers,
        vocab_size=vocab_size,
        intermediate_size=intermediate_size,
    )
    st.sidebar.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (‚âà)", format_params(est_params))
    
    # Model ID –¥–ª—è pretrain from scratch (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è HF –º–æ–¥–µ–ª–µ–π)
    model_id = None
    if selected_stage == "pretrain":
        st.sidebar.subheader("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
        use_hf_model = st.sidebar.checkbox(
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å HuggingFace –º–æ–¥–µ–ª—å",
            value=False,
            help="–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å HF model_id –¥–ª—è pretrain from scratch"
        )
        if use_hf_model:
            model_id = st.sidebar.text_input(
                "HF Model ID",
                placeholder="gpt2, microsoft/DialoGPT-small, etc.",
                help="HuggingFace model ID –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –Ω—É–ª—è"
            )
            if model_id:
                st.sidebar.info(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {model_id}")
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                st.sidebar.warning(
                    "‚ö†Ô∏è **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å `trust_remote_code=True` –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å "
                    "—á—É–∂–æ–π –∫–æ–¥. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏."
                )
    
    # –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞ (full/LoRA/QLoRA)
    st.sidebar.subheader("üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞")
    tuning_method = st.sidebar.selectbox(
        "–ú–µ—Ç–æ–¥",
        ["full", "lora", "qlora"],
        index=0,
        help="full: –ø–æ–ª–Ω—ã–π fine-tuning, lora: LoRA, qlora: QLoRA (4-bit + LoRA)"
    )
    
    lora_r = None
    lora_alpha = None
    lora_dropout = None
    lora_target_modules = None
    
    if tuning_method in ("lora", "qlora"):
        st.sidebar.markdown("**LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**")
        lora_r = st.sidebar.slider("LoRA r", min_value=4, max_value=128, value=16, step=4)
        lora_alpha = st.sidebar.slider("LoRA alpha", min_value=4, max_value=256, value=32, step=4)
        lora_dropout = st.sidebar.slider("LoRA dropout", min_value=0.0, max_value=0.5, value=0.1, step=0.05)
        
        lora_target_modules_input = st.sidebar.text_input(
            "Target modules (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            placeholder="q_proj,k_proj,v_proj,o_proj",
            help="–ú–æ–¥—É–ª–∏ –¥–ª—è LoRA (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é). –ï—Å–ª–∏ –ø—É—Å—Ç–æ - –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç"
        )
        if lora_target_modules_input:
            lora_target_modules = [m.strip() for m in lora_target_modules_input.split(",")]
    
    # –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
    config = {
        "stage": selected_stage,
        "base_model_path": base_model_path,
        "model_name_input": model_name,
        "model_id": model_id if model_id else None,
        "tuning_method": tuning_method,
        "lora_r": lora_r,
        "lora_alpha": lora_alpha,
        "lora_dropout": lora_dropout,
        "lora_target_modules": lora_target_modules,
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–∞–π–¥–µ—Ä–æ–≤ (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "n_heads": n_heads,
        "seq_len": seq_len,
    }

    if model_type == "Custom Blueprint (Visual Builder)":
        config["model_type"] = "blueprint"
        config["blueprint_path"] = blueprint_path
        if tokenizer_path:
             config["tokenizer_path"] = tokenizer_path
    else:
        config["model_type"] = "home"
        if "Llama" in model_type: config["arch_preset"] = "llama"
        if "Mistral" in model_type: config["arch_preset"] = "mistral"
        
    return config


def render_training_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
    
    batch_size = st.sidebar.slider(
        "Batch Size",
        min_value=1,
        max_value=256,
        value=4,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞"
    )
    
    grad_accum = st.sidebar.slider(
        "Gradient Accumulation",
        min_value=1,
        max_value=32,
        value=8,
        help="–®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞"
    )
    
    st.sidebar.caption(f"Effective batch: {batch_size * grad_accum}")
    
    learning_rate = st.sidebar.select_slider(
        "Learning Rate",
        options=[1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3],
        value=5e-4,
        format_func=lambda x: f"{x:.0e}"
    )

    lr_schedule_label = st.sidebar.selectbox(
        "LR scheduler",
        options=[
            "Cosine (with warmup)",
            "Linear (with warmup)",
            "Constant (with warmup)",
            "Cosine with Restarts (with warmup)",
        ],
        index=0,
        help=(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –º–µ–Ω—è—Ç—å learning rate –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è. "
            "–í–∞–∂–Ω–æ: scheduler —à–∞–≥–∞–µ—Ç –Ω–∞ update-step (–ø–æ—Å–ª–µ grad_accum), –Ω–µ –Ω–∞ –∫–∞–∂–¥–æ–º micro-batch."
        ),
    )
    lr_schedule_map = {
        "Cosine (with warmup)": "cosine",
        "Linear (with warmup)": "linear",
        "Constant (with warmup)": "constant_with_warmup",
        "Cosine with Restarts (with warmup)": "cosine_with_restarts",
    }
    lr_schedule = lr_schedule_map[lr_schedule_label]

    min_lr_ratio = st.sidebar.slider(
        "Min LR Ratio (Cosine floor)",
        min_value=0.0,
        max_value=0.2,
        value=0.0,
        step=0.01,
        help="0.0 = cosine –º–æ–∂–µ—Ç —É–π—Ç–∏ –ø–æ—á—Ç–∏ –≤ 0 –∫ –∫–æ–Ω—Ü—É. –ù–∞–ø—Ä–∏–º–µ—Ä 0.05 = –Ω–µ –Ω–∏–∂–µ 5% –æ—Ç base LR."
    )
    
    warmup_steps = st.sidebar.number_input(
        "Warmup Steps",
        min_value=0,
        max_value=10000,
        value=1000
    )

    scheduler_resync_on_resume = st.sidebar.checkbox(
        "Resync LR scheduler –ø—Ä–∏ resume (—Ñ–∏–∫—Å –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤)",
        value=True,
        help=(
            "–ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –±—ã–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å–æ scheduler, –∫–æ—Ç–æ—Ä—ã–π —à–∞–≥–∞–ª –ø–æ micro-batch (–∞ –Ω–µ –ø–æ update-step), "
            "LR –ø—Ä–∏ resume –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –ø–æ—á—Ç–∏ 0. –≠—Ç–æ—Ç —Ñ–ª–∞–≥ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç scheduler –Ω–∞ global_step."
        ),
    )
    
    # –í—ã–±–æ—Ä: epochs –∏–ª–∏ max_steps
    training_mode = st.sidebar.radio(
        "–†–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏",
        ["–ü–æ —ç–ø–æ—Ö–∞–º", "–ü–æ —à–∞–≥–∞–º"],
        help="–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏"
    )
    
    if training_mode == "–ü–æ —ç–ø–æ—Ö–∞–º":
        epochs = st.sidebar.number_input(
            "Epochs",
            min_value=1,
            max_value=10,
            value=1
        )
        max_steps = None
    else:
        epochs = 1
        max_steps = st.sidebar.number_input(
            "Max Steps",
            min_value=1,
            max_value=1000000,
            value=10000,
            step=1000,
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
        )
    
    mixed_precision = st.sidebar.selectbox(
        "Mixed Precision",
        ["no", "fp16", "bf16"],
        index=2,
        help="bf16 —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Ampere+ GPU"
    )

    # Sharding mode: –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥–≤–æ–π–Ω–æ–≥–æ —à–∞—Ä–¥–∏–Ω–≥–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É resume
    st.sidebar.divider()
    st.sidebar.subheader("üß© –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
    sharding_mode = st.sidebar.selectbox(
        "Sharding mode",
        options=["auto", "dataset", "accelerate"],
        index=0,
        help=(
            "auto: –¥–ª—è streaming (IterableDataset) –≤—ã–±–∏—Ä–∞–µ–º dataset-level —à–∞—Ä–¥–∏–Ω–≥ (—Å—Ç—Ä–æ–≥–æ –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å strict resume).\n"
            "dataset: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç —Å–∞–º –¥–∞—Ç–∞—Å–µ—Ç (shard=True), DataLoader –ù–ï –≥–æ—Ç–æ–≤–∏–º —á–µ—Ä–µ–∑ accelerate.\n"
            "accelerate: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç accelerate.prepare(DataLoader); —Å—Ç—Ä–æ–≥–∏–π resume –¥–ª—è streaming –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è."
        ),
    )
    
    grad_checkpoint = st.sidebar.checkbox(
        "Gradient Checkpointing",
        value=False,
        help="–≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ"
    )
    
    max_grad_norm = st.sidebar.number_input(
        "Max Gradient Norm",
        min_value=0.0,
        max_value=10.0,
        value=1.0,
        step=0.1,
        help="Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (0 = –æ—Ç–∫–ª—é—á–∏—Ç—å)"
    )
    
    # Validation / Eval
    st.sidebar.divider()
    st.sidebar.subheader("üìä –í–∞–ª–∏–¥–∞—Ü–∏—è")
    
    val_ratio = st.sidebar.slider(
        "Validation fraction",
        min_value=0.0,
        max_value=0.2,
        value=0.01,
        step=0.005,
        help="–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ validation, –µ—Å–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π val-—Ñ–∞–π–ª –Ω–µ –∑–∞–¥–∞–Ω"
    )
    
    eval_every = st.sidebar.number_input(
        "Eval Every N Steps",
        min_value=0,
        max_value=50000,
        value=200,
        step=10,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é (0 = –æ—Ç–∫–ª—é—á–∏—Ç—å)"
    )
    
    eval_batches = st.sidebar.number_input(
        "Eval Batches",
        min_value=1,
        max_value=500,
        value=20,
        step=1,
        help="–°–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –ø—Ä–æ–≥–æ–Ω—è—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ)"
    )
    
    return {
        "batch_size": batch_size,
        "gradient_accumulation": grad_accum,
        "learning_rate": learning_rate,
        "lr_schedule": lr_schedule,
        "min_lr_ratio": min_lr_ratio,
        "warmup_steps": warmup_steps,
        "scheduler_resync_on_resume": scheduler_resync_on_resume,
        "epochs": epochs,
        "max_steps": max_steps,
        "mixed_precision": mixed_precision,
        "grad_checkpoint": grad_checkpoint,
        "max_grad_norm": max_grad_norm,
        "sharding_mode": sharding_mode,
        "val_ratio": val_ratio,
        "eval_every": eval_every,
        "eval_batches": eval_batches,
    }


def render_dataset_config(stage="pretrain"):
    """–í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Ç–æ–ª—å–∫–æ –≤—ã–±–æ—Ä —Ñ–∞–π–ª–∞)."""
    st.sidebar.header("üìÅ –î–∞—Ç–∞—Å–µ—Ç")
    
    datasets = get_available_datasets()
    
    if datasets:
        dataset_options = [f"{name} ({size})" for name, size in datasets]
        selected = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç", dataset_options)
        selected_name = selected.split(" (")[0]
        data_path = str(DATASET_DIR / selected_name)
    else:
        st.sidebar.warning("–î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ datasets/")
        data_path = st.sidebar.text_input("–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É", "datasets/data.jsonl")
    
    return {"data_path": data_path}


def render_output_config(model_name="training_run"):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞."""
    st.sidebar.header("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å: out/{model_name}
    default_dir = f"out/{model_name}"
    
    output_dir = st.sidebar.text_input(
        "Output Directory (Experiment Root)",
        value=default_dir,
        help="–ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ —ç—Ç–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
    )
    
    save_every = st.sidebar.number_input(
        "Save Checkpoint Every N Steps",
        min_value=100,
        max_value=50000,
        value=200,
        step=100,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã"
    )

    export_on_checkpoint = st.sidebar.checkbox(
        "–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å final_model –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞",
        value=True,
        help=(
            "–ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å `final_model/` –Ω–∞ –∫–∞–∂–¥–æ–º checkpoint, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –±—ã–ª–æ —Å—Ä–∞–∑—É –≥—Ä—É–∑–∏—Ç—å –≤ —á–∞—Ç. "
            "–ú–∏–Ω—É—Å: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ."
        ),
    )
    
    log_every = st.sidebar.number_input(
        "Log Every N Steps",
        min_value=1,
        max_value=1000,
        value=10,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –º–µ—Ç—Ä–∏–∫–∏"
    )
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞—Ö
    output_path = PROJECT_ROOT / output_dir
    if output_path.exists():
        checkpoints = list(output_path.rglob("checkpoint_step*"))
        final_models = list(output_path.rglob("final_model"))
        final_model = final_models[0] if final_models else None
        
        if checkpoints or final_model:
            st.sidebar.caption(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {len(checkpoints)}")
            if final_model and final_model.exists():
                st.sidebar.caption("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    return {
        "output_dir": output_dir,
        "save_every": save_every,
        "export_on_checkpoint": export_on_checkpoint,
        "log_every": log_every,
        "tokenizer_path": "gpt2"
    }


def get_available_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫)."""
    models = []
    
    # 1. –ò—â–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤ out/ (–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)
    if OUTPUT_DIR.exists():
        # –ò—â–µ–º –ª—é–±—ã–µ config.json –≤–Ω—É—Ç—Ä–∏ out/
        for config_file in OUTPUT_DIR.rglob("config.json"):
            model_dir = config_file.parent
            
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–ø–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏)
            # –ö—Ä–∏—Ç–µ—Ä–∏–π –º–æ–¥–µ–ª–∏: –Ω–∞–ª–∏—á–∏–µ config.json + (pytorch_model.bin –∏–ª–∏ model.safetensors –∏–ª–∏ adapter_model.bin)
            has_weights = (
                (model_dir / "pytorch_model.bin").exists() or 
                (model_dir / "model.safetensors").exists() or
                (model_dir / "adapter_model.bin").exists()
            )
            
            if has_weights:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø (final –∏–ª–∏ checkpoint)
                m_type = "checkpoint" if "checkpoint" in model_dir.name else "final"
                if model_dir.name == "final_model": m_type = "final"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è
                # –ë–µ—Ä–µ–º –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ OUTPUT_DIR
                rel_path = model_dir.relative_to(OUTPUT_DIR)
                models.append({
                    "name": str(rel_path),
                    "path": str(model_dir),
                    "type": m_type,
                    "time": model_dir.stat().st_mtime
                })
    
    # 2. –ò—â–µ–º –≤ models/ (—Å–∫–∞—á–∞–Ω–Ω—ã–µ —Å HuggingFace)
    if MODELS_DIR.exists():
        for model_dir in MODELS_DIR.iterdir():
            if model_dir.is_dir():
                config_file = model_dir / "config.json"
                if config_file.exists():
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤
                    has_weights = (
                        (model_dir / "pytorch_model.bin").exists() or 
                        (model_dir / "model.safetensors").exists() or
                        any(model_dir.glob("*.safetensors")) or
                        any(model_dir.glob("pytorch_model*.bin"))
                    )
                    
                    if has_weights:
                        models.append({
                            "name": f"ü§ó {model_dir.name}",
                            "path": str(model_dir),
                            "type": "hf",  # HuggingFace –º–æ–¥–µ–ª—å
                            "time": model_dir.stat().st_mtime
                        })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
    models.sort(key=lambda x: x["time"], reverse=True)
    return models


def render_distributed_config(training_config: dict | None = None):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è distributed training."""
    st.sidebar.header("üñ•Ô∏è GPU –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    gpus = get_gpu_info()
    
    if gpus:
        st.sidebar.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ GPU: {len(gpus)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ GPU
        for gpu in gpus:
            st.sidebar.markdown(f"""
            **GPU {gpu['id']}**: {gpu['name']}  
            üìä VRAM: {gpu['memory_gb']} GB | CC: {gpu['compute_capability']}
            """)
        
        # –í—ã–±–æ—Ä GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        gpu_options = [f"GPU {g['id']}: {g['name']}" for g in gpus]
        if len(gpus) > 1:
            selected_gpus = st.sidebar.multiselect(
                "–í—ã–±–µ—Ä–∏—Ç–µ GPU",
                options=gpu_options,
                default=gpu_options,
                help="–í—ã–±–µ—Ä–∏—Ç–µ GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
            )
            num_gpus = len(selected_gpus)
            gpu_ids = [gpu_options.index(g) for g in selected_gpus]
        else:
            num_gpus = 1
            gpu_ids = [0]
            st.sidebar.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è GPU")
    else:
        st.sidebar.warning("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU")
        num_gpus = 0
        gpu_ids = []
    
    st.sidebar.markdown("---")
    
    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
    st.sidebar.subheader("‚ö° –¢–∏–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–µ–∂–∏–º
    if num_gpus == 0:
        available_modes = ["default"]
        recommended_idx = 0
    elif num_gpus == 1:
        available_modes = ["default", "deepspeed_zero3_offload"]
        recommended_idx = 0
    else:
        # –ü—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º multi_gpu –∏–ª–∏ fsdp
        available_modes = ["multi_gpu", "fsdp", "deepspeed_zero2", "deepspeed_zero3", "deepspeed_zero3_offload", "default"]
        recommended_idx = 0  # multi_gpu –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–ø—Ü–∏–∏ –¥–ª—è selectbox
    mode_options = []
    for i, mode in enumerate(available_modes):
        info = PARALLEL_TYPES[mode]
        label = f"{info['icon']} {info['name']}"
        if i == recommended_idx and num_gpus > 1:
            label += " ‚≠ê"  # –û—Ç–º–µ—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π
        mode_options.append(label)
    
    selected_mode_display = st.sidebar.selectbox(
        "–†–µ–∂–∏–º",
        options=mode_options,
        index=recommended_idx,
        help="–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
    )
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
    selected_idx = mode_options.index(selected_mode_display)
    selected_mode = available_modes[selected_idx]
    mode_info = PARALLEL_TYPES[selected_mode]
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ
    st.sidebar.markdown(f"""
    <div style="background: rgba(255,255,255,0.05); padding: 10px; border-radius: 8px; margin: 10px 0;">
    <b>–¢–∏–ø:</b> {mode_info['type']}<br>
    <small>{mode_info['description']}</small>
    </div>
    """, unsafe_allow_html=True)
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω single GPU –ø—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
    if num_gpus > 1 and selected_mode == "default":
        st.sidebar.warning(f"‚ö†Ô∏è –í—ã–±—Ä–∞–Ω Single GPU, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ {num_gpus} GPU. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º Multi-GPU!")
    
    # –ö–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª
    config_file = None
    if selected_mode != "default":
        config_path = CONFIGS_DIR / f"accelerate_{selected_mode}.yaml"
        if config_path.exists():
            config_file = str(config_path)
            st.sidebar.caption(f"üìÑ –ö–æ–Ω—Ñ–∏–≥: `{config_path.name}`")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∑–∞–ø—É—Å–∫–∞
    st.sidebar.markdown("---")
    st.sidebar.subheader("üöÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–ø—É—Å–∫–∞")
    
    if num_gpus == 0:
        launch_info = "**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** CPU"
    elif selected_mode == "default":
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** GPU {gpu_ids[0] if gpu_ids else 0}"
    else:
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:** {num_gpus} √ó GPU\n**–†–µ–∂–∏–º:** {mode_info['type']}"
    
    st.sidebar.info(launch_info)

    # Compute / precision (–Ω—É–∂–Ω–æ –∏ –¥–ª—è GRPO, –ø–æ—Ç–æ–º—É —á—Ç–æ training_config –¥–ª—è GRPO –ø—É—Å—Ç–æ–π)
    st.sidebar.markdown("---")
    st.sidebar.subheader("üß† Precision & Memory")

    # –ï—Å–ª–∏ training_config –ø–µ—Ä–µ–¥–∞–Ω (SFT/Pretrain) ‚Äî –±–µ—Ä—ë–º –¥–µ—Ñ–æ–ª—Ç –∏–∑ –Ω–µ–≥–æ, –∏–Ω–∞—á–µ bf16 (GRPO –¥–µ—Ñ–æ–ª—Ç)
    default_mp = (training_config.get("mixed_precision") if training_config else None) or "bf16"
    mixed_precision = st.sidebar.selectbox(
        "Mixed Precision",
        ["no", "fp16", "bf16"],
        index=["no", "fp16", "bf16"].index(default_mp) if default_mp in ("no", "fp16", "bf16") else 2,
        help="bf16 —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Ampere+ GPU. –î–ª—è FlashAttention –Ω—É–∂–µ–Ω fp16/bf16.",
    )

    default_gc = bool(training_config.get("grad_checkpoint", False)) if training_config else False
    grad_checkpoint = st.sidebar.checkbox(
        "Gradient Checkpointing",
        value=default_gc,
        help="–≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ. –î–ª—è GRPO (–æ—Å–æ–±–µ–Ω–Ω–æ full+–¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã) —á–∞—Å—Ç–æ must-have.",
    )

    # –ü–æ—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ batch semantics (—á–∞—Å—Ç–∞—è –ø—Ä–∏—á–∏–Ω–∞ "–ø–æ—á–µ–º—É —Ç–∞–∫ –º–Ω–æ–≥–æ VRAM –≤ DDP")
    if training_config:
        try:
            micro_bsz = int(training_config.get("batch_size", 1))
            grad_accum = int(training_config.get("gradient_accumulation", 1))
            eff_per_gpu = micro_bsz * grad_accum
            global_batch = eff_per_gpu * max(1, int(num_gpus or 1))
            st.sidebar.caption(
                f"Batch semantics: **per‚ÄëGPU microbatch** = {micro_bsz}, "
                f"accum = {grad_accum} ‚Üí **effective per‚ÄëGPU** = {eff_per_gpu}, "
                f"**global** = {global_batch} (√ó{max(1, int(num_gpus or 1))} GPU)"
            )
            st.sidebar.caption("–í–∞–∂–Ω–æ: –≤ DDP `batch_size` –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ, —Ç.–µ. —ç—Ç–æ –∏–º–µ–Ω–Ω–æ per‚ÄëGPU.")
        except Exception:
            pass
    
    return {
        "distributed_mode": selected_mode,
        "num_gpus": num_gpus,
        "gpu_ids": gpu_ids,
        "config_file": config_file,
        "parallel_type": mode_info['type'],
        "mixed_precision": mixed_precision,
        "grad_checkpoint": grad_checkpoint,
    }


# Graceful fallback –¥–ª—è @st.fragment (—Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö Streamlit)
try:
    fragment = st.fragment
except (AttributeError, Exception):
    # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π Streamlit
    fragment = lambda *args, **kwargs: lambda fn: fn

@fragment(run_every=3)  # –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã
def live_metrics_fragment():
    """Fragment –¥–ª—è –∂–∏–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    if not st.session_state.current_run_id:
        st.info("–í—ã–±–µ—Ä–∏—Ç–µ run –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫")
        return
    
    run_id = st.session_state.current_run_id
    metrics = load_metrics(run_id)
    process_alive = is_process_running(run_id)
    
    # –°—Ç–∞—Ç—É—Å
    if process_alive:
        st.success(f"üü¢ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–ø—É—â–µ–Ω (Run: {run_id})")
    else:
        if metrics and metrics.get("status") == "completed":
            duration = metrics.get("training_duration", "unknown")
            st.success(f"‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration} (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        elif metrics and metrics.get("status") == "error":
            st.error(f"‚ùå –û—à–∏–±–∫–∞ (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –æ—à–∏–±–∫–µ
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        elif metrics and metrics.get("status") == "stopped":
            st.warning(f"‚èπÔ∏è –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        else:
            st.info(f"üìã –ü—Ä–æ—Å–º–æ—Ç—Ä –º–µ—Ç—Ä–∏–∫ (Run: {run_id})")
    
    if metrics:
        render_metrics_dashboard(metrics)
    else:
        st.info("–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")


def render_metrics_dashboard(metrics: dict):
    """–î–∞—à–±–æ—Ä–¥ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è."""
    
    status = metrics.get("status", "unknown")
    
    # Status indicator
    status_emoji = {
        "training": "üü¢", 
        "completed": "‚úÖ", 
        "error": "‚ùå",
        "initializing": "‚è≥",
        "loading_tokenizer": "‚è≥",
        "loading_dataset": "‚è≥",
        "building_model": "‚è≥",
        "saving_model": "üíæ",
    }.get(status, "‚è≥")
    
    st.subheader(f"{status_emoji} –°—Ç–∞—Ç—É—Å: {status.upper()}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ metrics.json (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –∏–∑ config
    model_params = None
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ metrics.json (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ)
        if "num_parameters" in metrics:
            model_params = metrics["num_parameters"]
        else:
            # Fallback: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑ config
            run_id = st.session_state.get("current_run_id", "active")
            if run_id and run_id != "active":
                run_dir = RUNS_DIR / run_id
                config_path = run_dir / "config.json"
                if config_path.exists():
                    with open(config_path) as f:
                        rc = json.load(f)
                        if "hidden_size" in rc and "num_layers" in rc:
                            vocab_size = rc.get("vocab_size", 50257)
                            intermediate_size = rc.get("intermediate_size")
                            model_params = estimate_parameters(
                                rc["hidden_size"],
                                rc["num_layers"],
                                vocab_size=vocab_size,
                                intermediate_size=intermediate_size,
                            )
    except Exception:
        pass
    
    # Metrics cards
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    
    with col1:
        current_step = metrics.get("current_step", 0)
        total_steps = metrics.get("total_steps", None)
        progress = (current_step / total_steps * 100) if isinstance(total_steps, (int, float)) and total_steps > 0 else 0
        # –∑–∞—â–∏—Ç–∞ –æ—Ç "48000%" –∏ –ø—Ä–æ—á–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        progress = max(0.0, min(float(progress), 100.0))
        planned_total = metrics.get("planned_total_steps", None)
        suffix = f"Step {current_step}/{total_steps}" if isinstance(total_steps, (int, float)) and total_steps else f"Step {current_step} (–±–µ–∑ –ª–∏–º–∏—Ç–∞)"
        if planned_total is not None and int(planned_total) != int(total_steps):
            suffix = f"{suffix} (–ø–ª–∞–Ω: {planned_total})"
        st.metric("–ü—Ä–æ–≥—Ä–µ—Å—Å", f"{progress:.1f}%", suffix)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è GRPO: –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É (—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—Ä–æ—à–ª–æ)
    if metrics.get("stage") == "grpo":
        run_id = st.session_state.get("current_run_id", None)
        dataset_total = None
        prompt_bsz = metrics.get("prompt_batch_size", None)
        group_size = metrics.get("group_size", None)
        rollout_step = metrics.get("rollout_step", metrics.get("current_rollout_step", 0))
        num_gpus = 1
        try:
            if run_id:
                run_dir = RUNS_DIR / run_id
                cfg_path = run_dir / "config.json"
                if cfg_path.exists():
                    with open(cfg_path) as f:
                        rc = json.load(f) or {}
                    # num_gpus —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ config.json –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ GRPO
                    num_gpus = int(rc.get("num_gpus", 1) or 1)
                    # dataset_size: –±–µ—Ä—ë–º –∏–∑ run-config –µ—Å–ª–∏ –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ, –∏–Ω–∞—á–µ –ø–æ–ø—Ä–æ–±—É–µ–º dataset_info.json –∏–∑ output_dir
                    dataset_total = rc.get("dataset_size", None)
                    if dataset_total is None:
                        out_dir = rc.get("output_dir")
                        if out_dir:
                            info_path = Path(out_dir) / "dataset_info.json"
                            if info_path.exists():
                                with open(info_path) as inf:
                                    dataset_total = (json.load(inf) or {}).get("dataset_size", None)
        except Exception:
            pass
        try:
            dataset_total = int(dataset_total) if dataset_total is not None else None
        except Exception:
            dataset_total = None
        try:
            prompt_bsz = int(prompt_bsz) if prompt_bsz is not None else None
        except Exception:
            prompt_bsz = None
        try:
            group_size = int(group_size) if group_size is not None else None
        except Exception:
            group_size = None
        try:
            rollout_step = int(rollout_step)
        except Exception:
            rollout_step = 0

        # prompts/completions: –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –§–ê–ö–¢ –∏–∑ –º–µ—Ç—Ä–∏–∫, –∏–Ω–∞—á–µ fallback –Ω–∞ –æ—Ü–µ–Ω–∫—É
        prompts_seen = metrics.get("prompts_generated_total", None)
        prompts_used = metrics.get("prompts_used_total", None)
        completions_seen = metrics.get("completions_generated_total", None)
        experiences_tuned = metrics.get("experiences_tuned_total", None)

        # –û—Ü–µ–Ω–∫–∞ "—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—Ä–æ—à–ª–æ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É" –∏–∑ rollout_step (–≥–ª–æ–±–∞–ª—å–Ω–æ, —Å —É—á—ë—Ç–æ–º num_gpus).
        # –≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Å–µ–º–∞–Ω—Ç–∏–∫–µ "prompts/step".
        prompts_seen_est = None
        if prompt_bsz is not None:
            prompts_seen_est = rollout_step * prompt_bsz * max(1, num_gpus)
            if prompts_seen is None or (isinstance(prompts_seen, (int, float)) and float(prompts_seen) < float(prompts_seen_est)):
                prompts_seen = prompts_seen_est
        if completions_seen is None and prompts_seen is not None and group_size is not None:
            completions_seen = prompts_seen * group_size

        if prompts_seen is not None:
            if dataset_total is not None and dataset_total > 0:
                ds_progress = max(0.0, min(100.0, prompts_seen / dataset_total * 100.0))
                st.caption(f"–ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è): **{int(prompts_seen):,}/{dataset_total:,} –ø—Ä–æ–º–ø—Ç–æ–≤** ({ds_progress:.1f}%)")
                st.progress(ds_progress / 100.0)
            else:
                st.caption(f"–ü—Ä–æ–º–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è): **{int(prompts_seen):,}**")

            if prompts_used is not None:
                st.caption(f"–ü—Ä–æ–º–ø—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ –æ–±—É—á–µ–Ω–∏–∏ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏): **{int(prompts_used):,}**")
            if completions_seen is not None:
                st.caption(f"Completion'–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: **{int(completions_seen):,}**")
            if experiences_tuned is not None:
                st.caption(f"Experience'–æ–≤ –ø—Ä–æ—Ç—é–Ω–µ–Ω–æ (–±–∞—Ç—á–µ–π –≤ train): **{int(experiences_tuned):,}**")

        # –°–∫–æ—Ä–æ—Å—Ç–∏ (prompts/s, completions/s) –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ steps/timestamps
        try:
            elapsed = float(metrics.get("elapsed_seconds", 0.0))
            if elapsed > 0 and prompts_seen is not None:
                st.caption(f"–°–∫–æ—Ä–æ—Å—Ç—å: **{(float(prompts_seen)/elapsed):.2f} –ø—Ä–æ–º–ø—Ç–æ–≤/—Å**")
            if elapsed > 0 and completions_seen is not None:
                st.caption(f"–°–∫–æ—Ä–æ—Å—Ç—å: **{(float(completions_seen)/elapsed):.2f} completion/—Å**")
            if elapsed > 0 and experiences_tuned is not None:
                st.caption(f"–°–∫–æ—Ä–æ—Å—Ç—å: **{(float(experiences_tuned)/elapsed):.2f} tuned exp/—Å**")
        except Exception:
            pass
    
    with col2:
        if metrics.get("stage") == "grpo":
            reward = metrics.get("reward", metrics.get("batch_reward_mean", 0))
            st.metric("Reward", f"{reward:.4f}")
        else:
            loss = metrics.get("current_loss", 0)
            st.metric("Train Loss", f"{loss:.4f}")
    
    with col3:
        if metrics.get("stage") == "grpo":
            kl = metrics.get("kl", 0)
            st.metric("KL Divergence", f"{kl:.4f}")
        else:
            vloss = metrics.get("current_val_loss", None)
            if vloss is None:
                st.metric("Val Loss", "‚Äî")
            else:
                st.metric("Val Loss", f"{vloss:.4f}")
    
    with col4:
        lr = metrics.get("current_lr", 0)
        st.metric("Learning Rate", f"{lr:.2e}")
    
    with col5:
        if model_params:
            st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(model_params))
        else:
            st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", "‚Äî")

    # –î–æ–ø. –ø–æ—è—Å–Ω–µ–Ω–∏—è: –ø–ª–∞–Ω vs —Ñ–∞–∫—Ç, –ø—Ä–∏—á–∏–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, LR floor
    planned_total = metrics.get("planned_total_steps", None)
    total_steps = metrics.get("total_steps", None)
    stop_reason = metrics.get("stop_reason", None)
    min_lr_ratio = metrics.get("min_lr_ratio", None)
    if planned_total is not None and total_steps is not None and int(planned_total) != int(total_steps):
        st.caption(f"–ü–ª–∞–Ω —à–∞–≥–æ–≤: {planned_total} ‚Ä¢ –§–∞–∫—Ç (–¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞/ETA): {total_steps}")
    if stop_reason:
        st.caption(f"–ü—Ä–∏—á–∏–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: `{stop_reason}`")
    if min_lr_ratio is not None and float(min_lr_ratio) > 0:
        st.caption(f"Cosine LR floor –≤–∫–ª—é—á—ë–Ω: min_lr_ratio={float(min_lr_ratio):.2f}")
    
    with col6:
        eta = metrics.get("eta_seconds", 0)
        elapsed = metrics.get("elapsed_seconds", 0)
        st.metric("–í—Ä–µ–º—è", f"{format_time(elapsed)}", delta=f"–û—Å—Ç: {format_time(eta)}", delta_color="normal")
    
    # Progress bar
    st.progress(min(progress / 100, 1.0))
    
    # Charts
    # –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–ª—é—á –ø–æ run_id, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏
    rid = st.session_state.get("current_run_id", "active") or "active"
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è –¥–ª—è GRPO
    if metrics.get("stage") == "grpo":
        st.markdown("---")
        st.subheader("üß† GRPO –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ GRPO
        col_grpo1, col_grpo2, col_grpo3, col_grpo4 = st.columns(4)
        with col_grpo1:
            reward = metrics.get("reward", metrics.get("batch_reward_mean", 0))
            st.metric("Reward", f"{reward:.4f}")
        with col_grpo2:
            kl = metrics.get("kl", 0)
            st.metric("KL Divergence", f"{kl:.4f}")
        with col_grpo3:
            grad_norm = metrics.get("grad_norm", 0)
            st.metric("Grad Norm", f"{grad_norm:.4f}")
        with col_grpo4:
            buffer_size = metrics.get("buffer_size", 0)
            st.metric("Buffer Size", f"{buffer_size}")
        
        # –ì—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è GRPO
        if metrics.get("reward_history") and len(metrics["reward_history"]) > 0:
            col1, col2 = st.columns(2)
            
            with col1:
                # Reward chart
                fig_reward = go.Figure()
                steps = metrics.get("steps_history", list(range(len(metrics["reward_history"]))))
                fig_reward.add_trace(go.Scatter(
                    x=steps,
                    y=metrics["reward_history"],
                    mode='lines',
                    name='Reward',
                    line=dict(color='#10b981', width=2)
                ))
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
                if len(metrics["reward_history"]) > 10:
                    import pandas as pd
                    df_reward = pd.DataFrame({"reward": metrics["reward_history"]})
                    df_reward["reward_smooth"] = df_reward["reward"].rolling(window=min(10, len(df_reward)//4), min_periods=1).mean()
                    fig_reward.add_trace(go.Scatter(
                        x=steps,
                        y=df_reward["reward_smooth"].tolist(),
                        mode='lines',
                        name='Reward (smooth)',
                        line=dict(color='#34d399', width=2, dash='dash')
                    ))
                fig_reward.update_layout(
                    title="Reward Curve",
                    xaxis_title="Step",
                    yaxis_title="Reward",
                    template="plotly_dark",
                    height=300,
                    margin=dict(l=0, r=0, t=40, b=0)
                )
                st.plotly_chart(fig_reward, key=f"reward_chart_{rid}")
            
            with col2:
                # Loss chart –¥–ª—è GRPO
                if metrics.get("loss_history") and len(metrics["loss_history"]) > 0:
                    fig_loss = go.Figure()
                    steps = metrics.get("steps_history", list(range(len(metrics["loss_history"]))))
                    fig_loss.add_trace(go.Scatter(
                        x=steps,
                        y=metrics["loss_history"],
                        mode='lines',
                        name='GRPO Loss',
                        line=dict(color='#e94560', width=2)
                    ))
                    if metrics.get("kl_history") and len(metrics["kl_history"]) > 0:
                        fig_loss.add_trace(go.Scatter(
                            x=steps,
                            y=metrics["kl_history"],
                            mode='lines',
                            name='KL Divergence',
                            line=dict(color='#f59e0b', width=2, dash='dash')
                        ))
                    fig_loss.update_layout(
                        title="Loss & KL Divergence",
                        xaxis_title="Step",
                        yaxis_title="Loss / KL",
                        template="plotly_dark",
                        height=300,
                        margin=dict(l=0, r=0, t=40, b=0)
                    )
                    st.plotly_chart(fig_loss, key=f"grpo_loss_chart_{rid}")
                else:
                    st.info("–û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ loss...")
        
        # –û–∫–æ—à–∫–æ —Å —Å–µ–º–ø–ª–∞–º–∏
        st.markdown("---")
        st.subheader("üìù –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–π")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–µ–º–ø–ª—ã –∏–∑ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        run_id = st.session_state.get("current_run_id")
        if run_id and run_id != "active":
            run_dir = RUNS_DIR / run_id
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ samples.jsonl –≤ run_dir –∏–ª–∏ –≤ output_dir
            samples_file = None
            config_path = run_dir / "config.json"
            if config_path.exists():
                try:
                    with open(config_path) as f:
                        config = json.load(f)
                        output_dir = config.get("output_dir", "")
                        if output_dir:
                            samples_file = Path(output_dir) / "samples.jsonl"
                except:
                    pass
            
            # Fallback: –∏—â–µ–º –≤ run_dir
            if not samples_file or not samples_file.exists():
                samples_file = run_dir / "samples.jsonl"
            
            samples_data = []
            if samples_file.exists():
                try:
                    with open(samples_file, "r", encoding="utf-8") as f:
                        for line in f:
                            if line.strip():
                                samples_data.append(json.loads(line))
                except Exception as e:
                    st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–µ–º–ø–ª—ã: {e}")
            
            if samples_data:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–µ–º–ø–ª–æ–≤
                num_samples = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–º–ø–ª–æ–≤", 1, min(10, len(samples_data)), 3)
                recent_samples = samples_data[-num_samples:]
                
                for idx, sample in enumerate(reversed(recent_samples)):
                    with st.expander(f"–°–µ–º–ø–ª {len(samples_data) - idx} (Step {sample.get('step', '?')})", expanded=(idx == 0)):
                        prompt = sample.get("prompt", "")
                        reference = sample.get("reference_answer", "")
                        completions = sample.get("completions", [])
                        rewards = sample.get("rewards", [])
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç+–æ—Ç–≤–µ—Ç –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–µ–º–ø–ª–∞ (—á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —á—Ç–æ –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç)
                        if idx == 0 and completions:
                            st.markdown("**üîç –ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç + –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (—á—Ç–æ –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å):**")
                            st.caption("–≠—Ç–æ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ—Ç–æ—Ä—ã–π –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –æ —Ç–µ–≥–∞—Ö")
                            
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º full_texts –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º
                            full_texts = sample.get("full_texts", [])
                            if not full_texts:
                                full_texts = [prompt + comp for comp in completions]
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç (—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º reward)
                            if rewards:
                                best_completion_idx = max(range(len(rewards)), key=lambda i: rewards[i])
                                best_reward = rewards[best_completion_idx]
                                best_full_text = full_texts[best_completion_idx] if best_completion_idx < len(full_texts) else prompt + completions[best_completion_idx]
                                
                                # –í—ã–¥–µ–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                                system_prompt_start = best_full_text.find("system") if "system" in best_full_text.lower() else -1
                                if system_prompt_start == -1:
                                    # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
                                    for marker in ["<|im_start|>", "A conversation", "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ"]:
                                        if marker in best_full_text:
                                            system_prompt_start = best_full_text.find(marker)
                                            break
                                
                                st.code(best_full_text, language=None)
                                st.caption(f"‚úÖ –õ—É—á—à–∏–π –æ—Ç–≤–µ—Ç (reward={best_reward:.4f})")
                                
                                # –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ö—É–¥—à–∏–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                                worst_completion_idx = min(range(len(rewards)), key=lambda i: rewards[i])
                                worst_reward = rewards[worst_completion_idx]
                                if worst_reward < best_reward:
                                    with st.expander(f"üìâ –•—É–¥—à–∏–π –æ—Ç–≤–µ—Ç (reward={worst_reward:.4f})", expanded=False):
                                        worst_full_text = full_texts[worst_completion_idx] if worst_completion_idx < len(full_texts) else prompt + completions[worst_completion_idx]
                                        st.code(worst_full_text, language=None)
                                        st.caption("–°—Ä–∞–≤–Ω–∏—Ç–µ —Å –ª—É—á—à–∏–º –æ—Ç–≤–µ—Ç–æ–º –≤—ã—à–µ - –≤–∏–¥–Ω–æ –ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ —Ç–µ–≥–∞—Ö –≤ –ø—Ä–æ–º–ø—Ç–µ?")
                            else:
                                # –ï—Å–ª–∏ –Ω–µ—Ç rewards, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π
                                st.code(full_texts[0] if full_texts else prompt + completions[0], language=None)
                            
                            st.markdown("---")
                        
                        # –¢–µ–∫—É—â–µ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –ø—Ä–æ–º–ø—Ç –∏ –æ—Ç–≤–µ—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                        col_s1, col_s2 = st.columns([1, 1])
                        
                        with col_s1:
                            st.markdown("**üì• –ü—Ä–æ–º–ø—Ç:**")
                            st.code(prompt, language=None)
                        
                        with col_s2:
                            st.markdown("**‚úÖ –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:**")
                            st.code(reference, language=None)
                        
                        st.markdown("**ü§ñ –û—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏:**")
                        
                        if completions:
                            for i, (completion, reward) in enumerate(zip(completions, rewards)):
                                reward_color = "üü¢" if reward > 0.5 else "üü°" if reward > 0 else "üî¥"
                                with st.container():
                                    st.markdown(f"{reward_color} **–û—Ç–≤–µ—Ç {i+1}** (Reward: {reward:.4f})")
                                    st.code(completion[:500] + ("..." if len(completion) > 500 else ""), language=None)
                        else:
                            st.info("–û–∂–∏–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π...")
            else:
                st.info("–°–µ–º–ø–ª—ã –±—É–¥—É—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –∑–¥–µ—Å—å –ø–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª `samples.jsonl` –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ run.")
        
        st.markdown("---")
    
    # –û–±—ã—á–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç–∞–¥–∏–π
    elif metrics.get("loss_history"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Loss chart
            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["loss_history"],
                mode='lines',
                name='Train Loss',
                line=dict(color='#e94560', width=2)
            ))
            if metrics.get("val_loss_history"):
                fig_loss.add_trace(go.Scatter(
                    x=metrics["val_steps_history"],
                    y=metrics["val_loss_history"],
                    mode='lines',
                    name='Val Loss',
                    line=dict(width=2, dash="dash", color='#60a5fa')
                ))
            fig_loss.update_layout(
                title="Training & Validation Loss",
                xaxis_title="Step",
                yaxis_title="Loss",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_loss, key=f"loss_chart_{rid}")
        
        with col2:
            # LR chart
            fig_lr = go.Figure()
            fig_lr.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["lr_history"],
                mode='lines',
                name='LR',
                line=dict(color='#60a5fa', width=2)
            ))
            fig_lr.update_layout(
                title="Learning Rate Schedule",
                xaxis_title="Step",
                yaxis_title="LR",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_lr, key=f"lr_chart_{rid}")
    
    # Checkpoints
    if metrics.get("checkpoints"):
        with st.expander("üì¶ Checkpoints"):
            for ckpt in metrics["checkpoints"]:
                ckpt_loss = ckpt.get("loss")
                if ckpt_loss is not None:
                    st.text(f"Step {ckpt['step']}: Loss {ckpt_loss:.4f} | {ckpt['path']}")
                else:
                    st.text(f"Step {ckpt['step']}: {ckpt['path']}")
    
    # –ü—Ä–∏–º–µ—Ä —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–¥–ª—è SFT)
    sample_prompt = metrics.get("sample_prompt")
    if sample_prompt:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –Ω–∞–ª–∏—á–∏—é stage –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
        stage = metrics.get("stage", "pretrain")
        if stage == "sft":
            title = "üìù –ü—Ä–∏–º–µ—Ä —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (SFT)"
            caption = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:"
            tip = "üí° –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç–µ–≥–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ"
        else:
            title = "üìù –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (Pretrain)"
            caption = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:"
            tip = "üí° –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –≤ —Ç–µ–∫—Å—Ç–µ"
        
        with st.expander(title, expanded=True):
            st.caption(caption)
            st.code(sample_prompt, language=None)
            st.caption(tip)
    
    # GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    gpu_stats = metrics.get("gpu_stats", [])
    if gpu_stats:
        st.subheader("üñ•Ô∏è –ù–∞–≥—Ä—É–∑–∫–∞ GPU")
        
        cols = st.columns(len(gpu_stats))
        for i, (col, gpu) in enumerate(zip(cols, gpu_stats)):
            with col:
                st.markdown(f"**GPU {gpu['id']}**")
                
                # Memory bar
                mem_percent = gpu.get('memory_percent', 0)
                st.progress(min(mem_percent / 100, 1.0), text=f"VRAM: {gpu['memory_used_gb']:.1f} / {gpu['memory_total_gb']:.1f} GB ({mem_percent:.0f}%)")
                
                # Utilization
                util = gpu.get('utilization')
                if util is not None:
                    st.progress(min(util / 100, 1.0), text=f"–ó–∞–≥—Ä—É–∑–∫–∞: {util}%")
                else:
                    st.caption("–ó–∞–≥—Ä—É–∑–∫–∞: N/A")
    
    # Error
    if metrics.get("error"):
        st.error("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
        with st.expander("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏ (Traceback)", expanded=True):
            st.code(metrics['error'], language="python")
    
    # –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
    if st.session_state.current_run_id:
        run_dir = RUNS_DIR / st.session_state.current_run_id
        stderr_path = run_dir / "stderr.log"
        stdout_path = run_dir / "stdout.log"
        
        with st.expander("üìã –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.caption("stdout (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stdout_path.exists():
                    with open(stdout_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)
            
            with col2:
                st.caption("stderr (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stderr_path.exists():
                    with open(stderr_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)



def download_hf_dataset(repo_id, subset, split, limit_type, limit_val, limit_bytes, save_path, filters=None):
    """–§—É–Ω–∫—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    try:
        status_text = f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: {repo_id}..."
        st.toast(status_text)
        print(status_text)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã stream=True —á—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –≤—Å–µ –≤ –ø–∞–º—è—Ç—å
        # –î–ª—è –º–Ω–æ–≥–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ "default" –º–æ–∂–µ—Ç –ª–æ–º–∞—Ç—å load_dataset - –ø–µ—Ä–µ–¥–∞—ë–º None
        subset_arg = None if (not subset or subset.strip() == "" or subset.lower() == "default") else subset
        
        ds = load_dataset(repo_id, subset_arg, split=split, streaming=True)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
        save_path = DATASET_DIR / save_path
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        count = 0
        current_bytes = 0
        
        with open(save_path, "w", encoding="utf-8") as f:
            for item in ds:
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                if filters:
                    # –§–∏–ª—å—Ç—Ä –ø–æ score
                    if "score_col" in filters and "min_score" in filters:
                         col = filters["score_col"]
                         min_s = filters["min_score"]
                         # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Ç–∏–ø–∞
                         if col in item and item[col] is not None:
                             try:
                                 val = float(item[col])
                                 if val < min_s:
                                     continue
                             except ValueError:
                                 pass
                    
                    # –§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É
                    if "lang_col" in filters and "target_lang" in filters:
                         col = filters["lang_col"]
                         target = filters["target_lang"]
                         if col in item and item[col] is not None:
                             val = str(item[col])
                             if target.lower() not in val.lower():
                                 continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSONL
                line = json.dumps(item, ensure_ascii=False) + "\n"
                line_bytes = len(line.encode('utf-8'))
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)" and count >= limit_val:
                    break
                if limit_type == "–ì–ë (–†–∞–∑–º–µ—Ä)" and (current_bytes + line_bytes) > limit_bytes:
                    break
                    
                f.write(line)
                count += 1
                current_bytes += line_bytes
                
                if count % 1000 == 0:
                    print(f"Downloaded {count} lines, {current_bytes / 1024**2:.2f} MB")

        st.success(f"–ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {count} —Å—Ç—Ä–æ–∫ ({current_bytes / 1024**2:.2f} MB) –≤ {save_path}")
        return True

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        print(traceback.format_exc())
        return False


def render_data_manager():
    """–í–∫–ª–∞–¥–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏."""
    st.header("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
    
    col_upload, col_list = st.columns([1, 2])
    
    with col_upload:
        # –°–µ–∫—Ü–∏—è 1: –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        with st.expander("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤", expanded=False):
            uploaded_files = st.file_uploader(
                "–ü–µ—Ä–µ—Ç–∞—â–∏—Ç–µ —Ñ–∞–π–ª—ã —Å—é–¥–∞", 
                type=["jsonl", "txt"],  # –í–ê–ñ–ù–û: –Ω–µ –≤–∫–ª—é—á–∞–µ–º .json, —Ç.–∫. —ç—Ç–æ –æ–±—ã—á–Ω–æ –º–∞—Å—Å–∏–≤, –∞ –Ω–µ JSONL 
                accept_multiple_files=True
            )
            
            if uploaded_files:
                if st.button("üì• –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª—ã"):
                    for uploaded_file in uploaded_files:
                        save_path = DATASET_DIR / uploaded_file.name
                        with open(save_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        st.toast(f"–§–∞–π–ª {uploaded_file.name} —Å–æ—Ö—Ä–∞–Ω—ë–Ω!", icon="‚úÖ")
                    time.sleep(1)
                    st.rerun()

        # –°–µ–∫—Ü–∏—è 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å HuggingFace
        st.subheader("ü§ó –°–∫–∞—á–∞—Ç—å —Å HuggingFace")
        # –°–ª–æ–≤–∞—Ä—å –ø—Ä–µ—Å–µ—Ç–æ–≤: {–Ω–∞–∑–≤–∞–Ω–∏–µ: (repo_id, subset, split)}
        presets = {
            # Pretrain –¥–∞—Ç–∞—Å–µ—Ç—ã
            "üü¢ Pretrain: FineWeb-2 (Russian)": ("HuggingFaceFW/fineweb-2", "rus_Cyrl", "train"),
            "üü¢ Pretrain: FineWeb-Edu (Educational)": ("HuggingFaceFW/fineweb-edu", "default", "train"),
            "üü¢ Pretrain: Wikitext-103": ("wikitext", "wikitext-103-v1", "train"),
            # SFT –¥–∞—Ç–∞—Å–µ—Ç—ã
            "üîµ SFT: OpenOrca-ru": ("d0rj/OpenOrca-ru", "default", "train"),
            "üîµ SFT: ru-instruct": ("d0rj/ru-instruct", "default", "train"),
            "üîµ SFT: GrandMaster-PRO-MAX": ("Vikhrmodels/GrandMaster-PRO-MAX", "default", "train"),
            # Reasoning –¥–∞—Ç–∞—Å–µ—Ç—ã (–¥–ª—è GRPO)
            "üß† Reasoning: GSM8K (English)": ("gsm8k", "main", "train"),
            "üß† Reasoning: GSM8K-RU (–†—É—Å—Å–∫–∏–π)": ("d0rj/gsm8k-ru", "default", "train"),
            "üß† Reasoning: MATH-RU (–æ–ª–∏–º–ø–∏–∞–¥—ã)": ("d0rj/competition_math_ru", "default", "train"),
            "üß† Reasoning: MGSM (–º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π)": ("juletxara/mgsm", "ru", "train"),
            "üß† Reasoning: ARC-Challenge": ("allenai/ai2_arc", "ARC-Challenge", "train"),
            # –†—É—á–Ω–æ–π –≤–≤–æ–¥
            "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None, None),
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (FineWeb-2 Russian)
        if "hf_repo_id_input" not in st.session_state:
            st.session_state.hf_repo_id_input = "HuggingFaceFW/fineweb-2"
        if "hf_subset_default" not in st.session_state:
            st.session_state.hf_subset_default = "rus_Cyrl"
        if "hf_split_default" not in st.session_state:
            st.session_state.hf_split_default = "train"
        
        # –ü—Ä–µ–¥–∑–∞–ø–æ–ª–Ω—è–µ–º –∫—ç—à –¥–ª—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –ø—Ä–µ—Å–µ—Ç–∞ (FineWeb-2)
        # —á—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–≥ —Å—Ä–∞–∑—É —Å–∫–∞—á–∏–≤–∞—Ç—å –±–µ–∑ –Ω–∞–∂–∞—Ç–∏—è "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å"
        if "ds_repo_info" not in st.session_state:
            st.session_state.ds_repo_info = {}
        
        default_repo = "HuggingFaceFW/fineweb-2"
        if default_repo not in st.session_state.ds_repo_info:
            st.session_state.ds_repo_info[default_repo] = {
                "configs": ["rus_Cyrl"],  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —è–∑—ã–∫
                "splits": ["train", "test"],  # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ splits
                "features": {},  # –ó–∞–ø–æ–ª–Ω–∏—Ç—Å—è –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ
                "selected_config": "rus_Cyrl"
            }
        
        def on_preset_change():
            """Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ–ª–µ–π –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ø—Ä–µ—Å–µ—Ç–∞."""
            sel = st.session_state.dataset_preset_selector
            preset_data = presets.get(sel)
            if preset_data and preset_data[0]:
                st.session_state.hf_repo_id_input = preset_data[0]
                st.session_state.hf_subset_default = preset_data[1]
                st.session_state.hf_split_default = preset_data[2]
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º selectbox —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã
                if "hf_split_select" in st.session_state:
                    del st.session_state.hf_split_select
                if "hf_subset_select" in st.session_state:
                    del st.session_state.hf_subset_select

        # –°–µ–ª–µ–∫—Ç–æ—Ä –ø—Ä–µ—Å–µ—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é FineWeb-2 Russian)
        st.selectbox(
            "üìö –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã",
            options=list(presets.keys()),
            index=0,  # FineWeb-2 Russian –ø–µ—Ä–≤—ã–π
            key="dataset_preset_selector",
            on_change=on_preset_change,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –≥–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî –≤—Å–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω—è—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )

        repo_id = st.text_input("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (ID)", key="hf_repo_id_input")
        
        # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π"):
            try:
                with st.spinner(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {repo_id}..."):
                    # 1. –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥–∏
                    configs = get_dataset_config_names(repo_id)
                    
                    # 2. –í—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è splits/features
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π (rus_Cyrl) > –ø–µ—Ä–≤—ã–π –≤ —Å–ø–∏—Å–∫–µ
                    default_subset = st.session_state.hf_subset_default
                    if default_subset in configs:
                        selected_config = default_subset
                    else:
                        selected_config = configs[0] if configs else None
                    
                    splits = []
                    features_info = {}
                    
                    if selected_config:
                        splits = get_dataset_split_names(repo_id, selected_config)
                        # 3. –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (features)
                        try:
                            ds_builder = load_dataset_builder(repo_id, selected_config)
                            if ds_builder.info.features:
                                features_info = ds_builder.info.features
                        except Exception as e:
                            print(f"Could not load features: {e}")

                    st.session_state.ds_repo_info[repo_id] = {
                        "configs": configs,
                        "splits": splits,
                        "features": features_info,
                        "selected_config": selected_config  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –¥–ª—è –∫–∞–∫–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞ splits
                    }
                    
                    # –í–ê–ñ–ù–û: –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—ã–±–æ—Ä –≤–∏–¥–∂–µ—Ç–æ–≤ —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if "hf_split_select" in st.session_state:
                        del st.session_state.hf_split_select
                    if "hf_subset_select" in st.session_state:
                        del st.session_state.hf_subset_select
                    
                    st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(configs)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π, splits: {splits}")
            except Exception as e:
                st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: {e}")

        # –†–∞–±–æ—Ç–∞–µ–º —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        repo_info = st.session_state.ds_repo_info.get(repo_id, {})
        available_configs = repo_info.get("configs", [])
        available_splits = repo_info.get("splits", [])
        features = repo_info.get("features", {})
        
        if available_configs:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π subset, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –≤ —Å–ø–∏—Å–∫–µ
            default_idx = 0
            if st.session_state.hf_subset_default in available_configs:
                default_idx = available_configs.index(st.session_state.hf_subset_default)
            subset = st.selectbox("Subset (–∫–æ–Ω—Ñ–∏–≥)", available_configs, index=default_idx, key="hf_subset_select")
        else:
            subset = st.text_input("Subset (–∫–æ–Ω—Ñ–∏–≥)", st.session_state.hf_subset_default, key="hf_subset_input")
        
        if available_splits:
            default_idx = 0
            if st.session_state.hf_split_default in available_splits:
                default_idx = available_splits.index(st.session_state.hf_split_default)
            split = st.selectbox("Split", available_splits, index=default_idx, key="hf_split_select")
        else:
            split = st.text_input("Split", st.session_state.hf_split_default, key="hf_split_input")

        # --- –£–ú–ù–´–ï –§–ò–õ–¨–¢–†–´ ---
        with st.expander("üõ†Ô∏è –§–∏–ª—å—Ç—Ä—ã –∏ –õ–∏–º–∏—Ç—ã", expanded=True):
            # –õ–∏–º–∏—Ç—ã (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã)
            col_lim1, col_lim2 = st.columns(2)
            with col_lim1:
                limit_type = st.radio("–¢–∏–ø –ª–∏–º–∏—Ç–∞", ["–ì–ë (–†–∞–∑–º–µ—Ä)", "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)"], key="limit_type")
            
            with col_lim2:
                limit_val = 0
                limit_bytes = 0
                
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)":
                    limit_val = st.number_input("–ö–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫", value=100000, step=10000, key="limit_val")
                else:
                    limit_gb = st.number_input("–†–∞–∑–º–µ—Ä (–ì–ë)", value=2.0, step=0.5, min_value=0.1, key="limit_gb")
                    limit_bytes = int(limit_gb * 1024**3)
            
            st.divider()
            
            # –§–∏–ª—å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (features)
            active_filters = {}
            
            if features:
                st.caption("üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏ –∏—Ö —Ç–∏–ø–æ–≤
                # features —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å {col_name: feature_info}
                # feature_info –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π 'Value(dtype='string')' –∏–ª–∏ –æ–±—ä–µ–∫—Ç–æ–º
                
                # 1. –§–∏–ª—å—Ç—Ä —á–∏—Å–ª–æ–≤–æ–π (Score/Quality)
                float_cols = []
                string_cols = []
                
                for col_name, feature_def in features.items():
                    # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø
                    dtype = getattr(feature_def, 'dtype', str(feature_def))
                    if 'float' in str(dtype):
                        float_cols.append(col_name)
                    elif 'string' in str(dtype):
                        string_cols.append(col_name)
                
                col_f1, col_f2 = st.columns(2)
                
                with col_f1:
                    if float_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é (float)**")
                        selected_float_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + float_cols, key="sel_float_col")
                        if selected_float_col != "(–Ω–µ—Ç)":
                            min_val = st.slider(f"–ú–∏–Ω. –∑–Ω–∞—á–µ–Ω–∏–µ {selected_float_col}", 0.0, 1.0, 0.0, key="val_float_col")
                            active_filters["score_col"] = selected_float_col
                            active_filters["min_score"] = min_val
                    else:
                        st.caption("–ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

                with col_f2:
                    if string_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–∫—Å—Ç—É (contains)**")
                        selected_str_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + string_cols, key="sel_str_col")
                        if selected_str_col != "(–Ω–µ—Ç)":
                            target_str = st.text_input(f"–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:", key="val_str_col")
                            if target_str:
                                active_filters["lang_col"] = selected_str_col
                                active_filters["target_lang"] = target_str
                    else:
                        st.caption("–¢–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            else:
                st.info("‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ –ª–∏–º–∏—Ç—ã –ø–æ –æ–±—ä–µ–º—É.")


        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ repo_id
        # –ù–∞–ø—Ä–∏–º–µ—Ä: "HuggingFaceFW/fineweb-2" ‚Üí "fineweb-2.jsonl"
        # "d0rj/gsm8k-ru" ‚Üí "gsm8k-ru.jsonl"
        def compute_default_filename():
            """–í—ã—á–∏—Å–ª—è–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π repo_id –∏ subset."""
            computed_name = "dataset.jsonl"
            if repo_id:
                # –ë–µ—Ä—ë–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ "/" (–∏–ª–∏ –≤—Å—ë –µ—Å–ª–∏ –Ω–µ—Ç "/")
                name_part = repo_id.split("/")[-1] if "/" in repo_id else repo_id
                # –î–æ–±–∞–≤–ª—è–µ–º subset –µ—Å–ª–∏ –æ–Ω –Ω–µ default
                current_subset = st.session_state.get('hf_subset_select') or st.session_state.get('hf_subset_input') or st.session_state.get('hf_subset_default', '')
                if current_subset and current_subset not in ('default', 'main', ''):
                    name_part = f"{name_part}-{current_subset}"
                computed_name = f"{name_part}.jsonl"
            return computed_name
        
        computed_filename = compute_default_filename()
        
        # –°–æ–∑–¥–∞—ë–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ repo_id –∏ subset
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ—Ç key –¥–ª—è –≤–∏–¥–∂–µ—Ç–∞, —á—Ç–æ–±—ã –æ–Ω –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞–ª—Å—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ repo_id/subset
        current_subset = st.session_state.get('hf_subset_select') or st.session_state.get('hf_subset_input') or st.session_state.get('hf_subset_default', '')
        repo_subset_key = f"{repo_id}::{current_subset}"
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ session_state (—É–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã)
        normalized_key = repo_subset_key.replace('/', '_').replace(':', '_').replace('-', '_')
        widget_key = f"save_filename_{normalized_key}"
        
        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π –∫–ª—é—á (repo_id/subset –∏–∑–º–µ–Ω–∏–ª–∏—Å—å), –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–µ –∏–º—è
        if widget_key not in st.session_state:
            st.session_state[widget_key] = computed_filename
        
        save_filename = st.text_input(
            "–ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", 
            value=st.session_state.get(widget_key, computed_filename), 
            key=widget_key,
            help="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏ subset. –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é."
        )
        
        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º callback
        def on_download_click(active_filters_map):
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ session_state —è–≤–Ω–æ
            r_id = st.session_state.get('hf_repo_id_input')
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º subset –∏ split
            if st.session_state.get('hf_subset_select'):
                sub = st.session_state.get('hf_subset_select')
            else:
                sub = st.session_state.get('hf_subset_input')
                
            if st.session_state.get('hf_split_select'):
                spl = st.session_state.get('hf_split_select')
            else:
                spl = st.session_state.get('hf_split_input')
                
            l_type = st.session_state.get('limit_type')
            l_val = st.session_state.get('limit_val', 0) or 0
            
            l_gb = st.session_state.get('limit_gb', 2.0)
            l_bytes = int(l_gb * 1024**3)

            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—Ç –∂–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π key
            current_subset_for_key = sub or st.session_state.get('hf_subset_default', '')
            repo_subset_key_for_download = f"{r_id}::{current_subset_for_key}"
            normalized_key_for_download = repo_subset_key_for_download.replace('/', '_').replace(':', '_').replace('-', '_')
            widget_key_for_download = f"save_filename_{normalized_key_for_download}"
            s_path = st.session_state.get(widget_key_for_download, "dataset.jsonl")
            
            # –ü–µ—Ä–µ–¥–∞—ë–º —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞–∫ –µ—Å—Ç—å (active_filters_map —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏)
            filters_to_pass = active_filters_map or None
            
            download_hf_dataset(r_id, sub, spl, l_type, l_val, l_bytes, s_path, filters=filters_to_pass)

        st.button("–°–∫–∞—á–∞—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å", on_click=on_download_click, args=(active_filters,))
    
    with col_list:
        st.subheader("–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã")
        
        datasets = []
        if DATASET_DIR.exists():
            # JSONL / JSON
            for f in list(DATASET_DIR.glob("*.jsonl")) + list(DATASET_DIR.glob("*.json")):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "JSONL/JSON",
                    "size_mb": size_mb,
                    "path": f
                })
            # TXT
            for f in DATASET_DIR.glob("*.txt"):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "Text",
                    "size_mb": size_mb,
                    "path": f
                })
        
        if not datasets:
            st.info("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã —Å–ª–µ–≤–∞.")
        else:
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–ø–∏—Å–æ–∫
            for ds in datasets:
                with st.expander(f"üìÑ {ds['name']} ({ds['size_mb']:.1f} MB)"):
                    st.caption(f"–¢–∏–ø: {ds['type']}")
                    
                    # Preview
                    try:
                        with open(ds['path'], "r", encoding="utf-8") as f:
                            head = []
                            for i, line in enumerate(f):
                                if i >= 5:
                                    break
                                head.append(line.strip())
                        st.markdown("**Preview (–ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫):**")
                        if head:
                            st.code("\n".join(head), language="json" if "JSON" in ds['type'] else "text")
                        else:
                            st.info("–§–∞–π–ª –ø—É—Å—Ç")
                        
                        col_del, col_info = st.columns([1, 4])
                        with col_del:
                            if st.button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"del_{ds['name']}"):
                                ds['path'].unlink()
                                st.toast(f"–§–∞–π–ª {ds['name']} —É–¥–∞–ª—ë–Ω", icon="üóëÔ∏è")
                                time.sleep(1)
                                st.rerun()
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")


def download_hf_model(repo_id: str, save_name: str, revision: str = "main"):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å HuggingFace –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ –≤ MODELS_DIR.
    """
    from huggingface_hub import snapshot_download
    
    save_path = MODELS_DIR / save_name
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ
    if save_path.exists():
        st.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å `{save_name}` —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        return False
    
    try:
        with st.spinner(f"‚è≥ –°–∫–∞—á–∏–≤–∞–µ–º {repo_id}..."):
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar = st.progress(0, text="–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
            status_text = st.empty()
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            status_text.text(f"–°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∏–∑ {repo_id}...")
            progress_bar.progress(10, text="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤...")
            
            # snapshot_download —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å—é –º–æ–¥–µ–ª—å
            local_path = snapshot_download(
                repo_id=repo_id,
                revision=revision,
                local_dir=str(save_path),
                local_dir_use_symlinks=False,  # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã, –Ω–µ —Å–∏–º–ª–∏–Ω–∫–∏
                ignore_patterns=["*.md", "*.txt", "*.gitattributes", ".git*"],  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–Ω—É–∂–Ω–æ–µ
            )
            
            progress_bar.progress(90, text="–ü—Ä–æ–≤–µ—Ä–∫–∞...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–∫–∞—á–∞–ª–æ—Å—å
            config_file = save_path / "config.json"
            if not config_file.exists():
                st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω config.json –≤ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
                return False
            
            # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            import json
            with open(config_file) as f:
                model_config = json.load(f)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_type = model_config.get("model_type", "unknown")
            hidden_size = model_config.get("hidden_size", "?")
            num_layers = model_config.get("num_hidden_layers", model_config.get("n_layer", "?"))
            vocab_size = model_config.get("vocab_size", "?")
            
            progress_bar.progress(100, text="–ì–æ—Ç–æ–≤–æ!")
            status_text.empty()
            
            st.success(f"""‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞!
- **–ü—É—Ç—å:** `{save_path}`
- **–¢–∏–ø:** {model_type}
- **Hidden:** {hidden_size}, **Layers:** {num_layers}, **Vocab:** {vocab_size}
""")
            return True
            
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        import traceback
        print(traceback.format_exc())
        # –£–¥–∞–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω–æ–µ
        if save_path.exists():
            import shutil
            shutil.rmtree(save_path, ignore_errors=True)
        return False


def render_model_manager():
    """–í–∫–ª–∞–¥–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ (—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å HuggingFace)."""
    st.header("ü§ñ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏")
    
    col_download, col_list = st.columns([1, 2])
    
    with col_download:
        st.subheader("ü§ó –°–∫–∞—á–∞—Ç—å —Å HuggingFace")
        
        # –ü—Ä–µ—Å–µ—Ç—ã –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è continual pretraining / SFT
        model_presets = {
            "üî• SmolLM2-135M (135M params)": ("HuggingFaceTB/SmolLM2-135M", "SmolLM2-135M"),
            "üî• SmolLM2-360M (360M params)": ("HuggingFaceTB/SmolLM2-360M", "SmolLM2-360M"),
            "üî• SmolLM2-1.7B (1.7B params)": ("HuggingFaceTB/SmolLM2-1.7B", "SmolLM2-1.7B"),
            "ü¶ô TinyLlama-1.1B (1.1B params)": ("TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T", "TinyLlama-1.1B"),
            "üêç Pythia-70M (70M params)": ("EleutherAI/pythia-70m", "Pythia-70M"),
            "üêç Pythia-160M (160M params)": ("EleutherAI/pythia-160m", "Pythia-160M"),
            "üêç Pythia-410M (410M params)": ("EleutherAI/pythia-410m", "Pythia-410M"),
            "üêç Pythia-1B (1B params)": ("EleutherAI/pythia-1b", "Pythia-1B"),
            "ü§ñ GPT-2 Small (124M params)": ("openai-community/gpt2", "GPT2-Small"),
            "ü§ñ GPT-2 Medium (355M params)": ("openai-community/gpt2-medium", "GPT2-Medium"),
            "ü¶ä Qwen2.5-0.5B (0.5B params)": ("Qwen/Qwen2.5-0.5B", "Qwen2.5-0.5B"),
            "ü¶ä Qwen2.5-1.5B (1.5B params)": ("Qwen/Qwen2.5-1.5B", "Qwen2.5-1.5B"),
            "üá∑üá∫ ruGPT3-Small (125M, Russian)": ("ai-forever/rugpt3small_based_on_gpt2", "ruGPT3-Small"),
            "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None),
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        if "model_repo_id" not in st.session_state:
            st.session_state.model_repo_id = "HuggingFaceTB/SmolLM2-135M"
        if "model_save_name" not in st.session_state:
            st.session_state.model_save_name = "SmolLM2-135M"
        
        def on_model_preset_change():
            sel = st.session_state.model_preset_selector
            preset_data = model_presets.get(sel)
            if preset_data and preset_data[0]:
                st.session_state.model_repo_id = preset_data[0]
                st.session_state.model_save_name = preset_data[1]
        
        st.selectbox(
            "üìö –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏",
            options=list(model_presets.keys()),
            index=0,
            key="model_preset_selector",
            on_change=on_model_preset_change,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å ‚Äî repo_id –∑–∞–ø–æ–ª–Ω–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )
        
        repo_id = st.text_input("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (ID)", key="model_repo_id")
        save_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", key="model_save_name", 
                                   help="–ü–∞–ø–∫–∞ –≤ models/, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        st.markdown("""
<div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); 
            padding: 12px; border-radius: 8px; margin: 10px 0;
            border: 1px solid #0f3460; color: #e8e8e8;">
<b style="color: #4fc3f7;">üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b><br>
‚Ä¢ <b style="color: #81d4fa;">SmolLM2</b> ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç HuggingFace<br>
‚Ä¢ <b style="color: #81d4fa;">Pythia</b> ‚Äî –æ—Ç–ª–∏—á–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã<br>
‚Ä¢ <b style="color: #81d4fa;">TinyLlama</b> ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω–∞—è, —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 3T —Ç–æ–∫–µ–Ω–æ–≤<br>
‚Ä¢ <b style="color: #81d4fa;">Qwen2.5</b> ‚Äî —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç Alibaba
</div>
""", unsafe_allow_html=True)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        size_estimates = {
            "70m": "~150 MB", "135m": "~300 MB", "160m": "~350 MB",
            "360m": "~800 MB", "410m": "~900 MB", "0.5b": "~1 GB",
            "1b": "~2 GB", "1.1b": "~2.5 GB", "1.5b": "~3 GB", "1.7b": "~3.5 GB",
            "124m": "~500 MB", "355m": "~1.5 GB", "125m": "~500 MB",
        }
        
        estimated_size = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        repo_lower = repo_id.lower()
        for size_key, size_val in size_estimates.items():
            if size_key in repo_lower:
                estimated_size = size_val
                break
        
        st.caption(f"üì¶ –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä: **{estimated_size}**")
        
        if st.button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
            if not repo_id or not save_name:
                st.error("–£–∫–∞–∂–∏—Ç–µ repo_id –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ!")
            else:
                success = download_hf_model(repo_id, save_name)
                if success:
                    time.sleep(1)
                    st.rerun()
    
    with col_list:
        st.subheader("üìÅ –°–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        
        models = []
        if MODELS_DIR.exists():
            for model_dir in MODELS_DIR.iterdir():
                if model_dir.is_dir():
                    config_path = model_dir / "config.json"
                    if config_path.exists():
                        try:
                            import json
                            with open(config_path) as f:
                                cfg = json.load(f)
                            
                            # –†–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏
                            total_size = sum(
                                f.stat().st_size for f in model_dir.rglob("*") if f.is_file()
                            )
                            size_gb = total_size / (1024**3)
                            
                            models.append({
                                "name": model_dir.name,
                                "path": model_dir,
                                "model_type": cfg.get("model_type", "unknown"),
                                "hidden_size": cfg.get("hidden_size", "?"),
                                "num_layers": cfg.get("num_hidden_layers", cfg.get("n_layer", "?")),
                                "vocab_size": cfg.get("vocab_size", "?"),
                                "size_gb": size_gb,
                            })
                        except Exception:
                            models.append({
                                "name": model_dir.name,
                                "path": model_dir,
                                "model_type": "?",
                                "hidden_size": "?",
                                "num_layers": "?",
                                "vocab_size": "?",
                                "size_gb": 0,
                            })
        
        if not models:
            st.info("–ù–µ—Ç —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å —Å–ª–µ–≤–∞ –¥–ª—è Continual Pretraining –∏–ª–∏ SFT.")
        else:
            for m in sorted(models, key=lambda x: x["name"]):
                with st.expander(f"ü§ñ {m['name']} ({m['size_gb']:.2f} GB)"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"""
- **–¢–∏–ø:** `{m['model_type']}`
- **Hidden Size:** {m['hidden_size']}
- **Layers:** {m['num_layers']}
- **Vocab:** {m['vocab_size']}
""")
                    with col2:
                        st.caption(f"üìÇ –ü—É—Ç—å: `{m['path']}`")
                        
                        # –ö–Ω–æ–ø–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                        if st.button("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", key=f"use_{m['name']}", 
                                     help="–í—ã–±—Ä–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –¥–ª—è Continual Pretrain / SFT"):
                            st.session_state.selected_base_model = str(m['path'])
                            st.toast(f"–ú–æ–¥–µ–ª—å {m['name']} –≤—ã–±—Ä–∞–Ω–∞! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –ó–∞–ø—É—Å–∫ –∏ –≤—ã–±–µ—Ä–∏—Ç–µ Continual Pretrain –∏–ª–∏ SFT.", icon="‚úÖ")
                        
                        # –ö–Ω–æ–ø–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è
                        if st.button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"del_model_{m['name']}"):
                            import shutil
                            shutil.rmtree(m['path'])
                            st.toast(f"–ú–æ–¥–µ–ª—å {m['name']} —É–¥–∞–ª–µ–Ω–∞", icon="üóëÔ∏è")
                            time.sleep(1)
                            st.rerun()
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞
        st.markdown("---")
        st.info("""
üí° **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∫–∞—á–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å:**
1. –ù–∞–∂–º–∏—Ç–µ **üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å** –Ω–∞ –Ω—É–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üöÄ –ó–∞–ø—É—Å–∫**
3. –í —Å–∞–π–¥–±–∞—Ä–µ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º **Continual Pretrain** –∏–ª–∏ **SFT**
4. –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—Å—è –∫–∞–∫ –±–∞–∑–æ–≤–∞—è
""")


def _bytes_to_gb(x: int) -> float:
    return float(x) / (1024**3)


def _sum_tensor_bytes(obj) -> int:
    """
    –°—á–∏—Ç–∞–µ—Ç –±–∞–π—Ç—ã —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ CUDA –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Ç–µ–Ω–∑–æ—Ä/—Å–ø–∏—Å–æ–∫/–∫–æ—Ä—Ç–µ–∂/—Å–ª–æ–≤–∞—Ä—å).
    """
    import torch

    total = 0
    if obj is None:
        return 0
    if torch.is_tensor(obj):
        if obj.is_cuda:
            return int(obj.numel() * obj.element_size())
        return 0
    if isinstance(obj, dict):
        for v in obj.values():
            total += _sum_tensor_bytes(v)
        return total
    if isinstance(obj, (list, tuple)):
        for v in obj:
            total += _sum_tensor_bytes(v)
        return total
    return 0


def _estimate_memory_footprint(config, batch_size, distributed_mode="default", num_gpus=1):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ VRAM –¥–ª—è –ª—é–±–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π.
    """
    from homellm.models.memory_estimator import estimate_memory_footprint
    return estimate_memory_footprint(config, batch_size, distributed_mode, num_gpus)


def _profile_memory_footprint_cuda(config, batch_size: int):
    """
    –¢–æ—á–Ω—ã–π (–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω–æ) –∑–∞–º–µ—Ä –ø–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º CUDA –∞–ª–ª–æ–∫–∞—Ü–∏—è–º:
    - –¥–µ–ª–∞–µ–º warmup —à–∞–≥, —á—Ç–æ–±—ã AdamW –ø—Ä–æ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª state
    - –∑–∞—Ç–µ–º –º–µ—Ä—è–µ–º peak allocated/reserved –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º breakdown: Model+Optim (steady), Act (peak - steady), Buf (reserved - peak).
    """
    import torch

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

    # –°–æ–±–∏—Ä–∞–µ–º HomeForCausalLM (–¥–ª—è Blueprint —Ä–µ–∂–∏–º–∞ –ª—É—á—à–µ –¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ; –ø–æ–∫–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é Home –º–æ–¥–µ–ª—å)
    from homellm.models.home_model import HomeConfig, HomeForCausalLM

    vocab_size = int(config.get("vocab_size", 50257))
    hidden_size = int(config["hidden_size"])
    num_layers = int(config["num_layers"])
    n_heads = int(config["n_heads"])
    seq_len = int(config["seq_len"])

    mp = (config.get("mixed_precision") or "no").lower()
    if mp == "bf16":
        dtype = torch.bfloat16
    elif mp == "fp16":
        dtype = torch.float16
    else:
        dtype = torch.float32

    model_cfg = HomeConfig(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_layers,
        num_attention_heads=n_heads,
        max_position_embeddings=seq_len,
        dropout=float(config.get("dropout", 0.0)),
    )

    device = torch.device("cuda:0")
    torch.cuda.set_device(device)

    model = HomeForCausalLM(model_cfg).to(device=device, dtype=dtype)
    model.train()
    if config.get("grad_checkpoint", False) and hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()

    opt_name = (config.get("optimizer") or "adamw").lower()
    lr = float(config.get("lr", 1e-3))
    wd = float(config.get("weight_decay", 0.01))
    if opt_name == "adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    else:
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    def run_step():
        input_ids = torch.randint(0, vocab_size, (int(batch_size), seq_len), device=device)
        labels = input_ids.clone()
        out = model(input_ids=input_ids, labels=labels, use_cache=False)
        loss = out.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

    # Warmup: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è optimizer state + –ø—Ä–æ–≥—Ä–µ–≤ allocator'–∞
    torch.cuda.synchronize()
    run_step()
    torch.cuda.synchronize()

    # –ò–∑–º–µ—Ä–µ–Ω–∏–µ
    torch.cuda.reset_peak_memory_stats(device)
    alloc_before = torch.cuda.memory_allocated(device)
    reserved_before = torch.cuda.memory_reserved(device)

    run_step()
    torch.cuda.synchronize()

    peak_alloc = torch.cuda.max_memory_allocated(device)
    peak_reserved = torch.cuda.max_memory_reserved(device)
    alloc_after = torch.cuda.memory_allocated(device)

    # Tensor-based breakdown (–ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç "—á—Ç–æ –∏–º–µ–Ω–Ω–æ –∂–∏–≤—ë—Ç", –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç caching allocator)
    weights_bytes = 0
    for p in model.parameters():
        if p.is_cuda:
            weights_bytes += int(p.numel() * p.element_size())
    for b in model.buffers():
        if torch.is_tensor(b) and b.is_cuda:
            weights_bytes += int(b.numel() * b.element_size())

    opt_state_bytes = 0
    for st in optimizer.state.values():
        opt_state_bytes += _sum_tensor_bytes(st)

    # –ü–æ—Å–ª–µ zero_grad(set_to_none=True) –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –¥–µ—Ä–∂–∞—Ç—å –ø–∞–º—è—Ç—å
    grads_bytes = 0
    for p in model.parameters():
        if p.grad is not None and p.grad.is_cuda:
            grads_bytes += int(p.grad.numel() * p.grad.element_size())

    steady_alloc = alloc_after
    act_alloc = max(0, int(peak_alloc - steady_alloc))
    buf_alloc = max(0, int(peak_reserved - peak_alloc))

    total_peak = peak_reserved  # —Å–∞–º—ã–π —á–µ—Å—Ç–Ω—ã–π "—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—Ä–æ—Å–∏–ª —É –¥—Ä–∞–π–≤–µ—Ä–∞" –Ω–∞ –ø–∏–∫–µ —à–∞–≥–∞

    return {
        "method": "profile_cuda",
        "total_gb": round(_bytes_to_gb(total_peak), 2),
        "model_gb": round(_bytes_to_gb(steady_alloc), 2),
        "act_gb": round(_bytes_to_gb(act_alloc), 2),
        "buf_gb": round(_bytes_to_gb(buf_alloc), 2),
        "params": int(sum(p.numel() for p in model.parameters())),
        "detail": {
            "alloc_before_gb": round(_bytes_to_gb(alloc_before), 3),
            "reserved_before_gb": round(_bytes_to_gb(reserved_before), 3),
            "peak_alloc_gb": round(_bytes_to_gb(peak_alloc), 3),
            "peak_reserved_gb": round(_bytes_to_gb(peak_reserved), 3),
            "alloc_after_gb": round(_bytes_to_gb(alloc_after), 3),
            "tensor_weights_gb": round(_bytes_to_gb(weights_bytes), 3),
            "tensor_opt_state_gb": round(_bytes_to_gb(opt_state_bytes), 3),
            "tensor_grads_gb": round(_bytes_to_gb(grads_bytes), 3),
        },
        "notes": "–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ CUDA: warmup + –∏–∑–º–µ—Ä–µ–Ω–∏–µ peak. 'Buf' = caching allocator (reserved - peak allocated).",
    }


def calculate_memory_footprint(config, batch_size, distributed_mode="default", num_gpus=1, *, method: str = "estimate"):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫—É/–∑–∞–º–µ—Ä VRAM –¥–ª—è –ø—Ä–µ–≤—å—é.
    method:
      - 'estimate': –±—ã—Å—Ç—Ä–∞—è —Ñ–æ—Ä–º—É–ª–∞ (fallback)
      - 'profile_cuda': —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–º–µ—Ä –Ω–∞ —Ç–µ–∫—É—â–µ–π GPU (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ/–º–æ–∂–µ—Ç OOM)
    """
    try:
        if method == "profile_cuda":
            return _profile_memory_footprint_cuda(config, batch_size=int(batch_size))
        return _estimate_memory_footprint(config, batch_size=int(batch_size), distributed_mode=distributed_mode, num_gpus=num_gpus)
    except Exception as e:
        print(f"Error calculating VRAM ({method}): {e}")
        # –§–æ–ª–±—ç–∫ –Ω–∞ –æ—Ü–µ–Ω–∫—É
        out = _estimate_memory_footprint(config, batch_size=int(batch_size), distributed_mode=distributed_mode, num_gpus=num_gpus)
        out["notes"] = f"{out.get('notes','')} | profile error: {e}"
        return out


def render_model_preview(config: dict, distributed_config: dict = None):
    """–ü—Ä–µ–≤—å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞."""
    st.subheader("üìê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏")
    
    stage = config.get("stage", "pretrain")
    if stage == "sft":
        st.info(f"üîÑ **–†–µ–∂–∏–º SFT** (Fine-Tuning)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    elif stage == "continual_pretrain":
        st.info(f"üîÑ **–†–µ–∂–∏–º Continual Pretraining** (–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    elif stage == "grpo":
        st.info(f"üß† **–†–µ–∂–∏–º GRPO** (RL –¥–ª—è Reasoning)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    else:
        st.success("üèóÔ∏è **–†–µ–∂–∏–º Pretraining** (–° –Ω—É–ª—è)")

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å
    # –ù–∞–º –Ω—É–∂–µ–Ω batch_size –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (—ç—Ç–æ –±–∞—Ç—á –Ω–∞ –¥–µ–≤–∞–π—Å)
    batch_size = config.get("batch_size", 1)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º num_gpus –∏ distributed_mode –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    dist_mode = "default"
    n_gpus = 1
    if distributed_config:
        dist_mode = distributed_config.get("distributed_mode", "default")
        n_gpus = distributed_config.get("num_gpus", 1)
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º config –Ω–∞–ø—Ä—è–º—É—é (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ distributed_config –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)
    if config.get("num_gpus"):
        n_gpus = int(config.get("num_gpus", 1))
    if config.get("distributed_mode"):
        dist_mode = config.get("distributed_mode", "default")

    mem_method = "estimate"
    # if torch.cuda.is_available():
    #     with st.expander("üß† –ü–∞–º—è—Ç—å GPU: –æ—Ü–µ–Ω–∫–∞ vs —Ç–æ—á–Ω—ã–π –∑–∞–º–µ—Ä", expanded=False):
    #         st.caption("–û—Ü–µ–Ω–∫–∞ ‚Äî –º–≥–Ω–æ–≤–µ–Ω–Ω–æ, –Ω–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ. –¢–æ—á–Ω—ã–π –∑–∞–º–µ—Ä ‚Äî –∑–∞–ø—É—Å–∫–∞–µ—Ç 2 train-step –Ω–∞ GPU (warmup + –∏–∑–º–µ—Ä–µ–Ω–∏–µ) –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º/–º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –ø–æ OOM.")
    #         do_profile = st.checkbox("–°–¥–µ–ª–∞—Ç—å —Ç–æ—á–Ω—ã–π –∑–∞–º–µ—Ä –Ω–∞ CUDA (2 —à–∞–≥–∞)", value=False, key="profile_vram_cuda")
    #         if do_profile:
    #             mem_method = "profile_cuda"

    mem_info = calculate_memory_footprint(config, batch_size, dist_mode, n_gpus, method=mem_method)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Hidden Size", config["hidden_size"])
        st.metric("Layers", config["num_layers"])
    
    with col2:
        st.metric("Attention Heads", config["n_heads"])
        st.metric("Head Dim", config["hidden_size"] // config["n_heads"])
    
    with col3:
        st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(mem_info["params"]))
        
        # –¶–≤–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ –¥–ª—è 24GB –∫–∞—Ä—Ç—ã)
        val = mem_info["total_gb"]
        color = "normal"
        if val > 24: color = "off" # –∫—Ä–∞—Å–Ω—ã–π –æ—Ç—Ç–µ–Ω–æ–∫ –≤ –¥–µ–ª—å—Ç–µ –æ–±—ã—á–Ω–æ
        
        title = "VRAM (Profile)" if mem_info.get("method") == "profile_cuda" else "VRAM (Estimate)"
        st.metric(
            title,
            f"{val:.1f} GB",
            delta=f"M: {mem_info['model_gb']} + A: {mem_info['act_gb']} GB",
            delta_color=color,
            help=(mem_info.get("notes") or "M: Model+Optim (steady)\nA: Activations/temporaries (peak - steady)\nBuf: caching allocator")
        )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
    if mem_info["total_gb"] > 0:
        st.caption("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU:")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –±–∞—Ä —á–∞—Ä—Ç —á–µ—Ä–µ–∑ HTML/CSS –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
        total = mem_info["total_gb"]
        p_model = (mem_info["model_gb"] / total) * 100
        p_act = (mem_info["act_gb"] / total) * 100
        buf_gb = float(mem_info.get("buf_gb", 0.0))
        p_buff = (buf_gb / total) * 100 if total > 0 else 0
        
        st.markdown(f"""
        <div style="display: flex; height: 20px; width: 100%; background: #333; border-radius: 4px; overflow: hidden; margin-top: 5px;">
            <div style="width: {p_model}%; background: #3b82f6; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Model & Optim">Model</div>
            <div style="width: {p_act}%; background: #e94560; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Activations">Act</div>
            <div style="width: {p_buff}%; background: #777; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Buffer">Buf</div>
        </div>
        <div style="display: flex; justify-content: space-between; font-size: 12px; color: #888; margin-top: 2px;">
            <span>Model + Optim: {mem_info['model_gb']} GB</span>
            <span>Activations: {mem_info['act_gb']} GB</span>
        </div>
        """, unsafe_allow_html=True)

        if mem_info.get("notes"):
            st.caption(mem_info["notes"])

        if mem_info.get("method") == "profile_cuda" and isinstance(mem_info.get("detail"), dict):
            with st.expander("üîç –î–µ—Ç–∞–ª–∏ –∑–∞–º–µ—Ä–∞ (CUDA allocator / tensor sums)", expanded=False):
                st.json(mem_info["detail"])

        if mem_info["act_gb"] > mem_info["model_gb"] * 2:
            st.warning("‚ö†Ô∏è –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –∑–∞–Ω–∏–º–∞—é—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏! –í–∫–ª—é—á–∏—Ç–µ Gradient Checkpointing –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ Batch Size.")

    
    # –í–∏–∑—É–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    st.markdown(f"""
    <div class="model-ascii">
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         HomeForCausalLM             ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ  Embedding: 50257 ‚Üí {config['hidden_size']:4d}           ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇ  ‚îÇ HomeBlock √ó {config['num_layers']:2d}              ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí Attention      ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ {config['n_heads']:2d} heads √ó {config['hidden_size']//config['n_heads']:3d} dim      ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí FFN (SwiGLU)   ‚îÇ    ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
    ‚îÇ  RMSNorm ‚Üí LM Head                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    </div>
        """, unsafe_allow_html=True)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–µ

    if distributed_config:
        st.subheader("‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º")
        
        mode = distributed_config.get("distributed_mode", "default")
        mode_info = PARALLEL_TYPES.get(mode, PARALLEL_TYPES["default"])
        num_gpus = distributed_config.get("num_gpus", 0)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("–†–µ–∂–∏–º", mode_info["name"])
        
        with col2:
            st.metric("–¢–∏–ø", mode_info["type"])
        
        with col3:
            if num_gpus > 0:
                st.metric("GPU", f"{num_gpus} —à—Ç.")
            else:
                st.metric("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "CPU")
        
        # –°—Ö–µ–º–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        if mode == "default":
            parallel_diagram = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Single Device      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Full Model      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Optimizer  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Gradients  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
        elif mode == "multi_gpu":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Data Parallel ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (copy)  ‚îÇ  ‚îÇ (copy)  ‚îÇ       ‚îÇ (copy)  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Sync Gradients ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ö–∞–∂–¥–∞—è GPU: –ø–æ–ª–Ω–∞—è –∫–æ–ø–∏—è –º–æ–¥–µ–ª–∏, —á–∞—Å—Ç—å –±–∞—Ç—á–∞
"""
        elif mode == "fsdp":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FSDP (Fully Sharded) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Shard 0 ‚îÇ  ‚îÇ Shard 1 ‚îÇ       ‚îÇ Shard N ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Params  ‚îÇ  ‚îÇ Params  ‚îÇ       ‚îÇ Params  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ All-Gather for Forward ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ Reduce-Scatter Backward ‚îÄ‚îò       ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ú–æ–¥–µ–ª—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –º–µ–∂–¥—É GPU (—ç–∫–æ–Ω–æ–º–∏—è VRAM)
"""
        elif "deepspeed" in mode:
            if "zero3" in mode:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Params  ‚îÇ  ‚îÇ Params  ‚îÇ       ‚îÇ Params  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim   ‚îÇ  ‚îÇ Optim   ‚îÇ       ‚îÇ Optim   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  {'+ CPU Offload (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ CPU)' if 'offload' in mode else ''}          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–í—Å—ë —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–æ: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è VRAM
"""
            else:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Full    ‚îÇ  ‚îÇ Full    ‚îÇ       ‚îÇ Full    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim/N ‚îÇ  ‚îÇ Optim/N ‚îÇ       ‚îÇ Optim/N ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω—ã
"""
        else:
            parallel_diagram = ""
        
        if parallel_diagram:
            st.markdown(f"""
<div class="model-ascii">
{parallel_diagram}
</div>
            """, unsafe_allow_html=True)


def export_model_to_hf(model, tokenizer, source_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π HF —Ñ–æ—Ä–º–∞—Ç.
    
    –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LoRA/QLoRA, –º–µ—Ä–¥–∂–∏—Ç –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É,
    —á—Ç–æ–±—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±—ã–ª–∞ "–≥–æ—Ç–æ–≤–æ–π" –∏ –∑–∞–≥—Ä—É–∂–∞–ª–∞—Å—å –∫–∞–∫ –æ–±—ã—á–Ω–∞—è.
    """
    try:
        from peft import PeftModel
        
        source = Path(source_path)
        # –°–æ–∑–¥–∞–µ–º –∏–º—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: export_TIMESTAMP
        export_name = f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –ï—Å–ª–∏ —ç—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä—è–¥–æ–º —Å –Ω–∏–º, –Ω–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
        # out/model/run/checkpoint_X -> out/model/run/export_X
        if "checkpoint" in source.name:
            export_dir = source.parent / f"export_{source.name}"
        else:
            export_dir = source.parent / export_name
            
        export_dir.mkdir(parents=True, exist_ok=True)
        
        # –ï—Å–ª–∏ —ç—Ç–æ PEFT-–º–æ–¥–µ–ª—å (LoRA/QLoRA) ‚Äî –º–µ—Ä–¥–∂–∏–º –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É
        export_model = model
        try:
            if isinstance(model, PeftModel):
                logger.info("Merging LoRA adapter into base model for export...")
                export_model = model.merge_and_unload()
                logger.info("LoRA adapter merged successfully")
        except Exception as e:
            logger.warning(f"LoRA merge failed during export, saving as-is: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        export_model.save_pretrained(export_dir, safe_serialization=True)
        tokenizer.save_pretrained(export_dir)
        
        return str(export_dir)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
        return None


# ============================================================================
# Main App
# ============================================================================

def main():
    render_header()
    
    # Sidebar configs
    model_config = render_model_config()
    st.session_state.current_model_name = model_config.get("model_name_input", "home_model")
    
    current_stage = model_config.get("stage", "pretrain")
    
    # GRPO –∏–º–µ–µ—Ç —Å–≤–æ—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è
    if current_stage == "grpo":
        grpo_sidebar_config = render_grpo_sidebar_config()
        # –î–ª—è GRPO –Ω–µ –Ω—É–∂–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        training_config = {}
        # –í–ê–ñ–ù–û: –î–ª—è GRPO —Ç–æ–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º distributed config –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ multi-GPU
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π training_config –¥–ª—è render_distributed_config
        dummy_training_config = {
            # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —á–µ—Å—Ç–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É –≤ GPU —Ç–∞–±–µ: microbatch = train_batch_size
            "batch_size": grpo_sidebar_config.get("grpo_train_batch_size", 2),
            "gradient_accumulation": grpo_sidebar_config.get("gradient_accumulation", 4),
        }
        distributed_config = render_distributed_config(training_config=dummy_training_config)
    else:
        grpo_sidebar_config = {}
        training_config = render_training_config()
        distributed_config = render_distributed_config(training_config=training_config)
    
    # –ü–µ—Ä–µ–¥–∞–µ–º stage –≤ dataset_config
    # –î–ª—è GRPO –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ main area
    if current_stage != "grpo":
        dataset_config = render_dataset_config(stage=current_stage)
    else:
        dataset_config = {}
    
    output_config = render_output_config(st.session_state.current_model_name)
    
    # Merge configs
    # –í–ê–ñ–ù–û: grpo_sidebar_config –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –í–°–ï –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: grpo_sidebar_config > model_config (–¥–ª—è GRPO)
    full_config = {**model_config, **training_config, **dataset_config, **output_config, **grpo_sidebar_config}
    
    # –î–ª—è GRPO: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
    if current_stage == "grpo":
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã (–µ—Å–ª–∏ use_lora=True)
        # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ model_config (–∏–∑ render_model_config())
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º use_lora –∏–∑ tuning_method (lora/qlora = use_lora=True)
        tuning_method = model_config.get("tuning_method", "full")
        use_lora_from_model = tuning_method in ("lora", "qlora")
        
        # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥ lora/qlora, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
        if use_lora_from_model:
            if "lora_r" not in model_config or model_config.get("lora_r") is None:
                raise ValueError(
                    "‚ùå –í—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥ 'lora' –∏–ª–∏ 'qlora', –Ω–æ lora_r –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! "
                    "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤ —Å–µ–∫—Ü–∏–∏ 'üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞' —É–∫–∞–∑–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä 'LoRA r'."
                )
            if "lora_alpha" not in model_config or model_config.get("lora_alpha") is None:
                raise ValueError(
                    "‚ùå –í—ã–±—Ä–∞–Ω –º–µ—Ç–æ–¥ 'lora' –∏–ª–∏ 'qlora', –Ω–æ lora_alpha –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! "
                    "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤ —Å–µ–∫—Ü–∏–∏ 'üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞' —É–∫–∞–∑–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä 'LoRA alpha'."
                )
            # –ö–æ–ø–∏—Ä—É–µ–º LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ model_config –≤ full_config –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ train_gsm8k.py
            full_config["use_lora"] = True
            full_config["lora_r"] = model_config["lora_r"]
            full_config["lora_alpha"] = model_config["lora_alpha"]
            full_config["lora_dropout"] = model_config.get("lora_dropout")
            full_config["lora_target_modules"] = model_config.get("lora_target_modules")
            # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è qlora
            if tuning_method == "qlora":
                full_config["use_4bit"] = True  # QLoRA –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4-bit
                full_config["use_8bit"] = model_config.get("use_8bit", False)
            else:
                full_config["use_4bit"] = False
                full_config["use_8bit"] = False
        else:
            # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ full, use_lora=False
            full_config["use_lora"] = False
            full_config["use_4bit"] = False
            full_config["use_8bit"] = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
        required_grpo_params = [
            "grpo_algorithm", "grpo_group_size", "grpo_max_new_tokens",
            "grpo_temperature", "grpo_learning_rate", "grpo_kl_weight",
            "grpo_clip_eps_low"
        ]
        missing_params = [p for p in required_grpo_params if p not in full_config]
        if missing_params:
            raise ValueError(
                f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ GRPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_params}. "
                f"–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ render_grpo_sidebar_config() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã."
            )
    full_config["distributed_mode"] = distributed_config["distributed_mode"]
    full_config["num_gpus"] = distributed_config["num_gpus"]
    full_config["config_file"] = distributed_config["config_file"]
    full_config["gpu_ids"] = distributed_config.get("gpu_ids", [])
    # –í–ê–ñ–ù–û: –¥–ª—è GRPO training_config –ø—É—Å—Ç–æ–π, –ø–æ—ç—Ç–æ–º—É mixed_precision/grad_checkpoint –±–µ—Ä—ë–º –∏–∑ distributed_config.
    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç–∞–¥–∏–π —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ –ø—Ä–∏—Ö–æ–¥—è—Ç –∏–∑ training_config ‚Äî –Ω–µ –ø–µ—Ä–µ—Ç–∏—Ä–∞–µ–º –∏—Ö.
    if "mixed_precision" not in full_config:
        full_config["mixed_precision"] = distributed_config.get("mixed_precision", "bf16")
    if "grad_checkpoint" not in full_config:
        full_config["grad_checkpoint"] = distributed_config.get("grad_checkpoint", False)
    
    # –î–ª—è SFT, Continual Pretrain –∏ GRPO –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    if model_config.get("stage") in ("sft", "continual_pretrain", "grpo") and model_config.get("base_model_path"):
        full_config["tokenizer_path"] = model_config["base_model_path"]
    
    # Main content
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(["üöÄ –ó–∞–ø—É—Å–∫", "üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "üí¨ –ß–∞—Ç", "üìú –ò—Å—Ç–æ—Ä–∏—è", "üíæ –î–∞–Ω–Ω—ã–µ", "ü§ñ –ú–æ–¥–µ–ª–∏", "üìö –£—á–µ–±–Ω–∏–∫"])
    
    with tab1:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # –ü–µ—Ä–µ–¥–∞–µ–º full_config, —á—Ç–æ–±—ã –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –ø–∞–º—è—Ç–∏ –≤–∏–¥–µ–ª batch_size –∏ grad_checkpoint
            render_model_preview(full_config, distributed_config)
            
            # SFT Config (Main Area)
            if model_config.get("stage") == "sft" and dataset_config.get("data_path"):
                st.markdown("---")
                # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é (–¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∞, –≤—ã–∑–æ–≤–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è)
                sft_cfg = render_sft_main_config(dataset_config["data_path"])
                full_config.update(sft_cfg)
            
            # GRPO Config (Main Area) - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ reward —Ñ—É–Ω–∫—Ü–∏–π –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            if model_config.get("stage") == "grpo":
                st.markdown("---")
                grpo_main_cfg = render_grpo_main_config(dataset_config.get("data_path"))
                full_config.update(grpo_main_cfg)
            
            st.subheader("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
            st.json(full_config)
        
        with col2:
            st.subheader("üéÆ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
            
            if st.session_state.training_active:
                if st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", type="primary"):
                    with st.spinner("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É..."):
                        stopped = stop_training()
                    if stopped:
                        st.success("‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    else:
                        st.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å (–≤–æ–∑–º–æ–∂–Ω–æ —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞)")
                    time.sleep(1)
                    st.rerun()
            else:
                # –î–ª—è GRPO –¥—Ä—É–≥–∞—è –∫–Ω–æ–ø–∫–∞ –∏ –ª–æ–≥–∏–∫–∞
                if model_config.get("stage") == "grpo":
                    if st.button("üß† –ù–∞—á–∞—Ç—å GRPO –æ–±—É—á–µ–Ω–∏–µ", type="primary"):
                        with st.spinner("–ó–∞–ø—É—Å–∫ GRPO..."):
                            run_id, process = start_grpo_training(full_config)
                            st.session_state.current_run_id = run_id
                            st.session_state.training_process = process
                            st.session_state.training_active = True
                            
                            save_active_run(run_id, full_config)
                            
                            st.success(f"GRPO –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ! Run ID: {run_id}")
                            time.sleep(1)
                            st.rerun()
                else:
                    if st.button("‚ñ∂Ô∏è –ù–∞—á–∞—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É", type="primary"):
                        with st.spinner("–ó–∞–ø—É—Å–∫..."):
                            run_id, process = start_training(full_config)
                            st.session_state.current_run_id = run_id
                            st.session_state.training_process = process
                            st.session_state.training_active = True
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–π run –¥–ª—è persistence
                            save_active_run(run_id, full_config)
                            
                            st.success(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞! Run ID: {run_id}")
                            time.sleep(1)
                            st.rerun()
    
    with tab2:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fragment –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        live_metrics_fragment()
    
    with tab4:
        st.header("üìú –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—É—Å–∫–æ–≤")
        st.markdown("---")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —Ç–∏–ø–∞ active_run.json)
        runs = sorted([p for p in RUNS_DIR.iterdir() if p.is_dir()], reverse=True)
        
        if runs:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø—É—Å–∫–æ–≤
            for run_dir in runs[:30]: 
                run_id = run_dir.name
                metrics = load_metrics(run_id)
                
                if metrics:
                    status = metrics.get("status", "unknown")
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–∞–ø—É—Å–∫–∏ (–µ—Å–ª–∏ –æ–Ω–∏ —Å—Ç–∞—Ä—ã–µ –∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–¥–µ–ª–∞–ª–∏)
                    is_empty = metrics.get("current_step", 0) == 0 and not metrics.get("checkpoints")
                    if is_empty and status not in ("training", "running"):
                         continue

                    status_emoji = {"training": "üü¢", "completed": "‚úÖ", "error": "‚ùå", "stopped": "‚èπÔ∏è", "resumed": "‚ñ∂Ô∏è"}.get(status, "‚è≥")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    model_name_display = run_id
                    try:
                        config_path = run_dir / "config.json"
                        if config_path.exists():
                            with open(config_path) as f:
                                rc = json.load(f)
                                # –ò—â–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ output_dir
                                if "model_name_input" in rc:
                                    model_name_display = f"{run_id} | {rc['model_name_input']}"
                                elif "output_dir" in rc:
                                    out_d = Path(rc["output_dir"])
                                    # out/home_pretrain/run_id -> home_pretrain
                                    if out_d.name == run_id:
                                        model_name_display = f"{run_id} | {out_d.parent.name}"
                                    else:
                                        model_name_display = f"{run_id} | {out_d.name}"
                    except:
                        pass
                    
                    with st.expander(f"{status_emoji} {model_name_display}"):
                        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ metrics.json (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –∏–∑ config
                        model_params = None
                        try:
                            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ metrics.json (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ)
                            if "num_parameters" in metrics:
                                model_params = metrics["num_parameters"]
                            else:
                                # Fallback: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑ config
                                config_path = run_dir / "config.json"
                                if config_path.exists():
                                    with open(config_path) as f:
                                        rc = json.load(f)
                                        if "hidden_size" in rc and "num_layers" in rc:
                                            vocab_size = rc.get("vocab_size", 50257)
                                            intermediate_size = rc.get("intermediate_size")
                                            model_params = estimate_parameters(
                                                rc["hidden_size"],
                                                rc["num_layers"],
                                                vocab_size=vocab_size,
                                                intermediate_size=intermediate_size,
                                            )
                        except Exception:
                            pass
                        
                        col1, col2, col3, col4, col5 = st.columns(5)
                        with col1:
                            st.metric("Steps", metrics.get("current_step", 0))
                        with col2:
                            st.metric("Final Loss", f"{metrics.get('current_loss', 0):.4f}")
                        with col3:
                            if model_params:
                                st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(model_params))
                            else:
                                st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", "‚Äî")
                        with col4:
                            st.metric("Status", status)
                        with col5:
                            st.metric("Duration", metrics.get("training_duration", "-"))
                        
                        # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                        checkpoints = metrics.get("checkpoints", [])
                        if checkpoints:
                            st.markdown("**üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã:**")
                            # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è loss –∏–∑ history, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
                            loss_history = metrics.get("loss_history", [])
                            steps_history = metrics.get("steps_history", [])
                            
                            for ckpt in checkpoints:
                                ckpt_loss = ckpt.get("loss")
                                # –ï—Å–ª–∏ loss –Ω–µ—Ç –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ loss_history
                                if ckpt_loss is None and steps_history and loss_history:
                                    ckpt_step = ckpt.get("step")
                                    if ckpt_step in steps_history:
                                        idx = steps_history.index(ckpt_step)
                                        if idx < len(loss_history):
                                            ckpt_loss = loss_history[idx]
                                
                                if ckpt_loss is not None:
                                    st.caption(f"Step {ckpt['step']}: Loss {ckpt_loss:.4f} | {ckpt['path']}")
                                else:
                                    st.caption(f"Step {ckpt['step']}: {ckpt['path']}")
                        
                        # –ö–Ω–æ–ø–∫–∏
                        btn_col1, btn_col2, btn_col3 = st.columns(3)
                        with btn_col1:
                            if st.button(f"üìä –ú–µ—Ç—Ä–∏–∫–∏", key=f"metrics_{run_id}"):
                                st.session_state.current_run_id = run_id
                                st.toast(f"‚úÖ –í—ã–±—Ä–∞–Ω run: {run_id}. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", icon="üìä")
                        with btn_col2:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞
                            config_path = run_dir / "config.json"
                            if config_path.exists():
                                try:
                                    with open(config_path) as f:
                                        run_config = json.load(f)
                                    model_dir = PROJECT_ROOT / run_config.get("output_dir", "")
                                    final_model = model_dir / "final_model"
                                    if final_model.exists():
                                        if st.button("üí¨ –ß–∞—Ç", key=f"chat_run_{run_id}"):
                                            st.session_state.selected_chat_model = str(final_model)
                                            st.toast("‚úÖ –ú–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üí¨ –ß–∞—Ç", icon="üí¨")
                                except:
                                    pass
                        with btn_col3:
                            # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–µ—Å–ª–∏ –±—ã–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã)
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç –†–ï–ê–õ–¨–ù–û —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–∞ –¥–∏—Å–∫–µ
                            valid_ckpt = None
                            if checkpoints:
                                latest_ckpt_path = checkpoints[-1]['path']
                                # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –µ—Å–ª–∏ –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
                                abs_ckpt_path = Path(latest_ckpt_path)
                                if not abs_ckpt_path.is_absolute():
                                    abs_ckpt_path = PROJECT_ROOT / latest_ckpt_path
                                
                                if abs_ckpt_path.exists():
                                    valid_ckpt = str(abs_ckpt_path)

                            if valid_ckpt:
                                if st.button("‚ñ∂Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å", key=f"continue_{run_id}", help="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"):
                                    try:
                                        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å—Ç–∞—Ä–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                                        config_path = run_dir / "config.json"
                                        with open(config_path) as f:
                                            old_config = json.load(f)
                                        
                                        # 3. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º output_dir —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
                                        # –°—Ç–∞—Ä—ã–π output_dir —É–∫–∞–∑—ã–≤–∞–ª –Ω–∞ –ø–∞–ø–∫—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ (run_ID)
                                        # –ú—ã —Ö–æ—Ç–∏–º —á—Ç–æ–±—ã –Ω–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –±—ã–ª –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å —Å—Ç–∞—Ä—ã–º (–≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –ø–∞–ø–∫–µ)
                                        old_output_dir = Path(old_config.get("output_dir", ""))
                                        # –ï—Å–ª–∏ –ø—É—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π - –±–µ—Ä–µ–º —Ä–æ–¥–∏—Ç–µ–ª—è. –ï—Å–ª–∏ –Ω–µ—Ç - —Ç–æ–∂–µ (–Ω–∞–¥–µ–µ–º—Å—è)
                                        # start_training –æ–∂–∏–¥–∞–µ—Ç –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                                        old_config["output_dir"] = str(old_output_dir.parent)
                                        
                                        # 4. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ resume (–ê–ë–°–û–õ–Æ–¢–ù–´–ô –ü–£–¢–¨)
                                        old_config["resume_from_checkpoint"] = valid_ckpt
                                        
                                        # 5. –ó–∞–ø—É—Å–∫–∞–µ–º
                                        with st.spinner(f"–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å {valid_ckpt}..."):
                                            new_run_id, process = start_training(old_config)
                                            st.session_state.current_run_id = new_run_id
                                            st.session_state.training_process = process
                                            st.session_state.training_active = True
                                            
                                            save_active_run(new_run_id, old_config)
                                            
                                            st.success(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∞! Run ID: {new_run_id}")
                                            time.sleep(1)
                                            st.rerun()
                                            
                                    except Exception as e:
                                        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å: {e}")
                            elif checkpoints:
                                # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –±—ã–ª–∏ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö, –Ω–æ —É–¥–∞–ª–µ–Ω—ã —Å –¥–∏—Å–∫–∞
                                st.button("‚ö†Ô∏è –§–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã", key=f"gone_{run_id}", disabled=True, help=f"–ß–µ–∫–ø–æ–∏–Ω—Ç {checkpoints[-1]['path']} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –¥–∏—Å–∫–µ")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –≤—ã–±—Ä–∞–Ω–æ
                        if st.session_state.current_run_id == run_id:
                            st.info("üëÜ –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**")
        else:
            st.info("–ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤")
            
    with tab5:
        render_data_manager()
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø—Ä–æ —á–∞—Ç
        st.markdown("---")
        st.info("üí° –ß—Ç–æ–±—ã –ø–æ–æ–±—â–∞—Ç—å—Å—è —Å –º–æ–¥–µ–ª—å—é, –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üí¨ –ß–∞—Ç** (–≤ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)")

    with tab6:
        render_model_manager()
    
    with tab7:
        render_docs()
    
    with tab3:
        st.header("üí¨ –ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é")
        st.markdown("---")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        available_models = get_available_models()
        
        if available_models:
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            col1, col2 = st.columns([3, 1])
            
            with col1:
                model_options = [m["name"] for m in available_models]
                
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ - –Ω–∞—Ö–æ–¥–∏–º –µ—ë –∏–Ω–¥–µ–∫—Å
                default_idx = 0
                if st.session_state.selected_chat_model:
                    for i, m in enumerate(available_models):
                        if m["path"] == st.session_state.selected_chat_model:
                            default_idx = i
                            break
                
                selected_model_name = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç",
                    options=model_options,
                    index=default_idx,
                    help="–í—ã–±–µ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞"
                )
                selected_model = next(m for m in available_models if m["name"] == selected_model_name)
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º selected_chat_model –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                if st.session_state.selected_chat_model:
                    st.session_state.selected_chat_model = None
            
            with col2:
                # –î–ª—è —á–∞—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ final_model/export (HF —Ñ–æ—Ä–º–∞—Ç)
                # –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º AutoModelForCausalLM
                model_type = selected_model["type"]
                if model_type == "final":
                    st.success("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")
                else:
                    st.info("üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç")
            
            st.caption(f"–ü—É—Ç—å: `{selected_model['path']}`")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            with st.expander("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"):
                gen_col1, gen_col2, gen_col3 = st.columns(3)
                with gen_col1:
                    max_tokens = st.slider("Max Tokens", 10, 500, 128)
                with gen_col2:
                    temperature = st.slider("Temperature", 0.1, 2.0, 0.8, 0.1)
                with gen_col3:
                    top_p = st.slider("Top-p", 0.1, 1.0, 0.9, 0.05)

                # –†–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞ (2 —Ä–µ–∂–∏–º–∞, –Ω–æ –¥–µ—Ñ–æ–ª—Ç –≤—ã–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏):
                # - –µ—Å–ª–∏ —É –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å chat_template -> –î–∏–∞–ª–æ–≥
                # - –µ—Å–ª–∏ –Ω–µ—Ç -> Completion
                if "chat_prompt_mode" not in st.session_state:
                    st.session_state.chat_prompt_mode = "completion"  # –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
                prompt_mode_label = st.selectbox(
                    "–†–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞",
                    options=["–î–∏–∞–ª–æ–≥ (chat_template)", "Completion (plain text)"],
                    index=0 if st.session_state.chat_prompt_mode == "chat" else 1,
                    help="–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –µ—Å–ª–∏ —É –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å chat_template ‚Äî –≤–∫–ª—é—á–∞–µ–º –î–∏–∞–ª–æ–≥, –∏–Ω–∞—á–µ Completion. –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –≤—Ä—É—á–Ω—É—é.",
                    key="chat_prompt_mode_select",
                )
                prompt_mode = "chat" if prompt_mode_label.startswith("–î–∏–∞–ª–æ–≥") else "completion"
                st.session_state.chat_prompt_mode = prompt_mode
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞
            if "chat_model" not in st.session_state:
                st.session_state.chat_model = None
                st.session_state.chat_tokenizer = None
                st.session_state.chat_model_path = None
                st.session_state.chat_has_template = False
                st.session_state.chat_prompt_mode = "completion"
            
            if "messages" not in st.session_state:
                st.session_state.messages = []
            
            # –ö–Ω–æ–ø–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            if st.session_state.chat_model_path != selected_model["path"]:
                if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
                    with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å..."):
                        try:
                            from transformers import AutoTokenizer, AutoModelForCausalLM
                            from homellm.models.adapters import detect_model_type
                            from homellm.models.home_model import HomeForCausalLM
                            
                            model_path = Path(selected_model["path"])
                            device = "cuda" if torch.cuda.is_available() else "cpu"
                            dtype = torch.float16 if device == "cuda" else torch.float32
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ config.json
                            config_json = model_path / "config.json"
                            if not config_json.exists():
                                raise ValueError(f"config.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {model_path}")
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
                            model_type = detect_model_type(model_path)
                            st.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º {model_type.upper()} –º–æ–¥–µ–ª—å...")
                            
                            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                            try:
                                tokenizer = AutoTokenizer.from_pretrained(str(model_path), trust_remote_code=True)
                            except Exception:
                                # –î–ª—è accelerate checkpoint'–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –æ–±—ã—á–Ω–æ –Ω–µ—Ç –≤–Ω—É—Ç—Ä–∏ checkpoint_stepXXXX.
                                # –ü—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å tokenizer_path/base_model_path –∏–∑ run config.
                                tokenizer = None
                                try:
                                    run_root = model_path.parent if "checkpoint" in model_path.name else model_path
                                    run_id = run_root.name
                                    run_cfg_path = RUNS_DIR / run_id / "config.json"
                                    if run_cfg_path.exists():
                                        with open(run_cfg_path, "r", encoding="utf-8") as f:
                                            run_cfg = json.load(f)
                                        tok_src = run_cfg.get("tokenizer_path") or run_cfg.get("base_model_path")
                                        if tok_src:
                                            tokenizer = AutoTokenizer.from_pretrained(str(tok_src), trust_remote_code=True)
                                except Exception:
                                    tokenizer = None

                                if tokenizer is None:
                                    tokenizer = AutoTokenizer.from_pretrained("gpt2")
                            
                            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
                            if model_type == "home":
                                model = HomeForCausalLM.from_pretrained(str(model_path), torch_dtype=dtype)
                            else:
                                model = AutoModelForCausalLM.from_pretrained(
                                    str(model_path), trust_remote_code=True, torch_dtype=dtype
                                )
                            
                            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ device
                            model = model.to(device)
                            model.eval()
                            
                            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (pad_token = eos_token)
                            if tokenizer.pad_token is None:
                                if tokenizer.eos_token:
                                    tokenizer.pad_token = tokenizer.eos_token
                            
                            st.session_state.chat_model = model
                            st.session_state.chat_tokenizer = tokenizer
                            st.session_state.chat_model_path = str(model_path)
                            st.session_state.messages = []
                            # –ê–≤—Ç–æ–ø–æ–¥—Ç—è–≥–∏–≤–∞–Ω–∏–µ —Ä–µ–∂–∏–º–∞: –µ—Å–ª–∏ —É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –µ—Å—Ç—å chat_template, —Å—Ç–∞–≤–∏–º "–î–∏–∞–ª–æ–≥",
                            # –∏–Ω–∞—á–µ "Completion". –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –≤—Ä—É—á–Ω—É—é –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏.
                            st.session_state.chat_has_template = bool(getattr(tokenizer, "chat_template", None))
                            st.session_state.chat_prompt_mode = "chat" if st.session_state.chat_has_template else "completion"
                            st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                            st.rerun()
                        except Exception as e:
                            import traceback
                            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                            st.code(traceback.format_exc())
                            
                            # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ (–µ—Å–ª–∏ AutoModelForCausalLM –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
                            st.warning("–ü—Ä–æ–±—É–µ–º fallback –∑–∞–≥—Ä—É–∑–∫—É –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤...")
                            try:
                                from homellm.models.home_model import HomeForCausalLM, HomeConfig
                                from safetensors.torch import load_file
                                
                                model_safetensors = model_path / "model.safetensors"
                                model_bin = model_path / "pytorch_model.bin"
                                
                                if not (model_safetensors.exists() or model_bin.exists()):
                                    raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏")
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                                if not st.session_state.get("chat_tokenizer"):
                                    st.session_state.chat_tokenizer = AutoTokenizer.from_pretrained("gpt2")
                                    if st.session_state.chat_tokenizer.pad_token is None:
                                        st.session_state.chat_tokenizer.pad_token = st.session_state.chat_tokenizer.eos_token
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
                                if config_json.exists():
                                    config = HomeConfig.from_pretrained(str(model_path))
                                else:
                                    # –ò—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                                    run_config_path = model_path.parent / "run_config.json"
                                    if run_config_path.exists():
                                        import json as json_module
                                        with open(run_config_path) as f:
                                            run_cfg = json_module.load(f)
                                        config = HomeConfig(
                                            vocab_size=len(st.session_state.chat_tokenizer),
                                            hidden_size=run_cfg.get("hidden_size", 512),
                                            num_hidden_layers=run_cfg.get("num_layers", 8),
                                            num_attention_heads=run_cfg.get("n_heads", 8),
                                            max_position_embeddings=run_cfg.get("seq_len", 512),
                                        )
                                    else:
                                        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω config.json")
                                
                                # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                                st.session_state.chat_model = HomeForCausalLM(config)
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
                                if model_safetensors.exists():
                                    state_dict = load_file(str(model_safetensors))
                                else:
                                    state_dict = torch.load(str(model_bin), map_location="cpu")
                                
                                missing, unexpected = st.session_state.chat_model.load_state_dict(state_dict, strict=False)
                                
                                if missing:
                                    real_missing = [k for k in missing if k != "lm_head.weight"]
                                    if real_missing:
                                        st.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–µ—Å–∞: {real_missing[:5]}...")
                                
                                if hasattr(st.session_state.chat_model, "tie_weights"):
                                    st.session_state.chat_model.tie_weights()
                                
                                st.session_state.chat_model = st.session_state.chat_model.to(device)
                                st.session_state.chat_model.eval()
                                st.session_state.chat_model_path = str(model_path)
                                st.session_state.messages = []
                                st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (fallback –º–µ—Ç–æ–¥)!")
                                st.rerun()
                            except Exception as e2:
                                st.error(f"Fallback –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
                                import traceback
                                st.code(traceback.format_exc())
            else:
                st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {selected_model_name}")
                
                # –ö–Ω–æ–ø–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ (–¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–∞)
                if st.button("üíæ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HF —Ñ–æ—Ä–º–∞—Ç", help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (—Å –∫–æ–Ω—Ñ–∏–≥–æ–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º)"):
                    with st.spinner("–≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏..."):
                        export_path = export_model_to_hf(
                            st.session_state.chat_model, 
                            st.session_state.chat_tokenizer, 
                            st.session_state.chat_model_path
                        )
                        if export_path:
                            st.success(f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤:\n`{export_path}`")
                            time.sleep(2)
                            st.rerun() # –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —ç–∫—Å–ø–æ—Ä—Ç

                # --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–ü–¢–ê ---
                with st.expander("‚öôÔ∏è –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", expanded=False):
                    system_prompt_input = st.text_area(
                        "–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):",
                        value=st.session_state.get("system_prompt", ""),
                        help="–ï—Å–ª–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤–º–µ—Å—Ç–æ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –º–æ–¥–µ–ª–∏. –û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ.",
                        key="system_prompt_input"
                    )
                    st.session_state.system_prompt = system_prompt_input.strip()
                    if system_prompt_input.strip():
                        st.info("‚úÖ –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤–≤–µ–¥–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç")
                    else:
                        st.caption("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –º–æ–¥–µ–ª–∏")
                
                # --- –ò–ù–¢–ï–†–§–ï–ô–° –ß–ê–¢–ê –° –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ú –°–ö–†–û–õ–õ–û–ú ---
                chat_container = st.container(height=500) # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                
                with chat_container:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
                    for message in st.session_state.messages:
                        with st.chat_message(message["role"]):
                            st.write(message["content"])
                
                # –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤—Å–µ–≥–¥–∞ –≤–Ω–∏–∑—É)
                if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."):
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —é–∑–µ—Ä–∞ —Å—Ä–∞–∑—É)
                    with chat_container:
                        with st.chat_message("user"):
                            st.write(prompt)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                    with chat_container: # –û—Ç–≤–µ—Ç —Ç–æ–∂–µ –ø–∏—à–µ–º –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                        with st.chat_message("assistant"):
                            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è..."):
                                try:
                                    tokenizer = st.session_state.chat_tokenizer
                                    model = st.session_state.chat_model
                                    device = next(model.parameters()).device
                                    
                                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                                    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Ä–µ–∂–∏–º: auto/chat_template/plain
                                    
                                    # –ë–µ—Ä–µ–º –∏—Å—Ç–æ—Ä–∏—é + –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                                    conversation = st.session_state.messages.copy() # [{"role": "user", ...}, ...]
                                    
                                    has_template = bool(getattr(tokenizer, "chat_template", None))
                                    use_chat_template = (prompt_mode == "chat") and has_template
                                    if prompt_mode == "chat" and not has_template:
                                        st.warning("–£ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ—Ç chat_template ‚Äî –∏—Å–ø–æ–ª—å–∑—É—é Completion.")
                                        use_chat_template = False

                                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ chat_template)
                                    if use_chat_template:
                                        system_prompt = st.session_state.get("system_prompt", "").strip()
                                        
                                        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ conversation (–µ—Å–ª–∏ –µ—Å—Ç—å)
                                        # —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ —Å –≤–≤–µ–¥–µ–Ω–Ω—ã–º —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                                        if conversation and conversation[0].get("role") == "system":
                                            conversation.pop(0)
                                        
                                        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ –Ω–∞—á–∞–ª–æ
                                        if system_prompt:
                                            conversation.insert(0, {"role": "system", "content": system_prompt})
                                        # –ï—Å–ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø—É—Å—Ç–æ–π, —à–∞–±–ª–æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏–∑ –º–æ–¥–µ–ª–∏
                                    
                                    if use_chat_template:
                                        # –î–ª—è SFT –º–æ–¥–µ–ª–∏: –ø—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —Å —Ç–µ–≥–∞–º–∏
                                        prompt_text = tokenizer.apply_chat_template(
                                            conversation, 
                                            tokenize=False, 
                                            add_generation_prompt=True
                                        )
                                    else:
                                        # –î–ª—è Base/Pretrain –º–æ–¥–µ–ª–∏: –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
                                        # –û–±—ã—á–Ω–æ Base –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–∏–∞–ª–æ–≥, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ —Å–ª–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–º–ø—Ç
                                        # –∏–ª–∏ –≤–µ—Å—å –¥–∏–∞–ª–æ–≥ —Ç–µ–∫—Å—Ç–æ–º
                                        prompt_text = ""
                                        for m in conversation:
                                            prompt_text += f"{m['role']}: {m['content']}\n"
                                        prompt_text += "assistant: "
                                    
                                    # –í–ê–ñ–ù–û:
                                    # - inputs –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ –¢–û–ú –ñ–ï —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å (–º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ cuda:1)
                                    # - –∑–∞—Ä–∞–Ω–µ–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ token ids –Ω–µ –≤—ã—Ö–æ–¥—è—Ç –∑–∞ vocab –º–æ–¥–µ–ª–∏ (–∏–Ω–∞—á–µ –±—É–¥–µ—Ç CUDA assert)
                                    device = next(model.parameters()).device
                                    device_type = str(device.type)

                                    enc = tokenizer(prompt_text, return_tensors="pt")
                                    try:
                                        if hasattr(model, "get_input_embeddings") and model.get_input_embeddings() is not None:
                                            vocab_size = int(model.get_input_embeddings().weight.shape[0])
                                            max_id = int(enc["input_ids"].max().item())
                                            min_id = int(enc["input_ids"].min().item())
                                            if min_id < 0 or max_id >= vocab_size:
                                                raise ValueError(
                                                    f"Tokenizer –≤—ã–¥–∞—ë—Ç token_id –≤–Ω–µ vocab –º–æ–¥–µ–ª–∏: min_id={min_id}, max_id={max_id}, "
                                                    f"vocab_size(model)={vocab_size}. "
                                                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Å –º–æ–¥–µ–ª—å—é –∑–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π tokenizer (—Ç–æ—Ç –∂–µ, —á—Ç–æ –±—ã–ª –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)."
                                                )
                                    except Exception as e:
                                        raise RuntimeError(f"–ü—Ä–æ–±–ª–µ–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏/–≤–æ–∫–∞–±–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π: {e}")

                                    inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in enc.items()}
                                    
                                    # –ü—Ä–∏–≤–æ–¥–∏–º dtype –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ—Å–ª–µ SFT checkpoints bf16)
                                    model_dtype = next(model.parameters()).dtype
                                    autocast_enabled = (device_type == "cuda")
                                    autocast_dtype = torch.bfloat16 if model_dtype == torch.bfloat16 else torch.float16

                                    # attention_mask –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è int/bool (–Ω–µ bf16/fp16).

                                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ (–∏–Ω–∞—á–µ –≤–æ–∑–º–æ–∂–µ–Ω CUDA index out of bounds –ø—Ä–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ö).
                                    max_ctx = None
                                    try:
                                        cfg = getattr(model, "config", None)
                                        for key in ("max_position_embeddings", "n_positions", "seq_len", "max_seq_len"):
                                            if cfg is not None and hasattr(cfg, key):
                                                v = getattr(cfg, key)
                                                if v is not None:
                                                    max_ctx = int(v)
                                                    break
                                    except Exception:
                                        max_ctx = None

                                    if max_ctx is not None and max_ctx > 0 and "input_ids" in inputs:
                                        in_len = int(inputs["input_ids"].shape[1])
                                        if in_len > max_ctx:
                                            cut = in_len - max_ctx
                                            inputs["input_ids"] = inputs["input_ids"][:, cut:]
                                            if "attention_mask" in inputs and hasattr(inputs["attention_mask"], "shape"):
                                                inputs["attention_mask"] = inputs["attention_mask"][:, cut:]
                                            in_len = int(inputs["input_ids"].shape[1])

                                        allowed_new = int(max_ctx - in_len)
                                        if allowed_new <= 0:
                                            raise RuntimeError(
                                                f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —É–∂–µ –¥–æ—Å—Ç–∏–≥ –º–∞–∫—Å–∏–º—É–º–∞ –º–æ–¥–µ–ª–∏ (max_ctx={max_ctx}). "
                                                f"–£–º–µ–Ω—å—à–∏—Ç–µ –∏—Å—Ç–æ—Ä–∏—é/—Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
                                            )
                                        if int(max_tokens) > allowed_new:
                                            max_tokens = allowed_new

                                    with torch.no_grad(), torch.autocast(device_type=device_type, dtype=autocast_dtype, enabled=autocast_enabled):
                                        outputs = model.generate(
                                            **inputs,
                                            max_new_tokens=max_tokens,
                                            temperature=temperature,
                                            top_p=top_p,
                                            do_sample=True,
                                            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
                                            use_cache=False,  # –û—Ç–∫–ª—é—á–∞–µ–º KV-cache –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                                        )
                                    
                                    response = tokenizer.decode(
                                        outputs[0][inputs["input_ids"].shape[1]:], 
                                        skip_special_tokens=True
                                    )
                                    
                                    st.write(response)
                                    st.session_state.messages.append({"role": "assistant", "content": response})
                                except Exception as e:
                                    import traceback
                                    st.session_state.last_chat_error = traceback.format_exc()
                                    st.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                                    st.code(st.session_state.last_chat_error)
                
                # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
                if st.session_state.messages:
                    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
                        st.session_state.messages = []
                        st.rerun()
        else:
            st.info("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤–æ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–ø—É—Å–∫'!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥–¥–µ –∏—Å–∫–∞—Ç—å –º–æ–¥–µ–ª–∏
            st.markdown("""
            **–ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:**
            - `out/*/final_model/` ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
            - `out/*/checkpoint_*/` ‚Äî —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            """)


if __name__ == "__main__":
    main()


