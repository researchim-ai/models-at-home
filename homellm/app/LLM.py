"""
Motels at Home Training Studio ‚Äî –í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π
======================================================================

–ó–∞–ø—É—Å–∫:
    streamlit run homellm/app/main.py
    
–∏–ª–∏:
    ./scripts/run_studio.sh
"""

import streamlit as st
import logging
import subprocess
import json
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

import os
import signal
from pathlib import Path
from datetime import datetime
from typing import Tuple
from contextlib import suppress
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import torch
from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names, load_dataset_builder  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
try:
    from .docs import render_docs
except ImportError:
    from docs import render_docs

# –ü—É—Ç–∏
PROJECT_ROOT = Path(__file__).parent.parent.parent
CONFIGS_DIR = PROJECT_ROOT / "configs"
DATASET_DIR = PROJECT_ROOT / "datasets"  # datasets —Å "s"!
MODELS_DIR = PROJECT_ROOT / "models"  # –°–∫–∞—á–∞–Ω–Ω—ã–µ HF –º–æ–¥–µ–ª–∏
OUTPUT_DIR = PROJECT_ROOT / "out"
RUNS_DIR = PROJECT_ROOT / ".runs"
RUNS_DIR.mkdir(exist_ok=True)
MODELS_DIR.mkdir(exist_ok=True)

# ============================================================================
# Page Config
# ============================================================================

st.set_page_config(
    page_title="HomeLLM Training Studio",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS ‚Äî —á–∏—Å—Ç–∞—è —Ç—ë–º–Ω–∞—è —Ç–µ–º–∞ —Å —Ö–æ—Ä–æ—à–∏–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–º
st.markdown("""
<style>
    /* –ó–∞–≥–æ–ª–æ–≤–æ–∫ */
    .main-header {
        color: #ff6b6b;
        font-size: 2.5rem;
        font-weight: 800;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .sub-header {
        color: #888888;
        text-align: center;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    
    /* –°—Ç–∞—Ç—É—Å */
    .status-running {
        color: #22c55e !important;
        font-weight: bold;
    }
    
    .status-completed {
        color: #3b82f6 !important;
        font-weight: bold;
    }
    
    .status-error {
        color: #ef4444 !important;
        font-weight: bold;
    }
    
    /* ASCII art –±–ª–æ–∫ */
    .model-ascii {
        background: #1e1e1e;
        border: 1px solid #333;
        border-radius: 8px;
        padding: 1rem;
        font-family: 'Courier New', monospace;
        color: #00ff88;
        white-space: pre;
        overflow-x: auto;
    }
    
    /* –ö–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ç—Ä–∏–∫ */
    div[data-testid="metric-container"] {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 0.5rem;
    }
    
    /* –ö–Ω–æ–ø–∫–∏ –∑–∞–ø—É—Å–∫–∞ */
    .stButton > button[kind="primary"] {
        background: linear-gradient(90deg, #e94560, #ff6b6b);
        color: white !important;
        border: none;
        font-weight: 600;
    }
    
    /* Code –±–ª–æ–∫–∏ */
    pre {
        background: #0d1117 !important;
        color: #c9d1d9 !important;
        border: 1px solid #30363d !important;
    }
    
    code {
        color: #79c0ff !important;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# Persistence ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞–º–∏
# ============================================================================

ACTIVE_RUN_FILE = RUNS_DIR / "active_run.json"


def save_active_run(run_id: str, config: dict = None):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –≤ —Ñ–∞–π–ª."""
    data = {
        "run_id": run_id,
        "started_at": datetime.now().isoformat(),
        "config": config or {}
    }
    with open(ACTIVE_RUN_FILE, "w") as f:
        json.dump(data, f, indent=2)


def load_active_run() -> dict | None:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –∏–∑ —Ñ–∞–π–ª–∞."""
    if not ACTIVE_RUN_FILE.exists():
        return None
    try:
        with open(ACTIVE_RUN_FILE) as f:
            return json.load(f)
    except:
        return None


def clear_active_run():
    """–û—á–∏—Å—Ç–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run."""
    if ACTIVE_RUN_FILE.exists():
        ACTIVE_RUN_FILE.unlink()


def restore_session_state():
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π run
    active = load_active_run()
    if active and active.get("run_id"):
        run_id = active["run_id"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ run –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        run_dir = RUNS_DIR / run_id
        if run_dir.exists():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ metrics.json)
            pid_path = run_dir / "pid"
            metrics_path = run_dir / "metrics.json"
            process_alive = False
            metrics = None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∏–∑ metrics.json (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
            if metrics_path.exists():
                try:
                    import time
                    metrics_mtime = metrics_path.stat().st_mtime
                    metrics_age_minutes = (time.time() - metrics_mtime) / 60
                    
                    with open(metrics_path) as f:
                        metrics = json.load(f)
                    
                    status = metrics.get("status", "")
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–π - —Ç–æ—á–Ω–æ –Ω–µ –∂–∏–≤
                    if status in ("completed", "error", "stopped"):
                        process_alive = False
                    else:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å —á–µ—Ä–µ–∑ PID
                        if pid_path.exists():
                            try:
                                with open(pid_path) as f:
                                    pid = int(f.read().strip())
                                os.kill(pid, 0)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
                                process_alive = True
                            except ProcessLookupError:
                                process_alive = False
                            except (ValueError, PermissionError):
                                # PermissionError –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤
                                # –ù–æ –µ—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ —Å–≤–µ–∂–∏–µ (< 5 –º–∏–Ω—É—Ç) - —Å—á–∏—Ç–∞–µ–º –∂–∏–≤—ã–º
                                process_alive = metrics_age_minutes < 5
                    
                    # –ï—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å –¥–∞–≤–Ω–æ ‚Äî –ù–ï —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –º—ë—Ä—Ç–≤—ã–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
                    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å PID; PermissionError —Ç—Ä–∞–∫—Ç—É–µ–º –∫–∞–∫ "–∂–∏–≤".
                    if status not in ("completed", "error", "stopped") and metrics_age_minutes > 5:
                        pid_alive = False
                        if pid_path.exists():
                            try:
                                with open(pid_path) as f:
                                    pid = int(f.read().strip())
                                os.kill(pid, 0)
                                pid_alive = True
                            except PermissionError:
                                pid_alive = True
                            except (ProcessLookupError, ValueError, FileNotFoundError):
                                pid_alive = False
                        if pid_alive:
                            process_alive = True
                            logger.warning(
                                f"Metrics not updated for {metrics_age_minutes:.1f} minutes, but PID looks alive. "
                                f"Treating process as running (metrics may be stalled)."
                            )
                        else:
                            process_alive = False
                            logger.warning(f"Metrics not updated for {metrics_age_minutes:.1f} minutes and PID not alive, assuming process dead")
                            clear_active_run()
                except Exception as e:
                    logger.warning(f"Failed to check metrics: {e}")
                    # Fallback –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É PID
                    if pid_path.exists():
                        try:
                            with open(pid_path) as f:
                                pid = int(f.read().strip())
                            os.kill(pid, 0)
                            process_alive = True
                        except (ProcessLookupError, ValueError, PermissionError):
                            process_alive = False
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            st.session_state.current_run_id = run_id
            st.session_state.training_active = process_alive
            
            # –ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à—ë–Ω, –æ—á–∏—â–∞–µ–º active_run
            if not process_alive:
                if metrics and metrics.get("status") in ("completed", "error", "stopped"):
                    clear_active_run()


# ============================================================================
# Session State
# ============================================================================

if "training_process" not in st.session_state:
    st.session_state.training_process = None
if "current_run_id" not in st.session_state:
    st.session_state.current_run_id = None
if "training_active" not in st.session_state:
    st.session_state.training_active = False
if "selected_chat_model" not in st.session_state:
    st.session_state.selected_chat_model = None

# –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
if "session_restored" not in st.session_state:
    restore_session_state()
    st.session_state.session_restored = True


# ============================================================================
# Helper Functions
# ============================================================================

def get_available_datasets():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."""
    datasets = []
    if DATASET_DIR.exists():
        for f in DATASET_DIR.glob("*.jsonl"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        # –í–ê–ñ–ù–û: –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º .json, —Ç.–∫. —ç—Ç–æ –æ–±—ã—á–Ω–æ –º–∞—Å—Å–∏–≤, –∞ –Ω–µ JSONL (–ø–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        # –¥–ª—è f in DATASET_DIR.glob("*.json"):
        #     size_mb = f.stat().st_size / (1024 * 1024)
        #     datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt.gz"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.jsonl.gz"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
    return datasets


def get_gpu_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU."""
    gpus = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            gpus.append({
                "id": i,
                "name": props.name,
                "memory_gb": round(memory_gb, 1),
                "compute_capability": f"{props.major}.{props.minor}",
            })
    return gpus


def get_available_configs():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö accelerate –∫–æ–Ω—Ñ–∏–≥–æ–≤."""
    configs = []
    if CONFIGS_DIR.exists():
        for f in CONFIGS_DIR.glob("*.yaml"):
            name = f.stem.replace("accelerate_", "").replace("_", " ").title()
            configs.append({
                "file": f.name,
                "name": name,
                "path": str(f),
            })
    return configs


# –û–ø–∏—Å–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
PARALLEL_TYPES = {
    "default": {
        "name": "Single GPU / CPU",
        "type": "None",
        "description": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞",
        "icon": "üñ•Ô∏è",
    },
    "multi_gpu": {
        "name": "Multi-GPU (DDP)",
        "type": "Data Parallel",
        "description": "Distributed Data Parallel ‚Äî –∫–∞–∂–¥–∞—è GPU –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏ –∏ —á–∞—Å—Ç—å –±–∞—Ç—á–∞",
        "icon": "üîÑ",
    },
    "fsdp": {
        "name": "FSDP",
        "type": "Data Parallel + Model Parallel",
        "description": "Fully Sharded Data Parallel ‚Äî –º–æ–¥–µ–ª—å —à–∞—Ä–¥–∏—Ä—É–µ—Ç—Å—è –º–µ–∂–¥—É GPU (PyTorch native)",
        "icon": "‚ö°",
    },
    "deepspeed_zero2": {
        "name": "DeepSpeed ZeRO-2",
        "type": "Data Parallel + Optimizer Parallel",
        "description": "–®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É GPU",
        "icon": "üöÄ",
    },
    "deepspeed_zero3": {
        "name": "DeepSpeed ZeRO-3",
        "type": "Full Model Parallel",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å + –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä + –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã",
        "icon": "üí™",
    },
    "deepspeed_zero3_offload": {
        "name": "ZeRO-3 + CPU Offload",
        "type": "Model Parallel + CPU Offload",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ + –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM",
        "icon": "üßä",
    },
}


def estimate_parameters(
    hidden_size: int,
    num_layers: int,
    vocab_size: int = 50257,
    intermediate_size: int | None = None,
) -> int:
    """
    –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞—à–µ–π `HomeModel` (—Å–º. `homellm/models/home_model.py`).

    –í–ê–ñ–ù–û:
    - –£ –Ω–∞—Å RoPE (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ù–ï –æ–±—É—á–∞–µ–º—ã–µ) => `seq_len` –Ω–∞ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –≤–ª–∏—è–µ—Ç.
    - lm_head weight tied –∫ embed_tokens => –Ω–µ —É–¥–≤–∞–∏–≤–∞–µ–º vocab*hidden.
    """
    h = int(hidden_size)
    l = int(num_layers)
    v = int(vocab_size)
    i = int(intermediate_size) if intermediate_size is not None else int(h * 4)

    # Embedding
    embed = v * h

    # Per-layer:
    # - Attention: q/k/v/out, bias=False => 4 * (H*H)
    attn = 4 * h * h
    # - SwiGLU MLP: w1(H->I), w2(I->H), w3(H->I), bias=False => 3 * (H*I)
    mlp = 3 * h * i
    # - RMSNorm weights: 2 * H
    norms = 2 * h

    # Final norm
    final_norm = h

    return int(embed + l * (attn + mlp + norms) + final_norm)


def format_params(n: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    if n >= 1e9:
        return f"{n/1e9:.2f}B"
    elif n >= 1e6:
        return f"{n/1e6:.1f}M"
    elif n >= 1e3:
        return f"{n/1e3:.1f}K"
    return str(n)


def format_time(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏."""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        return f"{seconds/3600:.1f}h"


def load_metrics(run_id: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    metrics_path = RUNS_DIR / run_id / "metrics.json"
    if metrics_path.exists():
        try:
            with open(metrics_path) as f:
                return json.load(f)
        except:
            pass
    return None


def _close_run_log_files(run_id: str):
    """–ó–∞–∫—Ä—ã—Ç—å —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã stdout/stderr, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ session_state."""
    for k in (f"stdout_file_{run_id}", f"stderr_file_{run_id}"):
        f = st.session_state.get(k)
        if f:
            with suppress(Exception):
                f.close()
            with suppress(Exception):
                del st.session_state[k]


def start_training(config: dict) -> tuple[str, subprocess.Popen]:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤ —Ñ–æ–Ω–µ."""
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –õ–û–ì–ò–ö–ê –ü–£–¢–ï–ô
    # config["output_dir"] - —ç—Ç–æ –∫–æ—Ä–µ–Ω—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä out/my_model)
    # –ú—ã —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ out/my_model/run_2023.../checkpoint_...
    experiment_root = Path(PROJECT_ROOT) / config.get("output_dir", "out/default")
    run_output_dir = experiment_root / run_id
    
    # –û–±–Ω–æ–≤–ª—è–µ–º output_dir –≤ –∫–æ–Ω—Ñ–∏–≥–µ, —á—Ç–æ–±—ã worker —Å–æ—Ö—Ä–∞–Ω—è–ª —Ç—É–¥–∞
    config["output_dir"] = str(run_output_dir)
    
    # –ü–∞–ø–∫–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞ (–ª–æ–≥–∏, –º–µ—Ç—Ä–∏–∫–∏)
    # –ú–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö —Ç–∞–º –∂–µ, –≥–¥–µ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã, –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    run_dir = RUNS_DIR / run_id # –û—Å—Ç–∞–≤–ª—è–µ–º .runs –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ª–æ–≥–æ–≤ —Å—Ç—Ä–∏–º–ª–∏—Ç–∞
    run_dir.mkdir(parents=True, exist_ok=True)
    run_output_dir.mkdir(parents=True, exist_ok=True) # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤–µ—Å–æ–≤
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    # –°–æ–∑–¥–∞—ë–º –Ω–∞—á–∞–ª—å–Ω—ã–π metrics —Ñ–∞–π–ª
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0}, f)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ distributed
    distributed_mode = config.get("distributed_mode", "default")
    config_file = config.get("config_file")
    num_gpus = config.get("num_gpus", 1)
    
    if distributed_mode != "default" and config_file:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerate launch —Å –∫–æ–Ω—Ñ–∏–≥–æ–º
        # –í–ê–ñ–ù–û: gradient_accumulation_steps –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ config.json, –Ω–µ —á–µ—Ä–µ–∑ CLI —Ñ–ª–∞–≥–∏
        cmd = [
            "accelerate", "launch",
            "--config_file", config_file,
            "--num_processes", str(num_gpus),
            "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫
        cmd = [
            "python", "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        f.write(" ".join(cmd))
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    # –í–ê–ñ–ù–û: –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä GPU –∏–∑ UI
    env = os.environ.copy()
    gpu_ids = config.get("gpu_ids") or []
    if gpu_ids:
        env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,  # –û—Ç–¥–µ–ª—è–µ–º –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        env=env,
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
    st.session_state[f"stdout_file_{run_id}"] = stdout_file
    st.session_state[f"stderr_file_{run_id}"] = stderr_file
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PID –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def stop_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É."""
    stopped = False
    
    # –ü—Ä–æ–±—É–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ PID –∏–∑ —Ñ–∞–π–ª–∞ (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
    if st.session_state.current_run_id:
        pid_path = RUNS_DIR / st.session_state.current_run_id / "pid"
        if pid_path.exists():
            try:
                with open(pid_path) as f:
                    pid = int(f.read().strip())
                # –£–±–∏–≤–∞–µ–º process group (–≤–∞–∂–Ω–æ –¥–ª—è accelerate/DDP)
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è —É–±–∏—Ç—å process group
                    os.killpg(os.getpgid(pid), signal.SIGTERM)
                    stopped = True
                except (ProcessLookupError, OSError):
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å (–ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –≤ –≥—Ä—É–ø–ø–µ –∏–ª–∏ —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è), –ø—Ä–æ–±—É–µ–º –ø–æ PID
                    try:
                        os.kill(pid, signal.SIGTERM)
                        stopped = True
                    except ProcessLookupError:
                        pass
                
                # –ñ–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
                time.sleep(0.5)
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
                    os.kill(pid, 0)
                    # –ï—Å–ª–∏ –∂–∏–≤, —É–±–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ (process group)
                    try:
                        os.killpg(os.getpgid(pid), signal.SIGKILL)
                    except (ProcessLookupError, OSError):
                        os.kill(pid, signal.SIGKILL)
                except ProcessLookupError:
                    pass  # –ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
            except Exception as e:
                pass
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = RUNS_DIR / st.session_state.current_run_id / "metrics.json"
        if metrics_path.exists():
            try:
                with open(metrics_path) as f:
                    metrics = json.load(f)
                metrics["status"] = "stopped"
                with open(metrics_path, "w") as f:
                    json.dump(metrics, f, indent=2)
            except:
                pass
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ subprocess
    if st.session_state.training_process:
        try:
            st.session_state.training_process.terminate()
            st.session_state.training_process.wait(timeout=2)
        except:
            try:
                st.session_state.training_process.kill()
            except:
                pass
        st.session_state.training_process = None
    
    st.session_state.training_active = False
    
    # –û—á–∏—â–∞–µ–º active_run
    clear_active_run()
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –ª–æ–≥–æ–≤
    if st.session_state.current_run_id:
        _close_run_log_files(st.session_state.current_run_id)
    
    return stopped


def start_grpo_training(config: dict) -> tuple[str, subprocess.Popen]:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å GRPO –æ–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–µ."""
    run_id = datetime.now().strftime("grpo_%Y%m%d_%H%M%S")
    
    # –ü–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    experiment_root = Path(PROJECT_ROOT) / config.get("output_dir", "out/grpo")
    run_output_dir = experiment_root / run_id
    
    config["output_dir"] = str(run_output_dir)
    
    run_dir = RUNS_DIR / run_id
    run_dir.mkdir(parents=True, exist_ok=True)
    run_output_dir.mkdir(parents=True, exist_ok=True)
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    # –ù–∞—á–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0, "stage": "grpo"}, f)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ GRPO
    import sys
    cmd = [
        sys.executable, "-m", "homellm.training.rl.train_gsm8k",
        "--model", config.get("base_model_path", ""),
        "--algorithm", config.get("grpo_algorithm", "grpo"),
        "--output_dir", str(run_output_dir),
        "--group_size", str(config.get("grpo_group_size", 8)),
        "--batch_size", str(config.get("batch_size", 4)),
        "--max_new_tokens", str(config.get("grpo_max_new_tokens", 1024)),
        "--learning_rate", str(config.get("grpo_learning_rate", 5e-6)),
        "--max_steps", str(config.get("grpo_max_steps", 500)),
        "--log_steps", str(config.get("log_steps", 10)),
        "--save_steps", str(config.get("save_steps", 100)),
        "--reasoning_format", config.get("grpo_reasoning_format", "deepseek"),
    ]
    
    # LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    tuning_method = config.get("tuning_method", "lora")
    if tuning_method in ("lora", "qlora"):
        cmd.append("--use_lora")
        if config.get("lora_r"):
            cmd.extend(["--lora_r", str(config.get("lora_r"))])
    else:
        cmd.append("--no_lora")
    
    if tuning_method == "qlora" or config.get("use_4bit"):
        cmd.append("--use_4bit")
    
    # –î–∞—Ç–∞—Å–µ—Ç
    if config.get("grpo_dataset_source") == "GSM8K (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞)":
        if config.get("grpo_max_samples"):
            cmd.extend(["--max_samples", str(config.get("grpo_max_samples"))])
    elif config.get("grpo_dataset_path"):
        cmd.extend(["--dataset_file", config.get("grpo_dataset_path")])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        f.write(" ".join(cmd))
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    env = os.environ.copy()
    env["PYTHONUNBUFFERED"] = "1"
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,
        env=env,
    )
    
    st.session_state[f"stdout_file_{run_id}"] = stdout_file
    st.session_state[f"stderr_file_{run_id}"] = stderr_file
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PID
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def is_process_running(run_id: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–ø—É—â–µ–Ω –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å."""
    pid_path = RUNS_DIR / run_id / "pid"
    if not pid_path.exists():
        return False
    
    try:
        with open(pid_path) as f:
            pid = int(f.read().strip())
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
        os.kill(pid, 0)
        return True
    except PermissionError:
        # –ü—Ä–æ—Ü–µ—Å—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—É—â–µ–Ω –æ—Ç –¥—Ä—É–≥–æ–≥–æ —é–∑–µ—Ä–∞)
        # –°—á–∏—Ç–∞–µ–º, —á—Ç–æ –æ–Ω –∂–∏–≤
        return True
    except (ProcessLookupError, ValueError, FileNotFoundError):
        return False


# ============================================================================
# UI Components
# ============================================================================

def render_header():
    st.markdown("# üè† Models at Home Training Studio")
    st.caption("–í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–º–∞")


def get_nested_value(data: dict, path: str):
    """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - 'key1.key2' - –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏
    - 'messages [—Å–ø–∏—Å–æ–∫ –∏–∑ N —ç–ª.]' - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å—å —Å–ø–∏—Å–æ–∫
    - 'messages[].content' - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª—è –∏–∑ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    - 'messages[0]' - –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞
    """
    if not path: return None
    
    # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã —Ç–∏–ø–∞ " [—Å–ø–∏—Å–æ–∫]" –∏–ª–∏ " [—Å–ø–∏—Å–æ–∫ –∏–∑ 3 —ç–ª.]"
    import re
    path = re.sub(r' \[—Å–ø–∏—Å–æ–∫.*?\]$', '', path)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π —Ç–∏–ø–∞ 'messages[].content' (–≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞)
    if "[]." in path:
        parts = path.split("[].", 1)
        list_path = parts[0]
        remaining_path = parts[1]
        
        list_val = get_nested_value(data, list_path)
        if isinstance(list_val, list):
            results = []
            for item in list_val:
                if isinstance(item, dict):
                    val = get_nested_value(item, remaining_path)
                    results.append(val)
            return results
        return None
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π —Ç–∏–ø–∞ 'messages[0]' (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å)
    if "[" in path and "]" in path:
        # –ü–∞—Ä—Å–∏–º –∏–Ω–¥–µ–∫—Å
        match = re.search(r'\[(\d+)\]', path)
        if match:
            idx = int(match.group(1))
            base_path = path[:match.start()]
            after_path = path[match.end():]
            if after_path.startswith('.'):
                after_path = after_path[1:]
            
            base_val = get_nested_value(data, base_path)
            if isinstance(base_val, list) and 0 <= idx < len(base_val):
                if after_path:
                    return get_nested_value(base_val[idx], after_path)
                return base_val[idx]
            return None
    
    # –û–±—ã—á–Ω—ã–π –ø—É—Ç—å —á–µ—Ä–µ–∑ —Ç–æ—á–∫–∏
    keys = path.split('.')
    curr = data
    try:
        for k in keys:
            if isinstance(curr, dict):
                curr = curr.get(k)
            elif isinstance(curr, list) and k.isdigit():
                curr = curr[int(k)]
            else:
                return None
            if curr is None: return None
        return curr
    except:
        return None


def get_dataset_columns(file_path: str):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ (–≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ) –∏ –ø—Ä–∏–º–µ—Ä."""
    path = Path(file_path)
    if not path.exists():
        return [], {}
        
    try:
        sample_data = {}
        if path.suffix == ".jsonl":
            with open(path, "r", encoding="utf-8") as f:
                line = f.readline()
                if line:
                    sample_data = json.loads(line)
        elif path.suffix == ".json":
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list) and len(data) > 0:
                    sample_data = data[0]
                elif isinstance(data, dict):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º –∫–ª—é—á–∏
                    return list(data.keys()), {k: v[0] if isinstance(v, list) else v for k, v in data.items()}
        elif path.suffix == ".csv":
            import csv
            with open(path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                row = next(reader, None)
                if row:
                    sample_data = row
        
        if sample_data:
            # –î–ª—è get_dataset_columns –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–ª–æ—Å–∫–∏–µ –∫–ª—é—á–∏, 
            # —Ç–∞–∫ –∫–∞–∫ render_sft_main_config —Ç–µ–ø–µ—Ä—å —Å–∞–º —Å—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤–æ.
            # –ù–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º –≤–æ–∑–≤—Ä–∞—Ç sample_data
            return [], sample_data
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return [], {}
    
    return [], {}


def flatten_json_structure(d: dict, parent_path: str = '', depth: int = 0) -> list:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç JSON –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ –¥–µ—Ä–µ–≤–∞."""
    items = []
    for k, v in d.items():
        current_path = f"{parent_path}.{k}" if parent_path else k
        
        if isinstance(v, dict):
            # –ü–∞–ø–∫–∞
            items.append({"type": "folder", "key": k, "path": current_path, "depth": depth, "val": ""})
            items.extend(flatten_json_structure(v, current_path, depth + 1))
        elif isinstance(v, list):
            # –°–ø–∏—Å–æ–∫
            items.append({"type": "list", "key": k, "path": current_path, "depth": depth, "val": f"List[{len(v)}]"})
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            if v:
                if isinstance(v[0], dict):
                    items.extend(flatten_json_structure(v[0], current_path, depth + 1))
                else:
                    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ (—Å—Ç—Ä–æ–∫ –∏ —Ç.–¥.) - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ –ª–∏—Å—Ç
                    # –ù–æ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç, –ø—Ä–æ—Å—Ç–æ –¥–∞–µ–º –ø–æ–Ω—è—Ç—å —á—Ç–æ –≤–Ω—É—Ç—Ä–∏
                    # –î–ª—è SFT –º—ã –æ–±—ã—á–Ω–æ –Ω–µ –≤—ã–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ –≤–µ—Å—å —Å–ø–∏—Å–æ–∫
                    pass
        else:
            # –õ–∏—Å—Ç (–∑–Ω–∞—á–µ–Ω–∏–µ)
            items.append({"type": "leaf", "key": k, "path": current_path, "depth": depth, "val": str(v)})
    return items


def get_all_leaf_paths(data, parent_path: str = '', depth: int = 0, max_depth: int = 10) -> list:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –í–°–ï –ø—É—Ç–∏ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º, –≤–∫–ª—é—á–∞—è –≥–ª—É–±–æ–∫—É—é –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å."""
    if depth > max_depth:
        return []
    
    paths = []
    
    if isinstance(data, dict):
        for k, v in data.items():
            current_path = f"{parent_path}.{k}" if parent_path else k
            
            if isinstance(v, dict):
                # –í–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å - —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
                paths.extend(get_all_leaf_paths(v, current_path, depth + 1, max_depth))
            elif isinstance(v, list):
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º —Å–ø–∏—Å–æ–∫ –∫–∞–∫ –æ–ø—Ü–∏—é (–¥–ª—è chat-—Ñ–æ—Ä–º–∞—Ç–∞)
                paths.append(f"{current_path} [—Å–ø–∏—Å–æ–∫ –∏–∑ {len(v)} —ç–ª.]")
                # –†–∞—Å–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                if v:
                    if isinstance(v[0], dict):
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª—è –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–∏—Å–∫–∞
                        for inner_k, inner_v in v[0].items():
                            inner_path = f"{current_path}[].{inner_k}"
                            if isinstance(inner_v, dict):
                                paths.extend(get_all_leaf_paths(inner_v, inner_path, depth + 2, max_depth))
                            elif isinstance(inner_v, list):
                                paths.append(f"{inner_path} [—Å–ø–∏—Å–æ–∫]")
                                if inner_v and isinstance(inner_v[0], dict):
                                    paths.extend(get_all_leaf_paths(inner_v[0], f"{inner_path}[]", depth + 3, max_depth))
                            else:
                                paths.append(inner_path)
                    else:
                        # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤
                        paths.append(f"{current_path}[0]")
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                paths.append(current_path)
    
    return paths


def render_sft_main_config(data_path: str):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä SFT ‚Äî –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç + —Ä—É—á–Ω–æ–π –≤—ã–±–æ—Ä."""
    st.markdown("### üõ†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SFT")
    
    columns, sample = get_dataset_columns(data_path)
    
    if not sample:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏–ª–∏ –æ–Ω –ø—É—Å—Ç.")
        return {}
    
    # ===== –ê–í–¢–û–î–ï–¢–ï–ö–¢ –§–û–†–ú–ê–¢–ê =====
    def detect_chat_field(data: dict) -> tuple:
        """–ò—â–µ—Ç –ø–æ–ª–µ —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π (chat-—Ñ–æ—Ä–º–∞—Ç)."""
        for key, value in data.items():
            if isinstance(value, list) and value and isinstance(value[0], dict):
                first_item = value[0]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –ø–æ–ª—è –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ role/content
                has_role = any(k.lower() in ['role', 'from', 'type'] for k in first_item.keys())
                has_content = any(k.lower() in ['content', 'value', 'text', 'message'] for k in first_item.keys())
                if has_role and has_content:
                    return key, value
        return None, None
    
    chat_field, chat_value = detect_chat_field(sample)
    detected_format = "chat" if chat_field else "instruct"
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—É—Ç–∏ –¥–ª—è instruct —Ä–µ–∂–∏–º–∞
    all_paths = get_all_leaf_paths(sample)
    simple_fields = [k for k in sample.keys() if not isinstance(sample[k], (dict, list))]
    
    col_json, col_config = st.columns([1, 1])
    
    # ===== –õ–ï–í–ê–Ø –ö–û–õ–û–ù–ö–ê: JSON –ø—Ä–µ–≤—å—é =====
    with col_json:
        st.markdown("#### üìÑ –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
        with st.container(height=500):
            st.json(sample, expanded=True)
    
    # ===== –ü–†–ê–í–ê–Ø –ö–û–õ–û–ù–ö–ê: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è =====
    with col_config:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç –Ω–∞—à–µ–ª
        if detected_format == "chat":
            st.success(f"üîç **–ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç:** –Ω–∞–π–¥–µ–Ω Chat-—Ñ–æ—Ä–º–∞—Ç –≤ –ø–æ–ª–µ `{chat_field}`")
        else:
            st.info("üîç **–ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç:** Instruct-—Ñ–æ—Ä–º–∞—Ç (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è)")
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Ä–µ–∂–∏–º–∞
        format_choice = st.radio(
            "–§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö:",
            ["üí¨ Chat (—Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π)", "üìù Instruct (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è)"],
            index=0 if detected_format == "chat" else 1,
            key="sft_format_choice",
            horizontal=True
        )
        
        is_chat = "Chat" in format_choice
        
        st.markdown("---")
        
        sft_columns = {}
        
        if is_chat:
            # ===== CHAT –†–ï–ñ–ò–ú =====
            st.markdown("#### üí¨ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chat-—Ñ–æ—Ä–º–∞—Ç–∞")
            
            # –í—ã–±–æ—Ä –ø–æ–ª—è —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π
            list_fields = [k for k, v in sample.items() if isinstance(v, list) and v and isinstance(v[0], dict)]
            
            if not list_fields:
                st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ–π —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π!")
                return {}
            
            messages_field = st.selectbox(
                "üìã –ü–æ–ª–µ —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏:",
                list_fields,
                index=list_fields.index(chat_field) if chat_field in list_fields else 0,
                key="sft_messages_field"
            )
            
            messages = sample[messages_field]
            first_msg = messages[0]
            inner_fields = list(first_msg.keys())
            
            st.caption(f"–ù–∞–π–¥–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π
            c1, c2 = st.columns(2)
            
            role_guess = next((f for f in inner_fields if f.lower() in ['role', 'from', 'type']), inner_fields[0])
            content_guess = next((f for f in inner_fields if f.lower() in ['content', 'value', 'text', 'message']), inner_fields[-1])
            
            role_field = c1.selectbox(
                "–ü–æ–ª–µ **—Ä–æ–ª–∏**:",
                inner_fields,
                index=inner_fields.index(role_guess) if role_guess in inner_fields else 0,
                key="sft_chat_role"
            )
            content_field = c2.selectbox(
                "–ü–æ–ª–µ **—Ç–µ–∫—Å—Ç–∞**:",
                inner_fields,
                index=inner_fields.index(content_guess) if content_guess in inner_fields else 0,
                key="sft_chat_content"
            )
            
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–æ–ª–∏
            unique_roles = sorted(set(str(m.get(role_field, "")) for m in messages))
            st.caption(f"–†–æ–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: `{', '.join(unique_roles)}`")
            
            # –ú–∞–ø–ø–∏–Ω–≥ —Ä–æ–ª–µ–π
            st.markdown("**–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–æ–ª–µ–π:**")
            c1, c2, c3 = st.columns(3)
            
            sys_guess = next((r for r in unique_roles if 'system' in r.lower()), None)
            user_guess = next((r for r in unique_roles if r.lower() in ['user', 'human']), unique_roles[0] if unique_roles else "")
            asst_guess = next((r for r in unique_roles if r.lower() in ['assistant', 'gpt', 'bot']), unique_roles[-1] if len(unique_roles) > 1 else "")
            
            role_system = c1.selectbox("‚öôÔ∏è System =", ["(–Ω–µ—Ç)"] + unique_roles,
                index=(unique_roles.index(sys_guess) + 1) if sys_guess in unique_roles else 0, key="sft_map_sys")
            role_user = c2.selectbox("üë§ User =", unique_roles,
                index=unique_roles.index(user_guess) if user_guess in unique_roles else 0, key="sft_map_user")
            role_assistant = c3.selectbox("ü§ñ Assistant =", unique_roles,
                index=unique_roles.index(asst_guess) if asst_guess in unique_roles else 0, key="sft_map_asst")
            
            sft_columns = {
                "format": "chat",
                "messages_path": messages_field,
                "role_field": role_field,
                "content_field": content_field,
                "role_system": role_system if role_system != "(–Ω–µ—Ç)" else "",
                "role_user": role_user,
                "role_assistant": role_assistant
            }
            
        else:
            # ===== INSTRUCT –†–ï–ñ–ò–ú =====
            st.markdown("#### üìù –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Instruct-—Ñ–æ—Ä–º–∞—Ç–∞")
            st.caption("–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–ª—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–æ–ª–∏:")
            
            # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—É—Ç–∏
            field_options = ["(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)"] + all_paths
            
            system_path = st.selectbox("‚öôÔ∏è **System** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):", field_options, index=0, key="sft_inst_sys")
            user_path = st.selectbox("üë§ **User** (–≤–æ–ø—Ä–æ—Å/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è):", field_options, index=0, key="sft_inst_user")
            assistant_path = st.selectbox("ü§ñ **Assistant** (–æ—Ç–≤–µ—Ç):", field_options, index=0, key="sft_inst_asst")
            
            if user_path == "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" or assistant_path == "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)":
                st.warning("üëÜ –í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–ª—è **User** –∏ **Assistant**")
                return {}
            
            sft_columns = {
                "format": "instruct",
                "instruction": user_path,
                "output": assistant_path,
                "system_field": system_path if system_path != "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" else ""
            }
        
        # ===== –ù–ê–°–¢–†–û–ô–ö–ò –®–ê–ë–õ–û–ù–ê =====
        st.markdown("---")
        with st.expander("üè∑Ô∏è –¢–µ–≥–∏ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", expanded=False):
            default_system = st.text_input("System prompt (–ø–æ —É–º–æ–ª—á.):", "You are a helpful assistant.", key="sft_def_sys")
            tc1, tc2 = st.columns(2)
            user_tag = tc1.text_input("User tag:", "### User:", key="sft_tag_user")
            assistant_tag = tc2.text_input("Assistant tag:", "### Assistant:", key="sft_tag_asst")
        
        if 'default_system' not in dir():
            default_system, user_tag, assistant_tag = "You are a helpful assistant.", "### User:", "### Assistant:"
        
        sft_template = {
            "system": default_system,
            "separator": "\n\n",
            "user_tag": user_tag,
            "bot_tag": assistant_tag
        }
        
        # ===== –ü–†–ï–í–¨–Æ =====
        st.markdown("---")
        st.markdown("#### üëÅÔ∏è –ü—Ä–µ–≤—å—é:")
        
        try:
            sep = "\n\n"
            preview = ""
            
            if sft_columns["format"] == "chat":
                messages = sample[sft_columns["messages_path"]]
                sys_text = default_system
                
                for msg in messages:
                    role = str(msg.get(sft_columns["role_field"], ""))
                    content = str(msg.get(sft_columns["content_field"], ""))
                    
                    if role == sft_columns["role_system"]:
                        sys_text = content
                    elif role == sft_columns["role_user"]:
                        preview += f"{user_tag}\n{content[:200]}{'...' if len(content) > 200 else ''}{sep}"
                    elif role == sft_columns["role_assistant"]:
                        preview += f"{assistant_tag}\n{content[:200]}{'...' if len(content) > 200 else ''}{sep}"
                
                preview = f"{sys_text}{sep}" + preview + "<|endoftext|>"
            else:
                user_val = str(get_nested_value(sample, sft_columns["instruction"]) or "")[:300]
                asst_val = str(get_nested_value(sample, sft_columns["output"]) or "")[:300]
                
                # System prompt: —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Å–µ–º–ø–ª–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –ø–æ–ª–µ
                sys_val = default_system
                system_field = sft_columns.get("system_field")
                
                # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –ø–æ–ª–µ system_field, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å–µ–º–ø–ª–∞
                if system_field and system_field != "(–Ω–µ –≤—ã–±—Ä–∞–Ω–æ)" and system_field.strip():
                    field_sys = get_nested_value(sample, system_field)
                    # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ –∏ –Ω–µ –ø—É—Å—Ç–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –≤–º–µ—Å—Ç–æ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ
                    if field_sys is not None:
                        field_sys_str = str(field_sys).strip()
                        if field_sys_str:
                            sys_val = field_sys_str[:200]
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≤—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ –Ω–∞—á–∞–ª–µ
                # –í–ê–ñ–ù–û: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∏–¥–µ–Ω, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
                preview = f"{sys_val}{sep}{user_tag}\n{user_val}{sep}{assistant_tag}\n{asst_val}<|endoftext|>"
            
            with st.container(height=400):
                st.code(preview, language=None)
            
            st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞: {e}")

    return {"sft_columns": sft_columns, "sft_template": sft_template}


# ============================================================================
# GRPO Configuration
# ============================================================================

def render_grpo_sidebar_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.subheader("üß† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã GRPO")
    
    # –ê–ª–≥–æ—Ä–∏—Ç–º
    algorithm = st.sidebar.selectbox(
        "–ê–ª–≥–æ—Ä–∏—Ç–º",
        ["grpo", "drgrpo", "dapo"],
        format_func=lambda x: {
            "grpo": "GRPO (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)",
            "drgrpo": "Dr.GRPO (—É–ª—É—á—à–µ–Ω–Ω—ã–π)",
            "dapo": "DAPO (–ø–æ–ª–Ω—ã–π)",
        }[x],
        help="""
        **GRPO**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Group Relative Policy Optimization
        **Dr.GRPO**: –ë–µ–∑ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ std, —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        **DAPO**: + –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥, + dynamic sampling
        """
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    group_size = st.sidebar.slider(
        "Group size (G)",
        min_value=2,
        max_value=32,
        value=8,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç"
    )
    
    max_new_tokens = st.sidebar.slider(
        "Max new tokens",
        min_value=128,
        max_value=4096,
        value=1024,
        step=128,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"
    )
    
    temperature = st.sidebar.slider(
        "Temperature",
        min_value=0.1,
        max_value=2.0,
        value=0.7,
        step=0.1,
        help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    grpo_learning_rate = st.sidebar.select_slider(
        "Learning Rate (GRPO)",
        options=[1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5],
        value=5e-6,
        format_func=lambda x: f"{x:.0e}",
        help="–î–ª—è RL –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–µ–Ω—å—à–∏–π LR —á–µ–º –¥–ª—è SFT"
    )
    
    max_steps = st.sidebar.number_input(
        "Max steps",
        min_value=10,
        max_value=10000,
        value=500,
        step=50,
    )
    
    epochs_per_step = st.sidebar.slider(
        "Epochs per step",
        min_value=1,
        max_value=5,
        value=1,
        help="–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ rollout'–æ–≤"
    )
    
    # KL
    kl_weight = st.sidebar.slider(
        "KL weight",
        min_value=0.0,
        max_value=0.1,
        value=0.0,
        step=0.01,
        help="–í–µ—Å KL-—à—Ç—Ä–∞—Ñ–∞. –î–ª—è reasoning –æ–±—ã—á–Ω–æ 0"
    )
    
    # –ö–ª–∏–ø–ø–∏–Ω–≥
    with st.sidebar.expander("‚öôÔ∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"):
        clip_eps_low = st.slider("Clip Œµ (low)", 0.1, 0.3, 0.2, 0.01)
        clip_eps_high = st.slider(
            "Clip Œµ (high)", 
            0.1, 0.4, 
            0.28 if algorithm == "dapo" else 0.2, 
            0.01,
            help="DAPO —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç 0.28 –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã"
        )
        
        dynamic_sampling = st.checkbox(
            "Dynamic sampling",
            value=algorithm == "dapo",
            help="–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –≥—Ä—É–ø–ø—ã —Å –Ω—É–ª–µ–≤—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–º"
        )
        
        token_level_loss = st.checkbox(
            "Token-level loss",
            value=algorithm == "dapo",
            help="–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å loss –ø–æ —Ç–æ–∫–µ–Ω–∞–º, –∞ –Ω–µ –ø–æ —Å—ç–º–ø–ª–∞–º"
        )
    
    return {
        "grpo_algorithm": algorithm,
        "grpo_group_size": group_size,
        "grpo_max_new_tokens": max_new_tokens,
        "grpo_temperature": temperature,
        "grpo_learning_rate": grpo_learning_rate,
        "grpo_max_steps": max_steps,
        "grpo_epochs_per_step": epochs_per_step,
        "grpo_kl_weight": kl_weight,
        "grpo_clip_eps_low": clip_eps_low,
        "grpo_clip_eps_high": clip_eps_high,
        "grpo_dynamic_sampling": dynamic_sampling,
        "grpo_token_level_loss": token_level_loss,
    }


def render_grpo_main_config(data_path: str = None):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GRPO –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ (reward —Ñ—É–Ω–∫—Ü–∏–∏)."""
    st.markdown("### üéØ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Reward —Ñ—É–Ω–∫—Ü–∏–π")
    
    st.info("""
    **Reward —Ñ—É–Ω–∫—Ü–∏–∏** –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç, –∫–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏.
    –ú–æ–∂–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏–π —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏.
    """)
    
    # –î–∞—Ç–∞—Å–µ—Ç
    st.markdown("#### üìö –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    
    dataset_source = st.radio(
        "–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö",
        ["GSM8K (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞)", "–°–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç (JSONL)"],
        horizontal=True,
    )
    
    grpo_dataset_path = None
    grpo_max_samples = None
    
    if dataset_source == "GSM8K (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞)":
        st.caption("GSM8K ‚Äî –¥–∞—Ç–∞—Å–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è reasoning")
        grpo_max_samples = st.number_input(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤",
            min_value=10,
            max_value=10000,
            value=500,
            step=50,
        )
    else:
        if data_path and data_path.endswith(".jsonl"):
            grpo_dataset_path = data_path
            st.success(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±—Ä–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: `{Path(data_path).name}`")
        else:
            grpo_dataset_path = st.text_input(
                "–ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É",
                placeholder="/path/to/dataset.jsonl",
                help="JSONL —Ñ–∞–π–ª —Å –ø–æ–ª—è–º–∏: prompt (–∏–ª–∏ question), answer"
            )
    
    st.markdown("---")
    st.markdown("#### üèÜ Reward —Ñ—É–Ω–∫—Ü–∏–∏")
    
    # –ü—Ä–µ—Å–µ—Ç—ã reward —Ñ—É–Ω–∫—Ü–∏–π
    reward_preset = st.selectbox(
        "–ü—Ä–µ—Å–µ—Ç",
        [
            "üßÆ –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (GSM8K)",
            "üí¨ –û–±—â–∏–π reasoning",
            "üîß –ö–∞—Å—Ç–æ–º–Ω—ã–π",
        ],
        help="–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–µ—Å–µ—Ç –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ reward —Ñ—É–Ω–∫—Ü–∏–∏ –≤—Ä—É—á–Ω—É—é"
    )
    
    reward_configs = []
    
    if reward_preset == "üßÆ –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (GSM8K)":
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("**‚úÖ Format Check**")
            st.caption("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è <reasoning> –∏ <answer> —Ç–µ–≥–æ–≤")
            format_weight = st.slider("–í–µ—Å", 0.0, 2.0, 1.0, 0.1, key="format_w")
        with col2:
            st.markdown("**‚úÖ Reasoning Quality**")
            st.caption("–ë–æ–Ω—É—Å –∑–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–µ reasoning")
            reasoning_weight = st.slider("–í–µ—Å", 0.0, 2.0, 0.5, 0.1, key="reasoning_w")
        with col3:
            st.markdown("**‚úÖ Math Correctness**")
            st.caption("–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —á–∏—Å–ª–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")
            correctness_weight = st.slider("–í–µ—Å", 0.0, 5.0, 2.0, 0.1, key="correctness_w")
        
        reward_configs = [
            {"type": "format", "weight": format_weight},
            {"type": "reasoning_quality", "weight": reasoning_weight},
            {"type": "gsm8k_correctness", "weight": correctness_weight},
        ]
        
    elif reward_preset == "üí¨ –û–±—â–∏–π reasoning":
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**‚úÖ Format Check**")
            format_weight = st.slider("–í–µ—Å", 0.0, 2.0, 1.0, 0.1, key="format_w2")
        with col2:
            st.markdown("**‚úÖ Reasoning Quality**")
            reasoning_weight = st.slider("–í–µ—Å", 0.0, 2.0, 1.0, 0.1, key="reasoning_w2")
        
        reward_configs = [
            {"type": "format", "weight": format_weight},
            {"type": "reasoning_quality", "weight": reasoning_weight},
        ]
        
    else:  # –ö–∞—Å—Ç–æ–º–Ω—ã–π
        st.markdown("–î–æ–±–∞–≤—å—Ç–µ reward —Ñ—É–Ω–∫—Ü–∏–∏:")
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π
        if "custom_rewards" not in st.session_state:
            st.session_state.custom_rewards = [{"type": "format", "weight": 1.0}]
        
        reward_types = {
            "format": "Format Check (—Ç–µ–≥–∏ <reasoning>, <answer>)",
            "reasoning_quality": "Reasoning Quality (–¥–ª–∏–Ω–∞ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ)",
            "gsm8k_correctness": "GSM8K Correctness (—á–∏—Å–ª–æ–≤–æ–π –æ—Ç–≤–µ—Ç)",
            "exact_match": "Exact Match (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)",
            "contains_answer": "Contains Answer (–æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è)",
            "length_penalty": "Length Penalty (—à—Ç—Ä–∞—Ñ –∑–∞ –¥–ª–∏–Ω—É)",
        }
        
        for i, reward in enumerate(st.session_state.custom_rewards):
            col1, col2, col3 = st.columns([3, 1, 1])
            with col1:
                reward["type"] = st.selectbox(
                    f"–§—É–Ω–∫—Ü–∏—è {i+1}",
                    options=list(reward_types.keys()),
                    format_func=lambda x: reward_types.get(x, x),
                    index=list(reward_types.keys()).index(reward["type"]) if reward["type"] in reward_types else 0,
                    key=f"reward_type_{i}",
                )
            with col2:
                reward["weight"] = st.number_input(
                    "–í–µ—Å",
                    min_value=0.0,
                    max_value=10.0,
                    value=reward.get("weight", 1.0),
                    step=0.1,
                    key=f"reward_weight_{i}",
                )
            with col3:
                if st.button("üóëÔ∏è", key=f"remove_reward_{i}"):
                    st.session_state.custom_rewards.pop(i)
                    st.rerun()
        
        if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é"):
            st.session_state.custom_rewards.append({"type": "format", "weight": 1.0})
            st.rerun()
        
        reward_configs = st.session_state.custom_rewards.copy()
    
    # –§–æ—Ä–º–∞—Ç reasoning
    st.markdown("---")
    st.markdown("#### üìù –§–æ—Ä–º–∞—Ç Reasoning")
    
    reasoning_format = st.selectbox(
        "–§–æ—Ä–º–∞—Ç —Ç–µ–≥–æ–≤",
        ["deepseek", "simple", "russian"],
        format_func=lambda x: {
            "deepseek": "DeepSeek (<think>...</think>, <answer>...</answer>)",
            "simple": "Simple (<reasoning>...</reasoning>, <answer>...</answer>)",
            "russian": "Russian (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ)",
        }[x],
    )
    
    # –ü—Ä–µ–≤—å—é —Ñ–æ—Ä–º–∞—Ç–∞
    format_examples = {
        "deepseek": """<think>
–î–∞–Ω–æ: ... 
–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏: ...
–†–µ—à–µ–Ω–∏–µ: ...
</think>
<answer>
42
</answer>""",
        "simple": """<reasoning>
–®–∞–≥ 1: ...
–®–∞–≥ 2: ...
</reasoning>
<answer>
42
</answer>""",
        "russian": """<reasoning>
–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: ...
–í—ã—á–∏—Å–ª–µ–Ω–∏—è: ...
</reasoning>
<answer>
42
</answer>""",
    }
    
    with st.expander("üìã –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞"):
        st.code(format_examples[reasoning_format], language=None)
    
    return {
        "grpo_dataset_source": dataset_source,
        "grpo_dataset_path": grpo_dataset_path,
        "grpo_max_samples": grpo_max_samples,
        "grpo_reward_configs": reward_configs,
        "grpo_reasoning_format": reasoning_format,
    }


def render_model_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.header("üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –†–µ–∂–∏–º")
    
    # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    stage_options = {
        "pretrain": "Pretraining (—Å –Ω—É–ª—è)",
        "continual_pretrain": "Continual Pretraining (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)",
        "sft": "SFT (Fine-Tuning)",
        "grpo": "üß† GRPO (RL –¥–ª—è Reasoning)"
    }
    selected_stage = st.sidebar.selectbox(
        "–≠—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è",
        options=list(stage_options.keys()),
        format_func=lambda x: stage_options[x],
        help="–í—ã–±–µ—Ä–∏—Ç–µ —ç—Ç–∞–ø: –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è, –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ pretrain, –¥–æ–æ–±—É—á–µ–Ω–∏–µ (SFT) –∏–ª–∏ RL –æ–±—É—á–µ–Ω–∏–µ (GRPO)"
    )
    
    # –ò–º—è –º–æ–¥–µ–ª–∏ (–¥–ª—è –ø–∞–ø–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
    if selected_stage == "pretrain":
        model_name_default = "home_pretrain"
    elif selected_stage == "continual_pretrain":
        model_name_default = "home_continual_pretrain"
    elif selected_stage == "grpo":
        model_name_default = "home_grpo"
    else:
        model_name_default = "home_sft"
    model_name = st.sidebar.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", value=model_name_default, help="–ò–º—è –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    base_model_path = None
    
    if selected_stage in ("sft", "continual_pretrain", "grpo"):
        stage_label = {"sft": "SFT", "continual_pretrain": "Continual Pretraining", "grpo": "GRPO"}.get(selected_stage, selected_stage)
        st.sidebar.subheader("üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        available = get_available_models()
        
        if selected_stage == "continual_pretrain":
            # –î–ª—è continual_pretrain —Ñ–∏–ª—å—Ç—Ä—É–µ–º: –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º final/export –º–æ–¥–µ–ª–∏ –∏ HF –º–æ–¥–µ–ª–∏
            # Checkpoint'—ã —Ç–æ–∂–µ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã (–¥–ª—è resume), –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
            hf_models = [m for m in available if m["type"] == "hf"]
            final_models = [m for m in available if m["type"] == "final"]
            checkpoint_models = [m for m in available if m["type"] == "checkpoint"]
            
            if hf_models or final_models:
                st.sidebar.info("üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ü§ó HF –º–æ–¥–µ–ª—å –∏–ª–∏ final_model –¥–ª—è continual pretraining")
                available_filtered = hf_models + final_models + checkpoint_models
            else:
                if checkpoint_models:
                    st.sidebar.warning(
                        "‚ö†Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ checkpoint'—ã. –î–ª—è resume —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, "
                        "–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ continual pretraining –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å final_model."
                    )
                available_filtered = checkpoint_models if checkpoint_models else available
        else:
            # –î–ª—è SFT –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ (HF –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–º–∏)
            hf_models = [m for m in available if m["type"] == "hf"]
            other_models = [m for m in available if m["type"] != "hf"]
            available_filtered = hf_models + other_models
        
        if not available_filtered:
            st.sidebar.warning(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è {stage_label}. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ ü§ñ –ú–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–±—É—á–∏—Ç–µ Pretrain!")
            # –ú–æ–∂–Ω–æ –¥–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–≤–µ—Å—Ç–∏ –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é
            base_model_path = st.sidebar.text_input("–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –≤—Ä—É—á–Ω—É—é", placeholder="/path/to/model")
        else:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–ø—Ü–∏–π —Å –ø–æ–º–µ—Ç–∫–∞–º–∏ —Ç–∏–ø–æ–≤
            def get_model_label(m):
                if m["type"] == "hf":
                    return m["name"]  # –£–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ü§ó
                elif m["type"] == "final":
                    return f"{m['name']} (‚úÖ final)"
                else:
                    return f"{m['name']} (‚ö†Ô∏è checkpoint)"
            
            model_options = [get_model_label(m) for m in available_filtered]
            
            selected_base_name = st.sidebar.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å", 
                options=model_options,
                help="ü§ó ‚Äî –º–æ–¥–µ–ª–∏ —Å HuggingFace, ‚úÖ final ‚Äî –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, ‚ö†Ô∏è checkpoint ‚Äî –¥–ª—è resume"
            )
            
            # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å –ø–æ –∏–Ω–¥–µ–∫—Å—É (model_options –∏ available_filtered —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥—É)
            selected_idx = model_options.index(selected_base_name)
            selected_model = available_filtered[selected_idx]
            base_model_path = selected_model["path"]
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è checkpoint –≤ continual_pretrain
            if selected_stage == "continual_pretrain" and selected_model["type"] == "checkpoint":
                st.sidebar.info(
                    "‚ÑπÔ∏è –í—ã–±—Ä–∞–Ω checkpoint. –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω resume (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ scheduler). "
                    "–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–æ–≤–æ–≥–æ continual pretraining –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å final_model –∏–ª–∏ ü§ó HF –º–æ–¥–µ–ª—å."
                )
            elif selected_model["type"] == "hf":
                st.sidebar.success("‚úÖ HuggingFace –º–æ–¥–µ–ª—å ‚Äî –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è Continual Pretrain / SFT!")
            
            st.sidebar.caption(f"–ü—É—Ç—å: `{base_model_path}`")
    
    # –§–ª–∞–≥, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    loaded_config = None
    
    if selected_stage in ("sft", "continual_pretrain", "grpo") and base_model_path:
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥
        # –í–ê–ñ–ù–û: —Ä–∞–∑–ª–∏—á–∞–µ–º run_config.json (training params) –∏ config.json (model params)
        try:
            base_path = Path(base_model_path)
            cfg_path = None
            cfg_type = None  # "run" –∏–ª–∏ "model"
            
            # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º run_config.json (–ø–æ–ª–Ω—ã–π training config)
            # –î–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: checkpoint_step6800 -> parent/run_config.json
            if (base_path.parent / "run_config.json").exists():
                cfg_path = base_path.parent / "run_config.json"
                cfg_type = "run"
            elif (base_path / "run_config.json").exists():
                cfg_path = base_path / "run_config.json"
                cfg_type = "run"
            # 2. –ï—Å–ª–∏ run_config –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º config.json (—Ç–æ–ª—å–∫–æ model params)
            elif (base_path / "config.json").exists():
                cfg_path = base_path / "config.json"
                cfg_type = "model"
            
            if cfg_path and cfg_path.exists():
                with open(cfg_path) as f:
                    loaded_config = json.load(f)
                if cfg_type == "run":
                    st.sidebar.success("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ run_config.json")
                else:
                    # config.json —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–∏
                    # transformers config -> –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
                    if "num_hidden_layers" in loaded_config and "num_layers" not in loaded_config:
                        loaded_config["num_layers"] = loaded_config["num_hidden_layers"]
                    if "num_attention_heads" in loaded_config and "n_heads" not in loaded_config:
                        loaded_config["n_heads"] = loaded_config["num_attention_heads"]
                    if "max_position_embeddings" in loaded_config and "seq_len" not in loaded_config:
                        loaded_config["seq_len"] = loaded_config["max_position_embeddings"]
                    st.sidebar.info("‚ÑπÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ config.json (training params –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)")
            else:
                st.sidebar.warning("‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ä—É—á–Ω—É—é")
        except Exception as e:
             st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è config: {e}")

    st.sidebar.subheader("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")

    blueprint_path = ""
    model_type = "HomeModel (GPT-2 style)"  # default
    # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    default_h, default_l, default_n, default_seq = 512, 8, 8, 2048

    # –í—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢–û–õ–¨–ö–û –¥–ª—è pretrain (—Å –Ω—É–ª—è)
    # –î–ª—è SFT/Continual Pretrain –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
    if not loaded_config:
        # === –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° VISUAL MODEL BUILDER ===
        blueprints_dir = PROJECT_ROOT / "blueprints"
        blueprints_dir.mkdir(exist_ok=True)
        blueprints = list(blueprints_dir.glob("*.json"))
            
        arch_options = ["HomeModel (GPT-2 style)", "Llama (Custom)", "Mistral (Custom)", "Custom Blueprint (Visual Builder)"]
        
        model_type = st.sidebar.selectbox(
            "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏",
            options=arch_options,
            index=0,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É. Custom Blueprint –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑ Visual Builder."
        )

    if not loaded_config and model_type == "Custom Blueprint (Visual Builder)":
        blueprints_dir = PROJECT_ROOT / "blueprints"
        blueprints = list(blueprints_dir.glob("*.json"))
        # –õ–æ–≥–∏–∫–∞ –¥–ª—è Blueprint –ø—Ä–æ–µ–∫—Ç–æ–≤
        if blueprints:
            bp_names = [b.name for b in blueprints]
            selected_bp = st.sidebar.selectbox("–ü—Ä–æ–µ–∫—Ç (Visual Builder)", bp_names)
            blueprint_path = str(blueprints_dir / selected_bp)
        else:
            st.sidebar.warning("Blueprint –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–ø–∫–µ `blueprints/`. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç –≤ Visual Model Builder –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é.")
            blueprint_path = st.sidebar.text_input("–ü—É—Ç—å –∫ blueprint.json", value=str(blueprints_dir / "model.json"))
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ –∏–∑ blueprint
        try:
            with open(blueprint_path) as f:
                bp_data = json.load(f)
            st.sidebar.info(f"Blocks: {len(bp_data.get('blocks', []))} | Hidden: {bp_data.get('hidden_size')} | Vocab: {bp_data.get('vocab_size')}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç—ã –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–æ–≤ –Ω–∏–∂–µ (—á—Ç–æ–±—ã –æ–Ω–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏)
            default_h = bp_data.get("hidden_size", 512)
            default_seq = bp_data.get("max_position_embeddings", 2048)
            # –°–ª–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–∏ –∏–∑ Repeater, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –≥—Ä—É–±–æ
            default_l = len(bp_data.get("blocks", []))
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            st.sidebar.markdown("**–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä**")
            tokenizer_mode = st.sidebar.radio("Tokenizer Source", ["Standard (GPT-2)", "HF Repo", "Local Path"], horizontal=True)
            if tokenizer_mode == "Standard (GPT-2)":
                tokenizer_path = "gpt2"
            elif tokenizer_mode == "HF Repo":
                tokenizer_path = st.sidebar.text_input("HF ID", "meta-llama/Llama-2-7b-hf")
            else:
                tokenizer_path = st.sidebar.text_input("Local Path", str(PROJECT_ROOT / "tokenizers/my_tok"))
                
        except Exception as e:
            st.sidebar.error(f"Invalid Blueprint: {e}")
            tokenizer_path = "gpt2"
    elif not loaded_config:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è pretrain
        tokenizer_path = None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ
    else:
        # –î–ª—è SFT/Continual Pretrain —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±–µ—Ä–µ—Ç—Å—è –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        tokenizer_path = None

    if loaded_config:
        # –†–µ–∂–∏–º –¥–ª—è SFT/Continual Pretrain —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º
        # –°–ª–∞–π–¥–µ—Ä—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã
        disabled_sliders = True
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö –∏–º–µ–Ω –∫–ª—é—á–µ–π)
        hidden_size = loaded_config.get("hidden_size", 512)
        # num_hidden_layers - HF, num_layers - –Ω–∞—à –∫–æ–Ω—Ñ–∏–≥
        num_layers = loaded_config.get("num_hidden_layers", loaded_config.get("num_layers", 8))
        num_attention_heads = loaded_config.get("num_attention_heads", loaded_config.get("n_heads", 8))
        max_position_embeddings = loaded_config.get("max_position_embeddings", loaded_config.get("seq_len", 2048))
        
        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç—Å—è
        n_heads = num_attention_heads
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        base_model_type = loaded_config.get("model_type", loaded_config.get("architectures", ["Unknown"])[0] if "architectures" in loaded_config else "HomeModel")
        if isinstance(base_model_type, list):
            base_model_type = base_model_type[0] if base_model_type else "Unknown"
        st.sidebar.markdown(f"**–¢–∏–ø –º–æ–¥–µ–ª–∏:** `{base_model_type}`")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        c1, c2 = st.sidebar.columns(2)
        c1.metric("Hidden Size", hidden_size)
        c2.metric("Layers", num_layers)
        c1.metric("Heads", n_heads)
        c2.metric("Max Context", f"{max_position_embeddings:,}")
        
        st.sidebar.info("üîí –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞ (–æ—Ç –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏)")
        
        # seq_len –ú–û–ñ–ù–û –º–µ–Ω—è—Ç—å - —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –º–µ–Ω—å—à–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        st.sidebar.markdown("---")
        st.sidebar.markdown("**‚öôÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è**")
        
        # –û–ø—Ü–∏–∏ seq_len: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ + –º–∞–∫—Å–∏–º—É–º –º–æ–¥–µ–ª–∏
        seq_len_opts = [512, 1024, 2048, 4096, 8192]
        if max_position_embeddings not in seq_len_opts:
            seq_len_opts.append(max_position_embeddings)
        seq_len_opts = sorted([s for s in seq_len_opts if s <= max_position_embeddings])
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048 –∏–ª–∏ –º–∞–∫—Å–∏–º—É–º –µ—Å–ª–∏ –º–µ–Ω—å—à–µ
        default_seq = min(2048, max_position_embeddings)
        default_idx = seq_len_opts.index(default_seq) if default_seq in seq_len_opts else len(seq_len_opts) - 1
        
        seq_len = st.sidebar.selectbox(
            "Seq Length (–æ–±—É—á–µ–Ω–∏–µ)",
            seq_len_opts,
            index=default_idx,
            help=f"–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–∞–∫—Å. –º–æ–¥–µ–ª–∏: {max_position_embeddings:,}. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏."
        )
        
        if seq_len < max_position_embeddings:
            st.sidebar.caption(f"üí° –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {seq_len}, –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ {max_position_embeddings:,}")
        
    else:
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        d_hid, d_layers = 512, 8
        
        if selected_stage == "sft":
            st.sidebar.caption("‚ö†Ô∏è –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é!")

        # –ü—Ä–µ—Å–µ—Ç—ã
        preset = st.sidebar.selectbox(
            "–ü—Ä–µ—Å–µ—Ç",
            ["Tiny (25M)", "Small (80M)", "Medium (200M)", "Large (400M)", "Custom"],
            index=0
        )
        
        presets = {
            "Tiny (25M)": (512, 8, 8),
            "Small (80M)": (768, 12, 12),
            "Medium (200M)": (1024, 16, 16),
            "Large (400M)": (1280, 20, 20),
        }
        
        if preset != "Custom" and preset in presets:
            default_h, default_l, default_n = presets[preset]
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ blueprint mode, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ blueprint –∫–∞–∫ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–æ–≤
            # –ù–æ –Ω–µ –º–µ–Ω—è–µ–º default_h/l/n –≥–ª–æ–±–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å –ª–æ–≥–∏–∫—É –ø—Ä–µ—Å–µ—Ç–æ–≤ –µ—Å–ª–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ
            if model_type != "Custom Blueprint (Visual Builder)":
                 default_h, default_l, default_n = 512, 8, 8
        
        # –°–ª–∞–π–¥–µ—Ä—ã (—Ç–µ–ø–µ—Ä—å –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª—é–ø—Ä–∏–Ω—Ç–∞ –∏–ª–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –º–µ–Ω—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã HomeModel)
        disabled_sliders = (model_type == "Custom Blueprint (Visual Builder)")
        
        hidden_size = st.sidebar.slider(
            "Hidden Size", 
            min_value=128, 
            max_value=2048, 
            value=default_h, 
            step=64,
            help="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è",
            disabled=disabled_sliders
        )
        
        num_layers = st.sidebar.slider(
            "Num Layers", 
            min_value=2, 
            max_value=32, 
            value=default_l,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞",
            disabled=disabled_sliders
        )
        
        n_heads = st.sidebar.slider(
            "Attention Heads", 
            min_value=2, 
            max_value=32, 
            value=default_n,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è",
            disabled=disabled_sliders
        )
        
        seq_len_opts = [512, 1024, 2048, 4096, 6144, 8192]
        if default_seq not in seq_len_opts: seq_len_opts.append(default_seq)
        seq_len_opts = sorted(seq_len_opts)
        
        seq_len = st.sidebar.selectbox(
            "Seq Length",
            seq_len_opts,
            index=seq_len_opts.index(default_seq) if default_seq in seq_len_opts else 0,
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
            disabled=disabled_sliders
        )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ hidden_size –∏ n_heads
    if not disabled_sliders and hidden_size % n_heads != 0:
        st.sidebar.error(f"‚ö†Ô∏è hidden_size ({hidden_size}) –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ n_heads ({n_heads}) –±–µ–∑ –æ—Å—Ç–∞—Ç–∫–∞!")
        valid_heads = [str(i) for i in range(1, min(33, hidden_size+1)) if hidden_size % i == 0][:10]
        if valid_heads:
            st.sidebar.info(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è n_heads: {', '.join(valid_heads)}")
    
    # –û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (HomeModel RoPE/SwiGLU)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º loaded_config –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –¥–µ—Ñ–æ–ª—Ç—ã
    vocab_size = int(loaded_config.get("vocab_size", 50257)) if loaded_config else 50257
    intermediate_size = int(loaded_config.get("intermediate_size") or (int(hidden_size) * 4)) if loaded_config else (int(hidden_size) * 4)
    est_params = estimate_parameters(
        hidden_size,
        num_layers,
        vocab_size=vocab_size,
        intermediate_size=intermediate_size,
    )
    st.sidebar.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (‚âà)", format_params(est_params))
    
    # Model ID –¥–ª—è pretrain from scratch (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è HF –º–æ–¥–µ–ª–µ–π)
    model_id = None
    if selected_stage == "pretrain":
        st.sidebar.subheader("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
        use_hf_model = st.sidebar.checkbox(
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å HuggingFace –º–æ–¥–µ–ª—å",
            value=False,
            help="–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å HF model_id –¥–ª—è pretrain from scratch"
        )
        if use_hf_model:
            model_id = st.sidebar.text_input(
                "HF Model ID",
                placeholder="gpt2, microsoft/DialoGPT-small, etc.",
                help="HuggingFace model ID –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –Ω—É–ª—è"
            )
            if model_id:
                st.sidebar.info(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {model_id}")
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                st.sidebar.warning(
                    "‚ö†Ô∏è **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å `trust_remote_code=True` –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å "
                    "—á—É–∂–æ–π –∫–æ–¥. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏."
                )
    
    # –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞ (full/LoRA/QLoRA)
    st.sidebar.subheader("üéØ –ú–µ—Ç–æ–¥ —Ç—é–Ω–∏–Ω–≥–∞")
    tuning_method = st.sidebar.selectbox(
        "–ú–µ—Ç–æ–¥",
        ["full", "lora", "qlora"],
        index=0,
        help="full: –ø–æ–ª–Ω—ã–π fine-tuning, lora: LoRA, qlora: QLoRA (4-bit + LoRA)"
    )
    
    lora_r = None
    lora_alpha = None
    lora_dropout = None
    lora_target_modules = None
    
    if tuning_method in ("lora", "qlora"):
        st.sidebar.markdown("**LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**")
        lora_r = st.sidebar.slider("LoRA r", min_value=4, max_value=128, value=16, step=4)
        lora_alpha = st.sidebar.slider("LoRA alpha", min_value=4, max_value=256, value=32, step=4)
        lora_dropout = st.sidebar.slider("LoRA dropout", min_value=0.0, max_value=0.5, value=0.1, step=0.05)
        
        lora_target_modules_input = st.sidebar.text_input(
            "Target modules (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            placeholder="q_proj,k_proj,v_proj,o_proj",
            help="–ú–æ–¥—É–ª–∏ –¥–ª—è LoRA (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é). –ï—Å–ª–∏ –ø—É—Å—Ç–æ - –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç"
        )
        if lora_target_modules_input:
            lora_target_modules = [m.strip() for m in lora_target_modules_input.split(",")]
    
    # –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
    config = {
        "stage": selected_stage,
        "base_model_path": base_model_path,
        "model_name_input": model_name,
        "model_id": model_id if model_id else None,
        "tuning_method": tuning_method,
        "lora_r": lora_r,
        "lora_alpha": lora_alpha,
        "lora_dropout": lora_dropout,
        "lora_target_modules": lora_target_modules,
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–∞–π–¥–µ—Ä–æ–≤ (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "n_heads": n_heads,
        "seq_len": seq_len,
    }

    if model_type == "Custom Blueprint (Visual Builder)":
        config["model_type"] = "blueprint"
        config["blueprint_path"] = blueprint_path
        if tokenizer_path:
             config["tokenizer_path"] = tokenizer_path
    else:
        config["model_type"] = "home"
        if "Llama" in model_type: config["arch_preset"] = "llama"
        if "Mistral" in model_type: config["arch_preset"] = "mistral"
        
    return config


def render_training_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
    
    batch_size = st.sidebar.slider(
        "Batch Size",
        min_value=1,
        max_value=256,
        value=4,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞"
    )
    
    grad_accum = st.sidebar.slider(
        "Gradient Accumulation",
        min_value=1,
        max_value=32,
        value=8,
        help="–®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞"
    )
    
    st.sidebar.caption(f"Effective batch: {batch_size * grad_accum}")
    
    learning_rate = st.sidebar.select_slider(
        "Learning Rate",
        options=[1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3],
        value=5e-4,
        format_func=lambda x: f"{x:.0e}"
    )

    lr_schedule_label = st.sidebar.selectbox(
        "LR scheduler",
        options=[
            "Cosine (with warmup)",
            "Linear (with warmup)",
            "Constant (with warmup)",
            "Cosine with Restarts (with warmup)",
        ],
        index=0,
        help=(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –º–µ–Ω—è—Ç—å learning rate –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è. "
            "–í–∞–∂–Ω–æ: scheduler —à–∞–≥–∞–µ—Ç –Ω–∞ update-step (–ø–æ—Å–ª–µ grad_accum), –Ω–µ –Ω–∞ –∫–∞–∂–¥–æ–º micro-batch."
        ),
    )
    lr_schedule_map = {
        "Cosine (with warmup)": "cosine",
        "Linear (with warmup)": "linear",
        "Constant (with warmup)": "constant_with_warmup",
        "Cosine with Restarts (with warmup)": "cosine_with_restarts",
    }
    lr_schedule = lr_schedule_map[lr_schedule_label]

    min_lr_ratio = st.sidebar.slider(
        "Min LR Ratio (Cosine floor)",
        min_value=0.0,
        max_value=0.2,
        value=0.0,
        step=0.01,
        help="0.0 = cosine –º–æ–∂–µ—Ç —É–π—Ç–∏ –ø–æ—á—Ç–∏ –≤ 0 –∫ –∫–æ–Ω—Ü—É. –ù–∞–ø—Ä–∏–º–µ—Ä 0.05 = –Ω–µ –Ω–∏–∂–µ 5% –æ—Ç base LR."
    )
    
    warmup_steps = st.sidebar.number_input(
        "Warmup Steps",
        min_value=0,
        max_value=10000,
        value=1000
    )

    scheduler_resync_on_resume = st.sidebar.checkbox(
        "Resync LR scheduler –ø—Ä–∏ resume (—Ñ–∏–∫—Å –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤)",
        value=True,
        help=(
            "–ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –±—ã–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å–æ scheduler, –∫–æ—Ç–æ—Ä—ã–π —à–∞–≥–∞–ª –ø–æ micro-batch (–∞ –Ω–µ –ø–æ update-step), "
            "LR –ø—Ä–∏ resume –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –ø–æ—á—Ç–∏ 0. –≠—Ç–æ—Ç —Ñ–ª–∞–≥ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç scheduler –Ω–∞ global_step."
        ),
    )
    
    # –í—ã–±–æ—Ä: epochs –∏–ª–∏ max_steps
    training_mode = st.sidebar.radio(
        "–†–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏",
        ["–ü–æ —ç–ø–æ—Ö–∞–º", "–ü–æ —à–∞–≥–∞–º"],
        help="–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏"
    )
    
    if training_mode == "–ü–æ —ç–ø–æ—Ö–∞–º":
        epochs = st.sidebar.number_input(
            "Epochs",
            min_value=1,
            max_value=10,
            value=1
        )
        max_steps = None
    else:
        epochs = 1
        max_steps = st.sidebar.number_input(
            "Max Steps",
            min_value=1,
            max_value=1000000,
            value=10000,
            step=1000,
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
        )
    
    mixed_precision = st.sidebar.selectbox(
        "Mixed Precision",
        ["no", "fp16", "bf16"],
        index=2,
        help="bf16 —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Ampere+ GPU"
    )

    # Sharding mode: –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥–≤–æ–π–Ω–æ–≥–æ —à–∞—Ä–¥–∏–Ω–≥–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É resume
    st.sidebar.divider()
    st.sidebar.subheader("üß© –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
    sharding_mode = st.sidebar.selectbox(
        "Sharding mode",
        options=["auto", "dataset", "accelerate"],
        index=0,
        help=(
            "auto: –¥–ª—è streaming (IterableDataset) –≤—ã–±–∏—Ä–∞–µ–º dataset-level —à–∞—Ä–¥–∏–Ω–≥ (—Å—Ç—Ä–æ–≥–æ –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å strict resume).\n"
            "dataset: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç —Å–∞–º –¥–∞—Ç–∞—Å–µ—Ç (shard=True), DataLoader –ù–ï –≥–æ—Ç–æ–≤–∏–º —á–µ—Ä–µ–∑ accelerate.\n"
            "accelerate: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç accelerate.prepare(DataLoader); —Å—Ç—Ä–æ–≥–∏–π resume –¥–ª—è streaming –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è."
        ),
    )
    
    grad_checkpoint = st.sidebar.checkbox(
        "Gradient Checkpointing",
        value=False,
        help="–≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ"
    )
    
    max_grad_norm = st.sidebar.number_input(
        "Max Gradient Norm",
        min_value=0.0,
        max_value=10.0,
        value=1.0,
        step=0.1,
        help="Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (0 = –æ—Ç–∫–ª—é—á–∏—Ç—å)"
    )
    
    # Validation / Eval
    st.sidebar.divider()
    st.sidebar.subheader("üìä –í–∞–ª–∏–¥–∞—Ü–∏—è")
    
    val_ratio = st.sidebar.slider(
        "Validation fraction",
        min_value=0.0,
        max_value=0.2,
        value=0.01,
        step=0.005,
        help="–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ validation, –µ—Å–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π val-—Ñ–∞–π–ª –Ω–µ –∑–∞–¥–∞–Ω"
    )
    
    eval_every = st.sidebar.number_input(
        "Eval Every N Steps",
        min_value=0,
        max_value=50000,
        value=200,
        step=10,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é (0 = –æ—Ç–∫–ª—é—á–∏—Ç—å)"
    )
    
    eval_batches = st.sidebar.number_input(
        "Eval Batches",
        min_value=1,
        max_value=500,
        value=20,
        step=1,
        help="–°–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –ø—Ä–æ–≥–æ–Ω—è—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ)"
    )
    
    return {
        "batch_size": batch_size,
        "gradient_accumulation": grad_accum,
        "learning_rate": learning_rate,
        "lr_schedule": lr_schedule,
        "min_lr_ratio": min_lr_ratio,
        "warmup_steps": warmup_steps,
        "scheduler_resync_on_resume": scheduler_resync_on_resume,
        "epochs": epochs,
        "max_steps": max_steps,
        "mixed_precision": mixed_precision,
        "grad_checkpoint": grad_checkpoint,
        "max_grad_norm": max_grad_norm,
        "sharding_mode": sharding_mode,
        "val_ratio": val_ratio,
        "eval_every": eval_every,
        "eval_batches": eval_batches,
    }


def render_dataset_config(stage="pretrain"):
    """–í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Ç–æ–ª—å–∫–æ –≤—ã–±–æ—Ä —Ñ–∞–π–ª–∞)."""
    st.sidebar.header("üìÅ –î–∞—Ç–∞—Å–µ—Ç")
    
    datasets = get_available_datasets()
    
    if datasets:
        dataset_options = [f"{name} ({size})" for name, size in datasets]
        selected = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç", dataset_options)
        selected_name = selected.split(" (")[0]
        data_path = str(DATASET_DIR / selected_name)
    else:
        st.sidebar.warning("–î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ datasets/")
        data_path = st.sidebar.text_input("–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É", "datasets/data.jsonl")
    
    return {"data_path": data_path}


def render_output_config(model_name="training_run"):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞."""
    st.sidebar.header("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å: out/{model_name}
    default_dir = f"out/{model_name}"
    
    output_dir = st.sidebar.text_input(
        "Output Directory (Experiment Root)",
        value=default_dir,
        help="–ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ —ç—Ç–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
    )
    
    save_every = st.sidebar.number_input(
        "Save Checkpoint Every N Steps",
        min_value=100,
        max_value=50000,
        value=200,
        step=100,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã"
    )

    export_on_checkpoint = st.sidebar.checkbox(
        "–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å final_model –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞",
        value=True,
        help=(
            "–ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å `final_model/` –Ω–∞ –∫–∞–∂–¥–æ–º checkpoint, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –±—ã–ª–æ —Å—Ä–∞–∑—É –≥—Ä—É–∑–∏—Ç—å –≤ —á–∞—Ç. "
            "–ú–∏–Ω—É—Å: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ."
        ),
    )
    
    log_every = st.sidebar.number_input(
        "Log Every N Steps",
        min_value=1,
        max_value=1000,
        value=10,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –º–µ—Ç—Ä–∏–∫–∏"
    )
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞—Ö
    output_path = PROJECT_ROOT / output_dir
    if output_path.exists():
        checkpoints = list(output_path.rglob("checkpoint_step*"))
        final_models = list(output_path.rglob("final_model"))
        final_model = final_models[0] if final_models else None
        
        if checkpoints or final_model:
            st.sidebar.caption(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {len(checkpoints)}")
            if final_model and final_model.exists():
                st.sidebar.caption("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    return {
        "output_dir": output_dir,
        "save_every": save_every,
        "export_on_checkpoint": export_on_checkpoint,
        "log_every": log_every,
        "tokenizer_path": "gpt2"
    }


def get_available_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫)."""
    models = []
    
    # 1. –ò—â–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤ out/ (–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)
    if OUTPUT_DIR.exists():
        # –ò—â–µ–º –ª—é–±—ã–µ config.json –≤–Ω—É—Ç—Ä–∏ out/
        for config_file in OUTPUT_DIR.rglob("config.json"):
            model_dir = config_file.parent
            
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–ø–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏)
            # –ö—Ä–∏—Ç–µ—Ä–∏–π –º–æ–¥–µ–ª–∏: –Ω–∞–ª–∏—á–∏–µ config.json + (pytorch_model.bin –∏–ª–∏ model.safetensors –∏–ª–∏ adapter_model.bin)
            has_weights = (
                (model_dir / "pytorch_model.bin").exists() or 
                (model_dir / "model.safetensors").exists() or
                (model_dir / "adapter_model.bin").exists()
            )
            
            if has_weights:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø (final –∏–ª–∏ checkpoint)
                m_type = "checkpoint" if "checkpoint" in model_dir.name else "final"
                if model_dir.name == "final_model": m_type = "final"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è
                # –ë–µ—Ä–µ–º –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ OUTPUT_DIR
                rel_path = model_dir.relative_to(OUTPUT_DIR)
                models.append({
                    "name": str(rel_path),
                    "path": str(model_dir),
                    "type": m_type,
                    "time": model_dir.stat().st_mtime
                })
    
    # 2. –ò—â–µ–º –≤ models/ (—Å–∫–∞—á–∞–Ω–Ω—ã–µ —Å HuggingFace)
    if MODELS_DIR.exists():
        for model_dir in MODELS_DIR.iterdir():
            if model_dir.is_dir():
                config_file = model_dir / "config.json"
                if config_file.exists():
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤
                    has_weights = (
                        (model_dir / "pytorch_model.bin").exists() or 
                        (model_dir / "model.safetensors").exists() or
                        any(model_dir.glob("*.safetensors")) or
                        any(model_dir.glob("pytorch_model*.bin"))
                    )
                    
                    if has_weights:
                        models.append({
                            "name": f"ü§ó {model_dir.name}",
                            "path": str(model_dir),
                            "type": "hf",  # HuggingFace –º–æ–¥–µ–ª—å
                            "time": model_dir.stat().st_mtime
                        })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
    models.sort(key=lambda x: x["time"], reverse=True)
    return models


def render_distributed_config(training_config: dict | None = None):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è distributed training."""
    st.sidebar.header("üñ•Ô∏è GPU –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    gpus = get_gpu_info()
    
    if gpus:
        st.sidebar.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ GPU: {len(gpus)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ GPU
        for gpu in gpus:
            st.sidebar.markdown(f"""
            **GPU {gpu['id']}**: {gpu['name']}  
            üìä VRAM: {gpu['memory_gb']} GB | CC: {gpu['compute_capability']}
            """)
        
        # –í—ã–±–æ—Ä GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        gpu_options = [f"GPU {g['id']}: {g['name']}" for g in gpus]
        if len(gpus) > 1:
            selected_gpus = st.sidebar.multiselect(
                "–í—ã–±–µ—Ä–∏—Ç–µ GPU",
                options=gpu_options,
                default=gpu_options,
                help="–í—ã–±–µ—Ä–∏—Ç–µ GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
            )
            num_gpus = len(selected_gpus)
            gpu_ids = [gpu_options.index(g) for g in selected_gpus]
        else:
            num_gpus = 1
            gpu_ids = [0]
            st.sidebar.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è GPU")
    else:
        st.sidebar.warning("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU")
        num_gpus = 0
        gpu_ids = []
    
    st.sidebar.markdown("---")
    
    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
    st.sidebar.subheader("‚ö° –¢–∏–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–µ–∂–∏–º
    if num_gpus == 0:
        available_modes = ["default"]
        recommended_idx = 0
    elif num_gpus == 1:
        available_modes = ["default", "deepspeed_zero3_offload"]
        recommended_idx = 0
    else:
        # –ü—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º multi_gpu –∏–ª–∏ fsdp
        available_modes = ["multi_gpu", "fsdp", "deepspeed_zero2", "deepspeed_zero3", "deepspeed_zero3_offload", "default"]
        recommended_idx = 0  # multi_gpu –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–ø—Ü–∏–∏ –¥–ª—è selectbox
    mode_options = []
    for i, mode in enumerate(available_modes):
        info = PARALLEL_TYPES[mode]
        label = f"{info['icon']} {info['name']}"
        if i == recommended_idx and num_gpus > 1:
            label += " ‚≠ê"  # –û—Ç–º–µ—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π
        mode_options.append(label)
    
    selected_mode_display = st.sidebar.selectbox(
        "–†–µ–∂–∏–º",
        options=mode_options,
        index=recommended_idx,
        help="–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
    )
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
    selected_idx = mode_options.index(selected_mode_display)
    selected_mode = available_modes[selected_idx]
    mode_info = PARALLEL_TYPES[selected_mode]
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ
    st.sidebar.markdown(f"""
    <div style="background: rgba(255,255,255,0.05); padding: 10px; border-radius: 8px; margin: 10px 0;">
    <b>–¢–∏–ø:</b> {mode_info['type']}<br>
    <small>{mode_info['description']}</small>
    </div>
    """, unsafe_allow_html=True)
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω single GPU –ø—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
    if num_gpus > 1 and selected_mode == "default":
        st.sidebar.warning(f"‚ö†Ô∏è –í—ã–±—Ä–∞–Ω Single GPU, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ {num_gpus} GPU. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º Multi-GPU!")
    
    # –ö–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª
    config_file = None
    if selected_mode != "default":
        config_path = CONFIGS_DIR / f"accelerate_{selected_mode}.yaml"
        if config_path.exists():
            config_file = str(config_path)
            st.sidebar.caption(f"üìÑ –ö–æ–Ω—Ñ–∏–≥: `{config_path.name}`")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∑–∞–ø—É—Å–∫–∞
    st.sidebar.markdown("---")
    st.sidebar.subheader("üöÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–ø—É—Å–∫–∞")
    
    if num_gpus == 0:
        launch_info = "**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** CPU"
    elif selected_mode == "default":
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** GPU {gpu_ids[0] if gpu_ids else 0}"
    else:
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:** {num_gpus} √ó GPU\n**–†–µ–∂–∏–º:** {mode_info['type']}"
    
    st.sidebar.info(launch_info)

    # –ü–æ—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ batch semantics (—á–∞—Å—Ç–∞—è –ø—Ä–∏—á–∏–Ω–∞ "–ø–æ—á–µ–º—É —Ç–∞–∫ –º–Ω–æ–≥–æ VRAM –≤ DDP")
    if training_config:
        try:
            micro_bsz = int(training_config.get("batch_size", 1))
            grad_accum = int(training_config.get("gradient_accumulation", 1))
            eff_per_gpu = micro_bsz * grad_accum
            global_batch = eff_per_gpu * max(1, int(num_gpus or 1))
            st.sidebar.caption(
                f"Batch semantics: **per‚ÄëGPU microbatch** = {micro_bsz}, "
                f"accum = {grad_accum} ‚Üí **effective per‚ÄëGPU** = {eff_per_gpu}, "
                f"**global** = {global_batch} (√ó{max(1, int(num_gpus or 1))} GPU)"
            )
            st.sidebar.caption("–í–∞–∂–Ω–æ: –≤ DDP `batch_size` –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ, —Ç.–µ. —ç—Ç–æ –∏–º–µ–Ω–Ω–æ per‚ÄëGPU.")
        except Exception:
            pass
    
    return {
        "distributed_mode": selected_mode,
        "num_gpus": num_gpus,
        "gpu_ids": gpu_ids,
        "config_file": config_file,
        "parallel_type": mode_info['type'],
    }


# Graceful fallback –¥–ª—è @st.fragment (—Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö Streamlit)
try:
    fragment = st.fragment
except (AttributeError, Exception):
    # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π Streamlit
    fragment = lambda *args, **kwargs: lambda fn: fn

@fragment(run_every=3)  # –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã
def live_metrics_fragment():
    """Fragment –¥–ª—è –∂–∏–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    if not st.session_state.current_run_id:
        st.info("–í—ã–±–µ—Ä–∏—Ç–µ run –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫")
        return
    
    run_id = st.session_state.current_run_id
    metrics = load_metrics(run_id)
    process_alive = is_process_running(run_id)
    
    # –°—Ç–∞—Ç—É—Å
    if process_alive:
        st.success(f"üü¢ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–ø—É—â–µ–Ω (Run: {run_id})")
    else:
        if metrics and metrics.get("status") == "completed":
            duration = metrics.get("training_duration", "unknown")
            st.success(f"‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration} (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        elif metrics and metrics.get("status") == "error":
            st.error(f"‚ùå –û—à–∏–±–∫–∞ (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –æ—à–∏–±–∫–µ
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        elif metrics and metrics.get("status") == "stopped":
            st.warning(f"‚èπÔ∏è –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ (Run: {run_id})")
            clear_active_run()  # –û—á–∏—â–∞–µ–º active_run.json –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ
            _close_run_log_files(run_id)  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã
        else:
            st.info(f"üìã –ü—Ä–æ—Å–º–æ—Ç—Ä –º–µ—Ç—Ä–∏–∫ (Run: {run_id})")
    
    if metrics:
        render_metrics_dashboard(metrics)
    else:
        st.info("–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")


def render_metrics_dashboard(metrics: dict):
    """–î–∞—à–±–æ—Ä–¥ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è."""
    
    status = metrics.get("status", "unknown")
    
    # Status indicator
    status_emoji = {
        "training": "üü¢", 
        "completed": "‚úÖ", 
        "error": "‚ùå",
        "initializing": "‚è≥",
        "loading_tokenizer": "‚è≥",
        "loading_dataset": "‚è≥",
        "building_model": "‚è≥",
        "saving_model": "üíæ",
    }.get(status, "‚è≥")
    
    st.subheader(f"{status_emoji} –°—Ç–∞—Ç—É—Å: {status.upper()}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ metrics.json (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –∏–∑ config
    model_params = None
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ metrics.json (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ)
        if "num_parameters" in metrics:
            model_params = metrics["num_parameters"]
        else:
            # Fallback: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑ config
            run_id = st.session_state.get("current_run_id", "active")
            if run_id and run_id != "active":
                run_dir = RUNS_DIR / run_id
                config_path = run_dir / "config.json"
                if config_path.exists():
                    with open(config_path) as f:
                        rc = json.load(f)
                        if "hidden_size" in rc and "num_layers" in rc:
                            vocab_size = rc.get("vocab_size", 50257)
                            intermediate_size = rc.get("intermediate_size")
                            model_params = estimate_parameters(
                                rc["hidden_size"],
                                rc["num_layers"],
                                vocab_size=vocab_size,
                                intermediate_size=intermediate_size,
                            )
    except Exception:
        pass
    
    # Metrics cards
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    
    with col1:
        current_step = metrics.get("current_step", 0)
        total_steps = metrics.get("total_steps", 1)
        progress = current_step / total_steps * 100 if total_steps > 0 else 0
        planned_total = metrics.get("planned_total_steps", None)
        suffix = f"Step {current_step}/{total_steps}"
        if planned_total is not None and int(planned_total) != int(total_steps):
            suffix = f"{suffix} (–ø–ª–∞–Ω: {planned_total})"
        st.metric("–ü—Ä–æ–≥—Ä–µ—Å—Å", f"{progress:.1f}%", suffix)
    
    with col2:
        loss = metrics.get("current_loss", 0)
        st.metric("Train Loss", f"{loss:.4f}")
    
    with col3:
        vloss = metrics.get("current_val_loss", None)
        if vloss is None:
            st.metric("Val Loss", "‚Äî")
        else:
            st.metric("Val Loss", f"{vloss:.4f}")
    
    with col4:
        lr = metrics.get("current_lr", 0)
        st.metric("Learning Rate", f"{lr:.2e}")
    
    with col5:
        if model_params:
            st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(model_params))
        else:
            st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", "‚Äî")

    # –î–æ–ø. –ø–æ—è—Å–Ω–µ–Ω–∏—è: –ø–ª–∞–Ω vs —Ñ–∞–∫—Ç, –ø—Ä–∏—á–∏–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, LR floor
    planned_total = metrics.get("planned_total_steps", None)
    total_steps = metrics.get("total_steps", None)
    stop_reason = metrics.get("stop_reason", None)
    min_lr_ratio = metrics.get("min_lr_ratio", None)
    if planned_total is not None and total_steps is not None and int(planned_total) != int(total_steps):
        st.caption(f"–ü–ª–∞–Ω —à–∞–≥–æ–≤: {planned_total} ‚Ä¢ –§–∞–∫—Ç (–¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞/ETA): {total_steps}")
    if stop_reason:
        st.caption(f"–ü—Ä–∏—á–∏–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: `{stop_reason}`")
    if min_lr_ratio is not None and float(min_lr_ratio) > 0:
        st.caption(f"Cosine LR floor –≤–∫–ª—é—á—ë–Ω: min_lr_ratio={float(min_lr_ratio):.2f}")
    
    with col6:
        eta = metrics.get("eta_seconds", 0)
        elapsed = metrics.get("elapsed_seconds", 0)
        st.metric("–í—Ä–µ–º—è", f"{format_time(elapsed)}", delta=f"–û—Å—Ç: {format_time(eta)}", delta_color="normal")
    
    # Progress bar
    st.progress(min(progress / 100, 1.0))
    
    # Charts
    # –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–ª—é—á –ø–æ run_id, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏
    rid = st.session_state.get("current_run_id", "active") or "active"
    
    if metrics.get("loss_history"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Loss chart
            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["loss_history"],
                mode='lines',
                name='Train Loss',
                line=dict(color='#e94560', width=2)
            ))
            if metrics.get("val_loss_history"):
                fig_loss.add_trace(go.Scatter(
                    x=metrics["val_steps_history"],
                    y=metrics["val_loss_history"],
                    mode='lines',
                    name='Val Loss',
                    line=dict(width=2, dash="dash", color='#60a5fa')
                ))
            fig_loss.update_layout(
                title="Training & Validation Loss",
                xaxis_title="Step",
                yaxis_title="Loss",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_loss, key=f"loss_chart_{rid}")
        
        with col2:
            # LR chart
            fig_lr = go.Figure()
            fig_lr.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["lr_history"],
                mode='lines',
                name='LR',
                line=dict(color='#60a5fa', width=2)
            ))
            fig_lr.update_layout(
                title="Learning Rate Schedule",
                xaxis_title="Step",
                yaxis_title="LR",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_lr, key=f"lr_chart_{rid}")
    
    # Checkpoints
    if metrics.get("checkpoints"):
        with st.expander("üì¶ Checkpoints"):
            for ckpt in metrics["checkpoints"]:
                ckpt_loss = ckpt.get("loss")
                if ckpt_loss is not None:
                    st.text(f"Step {ckpt['step']}: Loss {ckpt_loss:.4f} | {ckpt['path']}")
                else:
                    st.text(f"Step {ckpt['step']}: {ckpt['path']}")
    
    # –ü—Ä–∏–º–µ—Ä —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–¥–ª—è SFT)
    sample_prompt = metrics.get("sample_prompt")
    if sample_prompt:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –Ω–∞–ª–∏—á–∏—é stage –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
        stage = metrics.get("stage", "pretrain")
        if stage == "sft":
            title = "üìù –ü—Ä–∏–º–µ—Ä —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (SFT)"
            caption = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:"
            tip = "üí° –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç–µ–≥–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ"
        else:
            title = "üìù –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (Pretrain)"
            caption = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:"
            tip = "üí° –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –≤ —Ç–µ–∫—Å—Ç–µ"
        
        with st.expander(title, expanded=True):
            st.caption(caption)
            st.code(sample_prompt, language=None)
            st.caption(tip)
    
    # GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    gpu_stats = metrics.get("gpu_stats", [])
    if gpu_stats:
        st.subheader("üñ•Ô∏è –ù–∞–≥—Ä—É–∑–∫–∞ GPU")
        
        cols = st.columns(len(gpu_stats))
        for i, (col, gpu) in enumerate(zip(cols, gpu_stats)):
            with col:
                st.markdown(f"**GPU {gpu['id']}**")
                
                # Memory bar
                mem_percent = gpu.get('memory_percent', 0)
                st.progress(min(mem_percent / 100, 1.0), text=f"VRAM: {gpu['memory_used_gb']:.1f} / {gpu['memory_total_gb']:.1f} GB ({mem_percent:.0f}%)")
                
                # Utilization
                util = gpu.get('utilization')
                if util is not None:
                    st.progress(min(util / 100, 1.0), text=f"–ó–∞–≥—Ä—É–∑–∫–∞: {util}%")
                else:
                    st.caption("–ó–∞–≥—Ä—É–∑–∫–∞: N/A")
    
    # Error
    if metrics.get("error"):
        st.error("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
        with st.expander("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏ (Traceback)", expanded=True):
            st.code(metrics['error'], language="python")
    
    # –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
    if st.session_state.current_run_id:
        run_dir = RUNS_DIR / st.session_state.current_run_id
        stderr_path = run_dir / "stderr.log"
        stdout_path = run_dir / "stdout.log"
        
        with st.expander("üìã –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.caption("stdout (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stdout_path.exists():
                    with open(stdout_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)
            
            with col2:
                st.caption("stderr (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stderr_path.exists():
                    with open(stderr_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)



def download_hf_dataset(repo_id, subset, split, limit_type, limit_val, limit_bytes, save_path, filters=None):
    """–§—É–Ω–∫—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    try:
        status_text = f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: {repo_id}..."
        st.toast(status_text)
        print(status_text)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã stream=True —á—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –≤—Å–µ –≤ –ø–∞–º—è—Ç—å
        # –î–ª—è –º–Ω–æ–≥–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ "default" –º–æ–∂–µ—Ç –ª–æ–º–∞—Ç—å load_dataset - –ø–µ—Ä–µ–¥–∞—ë–º None
        subset_arg = None if (not subset or subset.strip() == "" or subset.lower() == "default") else subset
        
        ds = load_dataset(repo_id, subset_arg, split=split, streaming=True)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
        save_path = DATASET_DIR / save_path
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        count = 0
        current_bytes = 0
        
        with open(save_path, "w", encoding="utf-8") as f:
            for item in ds:
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                if filters:
                    # –§–∏–ª—å—Ç—Ä –ø–æ score
                    if "score_col" in filters and "min_score" in filters:
                         col = filters["score_col"]
                         min_s = filters["min_score"]
                         # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Ç–∏–ø–∞
                         if col in item and item[col] is not None:
                             try:
                                 val = float(item[col])
                                 if val < min_s:
                                     continue
                             except ValueError:
                                 pass
                    
                    # –§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É
                    if "lang_col" in filters and "target_lang" in filters:
                         col = filters["lang_col"]
                         target = filters["target_lang"]
                         if col in item and item[col] is not None:
                             val = str(item[col])
                             if target.lower() not in val.lower():
                                 continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSONL
                line = json.dumps(item, ensure_ascii=False) + "\n"
                line_bytes = len(line.encode('utf-8'))
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)" and count >= limit_val:
                    break
                if limit_type == "–ì–ë (–†–∞–∑–º–µ—Ä)" and (current_bytes + line_bytes) > limit_bytes:
                    break
                    
                f.write(line)
                count += 1
                current_bytes += line_bytes
                
                if count % 1000 == 0:
                    print(f"Downloaded {count} lines, {current_bytes / 1024**2:.2f} MB")

        st.success(f"–ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {count} —Å—Ç—Ä–æ–∫ ({current_bytes / 1024**2:.2f} MB) –≤ {save_path}")
        return True

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        print(traceback.format_exc())
        return False


def render_data_manager():
    """–í–∫–ª–∞–¥–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏."""
    st.header("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
    
    col_upload, col_list = st.columns([1, 2])
    
    with col_upload:
        # –°–µ–∫—Ü–∏—è 1: –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        with st.expander("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤", expanded=False):
            uploaded_files = st.file_uploader(
                "–ü–µ—Ä–µ—Ç–∞—â–∏—Ç–µ —Ñ–∞–π–ª—ã —Å—é–¥–∞", 
                type=["jsonl", "txt"],  # –í–ê–ñ–ù–û: –Ω–µ –≤–∫–ª—é—á–∞–µ–º .json, —Ç.–∫. —ç—Ç–æ –æ–±—ã—á–Ω–æ –º–∞—Å—Å–∏–≤, –∞ –Ω–µ JSONL 
                accept_multiple_files=True
            )
            
            if uploaded_files:
                if st.button("üì• –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª—ã"):
                    for uploaded_file in uploaded_files:
                        save_path = DATASET_DIR / uploaded_file.name
                        with open(save_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        st.toast(f"–§–∞–π–ª {uploaded_file.name} —Å–æ—Ö—Ä–∞–Ω—ë–Ω!", icon="‚úÖ")
                    time.sleep(1)
                    st.rerun()

        # –°–µ–∫—Ü–∏—è 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å HuggingFace
        st.subheader("ü§ó –°–∫–∞—á–∞—Ç—å —Å HuggingFace")
        # –°–ª–æ–≤–∞—Ä—å –ø—Ä–µ—Å–µ—Ç–æ–≤: {–Ω–∞–∑–≤–∞–Ω–∏–µ: (repo_id, subset, split)}
        presets = {
            "üü¢ Pretrain: FineWeb-2 (Russian)": ("HuggingFaceFW/fineweb-2", "rus_Cyrl", "train"),
            "üü¢ Pretrain: FineWeb-Edu (Educational)": ("HuggingFaceFW/fineweb-edu", "default", "train"),
            "üü¢ Pretrain: Wikitext-103": ("wikitext", "wikitext-103-v1", "train"),
            "üîµ SFT: OpenOrca-ru": ("d0rj/OpenOrca-ru", "default", "train"),
            "üîµ SFT: ru-instruct": ("d0rj/ru-instruct", "default", "train"),
            "üîµ SFT: GrandMaster-PRO-MAX": ("Vikhrmodels/GrandMaster-PRO-MAX", "default", "train"),
            "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None, None),
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (FineWeb-2 Russian)
        if "hf_repo_id_input" not in st.session_state:
            st.session_state.hf_repo_id_input = "HuggingFaceFW/fineweb-2"
        if "hf_subset_default" not in st.session_state:
            st.session_state.hf_subset_default = "rus_Cyrl"
        if "hf_split_default" not in st.session_state:
            st.session_state.hf_split_default = "train"
        
        # –ü—Ä–µ–¥–∑–∞–ø–æ–ª–Ω—è–µ–º –∫—ç—à –¥–ª—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –ø—Ä–µ—Å–µ—Ç–∞ (FineWeb-2)
        # —á—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–≥ —Å—Ä–∞–∑—É —Å–∫–∞—á–∏–≤–∞—Ç—å –±–µ–∑ –Ω–∞–∂–∞—Ç–∏—è "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å"
        if "ds_repo_info" not in st.session_state:
            st.session_state.ds_repo_info = {}
        
        default_repo = "HuggingFaceFW/fineweb-2"
        if default_repo not in st.session_state.ds_repo_info:
            st.session_state.ds_repo_info[default_repo] = {
                "configs": ["rus_Cyrl"],  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —è–∑—ã–∫
                "splits": ["train", "test"],  # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ splits
                "features": {},  # –ó–∞–ø–æ–ª–Ω–∏—Ç—Å—è –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ
                "selected_config": "rus_Cyrl"
            }
        
        def on_preset_change():
            """Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ–ª–µ–π –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ø—Ä–µ—Å–µ—Ç–∞."""
            sel = st.session_state.dataset_preset_selector
            preset_data = presets.get(sel)
            if preset_data and preset_data[0]:
                st.session_state.hf_repo_id_input = preset_data[0]
                st.session_state.hf_subset_default = preset_data[1]
                st.session_state.hf_split_default = preset_data[2]
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º selectbox —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã
                if "hf_split_select" in st.session_state:
                    del st.session_state.hf_split_select
                if "hf_subset_select" in st.session_state:
                    del st.session_state.hf_subset_select

        # –°–µ–ª–µ–∫—Ç–æ—Ä –ø—Ä–µ—Å–µ—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é FineWeb-2 Russian)
        st.selectbox(
            "üìö –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã",
            options=list(presets.keys()),
            index=0,  # FineWeb-2 Russian –ø–µ—Ä–≤—ã–π
            key="dataset_preset_selector",
            on_change=on_preset_change,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –≥–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî –≤—Å–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω—è—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )

        repo_id = st.text_input("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (ID)", key="hf_repo_id_input")
        
        # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π"):
            try:
                with st.spinner(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {repo_id}..."):
                    # 1. –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥–∏
                    configs = get_dataset_config_names(repo_id)
                    
                    # 2. –í—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è splits/features
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π (rus_Cyrl) > –ø–µ—Ä–≤—ã–π –≤ —Å–ø–∏—Å–∫–µ
                    default_subset = st.session_state.hf_subset_default
                    if default_subset in configs:
                        selected_config = default_subset
                    else:
                        selected_config = configs[0] if configs else None
                    
                    splits = []
                    features_info = {}
                    
                    if selected_config:
                        splits = get_dataset_split_names(repo_id, selected_config)
                        # 3. –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (features)
                        try:
                            ds_builder = load_dataset_builder(repo_id, selected_config)
                            if ds_builder.info.features:
                                features_info = ds_builder.info.features
                        except Exception as e:
                            print(f"Could not load features: {e}")

                    st.session_state.ds_repo_info[repo_id] = {
                        "configs": configs,
                        "splits": splits,
                        "features": features_info,
                        "selected_config": selected_config  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –¥–ª—è –∫–∞–∫–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞ splits
                    }
                    
                    # –í–ê–ñ–ù–û: –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—ã–±–æ—Ä –≤–∏–¥–∂–µ—Ç–æ–≤ —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if "hf_split_select" in st.session_state:
                        del st.session_state.hf_split_select
                    if "hf_subset_select" in st.session_state:
                        del st.session_state.hf_subset_select
                    
                    st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(configs)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π, splits: {splits}")
            except Exception as e:
                st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: {e}")

        # –†–∞–±–æ—Ç–∞–µ–º —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        repo_info = st.session_state.ds_repo_info.get(repo_id, {})
        available_configs = repo_info.get("configs", [])
        available_splits = repo_info.get("splits", [])
        features = repo_info.get("features", {})
        
        if available_configs:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π subset, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –≤ —Å–ø–∏—Å–∫–µ
            default_idx = 0
            if st.session_state.hf_subset_default in available_configs:
                default_idx = available_configs.index(st.session_state.hf_subset_default)
            subset = st.selectbox("Subset (–∫–æ–Ω—Ñ–∏–≥)", available_configs, index=default_idx, key="hf_subset_select")
        else:
            subset = st.text_input("Subset (–∫–æ–Ω—Ñ–∏–≥)", st.session_state.hf_subset_default, key="hf_subset_input")
        
        if available_splits:
            default_idx = 0
            if st.session_state.hf_split_default in available_splits:
                default_idx = available_splits.index(st.session_state.hf_split_default)
            split = st.selectbox("Split", available_splits, index=default_idx, key="hf_split_select")
        else:
            split = st.text_input("Split", st.session_state.hf_split_default, key="hf_split_input")

        # --- –£–ú–ù–´–ï –§–ò–õ–¨–¢–†–´ ---
        with st.expander("üõ†Ô∏è –§–∏–ª—å—Ç—Ä—ã –∏ –õ–∏–º–∏—Ç—ã", expanded=True):
            # –õ–∏–º–∏—Ç—ã (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã)
            col_lim1, col_lim2 = st.columns(2)
            with col_lim1:
                limit_type = st.radio("–¢–∏–ø –ª–∏–º–∏—Ç–∞", ["–ì–ë (–†–∞–∑–º–µ—Ä)", "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)"], key="limit_type")
            
            with col_lim2:
                limit_val = 0
                limit_bytes = 0
                
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)":
                    limit_val = st.number_input("–ö–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫", value=100000, step=10000, key="limit_val")
                else:
                    limit_gb = st.number_input("–†–∞–∑–º–µ—Ä (–ì–ë)", value=2.0, step=0.5, min_value=0.1, key="limit_gb")
                    limit_bytes = int(limit_gb * 1024**3)
            
            st.divider()
            
            # –§–∏–ª—å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (features)
            active_filters = {}
            
            if features:
                st.caption("üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏ –∏—Ö —Ç–∏–ø–æ–≤
                # features —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å {col_name: feature_info}
                # feature_info –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π 'Value(dtype='string')' –∏–ª–∏ –æ–±—ä–µ–∫—Ç–æ–º
                
                # 1. –§–∏–ª—å—Ç—Ä —á–∏—Å–ª–æ–≤–æ–π (Score/Quality)
                float_cols = []
                string_cols = []
                
                for col_name, feature_def in features.items():
                    # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø
                    dtype = getattr(feature_def, 'dtype', str(feature_def))
                    if 'float' in str(dtype):
                        float_cols.append(col_name)
                    elif 'string' in str(dtype):
                        string_cols.append(col_name)
                
                col_f1, col_f2 = st.columns(2)
                
                with col_f1:
                    if float_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é (float)**")
                        selected_float_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + float_cols, key="sel_float_col")
                        if selected_float_col != "(–Ω–µ—Ç)":
                            min_val = st.slider(f"–ú–∏–Ω. –∑–Ω–∞—á–µ–Ω–∏–µ {selected_float_col}", 0.0, 1.0, 0.0, key="val_float_col")
                            active_filters["score_col"] = selected_float_col
                            active_filters["min_score"] = min_val
                    else:
                        st.caption("–ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

                with col_f2:
                    if string_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–∫—Å—Ç—É (contains)**")
                        selected_str_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + string_cols, key="sel_str_col")
                        if selected_str_col != "(–Ω–µ—Ç)":
                            target_str = st.text_input(f"–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:", key="val_str_col")
                            if target_str:
                                active_filters["lang_col"] = selected_str_col
                                active_filters["target_lang"] = target_str
                    else:
                        st.caption("–¢–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            else:
                st.info("‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ –ª–∏–º–∏—Ç—ã –ø–æ –æ–±—ä–µ–º—É.")


        save_filename = st.text_input("–ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", "dataset.jsonl", key="save_filename")
        
        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º callback
        def on_download_click(active_filters_map):
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ session_state —è–≤–Ω–æ
            r_id = st.session_state.get('hf_repo_id_input')
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º subset –∏ split
            if st.session_state.get('hf_subset_select'):
                sub = st.session_state.get('hf_subset_select')
            else:
                sub = st.session_state.get('hf_subset_input')
                
            if st.session_state.get('hf_split_select'):
                spl = st.session_state.get('hf_split_select')
            else:
                spl = st.session_state.get('hf_split_input')
                
            l_type = st.session_state.get('limit_type')
            l_val = st.session_state.get('limit_val', 0) or 0
            
            l_gb = st.session_state.get('limit_gb', 2.0)
            l_bytes = int(l_gb * 1024**3)

            s_path = st.session_state.get('save_filename')
            
            # –ü–µ—Ä–µ–¥–∞—ë–º —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞–∫ –µ—Å—Ç—å (active_filters_map —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏)
            filters_to_pass = active_filters_map or None
            
            download_hf_dataset(r_id, sub, spl, l_type, l_val, l_bytes, s_path, filters=filters_to_pass)

        st.button("–°–∫–∞—á–∞—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å", on_click=on_download_click, args=(active_filters,))
    
    with col_list:
        st.subheader("–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã")
        
        datasets = []
        if DATASET_DIR.exists():
            # JSONL / JSON
            for f in list(DATASET_DIR.glob("*.jsonl")) + list(DATASET_DIR.glob("*.json")):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "JSONL/JSON",
                    "size_mb": size_mb,
                    "path": f
                })
            # TXT
            for f in DATASET_DIR.glob("*.txt"):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "Text",
                    "size_mb": size_mb,
                    "path": f
                })
        
        if not datasets:
            st.info("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã —Å–ª–µ–≤–∞.")
        else:
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–ø–∏—Å–æ–∫
            for ds in datasets:
                with st.expander(f"üìÑ {ds['name']} ({ds['size_mb']:.1f} MB)"):
                    st.caption(f"–¢–∏–ø: {ds['type']}")
                    
                    # Preview
                    try:
                        with open(ds['path'], "r", encoding="utf-8") as f:
                            head = []
                            for i, line in enumerate(f):
                                if i >= 5:
                                    break
                                head.append(line.strip())
                        st.markdown("**Preview (–ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫):**")
                        if head:
                            st.code("\n".join(head), language="json" if "JSON" in ds['type'] else "text")
                        else:
                            st.info("–§–∞–π–ª –ø—É—Å—Ç")
                        
                        col_del, col_info = st.columns([1, 4])
                        with col_del:
                            if st.button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"del_{ds['name']}"):
                                ds['path'].unlink()
                                st.toast(f"–§–∞–π–ª {ds['name']} —É–¥–∞–ª—ë–Ω", icon="üóëÔ∏è")
                                time.sleep(1)
                                st.rerun()
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")


def download_hf_model(repo_id: str, save_name: str, revision: str = "main"):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å HuggingFace –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ –≤ MODELS_DIR.
    """
    from huggingface_hub import snapshot_download
    
    save_path = MODELS_DIR / save_name
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ
    if save_path.exists():
        st.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å `{save_name}` —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        return False
    
    try:
        with st.spinner(f"‚è≥ –°–∫–∞—á–∏–≤–∞–µ–º {repo_id}..."):
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar = st.progress(0, text="–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...")
            status_text = st.empty()
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            status_text.text(f"–°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∏–∑ {repo_id}...")
            progress_bar.progress(10, text="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤...")
            
            # snapshot_download —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å—é –º–æ–¥–µ–ª—å
            local_path = snapshot_download(
                repo_id=repo_id,
                revision=revision,
                local_dir=str(save_path),
                local_dir_use_symlinks=False,  # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã, –Ω–µ —Å–∏–º–ª–∏–Ω–∫–∏
                ignore_patterns=["*.md", "*.txt", "*.gitattributes", ".git*"],  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–Ω—É–∂–Ω–æ–µ
            )
            
            progress_bar.progress(90, text="–ü—Ä–æ–≤–µ—Ä–∫–∞...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–∫–∞—á–∞–ª–æ—Å—å
            config_file = save_path / "config.json"
            if not config_file.exists():
                st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω config.json –≤ —Å–∫–∞—á–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
                return False
            
            # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            import json
            with open(config_file) as f:
                model_config = json.load(f)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            model_type = model_config.get("model_type", "unknown")
            hidden_size = model_config.get("hidden_size", "?")
            num_layers = model_config.get("num_hidden_layers", model_config.get("n_layer", "?"))
            vocab_size = model_config.get("vocab_size", "?")
            
            progress_bar.progress(100, text="–ì–æ—Ç–æ–≤–æ!")
            status_text.empty()
            
            st.success(f"""‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞!
- **–ü—É—Ç—å:** `{save_path}`
- **–¢–∏–ø:** {model_type}
- **Hidden:** {hidden_size}, **Layers:** {num_layers}, **Vocab:** {vocab_size}
""")
            return True
            
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        import traceback
        print(traceback.format_exc())
        # –£–¥–∞–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω–æ–µ
        if save_path.exists():
            import shutil
            shutil.rmtree(save_path, ignore_errors=True)
        return False


def render_model_manager():
    """–í–∫–ª–∞–¥–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ (—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å HuggingFace)."""
    st.header("ü§ñ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏")
    
    col_download, col_list = st.columns([1, 2])
    
    with col_download:
        st.subheader("ü§ó –°–∫–∞—á–∞—Ç—å —Å HuggingFace")
        
        # –ü—Ä–µ—Å–µ—Ç—ã –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è continual pretraining / SFT
        model_presets = {
            "üî• SmolLM2-135M (135M params)": ("HuggingFaceTB/SmolLM2-135M", "SmolLM2-135M"),
            "üî• SmolLM2-360M (360M params)": ("HuggingFaceTB/SmolLM2-360M", "SmolLM2-360M"),
            "üî• SmolLM2-1.7B (1.7B params)": ("HuggingFaceTB/SmolLM2-1.7B", "SmolLM2-1.7B"),
            "ü¶ô TinyLlama-1.1B (1.1B params)": ("TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T", "TinyLlama-1.1B"),
            "üêç Pythia-70M (70M params)": ("EleutherAI/pythia-70m", "Pythia-70M"),
            "üêç Pythia-160M (160M params)": ("EleutherAI/pythia-160m", "Pythia-160M"),
            "üêç Pythia-410M (410M params)": ("EleutherAI/pythia-410m", "Pythia-410M"),
            "üêç Pythia-1B (1B params)": ("EleutherAI/pythia-1b", "Pythia-1B"),
            "ü§ñ GPT-2 Small (124M params)": ("openai-community/gpt2", "GPT2-Small"),
            "ü§ñ GPT-2 Medium (355M params)": ("openai-community/gpt2-medium", "GPT2-Medium"),
            "ü¶ä Qwen2.5-0.5B (0.5B params)": ("Qwen/Qwen2.5-0.5B", "Qwen2.5-0.5B"),
            "ü¶ä Qwen2.5-1.5B (1.5B params)": ("Qwen/Qwen2.5-1.5B", "Qwen2.5-1.5B"),
            "üá∑üá∫ ruGPT3-Small (125M, Russian)": ("ai-forever/rugpt3small_based_on_gpt2", "ruGPT3-Small"),
            "üìù –í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é...": (None, None),
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        if "model_repo_id" not in st.session_state:
            st.session_state.model_repo_id = "HuggingFaceTB/SmolLM2-135M"
        if "model_save_name" not in st.session_state:
            st.session_state.model_save_name = "SmolLM2-135M"
        
        def on_model_preset_change():
            sel = st.session_state.model_preset_selector
            preset_data = model_presets.get(sel)
            if preset_data and preset_data[0]:
                st.session_state.model_repo_id = preset_data[0]
                st.session_state.model_save_name = preset_data[1]
        
        st.selectbox(
            "üìö –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏",
            options=list(model_presets.keys()),
            index=0,
            key="model_preset_selector",
            on_change=on_model_preset_change,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å ‚Äî repo_id –∑–∞–ø–æ–ª–Ω–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
        )
        
        repo_id = st.text_input("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (ID)", key="model_repo_id")
        save_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", key="model_save_name", 
                                   help="–ü–∞–ø–∫–∞ –≤ models/, –∫—É–¥–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        st.markdown("""
<div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); 
            padding: 12px; border-radius: 8px; margin: 10px 0;
            border: 1px solid #0f3460; color: #e8e8e8;">
<b style="color: #4fc3f7;">üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b><br>
‚Ä¢ <b style="color: #81d4fa;">SmolLM2</b> ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç HuggingFace<br>
‚Ä¢ <b style="color: #81d4fa;">Pythia</b> ‚Äî –æ—Ç–ª–∏—á–Ω—ã–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã<br>
‚Ä¢ <b style="color: #81d4fa;">TinyLlama</b> ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω–∞—è, —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 3T —Ç–æ–∫–µ–Ω–æ–≤<br>
‚Ä¢ <b style="color: #81d4fa;">Qwen2.5</b> ‚Äî —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç Alibaba
</div>
""", unsafe_allow_html=True)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        size_estimates = {
            "70m": "~150 MB", "135m": "~300 MB", "160m": "~350 MB",
            "360m": "~800 MB", "410m": "~900 MB", "0.5b": "~1 GB",
            "1b": "~2 GB", "1.1b": "~2.5 GB", "1.5b": "~3 GB", "1.7b": "~3.5 GB",
            "124m": "~500 MB", "355m": "~1.5 GB", "125m": "~500 MB",
        }
        
        estimated_size = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        repo_lower = repo_id.lower()
        for size_key, size_val in size_estimates.items():
            if size_key in repo_lower:
                estimated_size = size_val
                break
        
        st.caption(f"üì¶ –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä: **{estimated_size}**")
        
        if st.button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
            if not repo_id or not save_name:
                st.error("–£–∫–∞–∂–∏—Ç–µ repo_id –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ!")
            else:
                success = download_hf_model(repo_id, save_name)
                if success:
                    time.sleep(1)
                    st.rerun()
    
    with col_list:
        st.subheader("üìÅ –°–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        
        models = []
        if MODELS_DIR.exists():
            for model_dir in MODELS_DIR.iterdir():
                if model_dir.is_dir():
                    config_path = model_dir / "config.json"
                    if config_path.exists():
                        try:
                            import json
                            with open(config_path) as f:
                                cfg = json.load(f)
                            
                            # –†–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏
                            total_size = sum(
                                f.stat().st_size for f in model_dir.rglob("*") if f.is_file()
                            )
                            size_gb = total_size / (1024**3)
                            
                            models.append({
                                "name": model_dir.name,
                                "path": model_dir,
                                "model_type": cfg.get("model_type", "unknown"),
                                "hidden_size": cfg.get("hidden_size", "?"),
                                "num_layers": cfg.get("num_hidden_layers", cfg.get("n_layer", "?")),
                                "vocab_size": cfg.get("vocab_size", "?"),
                                "size_gb": size_gb,
                            })
                        except Exception:
                            models.append({
                                "name": model_dir.name,
                                "path": model_dir,
                                "model_type": "?",
                                "hidden_size": "?",
                                "num_layers": "?",
                                "vocab_size": "?",
                                "size_gb": 0,
                            })
        
        if not models:
            st.info("–ù–µ—Ç —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å —Å–ª–µ–≤–∞ –¥–ª—è Continual Pretraining –∏–ª–∏ SFT.")
        else:
            for m in sorted(models, key=lambda x: x["name"]):
                with st.expander(f"ü§ñ {m['name']} ({m['size_gb']:.2f} GB)"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"""
- **–¢–∏–ø:** `{m['model_type']}`
- **Hidden Size:** {m['hidden_size']}
- **Layers:** {m['num_layers']}
- **Vocab:** {m['vocab_size']}
""")
                    with col2:
                        st.caption(f"üìÇ –ü—É—Ç—å: `{m['path']}`")
                        
                        # –ö–Ω–æ–ø–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                        if st.button("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", key=f"use_{m['name']}", 
                                     help="–í—ã–±—Ä–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –¥–ª—è Continual Pretrain / SFT"):
                            st.session_state.selected_base_model = str(m['path'])
                            st.toast(f"–ú–æ–¥–µ–ª—å {m['name']} –≤—ã–±—Ä–∞–Ω–∞! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –ó–∞–ø—É—Å–∫ –∏ –≤—ã–±–µ—Ä–∏—Ç–µ Continual Pretrain –∏–ª–∏ SFT.", icon="‚úÖ")
                        
                        # –ö–Ω–æ–ø–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è
                        if st.button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"del_model_{m['name']}"):
                            import shutil
                            shutil.rmtree(m['path'])
                            st.toast(f"–ú–æ–¥–µ–ª—å {m['name']} —É–¥–∞–ª–µ–Ω–∞", icon="üóëÔ∏è")
                            time.sleep(1)
                            st.rerun()
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞
        st.markdown("---")
        st.info("""
üí° **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∫–∞—á–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å:**
1. –ù–∞–∂–º–∏—Ç–µ **üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å** –Ω–∞ –Ω—É–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üöÄ –ó–∞–ø—É—Å–∫**
3. –í —Å–∞–π–¥–±–∞—Ä–µ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º **Continual Pretrain** –∏–ª–∏ **SFT**
4. –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—Å—è –∫–∞–∫ –±–∞–∑–æ–≤–∞—è
""")


def _bytes_to_gb(x: int) -> float:
    return float(x) / (1024**3)


def _sum_tensor_bytes(obj) -> int:
    """
    –°—á–∏—Ç–∞–µ—Ç –±–∞–π—Ç—ã —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ CUDA –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Ç–µ–Ω–∑–æ—Ä/—Å–ø–∏—Å–æ–∫/–∫–æ—Ä—Ç–µ–∂/—Å–ª–æ–≤–∞—Ä—å).
    """
    import torch

    total = 0
    if obj is None:
        return 0
    if torch.is_tensor(obj):
        if obj.is_cuda:
            return int(obj.numel() * obj.element_size())
        return 0
    if isinstance(obj, dict):
        for v in obj.values():
            total += _sum_tensor_bytes(v)
        return total
    if isinstance(obj, (list, tuple)):
        for v in obj:
            total += _sum_tensor_bytes(v)
        return total
    return 0


def _estimate_memory_footprint(config, batch_size, distributed_mode="default", num_gpus=1):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ VRAM –¥–ª—è –ª—é–±–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π.
    """
    from homellm.models.memory_estimator import estimate_memory_footprint
    return estimate_memory_footprint(config, batch_size, distributed_mode, num_gpus)


def _profile_memory_footprint_cuda(config, batch_size: int):
    """
    –¢–æ—á–Ω—ã–π (–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω–æ) –∑–∞–º–µ—Ä –ø–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º CUDA –∞–ª–ª–æ–∫–∞—Ü–∏—è–º:
    - –¥–µ–ª–∞–µ–º warmup —à–∞–≥, —á—Ç–æ–±—ã AdamW –ø—Ä–æ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª state
    - –∑–∞—Ç–µ–º –º–µ—Ä—è–µ–º peak allocated/reserved –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º breakdown: Model+Optim (steady), Act (peak - steady), Buf (reserved - peak).
    """
    import torch

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

    # –°–æ–±–∏—Ä–∞–µ–º HomeForCausalLM (–¥–ª—è Blueprint —Ä–µ–∂–∏–º–∞ –ª—É—á—à–µ –¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ; –ø–æ–∫–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é Home –º–æ–¥–µ–ª—å)
    from homellm.models.home_model import HomeConfig, HomeForCausalLM

    vocab_size = int(config.get("vocab_size", 50257))
    hidden_size = int(config["hidden_size"])
    num_layers = int(config["num_layers"])
    n_heads = int(config["n_heads"])
    seq_len = int(config["seq_len"])

    mp = (config.get("mixed_precision") or "no").lower()
    if mp == "bf16":
        dtype = torch.bfloat16
    elif mp == "fp16":
        dtype = torch.float16
    else:
        dtype = torch.float32

    model_cfg = HomeConfig(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_layers,
        num_attention_heads=n_heads,
        max_position_embeddings=seq_len,
        dropout=float(config.get("dropout", 0.0)),
    )

    device = torch.device("cuda:0")
    torch.cuda.set_device(device)

    model = HomeForCausalLM(model_cfg).to(device=device, dtype=dtype)
    model.train()
    if config.get("grad_checkpoint", False) and hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()

    opt_name = (config.get("optimizer") or "adamw").lower()
    lr = float(config.get("lr", 1e-3))
    wd = float(config.get("weight_decay", 0.01))
    if opt_name == "adam":
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    else:
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    def run_step():
        input_ids = torch.randint(0, vocab_size, (int(batch_size), seq_len), device=device)
        labels = input_ids.clone()
        out = model(input_ids=input_ids, labels=labels, use_cache=False)
        loss = out.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

    # Warmup: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è optimizer state + –ø—Ä–æ–≥—Ä–µ–≤ allocator'–∞
    torch.cuda.synchronize()
    run_step()
    torch.cuda.synchronize()

    # –ò–∑–º–µ—Ä–µ–Ω–∏–µ
    torch.cuda.reset_peak_memory_stats(device)
    alloc_before = torch.cuda.memory_allocated(device)
    reserved_before = torch.cuda.memory_reserved(device)

    run_step()
    torch.cuda.synchronize()

    peak_alloc = torch.cuda.max_memory_allocated(device)
    peak_reserved = torch.cuda.max_memory_reserved(device)
    alloc_after = torch.cuda.memory_allocated(device)

    # Tensor-based breakdown (–ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç "—á—Ç–æ –∏–º–µ–Ω–Ω–æ –∂–∏–≤—ë—Ç", –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç caching allocator)
    weights_bytes = 0
    for p in model.parameters():
        if p.is_cuda:
            weights_bytes += int(p.numel() * p.element_size())
    for b in model.buffers():
        if torch.is_tensor(b) and b.is_cuda:
            weights_bytes += int(b.numel() * b.element_size())

    opt_state_bytes = 0
    for st in optimizer.state.values():
        opt_state_bytes += _sum_tensor_bytes(st)

    # –ü–æ—Å–ª–µ zero_grad(set_to_none=True) –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –¥–µ—Ä–∂–∞—Ç—å –ø–∞–º—è—Ç—å
    grads_bytes = 0
    for p in model.parameters():
        if p.grad is not None and p.grad.is_cuda:
            grads_bytes += int(p.grad.numel() * p.grad.element_size())

    steady_alloc = alloc_after
    act_alloc = max(0, int(peak_alloc - steady_alloc))
    buf_alloc = max(0, int(peak_reserved - peak_alloc))

    total_peak = peak_reserved  # —Å–∞–º—ã–π —á–µ—Å—Ç–Ω—ã–π "—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—Ä–æ—Å–∏–ª —É –¥—Ä–∞–π–≤–µ—Ä–∞" –Ω–∞ –ø–∏–∫–µ —à–∞–≥–∞

    return {
        "method": "profile_cuda",
        "total_gb": round(_bytes_to_gb(total_peak), 2),
        "model_gb": round(_bytes_to_gb(steady_alloc), 2),
        "act_gb": round(_bytes_to_gb(act_alloc), 2),
        "buf_gb": round(_bytes_to_gb(buf_alloc), 2),
        "params": int(sum(p.numel() for p in model.parameters())),
        "detail": {
            "alloc_before_gb": round(_bytes_to_gb(alloc_before), 3),
            "reserved_before_gb": round(_bytes_to_gb(reserved_before), 3),
            "peak_alloc_gb": round(_bytes_to_gb(peak_alloc), 3),
            "peak_reserved_gb": round(_bytes_to_gb(peak_reserved), 3),
            "alloc_after_gb": round(_bytes_to_gb(alloc_after), 3),
            "tensor_weights_gb": round(_bytes_to_gb(weights_bytes), 3),
            "tensor_opt_state_gb": round(_bytes_to_gb(opt_state_bytes), 3),
            "tensor_grads_gb": round(_bytes_to_gb(grads_bytes), 3),
        },
        "notes": "–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ CUDA: warmup + –∏–∑–º–µ—Ä–µ–Ω–∏–µ peak. 'Buf' = caching allocator (reserved - peak allocated).",
    }


def calculate_memory_footprint(config, batch_size, distributed_mode="default", num_gpus=1, *, method: str = "estimate"):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫—É/–∑–∞–º–µ—Ä VRAM –¥–ª—è –ø—Ä–µ–≤—å—é.
    method:
      - 'estimate': –±—ã—Å—Ç—Ä–∞—è —Ñ–æ—Ä–º—É–ª–∞ (fallback)
      - 'profile_cuda': —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–º–µ—Ä –Ω–∞ —Ç–µ–∫—É—â–µ–π GPU (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ/–º–æ–∂–µ—Ç OOM)
    """
    try:
        if method == "profile_cuda":
            return _profile_memory_footprint_cuda(config, batch_size=int(batch_size))
        return _estimate_memory_footprint(config, batch_size=int(batch_size), distributed_mode=distributed_mode, num_gpus=num_gpus)
    except Exception as e:
        print(f"Error calculating VRAM ({method}): {e}")
        # –§–æ–ª–±—ç–∫ –Ω–∞ –æ—Ü–µ–Ω–∫—É
        out = _estimate_memory_footprint(config, batch_size=int(batch_size), distributed_mode=distributed_mode, num_gpus=num_gpus)
        out["notes"] = f"{out.get('notes','')} | profile error: {e}"
        return out


def render_model_preview(config: dict, distributed_config: dict = None):
    """–ü—Ä–µ–≤—å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞."""
    st.subheader("üìê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏")
    
    stage = config.get("stage", "pretrain")
    if stage == "sft":
        st.info(f"üîÑ **–†–µ–∂–∏–º SFT** (Fine-Tuning)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    elif stage == "continual_pretrain":
        st.info(f"üîÑ **–†–µ–∂–∏–º Continual Pretraining** (–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)\n–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `{Path(config.get('base_model_path') or 'Unknown').name}`")
    else:
        st.success("üèóÔ∏è **–†–µ–∂–∏–º Pretraining** (–° –Ω—É–ª—è)")

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å
    # –ù–∞–º –Ω—É–∂–µ–Ω batch_size –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (—ç—Ç–æ –±–∞—Ç—á –Ω–∞ –¥–µ–≤–∞–π—Å)
    batch_size = config.get("batch_size", 1)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º num_gpus –∏ distributed_mode –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    dist_mode = "default"
    n_gpus = 1
    if distributed_config:
        dist_mode = distributed_config.get("distributed_mode", "default")
        n_gpus = distributed_config.get("num_gpus", 1)
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º config –Ω–∞–ø—Ä—è–º—É—é (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ distributed_config –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)
    if config.get("num_gpus"):
        n_gpus = int(config.get("num_gpus", 1))
    if config.get("distributed_mode"):
        dist_mode = config.get("distributed_mode", "default")

    mem_method = "estimate"
    # if torch.cuda.is_available():
    #     with st.expander("üß† –ü–∞–º—è—Ç—å GPU: –æ—Ü–µ–Ω–∫–∞ vs —Ç–æ—á–Ω—ã–π –∑–∞–º–µ—Ä", expanded=False):
    #         st.caption("–û—Ü–µ–Ω–∫–∞ ‚Äî –º–≥–Ω–æ–≤–µ–Ω–Ω–æ, –Ω–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ. –¢–æ—á–Ω—ã–π –∑–∞–º–µ—Ä ‚Äî –∑–∞–ø—É—Å–∫–∞–µ—Ç 2 train-step –Ω–∞ GPU (warmup + –∏–∑–º–µ—Ä–µ–Ω–∏–µ) –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º/–º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –ø–æ OOM.")
    #         do_profile = st.checkbox("–°–¥–µ–ª–∞—Ç—å —Ç–æ—á–Ω—ã–π –∑–∞–º–µ—Ä –Ω–∞ CUDA (2 —à–∞–≥–∞)", value=False, key="profile_vram_cuda")
    #         if do_profile:
    #             mem_method = "profile_cuda"

    mem_info = calculate_memory_footprint(config, batch_size, dist_mode, n_gpus, method=mem_method)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Hidden Size", config["hidden_size"])
        st.metric("Layers", config["num_layers"])
    
    with col2:
        st.metric("Attention Heads", config["n_heads"])
        st.metric("Head Dim", config["hidden_size"] // config["n_heads"])
    
    with col3:
        st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(mem_info["params"]))
        
        # –¶–≤–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ –¥–ª—è 24GB –∫–∞—Ä—Ç—ã)
        val = mem_info["total_gb"]
        color = "normal"
        if val > 24: color = "off" # –∫—Ä–∞—Å–Ω—ã–π –æ—Ç—Ç–µ–Ω–æ–∫ –≤ –¥–µ–ª—å—Ç–µ –æ–±—ã—á–Ω–æ
        
        title = "VRAM (Profile)" if mem_info.get("method") == "profile_cuda" else "VRAM (Estimate)"
        st.metric(
            title,
            f"{val:.1f} GB",
            delta=f"M: {mem_info['model_gb']} + A: {mem_info['act_gb']} GB",
            delta_color=color,
            help=(mem_info.get("notes") or "M: Model+Optim (steady)\nA: Activations/temporaries (peak - steady)\nBuf: caching allocator")
        )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
    if mem_info["total_gb"] > 0:
        st.caption("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU:")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –±–∞—Ä —á–∞—Ä—Ç —á–µ—Ä–µ–∑ HTML/CSS –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
        total = mem_info["total_gb"]
        p_model = (mem_info["model_gb"] / total) * 100
        p_act = (mem_info["act_gb"] / total) * 100
        buf_gb = float(mem_info.get("buf_gb", 0.0))
        p_buff = (buf_gb / total) * 100 if total > 0 else 0
        
        st.markdown(f"""
        <div style="display: flex; height: 20px; width: 100%; background: #333; border-radius: 4px; overflow: hidden; margin-top: 5px;">
            <div style="width: {p_model}%; background: #3b82f6; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Model & Optim">Model</div>
            <div style="width: {p_act}%; background: #e94560; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Activations">Act</div>
            <div style="width: {p_buff}%; background: #777; text-align: center; color: white; font-size: 10px; line-height: 20px;" title="Buffer">Buf</div>
        </div>
        <div style="display: flex; justify-content: space-between; font-size: 12px; color: #888; margin-top: 2px;">
            <span>Model + Optim: {mem_info['model_gb']} GB</span>
            <span>Activations: {mem_info['act_gb']} GB</span>
        </div>
        """, unsafe_allow_html=True)

        if mem_info.get("notes"):
            st.caption(mem_info["notes"])

        if mem_info.get("method") == "profile_cuda" and isinstance(mem_info.get("detail"), dict):
            with st.expander("üîç –î–µ—Ç–∞–ª–∏ –∑–∞–º–µ—Ä–∞ (CUDA allocator / tensor sums)", expanded=False):
                st.json(mem_info["detail"])

        if mem_info["act_gb"] > mem_info["model_gb"] * 2:
            st.warning("‚ö†Ô∏è –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –∑–∞–Ω–∏–º–∞—é—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏! –í–∫–ª—é—á–∏—Ç–µ Gradient Checkpointing –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ Batch Size.")

    
    # –í–∏–∑—É–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    st.markdown(f"""
    <div class="model-ascii">
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         HomeForCausalLM             ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ  Embedding: 50257 ‚Üí {config['hidden_size']:4d}           ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇ  ‚îÇ HomeBlock √ó {config['num_layers']:2d}              ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí Attention      ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ {config['n_heads']:2d} heads √ó {config['hidden_size']//config['n_heads']:3d} dim      ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí FFN (SwiGLU)   ‚îÇ    ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
    ‚îÇ  RMSNorm ‚Üí LM Head                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    </div>
        """, unsafe_allow_html=True)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–µ

    if distributed_config:
        st.subheader("‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º")
        
        mode = distributed_config.get("distributed_mode", "default")
        mode_info = PARALLEL_TYPES.get(mode, PARALLEL_TYPES["default"])
        num_gpus = distributed_config.get("num_gpus", 0)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("–†–µ–∂–∏–º", mode_info["name"])
        
        with col2:
            st.metric("–¢–∏–ø", mode_info["type"])
        
        with col3:
            if num_gpus > 0:
                st.metric("GPU", f"{num_gpus} —à—Ç.")
            else:
                st.metric("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "CPU")
        
        # –°—Ö–µ–º–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        if mode == "default":
            parallel_diagram = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Single Device      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Full Model      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Optimizer  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Gradients  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
        elif mode == "multi_gpu":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Data Parallel ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (copy)  ‚îÇ  ‚îÇ (copy)  ‚îÇ       ‚îÇ (copy)  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Sync Gradients ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ö–∞–∂–¥–∞—è GPU: –ø–æ–ª–Ω–∞—è –∫–æ–ø–∏—è –º–æ–¥–µ–ª–∏, —á–∞—Å—Ç—å –±–∞—Ç—á–∞
"""
        elif mode == "fsdp":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FSDP (Fully Sharded) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Shard 0 ‚îÇ  ‚îÇ Shard 1 ‚îÇ       ‚îÇ Shard N ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Params  ‚îÇ  ‚îÇ Params  ‚îÇ       ‚îÇ Params  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ All-Gather for Forward ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ Reduce-Scatter Backward ‚îÄ‚îò       ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ú–æ–¥–µ–ª—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –º–µ–∂–¥—É GPU (—ç–∫–æ–Ω–æ–º–∏—è VRAM)
"""
        elif "deepspeed" in mode:
            if "zero3" in mode:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Params  ‚îÇ  ‚îÇ Params  ‚îÇ       ‚îÇ Params  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim   ‚îÇ  ‚îÇ Optim   ‚îÇ       ‚îÇ Optim   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  {'+ CPU Offload (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ CPU)' if 'offload' in mode else ''}          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–í—Å—ë —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–æ: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è VRAM
"""
            else:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Full    ‚îÇ  ‚îÇ Full    ‚îÇ       ‚îÇ Full    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim/N ‚îÇ  ‚îÇ Optim/N ‚îÇ       ‚îÇ Optim/N ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω—ã
"""
        else:
            parallel_diagram = ""
        
        if parallel_diagram:
            st.markdown(f"""
<div class="model-ascii">
{parallel_diagram}
</div>
            """, unsafe_allow_html=True)


def export_model_to_hf(model, tokenizer, source_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π HF —Ñ–æ—Ä–º–∞—Ç.
    
    –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LoRA/QLoRA, –º–µ—Ä–¥–∂–∏—Ç –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É,
    —á—Ç–æ–±—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±—ã–ª–∞ "–≥–æ—Ç–æ–≤–æ–π" –∏ –∑–∞–≥—Ä—É–∂–∞–ª–∞—Å—å –∫–∞–∫ –æ–±—ã—á–Ω–∞—è.
    """
    try:
        from peft import PeftModel
        
        source = Path(source_path)
        # –°–æ–∑–¥–∞–µ–º –∏–º—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: export_TIMESTAMP
        export_name = f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –ï—Å–ª–∏ —ç—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä—è–¥–æ–º —Å –Ω–∏–º, –Ω–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
        # out/model/run/checkpoint_X -> out/model/run/export_X
        if "checkpoint" in source.name:
            export_dir = source.parent / f"export_{source.name}"
        else:
            export_dir = source.parent / export_name
            
        export_dir.mkdir(parents=True, exist_ok=True)
        
        # –ï—Å–ª–∏ —ç—Ç–æ PEFT-–º–æ–¥–µ–ª—å (LoRA/QLoRA) ‚Äî –º–µ—Ä–¥–∂–∏–º –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É
        export_model = model
        try:
            if isinstance(model, PeftModel):
                logger.info("Merging LoRA adapter into base model for export...")
                export_model = model.merge_and_unload()
                logger.info("LoRA adapter merged successfully")
        except Exception as e:
            logger.warning(f"LoRA merge failed during export, saving as-is: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        export_model.save_pretrained(export_dir, safe_serialization=True)
        tokenizer.save_pretrained(export_dir)
        
        return str(export_dir)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
        return None


# ============================================================================
# Main App
# ============================================================================

def main():
    render_header()
    
    # Sidebar configs
    model_config = render_model_config()
    st.session_state.current_model_name = model_config.get("model_name_input", "home_model")
    
    current_stage = model_config.get("stage", "pretrain")
    
    # GRPO –∏–º–µ–µ—Ç —Å–≤–æ—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è
    if current_stage == "grpo":
        grpo_sidebar_config = render_grpo_sidebar_config()
        # –î–ª—è GRPO –Ω–µ –Ω—É–∂–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        training_config = {}
        distributed_config = {"distributed_mode": "single_gpu", "num_gpus": 1, "config_file": None, "gpu_ids": []}
    else:
        grpo_sidebar_config = {}
        training_config = render_training_config()
        distributed_config = render_distributed_config(training_config=training_config)
    
    # –ü–µ—Ä–µ–¥–∞–µ–º stage –≤ dataset_config
    # –î–ª—è GRPO –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ main area
    if current_stage != "grpo":
        dataset_config = render_dataset_config(stage=current_stage)
    else:
        dataset_config = {}
    
    output_config = render_output_config(st.session_state.current_model_name)
    
    # Merge configs
    full_config = {**model_config, **training_config, **dataset_config, **output_config, **grpo_sidebar_config}
    full_config["distributed_mode"] = distributed_config["distributed_mode"]
    full_config["num_gpus"] = distributed_config["num_gpus"]
    full_config["config_file"] = distributed_config["config_file"]
    full_config["gpu_ids"] = distributed_config.get("gpu_ids", [])
    
    # –î–ª—è SFT, Continual Pretrain –∏ GRPO –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    if model_config.get("stage") in ("sft", "continual_pretrain", "grpo") and model_config.get("base_model_path"):
        full_config["tokenizer_path"] = model_config["base_model_path"]
    
    # Main content
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(["üöÄ –ó–∞–ø—É—Å–∫", "üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "üí¨ –ß–∞—Ç", "üìú –ò—Å—Ç–æ—Ä–∏—è", "üíæ –î–∞–Ω–Ω—ã–µ", "ü§ñ –ú–æ–¥–µ–ª–∏", "üìö –£—á–µ–±–Ω–∏–∫"])
    
    with tab1:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # –ü–µ—Ä–µ–¥–∞–µ–º full_config, —á—Ç–æ–±—ã –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –ø–∞–º—è—Ç–∏ –≤–∏–¥–µ–ª batch_size –∏ grad_checkpoint
            render_model_preview(full_config, distributed_config)
            
            # SFT Config (Main Area)
            if model_config.get("stage") == "sft" and dataset_config.get("data_path"):
                st.markdown("---")
                # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é (–¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∞, –≤—ã–∑–æ–≤–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è)
                sft_cfg = render_sft_main_config(dataset_config["data_path"])
                full_config.update(sft_cfg)
            
            # GRPO Config (Main Area) - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ reward —Ñ—É–Ω–∫—Ü–∏–π –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            if model_config.get("stage") == "grpo":
                st.markdown("---")
                grpo_main_cfg = render_grpo_main_config(dataset_config.get("data_path"))
                full_config.update(grpo_main_cfg)
            
            st.subheader("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
            st.json(full_config)
        
        with col2:
            st.subheader("üéÆ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
            
            if st.session_state.training_active:
                if st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", type="primary"):
                    with st.spinner("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É..."):
                        stopped = stop_training()
                    if stopped:
                        st.success("‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    else:
                        st.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å (–≤–æ–∑–º–æ–∂–Ω–æ —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞)")
                    time.sleep(1)
                    st.rerun()
            else:
                # –î–ª—è GRPO –¥—Ä—É–≥–∞—è –∫–Ω–æ–ø–∫–∞ –∏ –ª–æ–≥–∏–∫–∞
                if model_config.get("stage") == "grpo":
                    if st.button("üß† –ù–∞—á–∞—Ç—å GRPO –æ–±—É—á–µ–Ω–∏–µ", type="primary"):
                        with st.spinner("–ó–∞–ø—É—Å–∫ GRPO..."):
                            run_id, process = start_grpo_training(full_config)
                            st.session_state.current_run_id = run_id
                            st.session_state.training_process = process
                            st.session_state.training_active = True
                            
                            save_active_run(run_id, full_config)
                            
                            st.success(f"GRPO –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ! Run ID: {run_id}")
                            time.sleep(1)
                            st.rerun()
                else:
                    if st.button("‚ñ∂Ô∏è –ù–∞—á–∞—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É", type="primary"):
                        with st.spinner("–ó–∞–ø—É—Å–∫..."):
                            run_id, process = start_training(full_config)
                            st.session_state.current_run_id = run_id
                            st.session_state.training_process = process
                            st.session_state.training_active = True
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–π run –¥–ª—è persistence
                            save_active_run(run_id, full_config)
                            
                            st.success(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞! Run ID: {run_id}")
                            time.sleep(1)
                            st.rerun()
    
    with tab2:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fragment –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        live_metrics_fragment()
    
    with tab4:
        st.header("üìú –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—É—Å–∫–æ–≤")
        st.markdown("---")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã —Ç–∏–ø–∞ active_run.json)
        runs = sorted([p for p in RUNS_DIR.iterdir() if p.is_dir()], reverse=True)
        
        if runs:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø—É—Å–∫–æ–≤
            for run_dir in runs[:30]: 
                run_id = run_dir.name
                metrics = load_metrics(run_id)
                
                if metrics:
                    status = metrics.get("status", "unknown")
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–∞–ø—É—Å–∫–∏ (–µ—Å–ª–∏ –æ–Ω–∏ —Å—Ç–∞—Ä—ã–µ –∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–¥–µ–ª–∞–ª–∏)
                    is_empty = metrics.get("current_step", 0) == 0 and not metrics.get("checkpoints")
                    if is_empty and status not in ("training", "running"):
                         continue

                    status_emoji = {"training": "üü¢", "completed": "‚úÖ", "error": "‚ùå", "stopped": "‚èπÔ∏è", "resumed": "‚ñ∂Ô∏è"}.get(status, "‚è≥")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    model_name_display = run_id
                    try:
                        config_path = run_dir / "config.json"
                        if config_path.exists():
                            with open(config_path) as f:
                                rc = json.load(f)
                                # –ò—â–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ output_dir
                                if "model_name_input" in rc:
                                    model_name_display = f"{run_id} | {rc['model_name_input']}"
                                elif "output_dir" in rc:
                                    out_d = Path(rc["output_dir"])
                                    # out/home_pretrain/run_id -> home_pretrain
                                    if out_d.name == run_id:
                                        model_name_display = f"{run_id} | {out_d.parent.name}"
                                    else:
                                        model_name_display = f"{run_id} | {out_d.name}"
                    except:
                        pass
                    
                    with st.expander(f"{status_emoji} {model_name_display}"):
                        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ metrics.json (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –∏–∑ config
                        model_params = None
                        try:
                            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ metrics.json (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ)
                            if "num_parameters" in metrics:
                                model_params = metrics["num_parameters"]
                            else:
                                # Fallback: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑ config
                                config_path = run_dir / "config.json"
                                if config_path.exists():
                                    with open(config_path) as f:
                                        rc = json.load(f)
                                        if "hidden_size" in rc and "num_layers" in rc:
                                            vocab_size = rc.get("vocab_size", 50257)
                                            intermediate_size = rc.get("intermediate_size")
                                            model_params = estimate_parameters(
                                                rc["hidden_size"],
                                                rc["num_layers"],
                                                vocab_size=vocab_size,
                                                intermediate_size=intermediate_size,
                                            )
                        except Exception:
                            pass
                        
                        col1, col2, col3, col4, col5 = st.columns(5)
                        with col1:
                            st.metric("Steps", metrics.get("current_step", 0))
                        with col2:
                            st.metric("Final Loss", f"{metrics.get('current_loss', 0):.4f}")
                        with col3:
                            if model_params:
                                st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(model_params))
                            else:
                                st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", "‚Äî")
                        with col4:
                            st.metric("Status", status)
                        with col5:
                            st.metric("Duration", metrics.get("training_duration", "-"))
                        
                        # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                        checkpoints = metrics.get("checkpoints", [])
                        if checkpoints:
                            st.markdown("**üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã:**")
                            # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è loss –∏–∑ history, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
                            loss_history = metrics.get("loss_history", [])
                            steps_history = metrics.get("steps_history", [])
                            
                            for ckpt in checkpoints:
                                ckpt_loss = ckpt.get("loss")
                                # –ï—Å–ª–∏ loss –Ω–µ—Ç –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ loss_history
                                if ckpt_loss is None and steps_history and loss_history:
                                    ckpt_step = ckpt.get("step")
                                    if ckpt_step in steps_history:
                                        idx = steps_history.index(ckpt_step)
                                        if idx < len(loss_history):
                                            ckpt_loss = loss_history[idx]
                                
                                if ckpt_loss is not None:
                                    st.caption(f"Step {ckpt['step']}: Loss {ckpt_loss:.4f} | {ckpt['path']}")
                                else:
                                    st.caption(f"Step {ckpt['step']}: {ckpt['path']}")
                        
                        # –ö–Ω–æ–ø–∫–∏
                        btn_col1, btn_col2, btn_col3 = st.columns(3)
                        with btn_col1:
                            if st.button(f"üìä –ú–µ—Ç—Ä–∏–∫–∏", key=f"metrics_{run_id}"):
                                st.session_state.current_run_id = run_id
                                st.toast(f"‚úÖ –í—ã–±—Ä–∞–Ω run: {run_id}. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", icon="üìä")
                        with btn_col2:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞
                            config_path = run_dir / "config.json"
                            if config_path.exists():
                                try:
                                    with open(config_path) as f:
                                        run_config = json.load(f)
                                    model_dir = PROJECT_ROOT / run_config.get("output_dir", "")
                                    final_model = model_dir / "final_model"
                                    if final_model.exists():
                                        if st.button("üí¨ –ß–∞—Ç", key=f"chat_run_{run_id}"):
                                            st.session_state.selected_chat_model = str(final_model)
                                            st.toast("‚úÖ –ú–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üí¨ –ß–∞—Ç", icon="üí¨")
                                except:
                                    pass
                        with btn_col3:
                            # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–µ—Å–ª–∏ –±—ã–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã)
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —á–µ–∫–ø–æ–∏–Ω—Ç –†–ï–ê–õ–¨–ù–û —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–∞ –¥–∏—Å–∫–µ
                            valid_ckpt = None
                            if checkpoints:
                                latest_ckpt_path = checkpoints[-1]['path']
                                # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –µ—Å–ª–∏ –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
                                abs_ckpt_path = Path(latest_ckpt_path)
                                if not abs_ckpt_path.is_absolute():
                                    abs_ckpt_path = PROJECT_ROOT / latest_ckpt_path
                                
                                if abs_ckpt_path.exists():
                                    valid_ckpt = str(abs_ckpt_path)

                            if valid_ckpt:
                                if st.button("‚ñ∂Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å", key=f"continue_{run_id}", help="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"):
                                    try:
                                        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å—Ç–∞—Ä–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                                        config_path = run_dir / "config.json"
                                        with open(config_path) as f:
                                            old_config = json.load(f)
                                        
                                        # 3. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º output_dir —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
                                        # –°—Ç–∞—Ä—ã–π output_dir —É–∫–∞–∑—ã–≤–∞–ª –Ω–∞ –ø–∞–ø–∫—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ (run_ID)
                                        # –ú—ã —Ö–æ—Ç–∏–º —á—Ç–æ–±—ã –Ω–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –±—ã–ª –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å —Å—Ç–∞—Ä—ã–º (–≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –ø–∞–ø–∫–µ)
                                        old_output_dir = Path(old_config.get("output_dir", ""))
                                        # –ï—Å–ª–∏ –ø—É—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π - –±–µ—Ä–µ–º —Ä–æ–¥–∏—Ç–µ–ª—è. –ï—Å–ª–∏ –Ω–µ—Ç - —Ç–æ–∂–µ (–Ω–∞–¥–µ–µ–º—Å—è)
                                        # start_training –æ–∂–∏–¥–∞–µ—Ç –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                                        old_config["output_dir"] = str(old_output_dir.parent)
                                        
                                        # 4. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ resume (–ê–ë–°–û–õ–Æ–¢–ù–´–ô –ü–£–¢–¨)
                                        old_config["resume_from_checkpoint"] = valid_ckpt
                                        
                                        # 5. –ó–∞–ø—É—Å–∫–∞–µ–º
                                        with st.spinner(f"–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å {valid_ckpt}..."):
                                            new_run_id, process = start_training(old_config)
                                            st.session_state.current_run_id = new_run_id
                                            st.session_state.training_process = process
                                            st.session_state.training_active = True
                                            
                                            save_active_run(new_run_id, old_config)
                                            
                                            st.success(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∞! Run ID: {new_run_id}")
                                            time.sleep(1)
                                            st.rerun()
                                            
                                    except Exception as e:
                                        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å: {e}")
                            elif checkpoints:
                                # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –±—ã–ª–∏ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö, –Ω–æ —É–¥–∞–ª–µ–Ω—ã —Å –¥–∏—Å–∫–∞
                                st.button("‚ö†Ô∏è –§–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã", key=f"gone_{run_id}", disabled=True, help=f"–ß–µ–∫–ø–æ–∏–Ω—Ç {checkpoints[-1]['path']} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –¥–∏—Å–∫–µ")
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –≤—ã–±—Ä–∞–Ω–æ
                        if st.session_state.current_run_id == run_id:
                            st.info("üëÜ –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**")
        else:
            st.info("–ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤")
            
    with tab5:
        render_data_manager()
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø—Ä–æ —á–∞—Ç
        st.markdown("---")
        st.info("üí° –ß—Ç–æ–±—ã –ø–æ–æ–±—â–∞—Ç—å—Å—è —Å –º–æ–¥–µ–ª—å—é, –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üí¨ –ß–∞—Ç** (–≤ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)")

    with tab6:
        render_model_manager()
    
    with tab7:
        render_docs()
    
    with tab3:
        st.header("üí¨ –ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é")
        st.markdown("---")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        available_models = get_available_models()
        
        if available_models:
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            col1, col2 = st.columns([3, 1])
            
            with col1:
                model_options = [m["name"] for m in available_models]
                
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ - –Ω–∞—Ö–æ–¥–∏–º –µ—ë –∏–Ω–¥–µ–∫—Å
                default_idx = 0
                if st.session_state.selected_chat_model:
                    for i, m in enumerate(available_models):
                        if m["path"] == st.session_state.selected_chat_model:
                            default_idx = i
                            break
                
                selected_model_name = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç",
                    options=model_options,
                    index=default_idx,
                    help="–í—ã–±–µ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞"
                )
                selected_model = next(m for m in available_models if m["name"] == selected_model_name)
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º selected_chat_model –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                if st.session_state.selected_chat_model:
                    st.session_state.selected_chat_model = None
            
            with col2:
                # –î–ª—è —á–∞—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ final_model/export (HF —Ñ–æ—Ä–º–∞—Ç)
                # –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º AutoModelForCausalLM
                model_type = selected_model["type"]
                if model_type == "final":
                    st.success("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")
                else:
                    st.info("üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç")
            
            st.caption(f"–ü—É—Ç—å: `{selected_model['path']}`")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            with st.expander("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"):
                gen_col1, gen_col2, gen_col3 = st.columns(3)
                with gen_col1:
                    max_tokens = st.slider("Max Tokens", 10, 500, 128)
                with gen_col2:
                    temperature = st.slider("Temperature", 0.1, 2.0, 0.8, 0.1)
                with gen_col3:
                    top_p = st.slider("Top-p", 0.1, 1.0, 0.9, 0.05)

                # –†–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞ (2 —Ä–µ–∂–∏–º–∞, –Ω–æ –¥–µ—Ñ–æ–ª—Ç –≤—ã–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏):
                # - –µ—Å–ª–∏ —É –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å chat_template -> –î–∏–∞–ª–æ–≥
                # - –µ—Å–ª–∏ –Ω–µ—Ç -> Completion
                if "chat_prompt_mode" not in st.session_state:
                    st.session_state.chat_prompt_mode = "completion"  # –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
                prompt_mode_label = st.selectbox(
                    "–†–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞",
                    options=["–î–∏–∞–ª–æ–≥ (chat_template)", "Completion (plain text)"],
                    index=0 if st.session_state.chat_prompt_mode == "chat" else 1,
                    help="–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –µ—Å–ª–∏ —É –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å chat_template ‚Äî –≤–∫–ª—é—á–∞–µ–º –î–∏–∞–ª–æ–≥, –∏–Ω–∞—á–µ Completion. –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –≤—Ä—É—á–Ω—É—é.",
                    key="chat_prompt_mode_select",
                )
                prompt_mode = "chat" if prompt_mode_label.startswith("–î–∏–∞–ª–æ–≥") else "completion"
                st.session_state.chat_prompt_mode = prompt_mode
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞
            if "chat_model" not in st.session_state:
                st.session_state.chat_model = None
                st.session_state.chat_tokenizer = None
                st.session_state.chat_model_path = None
                st.session_state.chat_has_template = False
                st.session_state.chat_prompt_mode = "completion"
            
            if "messages" not in st.session_state:
                st.session_state.messages = []
            
            # –ö–Ω–æ–ø–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            if st.session_state.chat_model_path != selected_model["path"]:
                if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
                    with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å..."):
                        try:
                            from transformers import AutoTokenizer, AutoModelForCausalLM
                            from homellm.models.adapters import detect_model_type
                            from homellm.models.home_model import HomeForCausalLM
                            
                            model_path = Path(selected_model["path"])
                            device = "cuda" if torch.cuda.is_available() else "cpu"
                            dtype = torch.float16 if device == "cuda" else torch.float32
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ config.json
                            config_json = model_path / "config.json"
                            if not config_json.exists():
                                raise ValueError(f"config.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {model_path}")
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
                            model_type = detect_model_type(model_path)
                            st.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º {model_type.upper()} –º–æ–¥–µ–ª—å...")
                            
                            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                            try:
                                tokenizer = AutoTokenizer.from_pretrained(str(model_path), trust_remote_code=True)
                            except Exception:
                                # –î–ª—è accelerate checkpoint'–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –æ–±—ã—á–Ω–æ –Ω–µ—Ç –≤–Ω—É—Ç—Ä–∏ checkpoint_stepXXXX.
                                # –ü—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å tokenizer_path/base_model_path –∏–∑ run config.
                                tokenizer = None
                                try:
                                    run_root = model_path.parent if "checkpoint" in model_path.name else model_path
                                    run_id = run_root.name
                                    run_cfg_path = RUNS_DIR / run_id / "config.json"
                                    if run_cfg_path.exists():
                                        with open(run_cfg_path, "r", encoding="utf-8") as f:
                                            run_cfg = json.load(f)
                                        tok_src = run_cfg.get("tokenizer_path") or run_cfg.get("base_model_path")
                                        if tok_src:
                                            tokenizer = AutoTokenizer.from_pretrained(str(tok_src), trust_remote_code=True)
                                except Exception:
                                    tokenizer = None

                                if tokenizer is None:
                                    tokenizer = AutoTokenizer.from_pretrained("gpt2")
                            
                            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
                            if model_type == "home":
                                model = HomeForCausalLM.from_pretrained(str(model_path), torch_dtype=dtype)
                            else:
                                model = AutoModelForCausalLM.from_pretrained(
                                    str(model_path), trust_remote_code=True, torch_dtype=dtype
                                )
                            
                            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ device
                            model = model.to(device)
                            model.eval()
                            
                            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (pad_token = eos_token)
                            if tokenizer.pad_token is None:
                                if tokenizer.eos_token:
                                    tokenizer.pad_token = tokenizer.eos_token
                            
                            st.session_state.chat_model = model
                            st.session_state.chat_tokenizer = tokenizer
                            st.session_state.chat_model_path = str(model_path)
                            st.session_state.messages = []
                            # –ê–≤—Ç–æ–ø–æ–¥—Ç—è–≥–∏–≤–∞–Ω–∏–µ —Ä–µ–∂–∏–º–∞: –µ—Å–ª–∏ —É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –µ—Å—Ç—å chat_template, —Å—Ç–∞–≤–∏–º "–î–∏–∞–ª–æ–≥",
                            # –∏–Ω–∞—á–µ "Completion". –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –≤—Ä—É—á–Ω—É—é –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏.
                            st.session_state.chat_has_template = bool(getattr(tokenizer, "chat_template", None))
                            st.session_state.chat_prompt_mode = "chat" if st.session_state.chat_has_template else "completion"
                            st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                            st.rerun()
                        except Exception as e:
                            import traceback
                            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                            st.code(traceback.format_exc())
                            
                            # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ (–µ—Å–ª–∏ AutoModelForCausalLM –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
                            st.warning("–ü—Ä–æ–±—É–µ–º fallback –∑–∞–≥—Ä—É–∑–∫—É –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤...")
                            try:
                                from homellm.models.home_model import HomeForCausalLM, HomeConfig
                                from safetensors.torch import load_file
                                
                                model_safetensors = model_path / "model.safetensors"
                                model_bin = model_path / "pytorch_model.bin"
                                
                                if not (model_safetensors.exists() or model_bin.exists()):
                                    raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏")
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                                if not st.session_state.get("chat_tokenizer"):
                                    st.session_state.chat_tokenizer = AutoTokenizer.from_pretrained("gpt2")
                                    if st.session_state.chat_tokenizer.pad_token is None:
                                        st.session_state.chat_tokenizer.pad_token = st.session_state.chat_tokenizer.eos_token
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
                                if config_json.exists():
                                    config = HomeConfig.from_pretrained(str(model_path))
                                else:
                                    # –ò—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                                    run_config_path = model_path.parent / "run_config.json"
                                    if run_config_path.exists():
                                        import json as json_module
                                        with open(run_config_path) as f:
                                            run_cfg = json_module.load(f)
                                        config = HomeConfig(
                                            vocab_size=len(st.session_state.chat_tokenizer),
                                            hidden_size=run_cfg.get("hidden_size", 512),
                                            num_hidden_layers=run_cfg.get("num_layers", 8),
                                            num_attention_heads=run_cfg.get("n_heads", 8),
                                            max_position_embeddings=run_cfg.get("seq_len", 512),
                                        )
                                    else:
                                        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω config.json")
                                
                                # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                                st.session_state.chat_model = HomeForCausalLM(config)
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
                                if model_safetensors.exists():
                                    state_dict = load_file(str(model_safetensors))
                                else:
                                    state_dict = torch.load(str(model_bin), map_location="cpu")
                                
                                missing, unexpected = st.session_state.chat_model.load_state_dict(state_dict, strict=False)
                                
                                if missing:
                                    real_missing = [k for k in missing if k != "lm_head.weight"]
                                    if real_missing:
                                        st.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–µ—Å–∞: {real_missing[:5]}...")
                                
                                if hasattr(st.session_state.chat_model, "tie_weights"):
                                    st.session_state.chat_model.tie_weights()
                                
                                st.session_state.chat_model = st.session_state.chat_model.to(device)
                                st.session_state.chat_model.eval()
                                st.session_state.chat_model_path = str(model_path)
                                st.session_state.messages = []
                                st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (fallback –º–µ—Ç–æ–¥)!")
                                st.rerun()
                            except Exception as e2:
                                st.error(f"Fallback –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
                                import traceback
                                st.code(traceback.format_exc())
            else:
                st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {selected_model_name}")
                
                # –ö–Ω–æ–ø–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ (–¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–∞)
                if st.button("üíæ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HF —Ñ–æ—Ä–º–∞—Ç", help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (—Å –∫–æ–Ω—Ñ–∏–≥–æ–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º)"):
                    with st.spinner("–≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏..."):
                        export_path = export_model_to_hf(
                            st.session_state.chat_model, 
                            st.session_state.chat_tokenizer, 
                            st.session_state.chat_model_path
                        )
                        if export_path:
                            st.success(f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤:\n`{export_path}`")
                            time.sleep(2)
                            st.rerun() # –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —ç–∫—Å–ø–æ—Ä—Ç

                # --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–ü–¢–ê ---
                with st.expander("‚öôÔ∏è –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", expanded=False):
                    system_prompt_input = st.text_area(
                        "–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):",
                        value=st.session_state.get("system_prompt", ""),
                        help="–ï—Å–ª–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤–º–µ—Å—Ç–æ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –º–æ–¥–µ–ª–∏. –û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ.",
                        key="system_prompt_input"
                    )
                    st.session_state.system_prompt = system_prompt_input.strip()
                    if system_prompt_input.strip():
                        st.info("‚úÖ –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤–≤–µ–¥–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç")
                    else:
                        st.caption("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –º–æ–¥–µ–ª–∏")
                
                # --- –ò–ù–¢–ï–†–§–ï–ô–° –ß–ê–¢–ê –° –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ú –°–ö–†–û–õ–õ–û–ú ---
                chat_container = st.container(height=500) # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                
                with chat_container:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
                    for message in st.session_state.messages:
                        with st.chat_message(message["role"]):
                            st.write(message["content"])
                
                # –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤—Å–µ–≥–¥–∞ –≤–Ω–∏–∑—É)
                if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."):
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —é–∑–µ—Ä–∞ —Å—Ä–∞–∑—É)
                    with chat_container:
                        with st.chat_message("user"):
                            st.write(prompt)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                    with chat_container: # –û—Ç–≤–µ—Ç —Ç–æ–∂–µ –ø–∏—à–µ–º –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                        with st.chat_message("assistant"):
                            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è..."):
                                try:
                                    tokenizer = st.session_state.chat_tokenizer
                                    model = st.session_state.chat_model
                                    device = next(model.parameters()).device
                                    
                                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                                    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Ä–µ–∂–∏–º: auto/chat_template/plain
                                    
                                    # –ë–µ—Ä–µ–º –∏—Å—Ç–æ—Ä–∏—é + –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                                    conversation = st.session_state.messages.copy() # [{"role": "user", ...}, ...]
                                    
                                    has_template = bool(getattr(tokenizer, "chat_template", None))
                                    use_chat_template = (prompt_mode == "chat") and has_template
                                    if prompt_mode == "chat" and not has_template:
                                        st.warning("–£ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ—Ç chat_template ‚Äî –∏—Å–ø–æ–ª—å–∑—É—é Completion.")
                                        use_chat_template = False

                                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ chat_template)
                                    if use_chat_template:
                                        system_prompt = st.session_state.get("system_prompt", "").strip()
                                        
                                        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ conversation (–µ—Å–ª–∏ –µ—Å—Ç—å)
                                        # —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ —Å –≤–≤–µ–¥–µ–Ω–Ω—ã–º —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                                        if conversation and conversation[0].get("role") == "system":
                                            conversation.pop(0)
                                        
                                        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ –Ω–∞—á–∞–ª–æ
                                        if system_prompt:
                                            conversation.insert(0, {"role": "system", "content": system_prompt})
                                        # –ï—Å–ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø—É—Å—Ç–æ–π, —à–∞–±–ª–æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏–∑ –º–æ–¥–µ–ª–∏
                                    
                                    if use_chat_template:
                                        # –î–ª—è SFT –º–æ–¥–µ–ª–∏: –ø—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —Å —Ç–µ–≥–∞–º–∏
                                        prompt_text = tokenizer.apply_chat_template(
                                            conversation, 
                                            tokenize=False, 
                                            add_generation_prompt=True
                                        )
                                    else:
                                        # –î–ª—è Base/Pretrain –º–æ–¥–µ–ª–∏: –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
                                        # –û–±—ã—á–Ω–æ Base –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–∏–∞–ª–æ–≥, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ —Å–ª–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–º–ø—Ç
                                        # –∏–ª–∏ –≤–µ—Å—å –¥–∏–∞–ª–æ–≥ —Ç–µ–∫—Å—Ç–æ–º
                                        prompt_text = ""
                                        for m in conversation:
                                            prompt_text += f"{m['role']}: {m['content']}\n"
                                        prompt_text += "assistant: "
                                    
                                    # –í–ê–ñ–ù–û:
                                    # - inputs –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ –¢–û–ú –ñ–ï —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å (–º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ cuda:1)
                                    # - –∑–∞—Ä–∞–Ω–µ–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ token ids –Ω–µ –≤—ã—Ö–æ–¥—è—Ç –∑–∞ vocab –º–æ–¥–µ–ª–∏ (–∏–Ω–∞—á–µ –±—É–¥–µ—Ç CUDA assert)
                                    device = next(model.parameters()).device
                                    device_type = str(device.type)

                                    enc = tokenizer(prompt_text, return_tensors="pt")
                                    try:
                                        if hasattr(model, "get_input_embeddings") and model.get_input_embeddings() is not None:
                                            vocab_size = int(model.get_input_embeddings().weight.shape[0])
                                            max_id = int(enc["input_ids"].max().item())
                                            min_id = int(enc["input_ids"].min().item())
                                            if min_id < 0 or max_id >= vocab_size:
                                                raise ValueError(
                                                    f"Tokenizer –≤—ã–¥–∞—ë—Ç token_id –≤–Ω–µ vocab –º–æ–¥–µ–ª–∏: min_id={min_id}, max_id={max_id}, "
                                                    f"vocab_size(model)={vocab_size}. "
                                                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Å –º–æ–¥–µ–ª—å—é –∑–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π tokenizer (—Ç–æ—Ç –∂–µ, —á—Ç–æ –±—ã–ª –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)."
                                                )
                                    except Exception as e:
                                        raise RuntimeError(f"–ü—Ä–æ–±–ª–µ–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏/–≤–æ–∫–∞–±–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π: {e}")

                                    inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in enc.items()}
                                    
                                    # –ü—Ä–∏–≤–æ–¥–∏–º dtype –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ—Å–ª–µ SFT checkpoints bf16)
                                    model_dtype = next(model.parameters()).dtype
                                    autocast_enabled = (device_type == "cuda")
                                    autocast_dtype = torch.bfloat16 if model_dtype == torch.bfloat16 else torch.float16

                                    # attention_mask –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è int/bool (–Ω–µ bf16/fp16).

                                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ (–∏–Ω–∞—á–µ –≤–æ–∑–º–æ–∂–µ–Ω CUDA index out of bounds –ø—Ä–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ö).
                                    max_ctx = None
                                    try:
                                        cfg = getattr(model, "config", None)
                                        for key in ("max_position_embeddings", "n_positions", "seq_len", "max_seq_len"):
                                            if cfg is not None and hasattr(cfg, key):
                                                v = getattr(cfg, key)
                                                if v is not None:
                                                    max_ctx = int(v)
                                                    break
                                    except Exception:
                                        max_ctx = None

                                    if max_ctx is not None and max_ctx > 0 and "input_ids" in inputs:
                                        in_len = int(inputs["input_ids"].shape[1])
                                        if in_len > max_ctx:
                                            cut = in_len - max_ctx
                                            inputs["input_ids"] = inputs["input_ids"][:, cut:]
                                            if "attention_mask" in inputs and hasattr(inputs["attention_mask"], "shape"):
                                                inputs["attention_mask"] = inputs["attention_mask"][:, cut:]
                                            in_len = int(inputs["input_ids"].shape[1])

                                        allowed_new = int(max_ctx - in_len)
                                        if allowed_new <= 0:
                                            raise RuntimeError(
                                                f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —É–∂–µ –¥–æ—Å—Ç–∏–≥ –º–∞–∫—Å–∏–º—É–º–∞ –º–æ–¥–µ–ª–∏ (max_ctx={max_ctx}). "
                                                f"–£–º–µ–Ω—å—à–∏—Ç–µ –∏—Å—Ç–æ—Ä–∏—é/—Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
                                            )
                                        if int(max_tokens) > allowed_new:
                                            max_tokens = allowed_new

                                    with torch.no_grad(), torch.autocast(device_type=device_type, dtype=autocast_dtype, enabled=autocast_enabled):
                                        outputs = model.generate(
                                            **inputs,
                                            max_new_tokens=max_tokens,
                                            temperature=temperature,
                                            top_p=top_p,
                                            do_sample=True,
                                            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
                                            use_cache=False,  # –û—Ç–∫–ª—é—á–∞–µ–º KV-cache –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                                        )
                                    
                                    response = tokenizer.decode(
                                        outputs[0][inputs["input_ids"].shape[1]:], 
                                        skip_special_tokens=True
                                    )
                                    
                                    st.write(response)
                                    st.session_state.messages.append({"role": "assistant", "content": response})
                                except Exception as e:
                                    import traceback
                                    st.session_state.last_chat_error = traceback.format_exc()
                                    st.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                                    st.code(st.session_state.last_chat_error)
                
                # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
                if st.session_state.messages:
                    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
                        st.session_state.messages = []
                        st.rerun()
        else:
            st.info("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤–æ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–ø—É—Å–∫'!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥–¥–µ –∏—Å–∫–∞—Ç—å –º–æ–¥–µ–ª–∏
            st.markdown("""
            **–ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:**
            - `out/*/final_model/` ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
            - `out/*/checkpoint_*/` ‚Äî —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            """)


if __name__ == "__main__":
    main()


