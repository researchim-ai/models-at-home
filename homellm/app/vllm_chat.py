"""
vLLM backend –¥–ª—è inference –≤ —á–∞—Ç–µ.

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ vLLM:
- –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (continuous batching, PagedAttention)
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ LoRA hot-swap
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏

–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
- –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ VRAM –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ (–∫–æ–º–ø–∏–ª—è—Ü–∏—è CUDA –≥—Ä–∞—Ñ–æ–≤)
- –ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ (–Ω—É–∂–Ω–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vLLM)
"""
from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional, List, Dict, Any

logger = logging.getLogger(__name__)


def is_vllm_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å vLLM."""
    try:
        import vllm
        return True
    except ImportError:
        return False


class VLLMChatBackend:
    """vLLM backend –¥–ª—è inference –≤ —á–∞—Ç–µ."""
    
    def __init__(
        self,
        model_path: str,
        dtype: str = "float16",
        gpu_memory_utilization: float = 0.9,
        max_model_len: Optional[int] = None,
        trust_remote_code: bool = True,
        enable_lora: bool = False,
        max_lora_rank: int = 64,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vLLM backend.
        
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∏–ª–∏ HuggingFace model ID
            dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö ("float16", "bfloat16", "float32")
            gpu_memory_utilization: –î–æ–ª—è VRAM –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (0.0-1.0)
            max_model_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (None = –∞–≤—Ç–æ)
            trust_remote_code: –†–∞–∑—Ä–µ—à–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ –∏–∑ –º–æ–¥–µ–ª–∏
            enable_lora: –í–∫–ª—é—á–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
            max_lora_rank: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–Ω–≥ LoRA (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= lora_r –º–æ–¥–µ–ª–∏)
        """
        from vllm import LLM
        
        self.model_path = model_path
        self.dtype = dtype
        self._lora_path: Optional[str] = None
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è vLLM
        llm_kwargs = {
            "model": model_path,
            "trust_remote_code": trust_remote_code,
            "dtype": dtype,
            "tensor_parallel_size": 1,  # Single GPU
            "gpu_memory_utilization": gpu_memory_utilization,
            "enforce_eager": True,  # –û—Ç–∫–ª—é—á–∞–µ–º CUDA graphs –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        }
        
        if max_model_len is not None:
            llm_kwargs["max_model_len"] = max_model_len
        
        if enable_lora:
            llm_kwargs["enable_lora"] = True
            llm_kwargs["max_loras"] = 1
            llm_kwargs["max_lora_rank"] = max_lora_rank
        
        logger.info(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ vLLM: {model_path}")
        logger.info(f"   dtype={dtype}, gpu_util={gpu_memory_utilization}, enable_lora={enable_lora}")
        
        self.llm = LLM(**llm_kwargs)
        self.tokenizer = self.llm.get_tokenizer()
        
        logger.info("‚úÖ vLLM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def set_lora(self, lora_path: Optional[str] = None):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
        Args:
            lora_path: –ü—É—Ç—å –∫ LoRA –∞–¥–∞–ø—Ç–µ—Ä—É (None = –æ—Ç–∫–ª—é—á–∏—Ç—å LoRA)
        """
        self._lora_path = lora_path
        if lora_path:
            logger.info(f"üîß –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω LoRA –∞–¥–∞–ø—Ç–µ—Ä: {lora_path}")
        else:
            logger.info("üîß LoRA –∞–¥–∞–ø—Ç–µ—Ä –æ—Ç–∫–ª—é—á—ë–Ω")
    
    def generate(
        self,
        prompt: str,
        max_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = -1,
        stop: Optional[List[str]] = None,
        stream: bool = False,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            prompt: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            top_p: Top-p (nucleus) sampling
            top_k: Top-k sampling (-1 = –æ—Ç–∫–ª—é—á–µ–Ω–æ)
            stop: –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å—Ç—Ä–æ–∫
            stream: –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ø–æ–∫–∞)
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        from vllm import SamplingParams
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
        params_dict = {
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
        }
        
        if top_k > 0:
            params_dict["top_k"] = top_k
        
        if stop:
            params_dict["stop"] = stop
        
        sampling_params = SamplingParams(**params_dict)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏–ª–∏ –±–µ–∑ LoRA
        generate_kwargs = {}
        if self._lora_path:
            try:
                from vllm.lora.request import LoRARequest
            except ImportError:
                from vllm.lora import LoRARequest
            
            lora_request = LoRARequest("chat_lora", 1, self._lora_path)
            generate_kwargs["lora_request"] = lora_request
        
        outputs = self.llm.generate([prompt], sampling_params, **generate_kwargs)
        
        if outputs and outputs[0].outputs:
            return outputs[0].outputs[0].text
        return ""
    
    def generate_batch(
        self,
        prompts: List[str],
        max_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> List[str]:
        """
        –ë–∞—Ç—á–µ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            prompts: –°–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            top_p: Top-p sampling
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        """
        from vllm import SamplingParams
        
        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        
        generate_kwargs = {}
        if self._lora_path:
            try:
                from vllm.lora.request import LoRARequest
            except ImportError:
                from vllm.lora import LoRARequest
            
            lora_request = LoRARequest("chat_lora", 1, self._lora_path)
            generate_kwargs["lora_request"] = lora_request
        
        outputs = self.llm.generate(prompts, sampling_params, **generate_kwargs)
        
        results = []
        for output in outputs:
            if output.outputs:
                results.append(output.outputs[0].text)
            else:
                results.append("")
        
        return results
    
    def apply_chat_template(
        self,
        messages: List[Dict[str, str]],
        add_generation_prompt: bool = True,
    ) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç chat template —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π [{"role": "user", "content": "..."}, ...]
            add_generation_prompt: –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if hasattr(self.tokenizer, "apply_chat_template"):
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt,
            )
        else:
            # Fallback: –ø—Ä–æ—Å—Ç–∞—è –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è
            result = ""
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role == "system":
                    result += f"System: {content}\n\n"
                elif role == "user":
                    result += f"User: {content}\n\n"
                elif role == "assistant":
                    result += f"Assistant: {content}\n\n"
            if add_generation_prompt:
                result += "Assistant: "
            return result
    
    @property
    def has_chat_template(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ chat template —É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
        return hasattr(self.tokenizer, "chat_template") and self.tokenizer.chat_template is not None


class TransformersChatBackend:
    """Transformers backend –¥–ª—è inference (fallback –µ—Å–ª–∏ vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)."""
    
    def __init__(
        self,
        model,
        tokenizer,
        device: str = "cuda",
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transformers backend.
        
        Args:
            model: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (PreTrainedModel)
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è inference
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
    
    def generate(
        self,
        prompt: str,
        max_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        stop: Optional[List[str]] = None,
        stream: bool = False,
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ transformers."""
        import torch
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k if top_k > 0 else None,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
        generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    def apply_chat_template(
        self,
        messages: List[Dict[str, str]],
        add_generation_prompt: bool = True,
    ) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç chat template —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
        if hasattr(self.tokenizer, "apply_chat_template"):
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt,
            )
        else:
            # Fallback
            result = ""
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role == "system":
                    result += f"System: {content}\n\n"
                elif role == "user":
                    result += f"User: {content}\n\n"
                elif role == "assistant":
                    result += f"Assistant: {content}\n\n"
            if add_generation_prompt:
                result += "Assistant: "
            return result
    
    @property
    def has_chat_template(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ chat template —É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
        return hasattr(self.tokenizer, "chat_template") and self.tokenizer.chat_template is not None
