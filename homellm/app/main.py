"""
Motels at Home Training Studio ‚Äî –í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π
======================================================================

–ó–∞–ø—É—Å–∫:
    streamlit run homellm/app/main.py
    
–∏–ª–∏:
    ./scripts/run_studio.sh
"""

import streamlit as st
import subprocess
import json
import time
import os
import signal
from pathlib import Path
from datetime import datetime
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import torch
from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names, load_dataset_builder  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
try:
    from .docs import render_docs
except ImportError:
    from docs import render_docs

# –ü—É—Ç–∏
PROJECT_ROOT = Path(__file__).parent.parent.parent
CONFIGS_DIR = PROJECT_ROOT / "configs"
DATASET_DIR = PROJECT_ROOT / "datasets"  # datasets —Å "s"!
OUTPUT_DIR = PROJECT_ROOT / "out"
RUNS_DIR = PROJECT_ROOT / ".runs"
RUNS_DIR.mkdir(exist_ok=True)

# ============================================================================
# Page Config
# ============================================================================

st.set_page_config(
    page_title="HomeLLM Training Studio",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS ‚Äî —á–∏—Å—Ç–∞—è —Ç—ë–º–Ω–∞—è —Ç–µ–º–∞ —Å —Ö–æ—Ä–æ—à–∏–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–º
st.markdown("""
<style>
    /* –ó–∞–≥–æ–ª–æ–≤–æ–∫ */
    .main-header {
        color: #ff6b6b;
        font-size: 2.5rem;
        font-weight: 800;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .sub-header {
        color: #888888;
        text-align: center;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    
    /* –°—Ç–∞—Ç—É—Å */
    .status-running {
        color: #22c55e !important;
        font-weight: bold;
    }
    
    .status-completed {
        color: #3b82f6 !important;
        font-weight: bold;
    }
    
    .status-error {
        color: #ef4444 !important;
        font-weight: bold;
    }
    
    /* ASCII art –±–ª–æ–∫ */
    .model-ascii {
        background: #1e1e1e;
        border: 1px solid #333;
        border-radius: 8px;
        padding: 1rem;
        font-family: 'Courier New', monospace;
        color: #00ff88;
        white-space: pre;
        overflow-x: auto;
    }
    
    /* –ö–∞—Ä—Ç–æ—á–∫–∏ –º–µ—Ç—Ä–∏–∫ */
    div[data-testid="metric-container"] {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 0.5rem;
    }
    
    /* –ö–Ω–æ–ø–∫–∏ –∑–∞–ø—É—Å–∫–∞ */
    .stButton > button[kind="primary"] {
        background: linear-gradient(90deg, #e94560, #ff6b6b);
        color: white !important;
        border: none;
        font-weight: 600;
    }
    
    /* Code –±–ª–æ–∫–∏ */
    pre {
        background: #0d1117 !important;
        color: #c9d1d9 !important;
        border: 1px solid #30363d !important;
    }
    
    code {
        color: #79c0ff !important;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# Persistence ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞–º–∏
# ============================================================================

ACTIVE_RUN_FILE = RUNS_DIR / "active_run.json"


def save_active_run(run_id: str, config: dict = None):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –≤ —Ñ–∞–π–ª."""
    data = {
        "run_id": run_id,
        "started_at": datetime.now().isoformat(),
        "config": config or {}
    }
    with open(ACTIVE_RUN_FILE, "w") as f:
        json.dump(data, f, indent=2)


def load_active_run() -> dict | None:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run –∏–∑ —Ñ–∞–π–ª–∞."""
    if not ACTIVE_RUN_FILE.exists():
        return None
    try:
        with open(ACTIVE_RUN_FILE) as f:
            return json.load(f)
    except:
        return None


def clear_active_run():
    """–û—á–∏—Å—Ç–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π run."""
    if ACTIVE_RUN_FILE.exists():
        ACTIVE_RUN_FILE.unlink()


def restore_session_state():
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π run
    active = load_active_run()
    if active and active.get("run_id"):
        run_id = active["run_id"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ run –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        run_dir = RUNS_DIR / run_id
        if run_dir.exists():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
            pid_path = run_dir / "pid"
            process_alive = False
            if pid_path.exists():
                try:
                    with open(pid_path) as f:
                        pid = int(f.read().strip())
                    os.kill(pid, 0)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
                    process_alive = True
                except PermissionError:
                    process_alive = True
                except (ProcessLookupError, ValueError, PermissionError):
                    pass
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            st.session_state.current_run_id = run_id
            st.session_state.training_active = process_alive
            
            # –ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à—ë–Ω, –æ—á–∏—â–∞–µ–º active_run
            if not process_alive:
                metrics_path = run_dir / "metrics.json"
                if metrics_path.exists():
                    try:
                        with open(metrics_path) as f:
                            metrics = json.load(f)
                        if metrics.get("status") in ["completed", "error", "stopped"]:
                            clear_active_run()
                    except:
                        pass


# ============================================================================
# Session State
# ============================================================================

if "training_process" not in st.session_state:
    st.session_state.training_process = None
if "current_run_id" not in st.session_state:
    st.session_state.current_run_id = None
if "training_active" not in st.session_state:
    st.session_state.training_active = False
if "selected_chat_model" not in st.session_state:
    st.session_state.selected_chat_model = None

# –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
if "session_restored" not in st.session_state:
    restore_session_state()
    st.session_state.session_restored = True


# ============================================================================
# Helper Functions
# ============================================================================

def get_available_datasets():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."""
    datasets = []
    if DATASET_DIR.exists():
        for f in DATASET_DIR.glob("*.jsonl"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
    return datasets


def get_gpu_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU."""
    gpus = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            gpus.append({
                "id": i,
                "name": props.name,
                "memory_gb": round(memory_gb, 1),
                "compute_capability": f"{props.major}.{props.minor}",
            })
    return gpus


def get_available_configs():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö accelerate –∫–æ–Ω—Ñ–∏–≥–æ–≤."""
    configs = []
    if CONFIGS_DIR.exists():
        for f in CONFIGS_DIR.glob("*.yaml"):
            name = f.stem.replace("accelerate_", "").replace("_", " ").title()
            configs.append({
                "file": f.name,
                "name": name,
                "path": str(f),
            })
    return configs


# –û–ø–∏—Å–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
PARALLEL_TYPES = {
    "default": {
        "name": "Single GPU / CPU",
        "type": "None",
        "description": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞",
        "icon": "üñ•Ô∏è",
    },
    "multi_gpu": {
        "name": "Multi-GPU (DDP)",
        "type": "Data Parallel",
        "description": "Distributed Data Parallel ‚Äî –∫–∞–∂–¥–∞—è GPU –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏ –∏ —á–∞—Å—Ç—å –±–∞—Ç—á–∞",
        "icon": "üîÑ",
    },
    "fsdp": {
        "name": "FSDP",
        "type": "Data Parallel + Model Parallel",
        "description": "Fully Sharded Data Parallel ‚Äî –º–æ–¥–µ–ª—å —à–∞—Ä–¥–∏—Ä—É–µ—Ç—Å—è –º–µ–∂–¥—É GPU (PyTorch native)",
        "icon": "‚ö°",
    },
    "deepspeed_zero2": {
        "name": "DeepSpeed ZeRO-2",
        "type": "Data Parallel + Optimizer Parallel",
        "description": "–®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É GPU",
        "icon": "üöÄ",
    },
    "deepspeed_zero3": {
        "name": "DeepSpeed ZeRO-3",
        "type": "Full Model Parallel",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –º–æ–¥–µ–ª—å + –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä + –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã",
        "icon": "üí™",
    },
    "deepspeed_zero3_offload": {
        "name": "ZeRO-3 + CPU Offload",
        "type": "Model Parallel + CPU Offload",
        "description": "–ü–æ–ª–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ + –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM",
        "icon": "üßä",
    },
}


def estimate_parameters(hidden_size: int, num_layers: int, vocab_size: int = 50257) -> int:
    """–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    # Embedding: vocab_size * hidden_size
    embed = vocab_size * hidden_size
    # Each layer: attention (4 * hidden^2) + FFN (8 * hidden^2) + norms
    per_layer = 4 * hidden_size ** 2 + 8 * hidden_size ** 2 + 2 * hidden_size
    # LM head is tied, so not counted
    total = embed + num_layers * per_layer
    return total


def format_params(n: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    if n >= 1e9:
        return f"{n/1e9:.2f}B"
    elif n >= 1e6:
        return f"{n/1e6:.1f}M"
    elif n >= 1e3:
        return f"{n/1e3:.1f}K"
    return str(n)


def format_time(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏."""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        return f"{seconds/3600:.1f}h"


def load_metrics(run_id: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    metrics_path = RUNS_DIR / run_id / "metrics.json"
    if metrics_path.exists():
        try:
            with open(metrics_path) as f:
                return json.load(f)
        except:
            pass
    return None


def start_training(config: dict) -> str:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤ —Ñ–æ–Ω–µ."""
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –õ–û–ì–ò–ö–ê –ü–£–¢–ï–ô
    # config["output_dir"] - —ç—Ç–æ –∫–æ—Ä–µ–Ω—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä out/my_model)
    # –ú—ã —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ out/my_model/run_2023.../checkpoint_...
    experiment_root = Path(PROJECT_ROOT) / config.get("output_dir", "out/default")
    run_output_dir = experiment_root / run_id
    
    # –û–±–Ω–æ–≤–ª—è–µ–º output_dir –≤ –∫–æ–Ω—Ñ–∏–≥–µ, —á—Ç–æ–±—ã worker —Å–æ—Ö—Ä–∞–Ω—è–ª —Ç—É–¥–∞
    config["output_dir"] = str(run_output_dir)
    
    # –ü–∞–ø–∫–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞ (–ª–æ–≥–∏, –º–µ—Ç—Ä–∏–∫–∏)
    # –ú–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö —Ç–∞–º –∂–µ, –≥–¥–µ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã, –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    run_dir = RUNS_DIR / run_id # –û—Å—Ç–∞–≤–ª—è–µ–º .runs –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ª–æ–≥–æ–≤ —Å—Ç—Ä–∏–º–ª–∏—Ç–∞
    run_dir.mkdir(parents=True, exist_ok=True)
    run_output_dir.mkdir(parents=True, exist_ok=True) # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤–µ—Å–æ–≤
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    # –°–æ–∑–¥–∞—ë–º –Ω–∞—á–∞–ª—å–Ω—ã–π metrics —Ñ–∞–π–ª
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0}, f)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ distributed
    distributed_mode = config.get("distributed_mode", "default")
    config_file = config.get("config_file")
    num_gpus = config.get("num_gpus", 1)
    
    if distributed_mode != "default" and config_file:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º accelerate launch —Å –∫–æ–Ω—Ñ–∏–≥–æ–º
        cmd = [
            "accelerate", "launch",
            "--config_file", config_file,
            "--num_processes", str(num_gpus),
            "--gradient_accumulation_steps", str(config.get("gradient_accumulation", 1)),
            "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫
        cmd = [
            "python", "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        f.write(" ".join(cmd))
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,  # –û—Ç–¥–µ–ª—è–µ–º –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PID –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def stop_training():
    """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É."""
    stopped = False
    
    # –ü—Ä–æ–±—É–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ PID –∏–∑ —Ñ–∞–π–ª–∞ (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
    if st.session_state.current_run_id:
        pid_path = RUNS_DIR / st.session_state.current_run_id / "pid"
        if pid_path.exists():
            try:
                with open(pid_path) as f:
                    pid = int(f.read().strip())
                # –°–Ω–∞—á–∞–ª–∞ SIGTERM, –ø–æ—Ç–æ–º SIGKILL –µ—Å–ª–∏ –Ω–µ –ø–æ–º–æ–≥–ª–æ
                os.kill(pid, signal.SIGTERM)
                stopped = True
                
                # –ñ–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
                time.sleep(0.5)
                try:
                    os.kill(pid, 0)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
                    os.kill(pid, signal.SIGKILL)  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–±–∏–≤–∞–µ–º
                except ProcessLookupError:
                    pass  # –ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
            except Exception as e:
                pass
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = RUNS_DIR / st.session_state.current_run_id / "metrics.json"
        if metrics_path.exists():
            try:
                with open(metrics_path) as f:
                    metrics = json.load(f)
                metrics["status"] = "stopped"
                with open(metrics_path, "w") as f:
                    json.dump(metrics, f, indent=2)
            except:
                pass
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ subprocess
    if st.session_state.training_process:
        try:
            st.session_state.training_process.terminate()
            st.session_state.training_process.wait(timeout=2)
        except:
            try:
                st.session_state.training_process.kill()
            except:
                pass
        st.session_state.training_process = None
    
    st.session_state.training_active = False
    
    # –û—á–∏—â–∞–µ–º active_run
    clear_active_run()
    
    return stopped


def is_process_running(run_id: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–ø—É—â–µ–Ω –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å."""
    pid_path = RUNS_DIR / run_id / "pid"
    if not pid_path.exists():
        return False
    
    try:
        with open(pid_path) as f:
            pid = int(f.read().strip())
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
        os.kill(pid, 0)
        return True
    except PermissionError:
        # –ü—Ä–æ—Ü–µ—Å—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—É—â–µ–Ω –æ—Ç –¥—Ä—É–≥–æ–≥–æ —é–∑–µ—Ä–∞)
        # –°—á–∏—Ç–∞–µ–º, —á—Ç–æ –æ–Ω –∂–∏–≤
        return True
    except (ProcessLookupError, ValueError, FileNotFoundError):
        return False


# ============================================================================
# UI Components
# ============================================================================

def render_header():
    st.markdown("# üè† Models at Home Training Studio")
    st.caption("–í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–º–∞")


def render_model_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.header("üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏")
    
    # –ò–º—è –º–æ–¥–µ–ª–∏ (–¥–ª—è –ø–∞–ø–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
    model_name = st.sidebar.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", value="my_first_model", help="–ò–º—è –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    # –ü—Ä–µ—Å–µ—Ç—ã
    preset = st.sidebar.selectbox(
        "–ü—Ä–µ—Å–µ—Ç",
        ["Tiny (25M)", "Small (80M)", "Medium (200M)", "Large (400M)", "Custom"],
        index=0
    )
    
    presets = {
        "Tiny (25M)": (512, 8, 8),
        "Small (80M)": (768, 12, 12),
        "Medium (200M)": (1024, 16, 16),
        "Large (400M)": (1280, 20, 20),
    }
    
    if preset != "Custom" and preset in presets:
        default_h, default_l, default_n = presets[preset]
    else:
        default_h, default_l, default_n = 512, 8, 8
    
    hidden_size = st.sidebar.slider(
        "Hidden Size", 
        min_value=128, 
        max_value=2048, 
        value=default_h, 
        step=64,
        help="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è"
    )
    
    num_layers = st.sidebar.slider(
        "Num Layers", 
        min_value=2, 
        max_value=32, 
        value=default_l,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"
    )
    
    n_heads = st.sidebar.slider(
        "Attention Heads", 
        min_value=2, 
        max_value=32, 
        value=default_n,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è"
    )
    
    seq_len = st.sidebar.selectbox(
        "Seq Length",
        [256, 512, 1024, 2048],
        index=1,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
    )
    
    # –û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    est_params = estimate_parameters(hidden_size, num_layers)
    st.sidebar.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã (‚âà)", format_params(est_params))
    
    return {
        "model_name_input": model_name,
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "n_heads": n_heads,
        "seq_len": seq_len,
    }


def render_training_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
    
    batch_size = st.sidebar.slider(
        "Batch Size",
        min_value=1,
        max_value=64,
        value=16,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞"
    )
    
    grad_accum = st.sidebar.slider(
        "Gradient Accumulation",
        min_value=1,
        max_value=32,
        value=4,
        help="–®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞"
    )
    
    st.sidebar.caption(f"Effective batch: {batch_size * grad_accum}")
    
    learning_rate = st.sidebar.select_slider(
        "Learning Rate",
        options=[1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3],
        value=5e-4,
        format_func=lambda x: f"{x:.0e}"
    )
    
    warmup_steps = st.sidebar.number_input(
        "Warmup Steps",
        min_value=0,
        max_value=10000,
        value=1000
    )
    
    # –í—ã–±–æ—Ä: epochs –∏–ª–∏ max_steps
    training_mode = st.sidebar.radio(
        "–†–µ–∂–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏",
        ["–ü–æ —ç–ø–æ—Ö–∞–º", "–ü–æ —à–∞–≥–∞–º"],
        help="–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏"
    )
    
    if training_mode == "–ü–æ —ç–ø–æ—Ö–∞–º":
        epochs = st.sidebar.number_input(
            "Epochs",
            min_value=1,
            max_value=10,
            value=1
        )
        max_steps = None
    else:
        epochs = 1
        max_steps = st.sidebar.number_input(
            "Max Steps",
            min_value=100,
            max_value=1000000,
            value=10000,
            step=1000,
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
        )
    
    mixed_precision = st.sidebar.selectbox(
        "Mixed Precision",
        ["no", "fp16", "bf16"],
        index=2,
        help="bf16 —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Ampere+ GPU"
    )
    
    grad_checkpoint = st.sidebar.checkbox(
        "Gradient Checkpointing",
        value=False,
        help="–≠–∫–æ–Ω–æ–º–∏—Ç VRAM, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ"
    )
    
    return {
        "batch_size": batch_size,
        "gradient_accumulation": grad_accum,
        "learning_rate": learning_rate,
        "warmup_steps": warmup_steps,
        "epochs": epochs,
        "max_steps": max_steps,
        "mixed_precision": mixed_precision,
        "grad_checkpoint": grad_checkpoint,
    }


def render_dataset_config():
    """–í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    st.sidebar.header("üìÅ –î–∞—Ç–∞—Å–µ—Ç")
    
    datasets = get_available_datasets()
    
    if datasets:
        dataset_options = [f"{name} ({size})" for name, size in datasets]
        selected = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç", dataset_options)
        selected_name = selected.split(" (")[0]
        data_path = str(DATASET_DIR / selected_name)
    else:
        st.sidebar.warning("–î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ dataset/")
        data_path = st.sidebar.text_input("–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É", "dataset/data.jsonl")
    
    return {"data_path": data_path}


def render_output_config(model_name="training_run"):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞."""
    st.sidebar.header("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å: out/{model_name}
    default_dir = f"out/{model_name}"
    
    output_dir = st.sidebar.text_input(
        "Output Directory (Experiment Root)",
        value=default_dir,
        help="–ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ —ç—Ç–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
    )
    
    save_every = st.sidebar.number_input(
        "Save Checkpoint Every N Steps",
        min_value=100,
        max_value=50000,
        value=2000,
        step=500,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã"
    )
    
    log_every = st.sidebar.number_input(
        "Log Every N Steps",
        min_value=1,
        max_value=1000,
        value=10,
        help="–ö–∞–∫ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –º–µ—Ç—Ä–∏–∫–∏"
    )
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞—Ö
    output_path = PROJECT_ROOT / output_dir
    if output_path.exists():
        checkpoints = list(output_path.glob("checkpoint_*"))
        final_model = output_path / "final_model"
        
        if checkpoints or final_model.exists():
            st.sidebar.caption(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {len(checkpoints)}")
            if final_model.exists():
                st.sidebar.caption("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    return {
        "output_dir": output_dir,
        "save_every": save_every,
        "log_every": log_every,
        "tokenizer_path": "gpt2"
    }


def get_available_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫)."""
    models = []
    
    # –ò—â–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤ out/
    if OUTPUT_DIR.exists():
        # –ò—â–µ–º –ª—é–±—ã–µ config.json –≤–Ω—É—Ç—Ä–∏ out/
        for config_file in OUTPUT_DIR.rglob("config.json"):
            model_dir = config_file.parent
            
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–∞–ø–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ—Ö–æ–∂–∏ –Ω–∞ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏)
            # –ö—Ä–∏—Ç–µ—Ä–∏–π –º–æ–¥–µ–ª–∏: –Ω–∞–ª–∏—á–∏–µ config.json + (pytorch_model.bin –∏–ª–∏ model.safetensors –∏–ª–∏ adapter_model.bin)
            has_weights = (
                (model_dir / "pytorch_model.bin").exists() or 
                (model_dir / "model.safetensors").exists() or
                (model_dir / "adapter_model.bin").exists()
            )
            
            if has_weights:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø (final –∏–ª–∏ checkpoint)
                m_type = "checkpoint" if "checkpoint" in model_dir.name else "final"
                if model_dir.name == "final_model": m_type = "final"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è
                # –ë–µ—Ä–µ–º –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ OUTPUT_DIR
                rel_path = model_dir.relative_to(OUTPUT_DIR)
                models.append({
                    "name": str(rel_path),
                    "path": str(model_dir),
                    "type": m_type,
                    "time": model_dir.stat().st_mtime
                })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
    models.sort(key=lambda x: x["time"], reverse=True)
    return models


def render_distributed_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è distributed training."""
    st.sidebar.header("üñ•Ô∏è GPU –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    gpus = get_gpu_info()
    
    if gpus:
        st.sidebar.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ GPU: {len(gpus)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ GPU
        for gpu in gpus:
            st.sidebar.markdown(f"""
            **GPU {gpu['id']}**: {gpu['name']}  
            üìä VRAM: {gpu['memory_gb']} GB | CC: {gpu['compute_capability']}
            """)
        
        # –í—ã–±–æ—Ä GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        gpu_options = [f"GPU {g['id']}: {g['name']}" for g in gpus]
        if len(gpus) > 1:
            selected_gpus = st.sidebar.multiselect(
                "–í—ã–±–µ—Ä–∏—Ç–µ GPU",
                options=gpu_options,
                default=gpu_options,
                help="–í—ã–±–µ—Ä–∏—Ç–µ GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
            )
            num_gpus = len(selected_gpus)
            gpu_ids = [gpu_options.index(g) for g in selected_gpus]
        else:
            num_gpus = 1
            gpu_ids = [0]
            st.sidebar.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è GPU")
    else:
        st.sidebar.warning("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU")
        num_gpus = 0
        gpu_ids = []
    
    st.sidebar.markdown("---")
    
    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
    st.sidebar.subheader("‚ö° –¢–∏–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–ø—Ü–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–µ–∂–∏–º
    if num_gpus == 0:
        available_modes = ["default"]
        recommended_idx = 0
    elif num_gpus == 1:
        available_modes = ["default", "deepspeed_zero3_offload"]
        recommended_idx = 0
    else:
        # –ü—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º multi_gpu –∏–ª–∏ fsdp
        available_modes = ["multi_gpu", "fsdp", "deepspeed_zero2", "deepspeed_zero3", "deepspeed_zero3_offload", "default"]
        recommended_idx = 0  # multi_gpu –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–ø—Ü–∏–∏ –¥–ª—è selectbox
    mode_options = []
    for i, mode in enumerate(available_modes):
        info = PARALLEL_TYPES[mode]
        label = f"{info['icon']} {info['name']}"
        if i == recommended_idx and num_gpus > 1:
            label += " ‚≠ê"  # –û—Ç–º–µ—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π
        mode_options.append(label)
    
    selected_mode_display = st.sidebar.selectbox(
        "–†–µ–∂–∏–º",
        options=mode_options,
        index=recommended_idx,
        help="–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
    )
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
    selected_idx = mode_options.index(selected_mode_display)
    selected_mode = available_modes[selected_idx]
    mode_info = PARALLEL_TYPES[selected_mode]
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ
    st.sidebar.markdown(f"""
    <div style="background: rgba(255,255,255,0.05); padding: 10px; border-radius: 8px; margin: 10px 0;">
    <b>–¢–∏–ø:</b> {mode_info['type']}<br>
    <small>{mode_info['description']}</small>
    </div>
    """, unsafe_allow_html=True)
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω single GPU –ø—Ä–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
    if num_gpus > 1 and selected_mode == "default":
        st.sidebar.warning(f"‚ö†Ô∏è –í—ã–±—Ä–∞–Ω Single GPU, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ {num_gpus} GPU. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º Multi-GPU!")
    
    # –ö–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª
    config_file = None
    if selected_mode != "default":
        config_path = CONFIGS_DIR / f"accelerate_{selected_mode}.yaml"
        if config_path.exists():
            config_file = str(config_path)
            st.sidebar.caption(f"üìÑ –ö–æ–Ω—Ñ–∏–≥: `{config_path.name}`")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∑–∞–ø—É—Å–∫–∞
    st.sidebar.markdown("---")
    st.sidebar.subheader("üöÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–ø—É—Å–∫–∞")
    
    if num_gpus == 0:
        launch_info = "**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** CPU"
    elif selected_mode == "default":
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** GPU {gpu_ids[0] if gpu_ids else 0}"
    else:
        launch_info = f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:** {num_gpus} √ó GPU\n**–†–µ–∂–∏–º:** {mode_info['type']}"
    
    st.sidebar.info(launch_info)
    
    return {
        "distributed_mode": selected_mode,
        "num_gpus": num_gpus,
        "gpu_ids": gpu_ids,
        "config_file": config_file,
        "parallel_type": mode_info['type'],
    }


@st.fragment(run_every=3)  # –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã
def live_metrics_fragment():
    """Fragment –¥–ª—è –∂–∏–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    if not st.session_state.current_run_id:
        st.info("–í—ã–±–µ—Ä–∏—Ç–µ run –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫")
        return
    
    run_id = st.session_state.current_run_id
    metrics = load_metrics(run_id)
    process_alive = is_process_running(run_id)
    
    # –°—Ç–∞—Ç—É—Å
    if process_alive:
        st.success(f"üü¢ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–ø—É—â–µ–Ω (Run: {run_id})")
    else:
        if metrics and metrics.get("status") == "completed":
            duration = metrics.get("training_duration", "unknown")
            st.success(f"‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration} (Run: {run_id})")
        elif metrics and metrics.get("status") == "error":
            st.error(f"‚ùå –û—à–∏–±–∫–∞ (Run: {run_id})")
        elif metrics and metrics.get("status") == "stopped":
            st.warning(f"‚èπÔ∏è –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ (Run: {run_id})")
        else:
            st.info(f"üìã –ü—Ä–æ—Å–º–æ—Ç—Ä –º–µ—Ç—Ä–∏–∫ (Run: {run_id})")
    
    if metrics:
        render_metrics_dashboard(metrics)
    else:
        st.info("–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")


def render_metrics_dashboard(metrics: dict):
    """–î–∞—à–±–æ—Ä–¥ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è."""
    
    status = metrics.get("status", "unknown")
    
    # Status indicator
    status_emoji = {
        "training": "üü¢", 
        "completed": "‚úÖ", 
        "error": "‚ùå",
        "initializing": "‚è≥",
        "loading_tokenizer": "‚è≥",
        "loading_dataset": "‚è≥",
        "building_model": "‚è≥",
        "saving_model": "üíæ",
    }.get(status, "‚è≥")
    
    st.subheader(f"{status_emoji} –°—Ç–∞—Ç—É—Å: {status.upper()}")
    
    # Metrics cards
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        current_step = metrics.get("current_step", 0)
        total_steps = metrics.get("total_steps", 1)
        progress = current_step / total_steps * 100 if total_steps > 0 else 0
        st.metric("–ü—Ä–æ–≥—Ä–µ—Å—Å", f"{progress:.1f}%", f"Step {current_step}/{total_steps}")
    
    with col2:
        loss = metrics.get("current_loss", 0)
        st.metric("Loss", f"{loss:.4f}")
    
    with col3:
        lr = metrics.get("current_lr", 0)
        st.metric("Learning Rate", f"{lr:.2e}")
    
    with col4:
        eta = metrics.get("eta_seconds", 0)
        st.metric("ETA", format_time(eta))
    
    # Progress bar
    st.progress(min(progress / 100, 1.0))
    
    # Charts
    if metrics.get("loss_history"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Loss chart
            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["loss_history"],
                mode='lines',
                name='Loss',
                line=dict(color='#e94560', width=2)
            ))
            fig_loss.update_layout(
                title="Training Loss",
                xaxis_title="Step",
                yaxis_title="Loss",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_loss, use_container_width=True, key=f"loss_chart_{metrics.get('current_step')}")
        
        with col2:
            # LR chart
            fig_lr = go.Figure()
            fig_lr.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["lr_history"],
                mode='lines',
                name='LR',
                line=dict(color='#60a5fa', width=2)
            ))
            fig_lr.update_layout(
                title="Learning Rate Schedule",
                xaxis_title="Step",
                yaxis_title="LR",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_lr, use_container_width=True, key=f"lr_chart_{metrics.get('current_step')}")
    
    # Checkpoints
    if metrics.get("checkpoints"):
        with st.expander("üì¶ Checkpoints"):
            for ckpt in metrics["checkpoints"]:
                st.text(f"Step {ckpt['step']}: {ckpt['path']}")
    
    # GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    gpu_stats = metrics.get("gpu_stats", [])
    if gpu_stats:
        st.subheader("üñ•Ô∏è –ù–∞–≥—Ä—É–∑–∫–∞ GPU")
        
        cols = st.columns(len(gpu_stats))
        for i, (col, gpu) in enumerate(zip(cols, gpu_stats)):
            with col:
                st.markdown(f"**GPU {gpu['id']}**")
                
                # Memory bar
                mem_percent = gpu.get('memory_percent', 0)
                st.progress(min(mem_percent / 100, 1.0), text=f"VRAM: {gpu['memory_used_gb']:.1f} / {gpu['memory_total_gb']:.1f} GB ({mem_percent:.0f}%)")
                
                # Utilization
                util = gpu.get('utilization')
                if util is not None:
                    st.progress(min(util / 100, 1.0), text=f"–ó–∞–≥—Ä—É–∑–∫–∞: {util}%")
                else:
                    st.caption("–ó–∞–≥—Ä—É–∑–∫–∞: N/A")
    
    # Error
    if metrics.get("error"):
        st.error("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
        with st.expander("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏ (Traceback)", expanded=True):
            st.code(metrics['error'], language="python")
    
    # –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
    if st.session_state.current_run_id:
        run_dir = RUNS_DIR / st.session_state.current_run_id
        stderr_path = run_dir / "stderr.log"
        stdout_path = run_dir / "stdout.log"
        
        with st.expander("üìã –õ–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.caption("stdout (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stdout_path.exists():
                    with open(stdout_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)
            
            with col2:
                st.caption("stderr (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫)")
                if stderr_path.exists():
                    with open(stderr_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(–ø—É—Å—Ç–æ)", language=None)



def download_hf_dataset(repo_id, subset, split, limit_type, limit_val, limit_bytes, save_path, filters=None):
    """–§—É–Ω–∫—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    try:
        status_text = f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: {repo_id}..."
        st.toast(status_text)
        print(status_text)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã stream=True —á—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –≤—Å–µ –≤ –ø–∞–º—è—Ç—å
        ds = load_dataset(repo_id, subset, split=split, streaming=True)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
        save_path = DATASET_DIR / save_path
        save_path.parent.mkdir(exist_ok=True)
        
        count = 0
        current_bytes = 0
        
        with open(save_path, "w", encoding="utf-8") as f:
            for item in ds:
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                if filters:
                    # –§–∏–ª—å—Ç—Ä –ø–æ score
                    if "score_col" in filters and "min_score" in filters:
                         col = filters["score_col"]
                         min_s = filters["min_score"]
                         # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Ç–∏–ø–∞
                         if col in item and item[col] is not None:
                             try:
                                 val = float(item[col])
                                 if val < min_s:
                                     continue
                             except ValueError:
                                 pass
                    
                    # –§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É
                    if "lang_col" in filters and "target_lang" in filters:
                         col = filters["lang_col"]
                         target = filters["target_lang"]
                         if col in item and item[col] is not None:
                             val = str(item[col])
                             if target.lower() not in val.lower():
                                 continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSONL
                line = json.dumps(item, ensure_ascii=False) + "\n"
                line_bytes = len(line.encode('utf-8'))
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)" and count >= limit_val:
                    break
                if limit_type == "–ì–ë (–†–∞–∑–º–µ—Ä)" and (current_bytes + line_bytes) > limit_bytes:
                    break
                    
                f.write(line)
                count += 1
                current_bytes += line_bytes
                
                if count % 1000 == 0:
                    print(f"Downloaded {count} lines, {current_bytes / 1024**2:.2f} MB")

        st.success(f"–ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {count} —Å—Ç—Ä–æ–∫ ({current_bytes / 1024**2:.2f} MB) –≤ {save_path}")
        return True

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        print(traceback.format_exc())
        return False


def render_data_manager():
    """–í–∫–ª–∞–¥–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏."""
    st.header("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
    
    col_upload, col_list = st.columns([1, 2])
    
    with col_upload:
        # –°–µ–∫—Ü–∏—è 1: –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        with st.expander("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤", expanded=False):
            uploaded_files = st.file_uploader(
                "–ü–µ—Ä–µ—Ç–∞—â–∏—Ç–µ —Ñ–∞–π–ª—ã —Å—é–¥–∞", 
                type=["jsonl", "txt", "json"], 
                accept_multiple_files=True
            )
            
            if uploaded_files:
                if st.button("üì• –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª—ã"):
                    for uploaded_file in uploaded_files:
                        save_path = DATASET_DIR / uploaded_file.name
                        with open(save_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        st.toast(f"–§–∞–π–ª {uploaded_file.name} —Å–æ—Ö—Ä–∞–Ω—ë–Ω!", icon="‚úÖ")
                    time.sleep(1)
                    st.rerun()

        # –°–µ–∫—Ü–∏—è 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Å HuggingFace
        st.subheader("ü§ó –°–∫–∞—á–∞—Ç—å —Å HuggingFace")
        
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        if "ds_repo_info" not in st.session_state:
            st.session_state.ds_repo_info = {} # {repo_id: {'configs': [], 'splits': [], 'features': {}}}

        repo_id = st.text_input("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (ID)", value="HuggingFaceFW/fineweb-2", key="hf_repo_id_input")
        
        # –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π"):
            try:
                with st.spinner(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {repo_id}..."):
                    # 1. –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥–∏
                    configs = get_dataset_config_names(repo_id)
                    
                    # 2. –ü–æ–ª—É—á–∞–µ–º —Å–ø–ª–∏—Ç—ã (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∫–æ–Ω—Ñ–∏–≥ –ø–æ –¥–µ—Ñ–æ–ª—Ç—É)
                    default_config = configs[0] if configs else None
                    splits = []
                    features_info = {}
                    
                    if default_config:
                        splits = get_dataset_split_names(repo_id, default_config)
                        # 3. –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (features)
                        try:
                            ds_builder = load_dataset_builder(repo_id, default_config)
                            if ds_builder.info.features:
                                features_info = ds_builder.info.features
                        except Exception as e:
                            print(f"Could not load features: {e}")

                    st.session_state.ds_repo_info[repo_id] = {
                        "configs": configs,
                        "splits": splits,
                        "features": features_info
                    }
                    st.success(f"–ù–∞–π–¥–µ–Ω–æ {len(configs)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")
            except Exception as e:
                st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: {e}")

        # –†–∞–±–æ—Ç–∞–µ–º —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        repo_info = st.session_state.ds_repo_info.get(repo_id, {})
        available_configs = repo_info.get("configs", [])
        available_splits = repo_info.get("splits", [])
        features = repo_info.get("features", {})
        
        if available_configs:
            subset = st.selectbox("Subset (–∫–æ–Ω—Ñ–∏–≥)", available_configs, key="hf_subset_select")
        else:
            subset = st.text_input("Subset (–∫–æ–Ω—Ñ–∏–≥)", "default", key="hf_subset_input")
        
        if available_splits:
             split = st.selectbox("Split", available_splits, key="hf_split_select")
        else:
             split = st.text_input("Split", "train", key="hf_split_input")

        # --- –£–ú–ù–´–ï –§–ò–õ–¨–¢–†–´ ---
        with st.expander("üõ†Ô∏è –§–∏–ª—å—Ç—Ä—ã –∏ –õ–∏–º–∏—Ç—ã", expanded=True):
            # –õ–∏–º–∏—Ç—ã (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã)
            col_lim1, col_lim2 = st.columns(2)
            with col_lim1:
                limit_type = st.radio("–¢–∏–ø –ª–∏–º–∏—Ç–∞", ["–ì–ë (–†–∞–∑–º–µ—Ä)", "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)"], key="limit_type")
            
            with col_lim2:
                limit_val = 0
                limit_bytes = 0
                
                if limit_type == "–°—Ç—Ä–æ–∫–∏ (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ)":
                    limit_val = st.number_input("–ö–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫", value=100000, step=10000, key="limit_val")
                else:
                    limit_gb = st.number_input("–†–∞–∑–º–µ—Ä (–ì–ë)", value=2.0, step=0.5, min_value=0.1, key="limit_gb")
                    limit_bytes = int(limit_gb * 1024**3)
            
            st.divider()
            
            # –§–∏–ª—å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (features)
            active_filters = {}
            
            if features:
                st.caption("üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏ –∏—Ö —Ç–∏–ø–æ–≤
                # features —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å {col_name: feature_info}
                # feature_info –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π 'Value(dtype='string')' –∏–ª–∏ –æ–±—ä–µ–∫—Ç–æ–º
                
                # 1. –§–∏–ª—å—Ç—Ä —á–∏—Å–ª–æ–≤–æ–π (Score/Quality)
                float_cols = []
                string_cols = []
                
                for col_name, feature_def in features.items():
                    # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø
                    dtype = getattr(feature_def, 'dtype', str(feature_def))
                    if 'float' in str(dtype):
                        float_cols.append(col_name)
                    elif 'string' in str(dtype):
                        string_cols.append(col_name)
                
                col_f1, col_f2 = st.columns(2)
                
                with col_f1:
                    if float_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é (float)**")
                        selected_float_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + float_cols, key="sel_float_col")
                        if selected_float_col != "(–Ω–µ—Ç)":
                            min_val = st.slider(f"–ú–∏–Ω. –∑–Ω–∞—á–µ–Ω–∏–µ {selected_float_col}", 0.0, 1.0, 0.0, key="val_float_col")
                            active_filters["score_col"] = selected_float_col
                            active_filters["min_score"] = min_val
                    else:
                        st.caption("–ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

                with col_f2:
                    if string_cols:
                        st.markdown("**–§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–∫—Å—Ç—É (contains)**")
                        selected_str_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É", ["(–Ω–µ—Ç)"] + string_cols, key="sel_str_col")
                        if selected_str_col != "(–Ω–µ—Ç)":
                            target_str = st.text_input(f"–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:", key="val_str_col")
                            if target_str:
                                active_filters["lang_col"] = selected_str_col
                                active_filters["target_lang"] = target_str
                    else:
                        st.caption("–¢–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            else:
                st.info("‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ –ª–∏–º–∏—Ç—ã –ø–æ –æ–±—ä–µ–º—É.")


        save_filename = st.text_input("–ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è", "dataset.jsonl", key="save_filename")
        
        # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º callback
        def on_download_click(active_filters_map):
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ session_state —è–≤–Ω–æ
            r_id = st.session_state.get('hf_repo_id_input')
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º subset –∏ split
            if st.session_state.get('hf_subset_select'):
                sub = st.session_state.get('hf_subset_select')
            else:
                sub = st.session_state.get('hf_subset_input')
                
            if st.session_state.get('hf_split_select'):
                spl = st.session_state.get('hf_split_select')
            else:
                spl = st.session_state.get('hf_split_input')
                
            l_type = st.session_state.get('limit_type')
            l_val = st.session_state.get('limit_val')
            
            l_gb = st.session_state.get('limit_gb', 2.0)
            l_bytes = int(l_gb * 1024**3)

            s_path = st.session_state.get('save_filename')
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            filters_to_pass = {}
            
            # 1. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
            if active_filters_map:
                if "score_col" in active_filters_map:
                    filters_to_pass["score_col"] = active_filters_map["score_col"]
                    filters_to_pass["min_score"] = st.session_state.get("filter_score", 0.0)
                
                if "lang_col" in active_filters_map:
                    filters_to_pass["lang_col"] = active_filters_map["lang_col"]
                    filters_to_pass["target_lang"] = st.session_state.get("filter_lang", "ru")
            
            # 2. –§–æ–ª–±—ç–∫ –¥–ª—è FineWeb (—É–¥–∞–ª–µ–Ω, —Ç–∞–∫ –∫–∞–∫ –≤—ã–∑—ã–≤–∞–ª –ø—É—Ç–∞–Ω–∏—Ü—É)
            # elif "fineweb" in r_id.lower():
            #      pass
            
            download_hf_dataset(r_id, sub, spl, l_type, l_val, l_bytes, s_path, filters=filters_to_pass)

        st.button("–°–∫–∞—á–∞—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å", on_click=on_download_click, args=(active_filters,))
    
    with col_list:
        st.subheader("–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã")
        
        datasets = []
        if DATASET_DIR.exists():
            # JSONL / JSON
            for f in list(DATASET_DIR.glob("*.jsonl")) + list(DATASET_DIR.glob("*.json")):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "JSONL/JSON",
                    "size_mb": size_mb,
                    "path": f
                })
            # TXT
            for f in DATASET_DIR.glob("*.txt"):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "Text",
                    "size_mb": size_mb,
                    "path": f
                })
        
        if not datasets:
            st.info("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã —Å–ª–µ–≤–∞.")
        else:
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–ø–∏—Å–æ–∫
            for ds in datasets:
                with st.expander(f"üìÑ {ds['name']} ({ds['size_mb']:.1f} MB)"):
                    st.caption(f"–¢–∏–ø: {ds['type']}")
                    
                    # Preview
                    try:
                        with open(ds['path'], "r", encoding="utf-8") as f:
                            head = [next(f).strip() for _ in range(5)]
                        st.markdown("**Preview (–ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫):**")
                        st.code("\n".join(head), language="json" if "JSON" in ds['type'] else "text")
                        
                        col_del, col_info = st.columns([1, 4])
                        with col_del:
                            if st.button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å", key=f"del_{ds['name']}"):
                                ds['path'].unlink()
                                st.toast(f"–§–∞–π–ª {ds['name']} —É–¥–∞–ª—ë–Ω", icon="üóëÔ∏è")
                                time.sleep(1)
                                st.rerun()
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")


def render_model_preview(config: dict, distributed_config: dict = None):
    """–ü—Ä–µ–≤—å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞."""
    st.subheader("üìê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏")
    
    params = estimate_parameters(config["hidden_size"], config["num_layers"])
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Hidden Size", config["hidden_size"])
        st.metric("Layers", config["num_layers"])
    
    with col2:
        st.metric("Attention Heads", config["n_heads"])
        st.metric("Head Dim", config["hidden_size"] // config["n_heads"])
    
    with col3:
        st.metric("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", format_params(params))
        vram_est = params * 4 / 1e9  # fp32
        st.metric("VRAM (‚âà fp32)", f"{vram_est:.1f} GB")
    
    # –í–∏–∑—É–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    st.markdown(f"""
<div class="model-ascii">
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         HomeForCausalLM             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Embedding: 50257 ‚Üí {config['hidden_size']:4d}           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ HomeBlock √ó {config['num_layers']:2d}              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí Attention      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ {config['n_heads']:2d} heads √ó {config['hidden_size']//config['n_heads']:3d} dim      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ RMSNorm ‚Üí FFN (SwiGLU)   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ  RMSNorm ‚Üí LM Head                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</div>
    """, unsafe_allow_html=True)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–µ
    if distributed_config:
        st.subheader("‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º")
        
        mode = distributed_config.get("distributed_mode", "default")
        mode_info = PARALLEL_TYPES.get(mode, PARALLEL_TYPES["default"])
        num_gpus = distributed_config.get("num_gpus", 0)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("–†–µ–∂–∏–º", mode_info["name"])
        
        with col2:
            st.metric("–¢–∏–ø", mode_info["type"])
        
        with col3:
            if num_gpus > 0:
                st.metric("GPU", f"{num_gpus} —à—Ç.")
            else:
                st.metric("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "CPU")
        
        # –°—Ö–µ–º–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        if mode == "default":
            parallel_diagram = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Single Device      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Full Model      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Optimizer  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Full Gradients  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
        elif mode == "multi_gpu":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Data Parallel ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (copy)  ‚îÇ  ‚îÇ (copy)  ‚îÇ       ‚îÇ (copy)  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Sync Gradients ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ö–∞–∂–¥–∞—è GPU: –ø–æ–ª–Ω–∞—è –∫–æ–ø–∏—è –º–æ–¥–µ–ª–∏, —á–∞—Å—Ç—å –±–∞—Ç—á–∞
"""
        elif mode == "fsdp":
            parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FSDP (Fully Sharded) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Shard 0 ‚îÇ  ‚îÇ Shard 1 ‚îÇ       ‚îÇ Shard N ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Params  ‚îÇ  ‚îÇ Params  ‚îÇ       ‚îÇ Params  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                 ‚îÇ      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ All-Gather for Forward ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ Reduce-Scatter Backward ‚îÄ‚îò       ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–ú–æ–¥–µ–ª—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –º–µ–∂–¥—É GPU (—ç–∫–æ–Ω–æ–º–∏—è VRAM)
"""
        elif "deepspeed" in mode:
            if "zero3" in mode:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Params  ‚îÇ  ‚îÇ Params  ‚îÇ       ‚îÇ Params  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim   ‚îÇ  ‚îÇ Optim   ‚îÇ       ‚îÇ Optim   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1/N    ‚îÇ  ‚îÇ  1/N    ‚îÇ       ‚îÇ  1/N    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  {'+ CPU Offload (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ CPU)' if 'offload' in mode else ''}          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–í—Å—ë —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–æ: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è VRAM
"""
            else:
                parallel_diagram = f"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DeepSpeed ZeRO-2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  GPU 0  ‚îÇ  ‚îÇ  GPU 1  ‚îÇ  ...  ‚îÇ  GPU N  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Full    ‚îÇ  ‚îÇ Full    ‚îÇ       ‚îÇ Full    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Model   ‚îÇ  ‚îÇ Model   ‚îÇ       ‚îÇ Model   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Optim/N ‚îÇ  ‚îÇ Optim/N ‚îÇ       ‚îÇ Optim/N ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω—ã
"""
        else:
            parallel_diagram = ""
        
        if parallel_diagram:
            st.markdown(f"""
<div class="model-ascii">
{parallel_diagram}
</div>
            """, unsafe_allow_html=True)


# ============================================================================
# Main App
# ============================================================================

def main():
    render_header()
    
    # Sidebar configs
    model_config = render_model_config()
    st.session_state.current_model_name = model_config.get("model_name_input", "home_model")
    
    training_config = render_training_config()
    distributed_config = render_distributed_config()
    dataset_config = render_dataset_config()
    output_config = render_output_config(st.session_state.current_model_name)
    
    # Merge configs
    full_config = {**model_config, **training_config, **dataset_config, **output_config}
    full_config["distributed_mode"] = distributed_config["distributed_mode"]
    full_config["num_gpus"] = distributed_config["num_gpus"]
    full_config["config_file"] = distributed_config["config_file"]
    
    # Main content
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["üöÄ –ó–∞–ø—É—Å–∫", "üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "üí¨ –ß–∞—Ç", "üìú –ò—Å—Ç–æ—Ä–∏—è", "üíæ –î–∞–Ω–Ω—ã–µ", "üìö –£—á–µ–±–Ω–∏–∫"])
    
    with tab1:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            render_model_preview(model_config, distributed_config)
            
            st.subheader("üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
            st.json(full_config)
        
        with col2:
            st.subheader("üéÆ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
            
            if st.session_state.training_active:
                if st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", type="primary"):
                    with st.spinner("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É..."):
                        stopped = stop_training()
                    if stopped:
                        st.success("‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    else:
                        st.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å (–≤–æ–∑–º–æ–∂–Ω–æ —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞)")
                    time.sleep(1)
                    st.rerun()
            else:
                if st.button("‚ñ∂Ô∏è –ù–∞—á–∞—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É", type="primary"):
                    with st.spinner("–ó–∞–ø—É—Å–∫..."):
                        run_id, process = start_training(full_config)
                        st.session_state.current_run_id = run_id
                        st.session_state.training_process = process
                        st.session_state.training_active = True
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–π run –¥–ª—è persistence
                        save_active_run(run_id, full_config)
                        
                        st.success(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞! Run ID: {run_id}")
                        time.sleep(1)
                        st.rerun()
    
    with tab2:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fragment –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        live_metrics_fragment()
    
    with tab4:
        st.header("üìú –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—É—Å–∫–æ–≤")
        st.markdown("---")
        
        runs = sorted(RUNS_DIR.glob("*"), reverse=True)
        
        if runs:
            for run_dir in runs[:10]:  # Last 10 runs
                run_id = run_dir.name
                metrics = load_metrics(run_id)
                
                if metrics:
                    status = metrics.get("status", "unknown")
                    status_emoji = {"training": "üü¢", "completed": "‚úÖ", "error": "‚ùå", "stopped": "‚èπÔ∏è"}.get(status, "‚è≥")
                    
                    with st.expander(f"{status_emoji} {run_id}"):
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Steps", metrics.get("current_step", 0))
                        with col2:
                            st.metric("Final Loss", f"{metrics.get('current_loss', 0):.4f}")
                        with col3:
                            st.metric("Status", status)
                        with col4:
                            st.metric("Duration", metrics.get("training_duration", "-"))
                        
                        # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                        checkpoints = metrics.get("checkpoints", [])
                        if checkpoints:
                            st.markdown("**üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã:**")
                            for ckpt in checkpoints[-5:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5
                                st.caption(f"Step {ckpt['step']}: `{ckpt['path']}`")
                        
                        # –ö–Ω–æ–ø–∫–∏
                        btn_col1, btn_col2 = st.columns(2)
                        with btn_col1:
                            if st.button(f"üìä –ú–µ—Ç—Ä–∏–∫–∏", key=f"metrics_{run_id}"):
                                st.session_state.current_run_id = run_id
                                st.toast(f"‚úÖ –í—ã–±—Ä–∞–Ω run: {run_id}. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", icon="üìä")
                        with btn_col2:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞
                            config_path = run_dir / "config.json"
                            if config_path.exists():
                                try:
                                    with open(config_path) as f:
                                        run_config = json.load(f)
                                    model_dir = PROJECT_ROOT / run_config.get("output_dir", "")
                                    final_model = model_dir / "final_model"
                                    if final_model.exists():
                                        if st.button("üí¨ –ß–∞—Ç", key=f"chat_run_{run_id}"):
                                            st.session_state.selected_chat_model = str(final_model)
                                            st.toast("‚úÖ –ú–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É üí¨ –ß–∞—Ç", icon="üí¨")
                                except:
                                    pass
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –≤—ã–±—Ä–∞–Ω–æ
                        if st.session_state.current_run_id == run_id:
                            st.info("üëÜ –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**")
        else:
            st.info("–ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤")
            
    with tab5:
        render_data_manager()
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø—Ä–æ —á–∞—Ç
        st.markdown("---")
        st.info("üí° –ß—Ç–æ–±—ã –ø–æ–æ–±—â–∞—Ç—å—Å—è —Å –º–æ–¥–µ–ª—å—é, –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É **üí¨ –ß–∞—Ç** (–≤ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)")

    with tab6:
        render_docs()
    
    with tab3:
        st.header("üí¨ –ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é")
        st.markdown("---")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        available_models = get_available_models()
        
        if available_models:
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            col1, col2 = st.columns([3, 1])
            
            with col1:
                model_options = [m["name"] for m in available_models]
                
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ - –Ω–∞—Ö–æ–¥–∏–º –µ—ë –∏–Ω–¥–µ–∫—Å
                default_idx = 0
                if st.session_state.selected_chat_model:
                    for i, m in enumerate(available_models):
                        if m["path"] == st.session_state.selected_chat_model:
                            default_idx = i
                            break
                
                selected_model_name = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç",
                    options=model_options,
                    index=default_idx,
                    help="–í—ã–±–µ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞"
                )
                selected_model = next(m for m in available_models if m["name"] == selected_model_name)
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º selected_chat_model –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                if st.session_state.selected_chat_model:
                    st.session_state.selected_chat_model = None
            
            with col2:
                model_type = selected_model["type"]
                if model_type == "final":
                    st.success("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")
                else:
                    st.info("üì¶ –ß–µ–∫–ø–æ–∏–Ω—Ç")
            
            st.caption(f"–ü—É—Ç—å: `{selected_model['path']}`")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            with st.expander("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"):
                gen_col1, gen_col2, gen_col3 = st.columns(3)
                with gen_col1:
                    max_tokens = st.slider("Max Tokens", 10, 500, 128)
                with gen_col2:
                    temperature = st.slider("Temperature", 0.1, 2.0, 0.8, 0.1)
                with gen_col3:
                    top_p = st.slider("Top-p", 0.1, 1.0, 0.9, 0.05)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞
            if "chat_model" not in st.session_state:
                st.session_state.chat_model = None
                st.session_state.chat_tokenizer = None
                st.session_state.chat_model_path = None
            
            if "messages" not in st.session_state:
                st.session_state.messages = []
            
            # –ö–Ω–æ–ø–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            if st.session_state.chat_model_path != selected_model["path"]:
                if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", type="primary"):
                    with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å..."):
                        try:
                            from transformers import AutoTokenizer
                            from homellm.models.home_model import HomeForCausalLM, HomeConfig
                            from safetensors.torch import load_file
                            
                            model_path = Path(selected_model["path"])
                            device = "cuda" if torch.cuda.is_available() else "cpu"
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —á–µ–∫–ø–æ–∏–Ω—Ç–∞
                            config_json = model_path / "config.json"
                            model_safetensors = model_path / "model.safetensors"
                            tokenizer_json = model_path / "tokenizer.json"
                            tokenizer_config = model_path / "tokenizer_config.json"
                            
                            # HuggingFace —Ñ–æ—Ä–º–∞—Ç = –µ—Å—Ç—å tokenizer —Ñ–∞–π–ª—ã
                            is_hf_format = tokenizer_json.exists() or tokenizer_config.exists()
                            
                            if is_hf_format and config_json.exists():
                                # HuggingFace —Ñ–æ—Ä–º–∞—Ç (final_model —Å tokenizer)
                                st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º HuggingFace –º–æ–¥–µ–ª—å...")
                                st.session_state.chat_tokenizer = AutoTokenizer.from_pretrained(
                                    str(model_path), 
                                    trust_remote_code=True
                                )
                                st.session_state.chat_model = HomeForCausalLM.from_pretrained(
                                    str(model_path)
                                ).to(device)
                            elif model_safetensors.exists():
                                # Accelerate checkpoint —Ñ–æ—Ä–º–∞—Ç
                                st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º Accelerate —á–µ–∫–ø–æ–∏–Ω—Ç...")
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä GPT-2 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                                st.session_state.chat_tokenizer = AutoTokenizer.from_pretrained("gpt2")
                                if st.session_state.chat_tokenizer.pad_token is None:
                                    st.session_state.chat_tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
                                
                                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥ –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
                                if config_json.exists():
                                    config = HomeConfig.from_pretrained(str(model_path))
                                    st.info(f"–ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω: hidden_size={config.hidden_size}, layers={config.num_hidden_layers}")
                                else:
                                    # –ò—â–µ–º –∫–æ–Ω—Ñ–∏–≥ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (run config)
                                    run_config_path = model_path.parent / "run_config.json"
                                    if run_config_path.exists():
                                        import json as json_module
                                        with open(run_config_path) as f:
                                            run_cfg = json_module.load(f)
                                        config = HomeConfig(
                                            vocab_size=len(st.session_state.chat_tokenizer),
                                            hidden_size=run_cfg.get("hidden_size", 512),
                                            num_hidden_layers=run_cfg.get("num_layers", 8),
                                            num_attention_heads=run_cfg.get("n_heads", 8),
                                            max_position_embeddings=run_cfg.get("seq_len", 512),
                                        )
                                        st.info(f"–ö–æ–Ω—Ñ–∏–≥ –∏–∑ run_config: hidden_size={config.hidden_size}")
                                    else:
                                        st.warning("‚ö†Ô∏è config.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
                                        config = HomeConfig(
                                            vocab_size=len(st.session_state.chat_tokenizer),
                                            hidden_size=512,
                                            num_hidden_layers=8,
                                            num_attention_heads=8,
                                            max_position_embeddings=512,
                                        )
                                
                                st.session_state.chat_model = HomeForCausalLM(config)
                                
                                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
                                state_dict = load_file(str(model_safetensors))
                                st.session_state.chat_model.load_state_dict(state_dict)
                                st.session_state.chat_model = st.session_state.chat_model.to(device)
                            else:
                                raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω config.json –∏–ª–∏ model.safetensors –≤ {model_path}")
                            
                            st.session_state.chat_model.eval()
                            st.session_state.chat_model_path = str(model_path)
                            st.session_state.messages = []
                            st.success("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                            st.rerun()
                        except Exception as e:
                            import traceback
                            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                            st.code(traceback.format_exc())
            else:
                st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {selected_model_name}")
                
                # --- –ò–ù–¢–ï–†–§–ï–ô–° –ß–ê–¢–ê –° –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ú –°–ö–†–û–õ–õ–û–ú ---
                chat_container = st.container(height=500) # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                
                with chat_container:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
                    for message in st.session_state.messages:
                        with st.chat_message(message["role"]):
                            st.write(message["content"])
                
                # –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤—Å–µ–≥–¥–∞ –≤–Ω–∏–∑—É)
                if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."):
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —é–∑–µ—Ä–∞ —Å—Ä–∞–∑—É)
                    with chat_container:
                        with st.chat_message("user"):
                            st.write(prompt)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                    with chat_container: # –û—Ç–≤–µ—Ç —Ç–æ–∂–µ –ø–∏—à–µ–º –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                        with st.chat_message("assistant"):
                            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è..."):
                                try:
                                    tokenizer = st.session_state.chat_tokenizer
                                    model = st.session_state.chat_model
                                    device = next(model.parameters()).device
                                    
                                    inputs = tokenizer(prompt, return_tensors="pt").to(device)
                                    
                                    with torch.no_grad():
                                        outputs = model.generate(
                                            **inputs,
                                            max_new_tokens=max_tokens,
                                            temperature=temperature,
                                            top_p=top_p,
                                            do_sample=True,
                                            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
                                            use_cache=False,  # –û—Ç–∫–ª—é—á–∞–µ–º KV-cache –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                                        )
                                    
                                    response = tokenizer.decode(
                                        outputs[0][inputs["input_ids"].shape[1]:], 
                                        skip_special_tokens=True
                                    )
                                    
                                    st.write(response)
                                    st.session_state.messages.append({"role": "assistant", "content": response})
                                except Exception as e:
                                    st.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                
                # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
                if st.session_state.messages:
                    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
                        st.session_state.messages = []
                        st.rerun()
        else:
            st.info("–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –≤–æ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–ø—É—Å–∫'!")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥–¥–µ –∏—Å–∫–∞—Ç—å –º–æ–¥–µ–ª–∏
            st.markdown("""
            **–ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:**
            - `out/*/final_model/` ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
            - `out/*/checkpoint_*/` ‚Äî —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            """)


if __name__ == "__main__":
    main()

