"""
Motels at Home Training Studio â€” Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
======================================================================

Ğ—Ğ°Ğ¿ÑƒÑĞº:
    streamlit run homellm/app/main.py
    
Ğ¸Ğ»Ğ¸:
    ./scripts/run_studio.sh
"""

import streamlit as st
import subprocess
import json
import time
import os
import signal
from pathlib import Path
from datetime import datetime
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import torch
from datasets import load_dataset  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚

# ĞŸÑƒÑ‚Ğ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent
CONFIGS_DIR = PROJECT_ROOT / "configs"
DATASET_DIR = PROJECT_ROOT / "datasets"  # datasets Ñ "s"!
OUTPUT_DIR = PROJECT_ROOT / "out"
RUNS_DIR = PROJECT_ROOT / ".runs"
RUNS_DIR.mkdir(exist_ok=True)

# ============================================================================
# Page Config
# ============================================================================

st.set_page_config(
    page_title="HomeLLM Training Studio",
    page_icon="ğŸ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS â€” Ñ‡Ğ¸ÑÑ‚Ğ°Ñ Ñ‚Ñ‘Ğ¼Ğ½Ğ°Ñ Ñ‚ĞµĞ¼Ğ° Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¾Ğ¼
st.markdown("""
<style>
    /* Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº */
    .main-header {
        color: #ff6b6b;
        font-size: 2.5rem;
        font-weight: 800;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    
    .sub-header {
        color: #888888;
        text-align: center;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    
    /* Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ */
    .status-running {
        color: #22c55e !important;
        font-weight: bold;
    }
    
    .status-completed {
        color: #3b82f6 !important;
        font-weight: bold;
    }
    
    .status-error {
        color: #ef4444 !important;
        font-weight: bold;
    }
    
    /* ASCII art Ğ±Ğ»Ğ¾Ğº */
    .model-ascii {
        background: #1e1e1e;
        border: 1px solid #333;
        border-radius: 8px;
        padding: 1rem;
        font-family: 'Courier New', monospace;
        color: #00ff88;
        white-space: pre;
        overflow-x: auto;
    }
    
    /* ĞšĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº */
    div[data-testid="metric-container"] {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 0.5rem;
    }
    
    /* ĞšĞ½Ğ¾Ğ¿ĞºĞ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° */
    .stButton > button[kind="primary"] {
        background: linear-gradient(90deg, #e94560, #ff6b6b);
        color: white !important;
        border: none;
        font-weight: 600;
    }
    
    /* Code Ğ±Ğ»Ğ¾ĞºĞ¸ */
    pre {
        background: #0d1117 !important;
        color: #c9d1d9 !important;
        border: 1px solid #30363d !important;
    }
    
    code {
        color: #79c0ff !important;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# Persistence â€” ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°Ğ¼Ğ¸
# ============================================================================

ACTIVE_RUN_FILE = RUNS_DIR / "active_run.json"


def save_active_run(run_id: str, config: dict = None):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ run Ğ² Ñ„Ğ°Ğ¹Ğ»."""
    data = {
        "run_id": run_id,
        "started_at": datetime.now().isoformat(),
        "config": config or {}
    }
    with open(ACTIVE_RUN_FILE, "w") as f:
        json.dump(data, f, indent=2)


def load_active_run() -> dict | None:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ run Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°."""
    if not ACTIVE_RUN_FILE.exists():
        return None
    try:
        with open(ACTIVE_RUN_FILE) as f:
            return json.load(f)
    except:
        return None


def clear_active_run():
    """ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ run."""
    if ACTIVE_RUN_FILE.exists():
        ACTIVE_RUN_FILE.unlink()


def restore_session_state():
    """Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ÑĞµÑÑĞ¸Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸."""
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞµÑÑ‚ÑŒ Ğ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ run
    active = load_active_run()
    if active and active.get("run_id"):
        run_id = active["run_id"]
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ¸ run Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ
        run_dir = RUNS_DIR / run_id
        if run_dir.exists():
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¶Ğ¸Ğ² Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
            pid_path = run_dir / "pid"
            process_alive = False
            if pid_path.exists():
                try:
                    with open(pid_path) as f:
                        pid = int(f.read().strip())
                    os.kill(pid, 0)  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°
                    process_alive = True
                except PermissionError:
                    process_alive = True
                except (ProcessLookupError, ValueError, PermissionError):
                    pass
            
            # Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ
            st.session_state.current_run_id = run_id
            st.session_state.training_active = process_alive
            
            # Ğ•ÑĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½, Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ active_run
            if not process_alive:
                metrics_path = run_dir / "metrics.json"
                if metrics_path.exists():
                    try:
                        with open(metrics_path) as f:
                            metrics = json.load(f)
                        if metrics.get("status") in ["completed", "error", "stopped"]:
                            clear_active_run()
                    except:
                        pass


# ============================================================================
# Session State
# ============================================================================

if "training_process" not in st.session_state:
    st.session_state.training_process = None
if "current_run_id" not in st.session_state:
    st.session_state.current_run_id = None
if "training_active" not in st.session_state:
    st.session_state.training_active = False
if "selected_chat_model" not in st.session_state:
    st.session_state.selected_chat_model = None

# Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ
if "session_restored" not in st.session_state:
    restore_session_state()
    st.session_state.session_restored = True


# ============================================================================
# Helper Functions
# ============================================================================

def get_available_datasets():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²."""
    datasets = []
    if DATASET_DIR.exists():
        for f in DATASET_DIR.glob("*.jsonl"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
        for f in DATASET_DIR.glob("*.txt"):
            size_mb = f.stat().st_size / (1024 * 1024)
            datasets.append((f.name, f"{size_mb:.1f} MB"))
    return datasets


def get_gpu_info():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… GPU."""
    gpus = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            gpus.append({
                "id": i,
                "name": props.name,
                "memory_gb": round(memory_gb, 1),
                "compute_capability": f"{props.major}.{props.minor}",
            })
    return gpus


def get_available_configs():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… accelerate ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¾Ğ²."""
    configs = []
    if CONFIGS_DIR.exists():
        for f in CONFIGS_DIR.glob("*.yaml"):
            name = f.stem.replace("accelerate_", "").replace("_", " ").title()
            configs.append({
                "file": f.name,
                "name": name,
                "path": str(f),
            })
    return configs


# ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°
PARALLEL_TYPES = {
    "default": {
        "name": "Single GPU / CPU",
        "type": "None",
        "description": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°",
        "icon": "ğŸ–¥ï¸",
    },
    "multi_gpu": {
        "name": "Multi-GPU (DDP)",
        "type": "Data Parallel",
        "description": "Distributed Data Parallel â€” ĞºĞ°Ğ¶Ğ´Ğ°Ñ GPU Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ¿Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ±Ğ°Ñ‚Ñ‡Ğ°",
        "icon": "ğŸ”„",
    },
    "fsdp": {
        "name": "FSDP",
        "type": "Data Parallel + Model Parallel",
        "description": "Fully Sharded Data Parallel â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ GPU (PyTorch native)",
        "icon": "âš¡",
    },
    "deepspeed_zero2": {
        "name": "DeepSpeed ZeRO-2",
        "type": "Data Parallel + Optimizer Parallel",
        "description": "Ğ¨Ğ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ GPU",
        "icon": "ğŸš€",
    },
    "deepspeed_zero3": {
        "name": "DeepSpeed ZeRO-3",
        "type": "Full Model Parallel",
        "description": "ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ + Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ + Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹",
        "icon": "ğŸ’ª",
    },
    "deepspeed_zero3_offload": {
        "name": "ZeRO-3 + CPU Offload",
        "type": "Model Parallel + CPU Offload",
        "description": "ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ + Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ½Ğ° CPU Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ VRAM",
        "icon": "ğŸ§Š",
    },
}


def estimate_parameters(hidden_size: int, num_layers: int, vocab_size: int = 50257) -> int:
    """ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."""
    # Embedding: vocab_size * hidden_size
    embed = vocab_size * hidden_size
    # Each layer: attention (4 * hidden^2) + FFN (8 * hidden^2) + norms
    per_layer = 4 * hidden_size ** 2 + 8 * hidden_size ** 2 + 2 * hidden_size
    # LM head is tied, so not counted
    total = embed + num_layers * per_layer
    return total


def format_params(n: int) -> str:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."""
    if n >= 1e9:
        return f"{n/1e9:.2f}B"
    elif n >= 1e6:
        return f"{n/1e6:.1f}M"
    elif n >= 1e3:
        return f"{n/1e3:.1f}K"
    return str(n)


def format_time(seconds: float) -> str:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        return f"{seconds/3600:.1f}h"


def load_metrics(run_id: str) -> dict:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°."""
    metrics_path = RUNS_DIR / run_id / "metrics.json"
    if metrics_path.exists():
        try:
            with open(metrics_path) as f:
                return json.load(f)
        except:
            pass
    return None


def start_training(config: dict) -> str:
    """Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ² Ñ„Ğ¾Ğ½Ğµ."""
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = RUNS_DIR / run_id
    run_dir.mkdir(parents=True, exist_ok=True)
    
    config_path = run_dir / "config.json"
    metrics_path = run_dir / "metrics.json"
    stdout_path = run_dir / "stdout.log"
    stderr_path = run_dir / "stderr.log"
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ metrics Ñ„Ğ°Ğ¹Ğ»
    with open(metrics_path, "w") as f:
        json.dump({"status": "starting", "current_step": 0}, f)
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° distributed
    distributed_mode = config.get("distributed_mode", "default")
    config_file = config.get("config_file")
    num_gpus = config.get("num_gpus", 1)
    
    if distributed_mode != "default" and config_file:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ accelerate launch Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¾Ğ¼
        cmd = [
            "accelerate", "launch",
            "--config_file", config_file,
            "--num_processes", str(num_gpus),
            "--gradient_accumulation_steps", str(config.get("gradient_accumulation", 1)),
            "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    else:
        # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº
        cmd = [
            "python", "-m", "homellm.app.trainer_worker",
            "--config", str(config_path),
            "--metrics", str(metrics_path)
        ]
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸
    cmd_path = run_dir / "command.txt"
    with open(cmd_path, "w") as f:
        f.write(" ".join(cmd))
    
    stdout_file = open(stdout_path, "w")
    stderr_file = open(stderr_path, "w")
    
    process = subprocess.Popen(
        cmd,
        cwd=str(PROJECT_ROOT),
        stdout=stdout_file,
        stderr=stderr_file,
        start_new_session=True,  # ĞÑ‚Ğ´ĞµĞ»ÑĞµĞ¼ Ğ¾Ñ‚ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°
    )
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ PID Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°
    pid_path = run_dir / "pid"
    with open(pid_path, "w") as f:
        f.write(str(process.pid))
    
    return run_id, process


def stop_training():
    """ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ."""
    stopped = False
    
    # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ PID Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ° (Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾)
    if st.session_state.current_run_id:
        pid_path = RUNS_DIR / st.session_state.current_run_id / "pid"
        if pid_path.exists():
            try:
                with open(pid_path) as f:
                    pid = int(f.read().strip())
                # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° SIGTERM, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ SIGKILL ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ»Ğ¾
                os.kill(pid, signal.SIGTERM)
                stopped = True
                
                # Ğ–Ğ´Ñ‘Ğ¼ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼
                time.sleep(0.5)
                try:
                    os.kill(pid, 0)  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¶Ğ¸Ğ² Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
                    os.kill(pid, signal.SIGKILL)  # ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ±Ğ¸Ğ²Ğ°ĞµĞ¼
                except ProcessLookupError:
                    pass  # ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»ÑÑ
            except Exception as e:
                pass
        
        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        metrics_path = RUNS_DIR / st.session_state.current_run_id / "metrics.json"
        if metrics_path.exists():
            try:
                with open(metrics_path) as f:
                    metrics = json.load(f)
                metrics["status"] = "stopped"
                with open(metrics_path, "w") as f:
                    json.dump(metrics, f, indent=2)
            except:
                pass
    
    # Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· subprocess
    if st.session_state.training_process:
        try:
            st.session_state.training_process.terminate()
            st.session_state.training_process.wait(timeout=2)
        except:
            try:
                st.session_state.training_process.kill()
            except:
                pass
        st.session_state.training_process = None
    
    st.session_state.training_active = False
    
    # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ active_run
    clear_active_run()
    
    return stopped


def is_process_running(run_id: str) -> bool:
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ, Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ."""
    pid_path = RUNS_DIR / run_id / "pid"
    if not pid_path.exists():
        return False
    
    try:
        with open(pid_path) as f:
            pid = int(f.read().strip())
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
        os.kill(pid, 0)
        return True
    except PermissionError:
        # ĞŸÑ€Ğ¾Ñ†ĞµÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚, Ğ½Ğ¾ Ñƒ Ğ½Ğ°Ñ Ğ½ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ² (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾ ÑĞ·ĞµÑ€Ğ°)
        # Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½ Ğ¶Ğ¸Ğ²
        return True
    except (ProcessLookupError, ValueError, FileNotFoundError):
        return False


# ============================================================================
# UI Components
# ============================================================================

def render_header():
    st.markdown("# ğŸ  Models at Home Training Studio")
    st.caption("Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾Ğ¼Ğ°")


def render_model_config():
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ°Ğ¹Ğ´Ğ±Ğ°Ñ€Ğµ."""
    st.sidebar.header("ğŸ§  ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
    
    # ĞŸÑ€ĞµÑĞµÑ‚Ñ‹
    preset = st.sidebar.selectbox(
        "ĞŸÑ€ĞµÑĞµÑ‚",
        ["Tiny (25M)", "Small (80M)", "Medium (200M)", "Large (400M)", "Custom"],
        index=0
    )
    
    presets = {
        "Tiny (25M)": (512, 8, 8),
        "Small (80M)": (768, 12, 12),
        "Medium (200M)": (1024, 16, 16),
        "Large (400M)": (1280, 20, 20),
    }
    
    if preset != "Custom" and preset in presets:
        default_h, default_l, default_n = presets[preset]
    else:
        default_h, default_l, default_n = 512, 8, 8
    
    hidden_size = st.sidebar.slider(
        "Hidden Size", 
        min_value=128, 
        max_value=2048, 
        value=default_h, 
        step=64,
        help="Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ"
    )
    
    num_layers = st.sidebar.slider(
        "Num Layers", 
        min_value=2, 
        max_value=32, 
        value=default_l,
        help="ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ»Ğ¾Ñ‘Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°"
    )
    
    n_heads = st.sidebar.slider(
        "Attention Heads", 
        min_value=2, 
        max_value=32, 
        value=default_n,
        help="ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ"
    )
    
    seq_len = st.sidebar.selectbox(
        "Seq Length",
        [256, 512, 1024, 2048],
        index=1,
        help="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸"
    )
    
    # ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
    est_params = estimate_parameters(hidden_size, num_layers)
    st.sidebar.metric("ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ (â‰ˆ)", format_params(est_params))
    
    return {
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "n_heads": n_heads,
        "seq_len": seq_len,
    }


def render_training_config():
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ°Ğ¹Ğ´Ğ±Ğ°Ñ€Ğµ."""
    st.sidebar.header("âš™ï¸ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ")
    
    batch_size = st.sidebar.slider(
        "Batch Size",
        min_value=1,
        max_value=64,
        value=16,
        help="Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ°"
    )
    
    grad_accum = st.sidebar.slider(
        "Gradient Accumulation",
        min_value=1,
        max_value=32,
        value=4,
        help="Ğ¨Ğ°Ğ³Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°"
    )
    
    st.sidebar.caption(f"Effective batch: {batch_size * grad_accum}")
    
    learning_rate = st.sidebar.select_slider(
        "Learning Rate",
        options=[1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3],
        value=5e-4,
        format_func=lambda x: f"{x:.0e}"
    )
    
    warmup_steps = st.sidebar.number_input(
        "Warmup Steps",
        min_value=0,
        max_value=10000,
        value=1000
    )
    
    # Ğ’Ñ‹Ğ±Ğ¾Ñ€: epochs Ğ¸Ğ»Ğ¸ max_steps
    training_mode = st.sidebar.radio(
        "Ğ ĞµĞ¶Ğ¸Ğ¼ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸",
        ["ĞŸĞ¾ ÑĞ¿Ğ¾Ñ…Ğ°Ğ¼", "ĞŸĞ¾ ÑˆĞ°Ğ³Ğ°Ğ¼"],
        help="Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ĞºĞ°Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸"
    )
    
    if training_mode == "ĞŸĞ¾ ÑĞ¿Ğ¾Ñ…Ğ°Ğ¼":
        epochs = st.sidebar.number_input(
            "Epochs",
            min_value=1,
            max_value=10,
            value=1
        )
        max_steps = None
    else:
        epochs = 1
        max_steps = st.sidebar.number_input(
            "Max Steps",
            min_value=100,
            max_value=1000000,
            value=10000,
            step=1000,
            help="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"
        )
    
    mixed_precision = st.sidebar.selectbox(
        "Mixed Precision",
        ["no", "fp16", "bf16"],
        index=2,
        help="bf16 Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ampere+ GPU"
    )
    
    grad_checkpoint = st.sidebar.checkbox(
        "Gradient Checkpointing",
        value=False,
        help="Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ VRAM, Ğ½Ğ¾ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ"
    )
    
    return {
        "batch_size": batch_size,
        "gradient_accumulation": grad_accum,
        "learning_rate": learning_rate,
        "warmup_steps": warmup_steps,
        "epochs": epochs,
        "max_steps": max_steps,
        "mixed_precision": mixed_precision,
        "grad_checkpoint": grad_checkpoint,
    }


def render_dataset_config():
    """Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°."""
    st.sidebar.header("ğŸ“ Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚")
    
    datasets = get_available_datasets()
    
    if datasets:
        dataset_options = [f"{name} ({size})" for name, size in datasets]
        selected = st.sidebar.selectbox("Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚", dataset_options)
        selected_name = selected.split(" (")[0]
        data_path = str(DATASET_DIR / selected_name)
    else:
        st.sidebar.warning("Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ² dataset/")
        data_path = st.sidebar.text_input("ĞŸÑƒÑ‚ÑŒ Ğº Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ", "dataset/data.jsonl")
    
    return {"data_path": data_path}


def render_output_config():
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."""
    st.sidebar.header("ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ")
    
    output_dir = st.sidebar.text_input(
        "Output Directory",
        value="out/training_run"
    )
    
    save_every = st.sidebar.number_input(
        "Save Checkpoint Every N Steps",
        min_value=100,
        max_value=50000,
        value=2000,
        step=500,
        help="ĞšĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹"
    )
    
    log_every = st.sidebar.number_input(
        "Log Every N Steps",
        min_value=1,
        max_value=1000,
        value=10,
        help="ĞšĞ°Ğº Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸"
    )
    
    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°Ñ…
    output_path = PROJECT_ROOT / output_dir
    if output_path.exists():
        checkpoints = list(output_path.glob("checkpoint_*"))
        final_model = output_path / "final_model"
        
        if checkpoints or final_model.exists():
            st.sidebar.caption(f"ğŸ“¦ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ²: {len(checkpoints)}")
            if final_model.exists():
                st.sidebar.caption("âœ… Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°")
    
    return {
        "output_dir": output_dir,
        "save_every": save_every,
        "log_every": log_every,
        "tokenizer_path": "gpt2"
    }


def get_available_models():
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."""
    models = []
    
    # Ğ˜Ñ‰ĞµĞ¼ Ğ² out/
    if OUTPUT_DIR.exists():
        for model_dir in OUTPUT_DIR.iterdir():
            if model_dir.is_dir():
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞµÑÑ‚ÑŒ Ğ»Ğ¸ final_model
                final_model = model_dir / "final_model"
                if final_model.exists() and (final_model / "config.json").exists():
                    models.append({
                        "name": f"{model_dir.name}/final_model",
                        "path": str(final_model),
                        "type": "final",
                    })
                
                # Ğ˜Ñ‰ĞµĞ¼ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹
                for ckpt in sorted(model_dir.glob("checkpoint_*"), reverse=True):
                    if ckpt.is_dir():
                        models.append({
                            "name": f"{model_dir.name}/{ckpt.name}",
                            "path": str(ckpt),
                            "type": "checkpoint",
                        })
    
    return models


def render_distributed_config():
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ distributed training."""
    st.sidebar.header("ğŸ–¥ï¸ GPU Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼")
    
    # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ GPU
    gpus = get_gpu_info()
    
    if gpus:
        st.sidebar.success(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ GPU: {len(gpus)}")
        
        # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸ GPU
        for gpu in gpus:
            st.sidebar.markdown(f"""
            **GPU {gpu['id']}**: {gpu['name']}  
            ğŸ“Š VRAM: {gpu['memory_gb']} GB | CC: {gpu['compute_capability']}
            """)
        
        # Ğ’Ñ‹Ğ±Ğ¾Ñ€ GPU Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        gpu_options = [f"GPU {g['id']}: {g['name']}" for g in gpus]
        if len(gpus) > 1:
            selected_gpus = st.sidebar.multiselect(
                "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ GPU",
                options=gpu_options,
                default=gpu_options,
                help="Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ GPU Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"
            )
            num_gpus = len(selected_gpus)
            gpu_ids = [gpu_options.index(g) for g in selected_gpus]
        else:
            num_gpus = 1
            gpu_ids = [0]
            st.sidebar.info("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ GPU")
    else:
        st.sidebar.warning("âš ï¸ GPU Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹, Ğ±ÑƒĞ´ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ CPU")
        num_gpus = 0
        gpu_ids = []
    
    st.sidebar.markdown("---")
    
    # Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¸Ğ¿Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°
    st.sidebar.subheader("âš¡ Ğ¢Ğ¸Ğ¿ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°")
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
    if num_gpus == 0:
        available_modes = ["default"]
        recommended_idx = 0
    elif num_gpus == 1:
        available_modes = ["default", "deepspeed_zero3_offload"]
        recommended_idx = 0
    else:
        # ĞŸÑ€Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… GPU Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼ multi_gpu Ğ¸Ğ»Ğ¸ fsdp
        available_modes = ["multi_gpu", "fsdp", "deepspeed_zero2", "deepspeed_zero3", "deepspeed_zero3_offload", "default"]
        recommended_idx = 0  # multi_gpu Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ selectbox
    mode_options = []
    for i, mode in enumerate(available_modes):
        info = PARALLEL_TYPES[mode]
        label = f"{info['icon']} {info['name']}"
        if i == recommended_idx and num_gpus > 1:
            label += " â­"  # ĞÑ‚Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğ¹
        mode_options.append(label)
    
    selected_mode_display = st.sidebar.selectbox(
        "Ğ ĞµĞ¶Ğ¸Ğ¼",
        options=mode_options,
        index=recommended_idx,
        help="Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"
    )
    
    # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
    selected_idx = mode_options.index(selected_mode_display)
    selected_mode = available_modes[selected_idx]
    mode_info = PARALLEL_TYPES[selected_mode]
    
    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ
    st.sidebar.markdown(f"""
    <div style="background: rgba(255,255,255,0.05); padding: 10px; border-radius: 8px; margin: 10px 0;">
    <b>Ğ¢Ğ¸Ğ¿:</b> {mode_info['type']}<br>
    <small>{mode_info['description']}</small>
    </div>
    """, unsafe_allow_html=True)
    
    # ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞµÑĞ»Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½ single GPU Ğ¿Ñ€Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ…
    if num_gpus > 1 and selected_mode == "default":
        st.sidebar.warning(f"âš ï¸ Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½ Single GPU, Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ {num_gpus} GPU. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼ Multi-GPU!")
    
    # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ Ñ„Ğ°Ğ¹Ğ»
    config_file = None
    if selected_mode != "default":
        config_path = CONFIGS_DIR / f"accelerate_{selected_mode}.yaml"
        if config_path.exists():
            config_file = str(config_path)
            st.sidebar.caption(f"ğŸ“„ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³: `{config_path.name}`")
    
    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸš€ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°")
    
    if num_gpus == 0:
        launch_info = "**Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾:** CPU"
    elif selected_mode == "default":
        launch_info = f"**Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾:** GPU {gpu_ids[0] if gpu_ids else 0}"
    else:
        launch_info = f"**Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°:** {num_gpus} Ã— GPU\n**Ğ ĞµĞ¶Ğ¸Ğ¼:** {mode_info['type']}"
    
    st.sidebar.info(launch_info)
    
    return {
        "distributed_mode": selected_mode,
        "num_gpus": num_gpus,
        "gpu_ids": gpu_ids,
        "config_file": config_file,
        "parallel_type": mode_info['type'],
    }


@st.fragment(run_every=3)  # ĞĞ²Ñ‚Ğ¾Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹
def live_metrics_fragment():
    """Fragment Ğ´Ğ»Ñ Ğ¶Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ²ÑĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹."""
    if not st.session_state.current_run_id:
        st.info("Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ run Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº")
        return
    
    run_id = st.session_state.current_run_id
    metrics = load_metrics(run_id)
    process_alive = is_process_running(run_id)
    
    # Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ
    if process_alive:
        st.success(f"ğŸŸ¢ ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ (Run: {run_id})")
    else:
        if metrics and metrics.get("status") == "completed":
            st.success(f"âœ… Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° (Run: {run_id})")
        elif metrics and metrics.get("status") == "error":
            st.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° (Run: {run_id})")
        elif metrics and metrics.get("status") == "stopped":
            st.warning(f"â¹ï¸ Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° (Run: {run_id})")
        else:
            st.info(f"ğŸ“‹ ĞŸÑ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº (Run: {run_id})")
    
    if metrics:
        render_metrics_dashboard(metrics)
    else:
        st.info("ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")


def render_metrics_dashboard(metrics: dict):
    """Ğ”Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´ Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."""
    
    status = metrics.get("status", "unknown")
    
    # Status indicator
    status_emoji = {
        "training": "ğŸŸ¢", 
        "completed": "âœ…", 
        "error": "âŒ",
        "initializing": "â³",
        "loading_tokenizer": "â³",
        "loading_dataset": "â³",
        "building_model": "â³",
        "saving_model": "ğŸ’¾",
    }.get(status, "â³")
    
    st.subheader(f"{status_emoji} Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: {status.upper()}")
    
    # Metrics cards
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        current_step = metrics.get("current_step", 0)
        total_steps = metrics.get("total_steps", 1)
        progress = current_step / total_steps * 100 if total_steps > 0 else 0
        st.metric("ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ", f"{progress:.1f}%", f"Step {current_step}/{total_steps}")
    
    with col2:
        loss = metrics.get("current_loss", 0)
        st.metric("Loss", f"{loss:.4f}")
    
    with col3:
        lr = metrics.get("current_lr", 0)
        st.metric("Learning Rate", f"{lr:.2e}")
    
    with col4:
        eta = metrics.get("eta_seconds", 0)
        st.metric("ETA", format_time(eta))
    
    # Progress bar
    st.progress(min(progress / 100, 1.0))
    
    # Charts
    if metrics.get("loss_history"):
        col1, col2 = st.columns(2)
        
        with col1:
            # Loss chart
            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["loss_history"],
                mode='lines',
                name='Loss',
                line=dict(color='#e94560', width=2)
            ))
            fig_loss.update_layout(
                title="Training Loss",
                xaxis_title="Step",
                yaxis_title="Loss",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_loss, width="stretch")
        
        with col2:
            # LR chart
            fig_lr = go.Figure()
            fig_lr.add_trace(go.Scatter(
                x=metrics["steps_history"],
                y=metrics["lr_history"],
                mode='lines',
                name='LR',
                line=dict(color='#60a5fa', width=2)
            ))
            fig_lr.update_layout(
                title="Learning Rate Schedule",
                xaxis_title="Step",
                yaxis_title="LR",
                template="plotly_dark",
                height=300,
                margin=dict(l=0, r=0, t=40, b=0)
            )
            st.plotly_chart(fig_lr, width="stretch")
    
    # Checkpoints
    if metrics.get("checkpoints"):
        with st.expander("ğŸ“¦ Checkpoints"):
            for ckpt in metrics["checkpoints"]:
                st.text(f"Step {ckpt['step']}: {ckpt['path']}")
    
    # GPU ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    gpu_stats = metrics.get("gpu_stats", [])
    if gpu_stats:
        st.subheader("ğŸ–¥ï¸ ĞĞ°Ğ³Ñ€ÑƒĞ·ĞºĞ° GPU")
        
        cols = st.columns(len(gpu_stats))
        for i, (col, gpu) in enumerate(zip(cols, gpu_stats)):
            with col:
                st.markdown(f"**GPU {gpu['id']}**")
                
                # Memory bar
                mem_percent = gpu.get('memory_percent', 0)
                st.progress(min(mem_percent / 100, 1.0), text=f"VRAM: {gpu['memory_used_gb']:.1f} / {gpu['memory_total_gb']:.1f} GB ({mem_percent:.0f}%)")
                
                # Utilization
                util = gpu.get('utilization')
                if util is not None:
                    st.progress(min(util / 100, 1.0), text=f"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°: {util}%")
                else:
                    st.caption("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°: N/A")
    
    # Error
    if metrics.get("error"):
        st.error("âŒ ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸")
        with st.expander("ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ (Traceback)", expanded=True):
            st.code(metrics['error'], language="python")
    
    # Ğ›Ğ¾Ğ³Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°
    if st.session_state.current_run_id:
        run_dir = RUNS_DIR / st.session_state.current_run_id
        stderr_path = run_dir / "stderr.log"
        stdout_path = run_dir / "stdout.log"
        
        with st.expander("ğŸ“‹ Ğ›Ğ¾Ğ³Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.caption("stdout (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 500 ÑÑ‚Ñ€Ğ¾Ğº)")
                if stdout_path.exists():
                    with open(stdout_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(Ğ¿ÑƒÑÑ‚Ğ¾)", language=None)
            
            with col2:
                st.caption("stderr (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 500 ÑÑ‚Ñ€Ğ¾Ğº)")
                if stderr_path.exists():
                    with open(stderr_path) as f:
                        lines = f.readlines()
                        content = "".join(lines[-500:])
                        st.code(content if content else "(Ğ¿ÑƒÑÑ‚Ğ¾)", language=None)



def download_hf_dataset(repo_id, subset, split, limit_val, limit_bytes, filter_lang, filter_score, filename):
    """Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° (Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ¶Ğ°Ñ‚Ğ¸Ğ¸ ĞºĞ½Ğ¾Ğ¿ĞºĞ¸)."""
    st.info(f"ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: {repo_id}...")
    
    target_filename = filename if filename else f"{repo_id.split('/')[-1]}_{split}.jsonl"
    if not target_filename.endswith('.jsonl'):
        target_filename += '.jsonl'
    
    save_path = DATASET_DIR / target_filename
    
    status_text = st.empty()
    progress_bar = st.progress(0)
    metric_col1, metric_col2, metric_col3 = st.columns(3)
    
    try:
        status_text.info(f"ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº {repo_id}...")
        
        # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ¼ĞµĞ½Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ°
        config_name = subset if subset and subset != "default" else None
        
        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ
        try:
            ds = load_dataset(
                repo_id, 
                name=config_name, 
                split=split, 
                streaming=True,
                trust_remote_code=True
            )
        except ValueError as e:
            if "BuilderConfig" in str(e) and "not found" in str(e):
                # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸ Ğ¸Ğ· ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
                import re
                match = re.search(r"Available: \[(.*?)\]", str(e))
                if match:
                    available = match.group(1).replace("'", "")
                    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºÑƒ
                    st.error(f"ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ '{subset}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½! Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸ (ÑĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ğ² Ğ¿Ğ¾Ğ»Ğµ Subset):")
                    st.code(available, language=None)
                    # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ rus_Cyrl, Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ ĞµĞ³Ğ¾
                    if "rus_Cyrl" in available:
                        st.info("ğŸ’¡ Ğ”Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Subset: **rus_Cyrl**")
                    return
            raise e
        
        status_text.info(f"Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² {target_filename}...")
        
        count = 0
        current_bytes = 0
        skipped_count = 0
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚
        iterator = iter(ds)
        try:
            first_item = next(iterator)
            items_to_process = [first_item]
        except StopIteration:
            st.error("Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿ÑƒÑÑ‚!")
            return
        
        with open(save_path, "w", encoding="utf-8") as f:
            import itertools
            for item in itertools.chain(items_to_process, iterator):
                # 1. Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
                if filter_lang:
                    item_lang = item.get("language")
                    if item_lang and filter_lang.lower() not in item_lang.lower():
                        skipped_count += 1
                        continue
                
                if filter_score > 0:
                    item_score = item.get("language_score")
                    if item_score is not None and float(item_score) < filter_score:
                        skipped_count += 1
                        continue
                
                # 2. Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°
                text = item.get("text") or item.get("content") or item.get("body")
                
                if text:
                    row_data = json.dumps({"text": text}, ensure_ascii=False)
                    row_bytes = len(row_data.encode('utf-8')) + 1
                    
                    f.write(row_data + "\n")
                    
                    count += 1
                    current_bytes += row_bytes
                    
                    if count % 100 == 0:
                        status_text.text(f"Ğ¡ĞºĞ°Ñ‡Ğ°Ğ½Ğ¾: {count} | {current_bytes / 1024**2:.1f} MB")
                        if limit_val > 0 and count >= limit_val:
                            break
                        if limit_bytes > 0 and current_bytes >= limit_bytes:
                            break
        
        status_text.success(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ² {target_filename}")
        time.sleep(2)
        st.rerun()
        
    except Exception as e:
        st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")


def render_data_manager():
    """Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."""
    st.header("ğŸ’¾ Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸")
    
    col_upload, col_list = st.columns([1, 2])
    
    with col_upload:
        # Ğ¡ĞµĞºÑ†Ğ¸Ñ 1: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
        with st.expander("ğŸ“¤ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²", expanded=False):
            uploaded_files = st.file_uploader(
                "ĞŸĞµÑ€ĞµÑ‚Ğ°Ñ‰Ğ¸Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ ÑÑĞ´Ğ°", 
                type=["jsonl", "txt", "json"], 
                accept_multiple_files=True
            )
            
            if uploaded_files:
                if st.button("ğŸ“¥ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»Ñ‹"):
                    for uploaded_file in uploaded_files:
                        save_path = DATASET_DIR / uploaded_file.name
                        with open(save_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        st.toast(f"Ğ¤Ğ°Ğ¹Ğ» {uploaded_file.name} ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½!", icon="âœ…")
                    time.sleep(1)
                    st.rerun()

        # Ğ¡ĞµĞºÑ†Ğ¸Ñ 2: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ HuggingFace
        st.subheader("ğŸ¤— Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ HuggingFace")
        
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ session_state Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹
        if "hf_repo_id" not in st.session_state: st.session_state.hf_repo_id = "HuggingFaceFW/fineweb-2"
        if "hf_subset" not in st.session_state: st.session_state.hf_subset = "default"
        if "hf_split" not in st.session_state: st.session_state.hf_split = "train"
        if "hf_filename" not in st.session_state: st.session_state.hf_filename = ""
        
        # Ğ˜Ğ½Ğ¿ÑƒÑ‚Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ session_state
        st.text_input("Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ (ID)", key="hf_repo_id", help="ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: HuggingFaceFW/fineweb-2")
        st.text_input("Subset (ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³)", key="hf_subset", help="ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: rus_Cyrl (Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾) Ğ¸Ğ»Ğ¸ default. Ğ•ÑĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° 'Config not found', ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ² Ğ¾ÑˆĞ¸Ğ±ĞºĞµ.")
        st.text_input("Split", key="hf_split")
        
        with st.expander("ğŸ› ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ¸ Ğ›Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹", expanded=True):
            col1, col2 = st.columns(2)
            with col1:
                filter_lang = st.text_input("Ğ¯Ğ·Ñ‹Ğº (rus)", value="rus", key="filter_lang")
                filter_score = st.slider("ĞœĞ¸Ğ½. score", 0.0, 1.0, 0.0, key="filter_score")
            with col2:
                # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ - Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ² Ğ“Ğ‘
                limit_type = st.radio("Ğ¢Ğ¸Ğ¿ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°", ["Ğ“Ğ‘ (Ğ Ğ°Ğ·Ğ¼ĞµÑ€)", "Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ¸ (ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾)"], key="limit_type")
                
                limit_val = 0
                limit_bytes = 0
                
                if limit_type == "Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ¸ (ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾)":
                    limit_val = st.number_input("ĞšĞ¾Ğ»-Ğ²Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº", value=100000, step=10000, key="limit_val")
                else:
                    limit_gb = st.number_input("Ğ Ğ°Ğ·Ğ¼ĞµÑ€ (Ğ“Ğ‘)", value=2.0, step=0.5, min_value=0.1, key="limit_gb")
                    limit_bytes = int(limit_gb * 1024**3)
        
        st.text_input("Ğ˜Ğ¼Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°", key="hf_filename", placeholder="dataset.jsonl")
        
        # ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
        if st.button("ğŸš€ Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ", type="primary"):
            if not st.session_state.hf_repo_id:
                st.error("Ğ£ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ ID Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ!")
            else:
                download_hf_dataset(
                    st.session_state.hf_repo_id,
                    st.session_state.hf_subset,
                    st.session_state.hf_split,
                    limit_val,
                    limit_bytes,
                    filter_lang,
                    filter_score,
                    st.session_state.hf_filename
                )
    
    with col_list:
        st.subheader("Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹")
        
        datasets = []
        if DATASET_DIR.exists():
            # JSONL / JSON
            for f in list(DATASET_DIR.glob("*.jsonl")) + list(DATASET_DIR.glob("*.json")):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "JSONL/JSON",
                    "size_mb": size_mb,
                    "path": f
                })
            # TXT
            for f in DATASET_DIR.glob("*.txt"):
                size_mb = f.stat().st_size / (1024 * 1024)
                datasets.append({
                    "name": f.name,
                    "type": "Text",
                    "size_mb": size_mb,
                    "path": f
                })
        
        if not datasets:
            st.info("ĞĞµÑ‚ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ ÑĞ»ĞµĞ²Ğ°.")
        else:
            # ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº
            for ds in datasets:
                with st.expander(f"ğŸ“„ {ds['name']} ({ds['size_mb']:.1f} MB)"):
                    st.caption(f"Ğ¢Ğ¸Ğ¿: {ds['type']}")
                    
                    # Preview
                    try:
                        with open(ds['path'], "r", encoding="utf-8") as f:
                            head = [next(f).strip() for _ in range(5)]
                        st.markdown("**Preview (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5 ÑÑ‚Ñ€Ğ¾Ğº):**")
                        st.code("\n".join(head), language="json" if "JSON" in ds['type'] else "text")
                        
                        col_del, col_info = st.columns([1, 4])
                        with col_del:
                            if st.button("ğŸ—‘ï¸ Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ", key=f"del_{ds['name']}"):
                                ds['path'].unlink()
                                st.toast(f"Ğ¤Ğ°Ğ¹Ğ» {ds['name']} ÑƒĞ´Ğ°Ğ»Ñ‘Ğ½", icon="ğŸ—‘ï¸")
                                time.sleep(1)
                                st.rerun()
                    except Exception as e:
                        st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°: {e}")


def render_model_preview(config: dict, distributed_config: dict = None):
    """ĞŸÑ€ĞµĞ²ÑŒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°."""
    st.subheader("ğŸ“ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
    
    params = estimate_parameters(config["hidden_size"], config["num_layers"])
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Hidden Size", config["hidden_size"])
        st.metric("Layers", config["num_layers"])
    
    with col2:
        st.metric("Attention Heads", config["n_heads"])
        st.metric("Head Dim", config["hidden_size"] // config["n_heads"])
    
    with col3:
        st.metric("ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹", format_params(params))
        vram_est = params * 4 / 1e9  # fp32
        st.metric("VRAM (â‰ˆ fp32)", f"{vram_est:.1f} GB")
    
    # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹
    st.markdown(f"""
<div class="model-ascii">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HomeForCausalLM             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Embedding: 50257 â†’ {config['hidden_size']:4d}           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ HomeBlock Ã— {config['num_layers']:2d}              â”‚    â”‚
â”‚  â”‚  â€¢ RMSNorm â†’ Attention      â”‚    â”‚
â”‚  â”‚  â€¢ {config['n_heads']:2d} heads Ã— {config['hidden_size']//config['n_heads']:3d} dim      â”‚    â”‚
â”‚  â”‚  â€¢ RMSNorm â†’ FFN (SwiGLU)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  RMSNorm â†’ LM Head                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</div>
    """, unsafe_allow_html=True)
    
    # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğµ
    if distributed_config:
        st.subheader("âš¡ ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼")
        
        mode = distributed_config.get("distributed_mode", "default")
        mode_info = PARALLEL_TYPES.get(mode, PARALLEL_TYPES["default"])
        num_gpus = distributed_config.get("num_gpus", 0)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Ğ ĞµĞ¶Ğ¸Ğ¼", mode_info["name"])
        
        with col2:
            st.metric("Ğ¢Ğ¸Ğ¿", mode_info["type"])
        
        with col3:
            if num_gpus > 0:
                st.metric("GPU", f"{num_gpus} ÑˆÑ‚.")
            else:
                st.metric("Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾", "CPU")
        
        # Ğ¡Ñ…ĞµĞ¼Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°
        if mode == "default":
            parallel_diagram = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Single Device      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Full Model      â”‚  â”‚
â”‚  â”‚   Full Optimizer  â”‚  â”‚
â”‚  â”‚   Full Gradients  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
        elif mode == "multi_gpu":
            parallel_diagram = f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Parallel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  GPU 0  â”‚  â”‚  GPU 1  â”‚  ...  â”‚  GPU N  â”‚ â”‚
â”‚  â”‚ Model   â”‚  â”‚ Model   â”‚       â”‚ Model   â”‚ â”‚
â”‚  â”‚ (copy)  â”‚  â”‚ (copy)  â”‚       â”‚ (copy)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚            â”‚                 â”‚      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€ Sync Gradients â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ĞšĞ°Ğ¶Ğ´Ğ°Ñ GPU: Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ ĞºĞ¾Ğ¿Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ğ°ÑÑ‚ÑŒ Ğ±Ğ°Ñ‚Ñ‡Ğ°
"""
        elif mode == "fsdp":
            parallel_diagram = f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FSDP (Fully Sharded) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  GPU 0  â”‚  â”‚  GPU 1  â”‚  ...  â”‚  GPU N  â”‚ â”‚
â”‚  â”‚ Shard 0 â”‚  â”‚ Shard 1 â”‚       â”‚ Shard N â”‚ â”‚
â”‚  â”‚ Params  â”‚  â”‚ Params  â”‚       â”‚ Params  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚            â”‚                 â”‚      â”‚
â”‚       â””â”€â”€â”€ All-Gather for Forward â”€â”€â”˜       â”‚
â”‚       â””â”€â”€â”€ Reduce-Scatter Backward â”€â”˜       â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ GPU (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ VRAM)
"""
        elif "deepspeed" in mode:
            if "zero3" in mode:
                parallel_diagram = f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DeepSpeed ZeRO-3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  GPU 0  â”‚  â”‚  GPU 1  â”‚  ...  â”‚  GPU N  â”‚ â”‚
â”‚  â”‚ Params  â”‚  â”‚ Params  â”‚       â”‚ Params  â”‚ â”‚
â”‚  â”‚  1/N    â”‚  â”‚  1/N    â”‚       â”‚  1/N    â”‚ â”‚
â”‚  â”‚ Optim   â”‚  â”‚ Optim   â”‚       â”‚ Optim   â”‚ â”‚
â”‚  â”‚  1/N    â”‚  â”‚  1/N    â”‚       â”‚  1/N    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                              â”‚
â”‚  {'+ CPU Offload (Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğ° CPU)' if 'offload' in mode else ''}          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Ğ’ÑÑ‘ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ VRAM
"""
            else:
                parallel_diagram = f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DeepSpeed ZeRO-2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  GPU 0  â”‚  â”‚  GPU 1  â”‚  ...  â”‚  GPU N  â”‚ â”‚
â”‚  â”‚ Full    â”‚  â”‚ Full    â”‚       â”‚ Full    â”‚ â”‚
â”‚  â”‚ Model   â”‚  â”‚ Model   â”‚       â”‚ Model   â”‚ â”‚
â”‚  â”‚ Optim/N â”‚  â”‚ Optim/N â”‚       â”‚ Optim/N â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹
"""
        else:
            parallel_diagram = ""
        
        if parallel_diagram:
            st.markdown(f"""
<div class="model-ascii">
{parallel_diagram}
</div>
            """, unsafe_allow_html=True)


# ============================================================================
# Main App
# ============================================================================

def main():
    render_header()
    
    # Sidebar configs
    model_config = render_model_config()
    training_config = render_training_config()
    distributed_config = render_distributed_config()
    dataset_config = render_dataset_config()
    output_config = render_output_config()
    
    # Merge configs
    full_config = {**model_config, **training_config, **dataset_config, **output_config}
    full_config["distributed_mode"] = distributed_config["distributed_mode"]
    full_config["num_gpus"] = distributed_config["num_gpus"]
    full_config["config_file"] = distributed_config["config_file"]
    
    # Main content
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº", "ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³", "ğŸ’¬ Ğ§Ğ°Ñ‚", "ğŸ“œ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ", "ğŸ’¾ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ"])
    
    with tab1:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            render_model_preview(model_config, distributed_config)
            
            st.subheader("ğŸ“‹ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ")
            st.json(full_config)
        
        with col2:
            st.subheader("ğŸ® Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ")
            
            if st.session_state.training_active:
                if st.button("â¹ï¸ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ", type="primary"):
                    with st.spinner("ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ..."):
                        stopped = stop_training()
                    if stopped:
                        st.success("âœ… Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°")
                    else:
                        st.warning("âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ (Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°)")
                    time.sleep(1)
                    st.rerun()
            else:
                if st.button("â–¶ï¸ ĞĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ", type="primary"):
                    with st.spinner("Ğ—Ğ°Ğ¿ÑƒÑĞº..."):
                        run_id, process = start_training(full_config)
                        st.session_state.current_run_id = run_id
                        st.session_state.training_process = process
                        st.session_state.training_active = True
                        
                        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ run Ğ´Ğ»Ñ persistence
                        save_active_run(run_id, full_config)
                        
                        st.success(f"Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ°! Run ID: {run_id}")
                        time.sleep(1)
                        st.rerun()
    
    with tab2:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fragment Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        live_metrics_fragment()
    
    with tab3:
        st.header("ğŸ“œ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ²")
        st.markdown("---")
        
        runs = sorted(RUNS_DIR.glob("*"), reverse=True)
        
        if runs:
            for run_dir in runs[:10]:  # Last 10 runs
                run_id = run_dir.name
                metrics = load_metrics(run_id)
                
                if metrics:
                    status = metrics.get("status", "unknown")
                    status_emoji = {"training": "ğŸŸ¢", "completed": "âœ…", "error": "âŒ", "stopped": "â¹ï¸"}.get(status, "â³")
                    
                    with st.expander(f"{status_emoji} {run_id}"):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Steps", metrics.get("current_step", 0))
                        with col2:
                            st.metric("Final Loss", f"{metrics.get('current_loss', 0):.4f}")
                        with col3:
                            st.metric("Status", status)
                        
                        # Ğ§ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
                        checkpoints = metrics.get("checkpoints", [])
                        if checkpoints:
                            st.markdown("**ğŸ“¦ Ğ§ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹:**")
                            for ckpt in checkpoints[-5:]:  # ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 5
                                st.caption(f"Step {ckpt['step']}: `{ckpt['path']}`")
                        
                        # ĞšĞ½Ğ¾Ğ¿ĞºĞ¸
                        btn_col1, btn_col2 = st.columns(2)
                        with btn_col1:
                            if st.button(f"ğŸ“Š ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸", key=f"metrics_{run_id}"):
                                st.session_state.current_run_id = run_id
                                st.toast(f"âœ… Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½ run: {run_id}. ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³", icon="ğŸ“Š")
                        with btn_col2:
                            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ°
                            config_path = run_dir / "config.json"
                            if config_path.exists():
                                try:
                                    with open(config_path) as f:
                                        run_config = json.load(f)
                                    model_dir = PROJECT_ROOT / run_config.get("output_dir", "")
                                    final_model = model_dir / "final_model"
                                    if final_model.exists():
                                        if st.button("ğŸ’¬ Ğ§Ğ°Ñ‚", key=f"chat_run_{run_id}"):
                                            st.session_state.selected_chat_model = str(final_model)
                                            st.toast("âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ°! ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ ğŸ’¬ Ğ§Ğ°Ñ‚", icon="ğŸ’¬")
                                except:
                                    pass
                        
                        # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾
                        if st.session_state.current_run_id == run_id:
                            st.info("ğŸ‘† ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ **ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³**")
        else:
            st.info("ĞĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ²")
            
    with tab5:
        render_data_manager()
        
        # ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ° Ğ¿Ñ€Ğ¾ Ñ‡Ğ°Ñ‚
        st.markdown("---")
        st.info("ğŸ’¡ Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ¿ĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ **ğŸ’¬ Ğ§Ğ°Ñ‚** (Ğ² Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹)")
    
    with tab4:
        st.header("ğŸ’¬ Ğ§Ğ°Ñ‚ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ")
        st.markdown("---")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
        available_models = get_available_models()
        
        if available_models:
            # Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            col1, col2 = st.columns([3, 1])
            
            with col1:
                model_options = [m["name"] for m in available_models]
                
                # Ğ•ÑĞ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ° Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ - Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ ĞµÑ‘ Ğ¸Ğ½Ğ´ĞµĞºÑ
                default_idx = 0
                if st.session_state.selected_chat_model:
                    for i, m in enumerate(available_models):
                        if m["path"] == st.session_state.selected_chat_model:
                            default_idx = i
                            break
                
                selected_model_name = st.selectbox(
                    "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚",
                    options=model_options,
                    index=default_idx,
                    help="Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ°"
                )
                selected_model = next(m for m in available_models if m["name"] == selected_model_name)
                
                # Ğ¡Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ selected_chat_model Ğ¿Ğ¾ÑĞ»Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
                if st.session_state.selected_chat_model:
                    st.session_state.selected_chat_model = None
            
            with col2:
                model_type = selected_model["type"]
                if model_type == "final":
                    st.success("âœ… Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
                else:
                    st.info("ğŸ“¦ Ğ§ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚")
            
            st.caption(f"ĞŸÑƒÑ‚ÑŒ: `{selected_model['path']}`")
            
            # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
            with st.expander("âš™ï¸ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸"):
                gen_col1, gen_col2, gen_col3 = st.columns(3)
                with gen_col1:
                    max_tokens = st.slider("Max Tokens", 10, 500, 128)
                with gen_col2:
                    temperature = st.slider("Temperature", 0.1, 2.0, 0.8, 0.1)
                with gen_col3:
                    top_p = st.slider("Top-p", 0.1, 1.0, 0.9, 0.05)
            
            # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ°Ñ‚Ğ°
            if "chat_model" not in st.session_state:
                st.session_state.chat_model = None
                st.session_state.chat_tokenizer = None
                st.session_state.chat_model_path = None
            
            if "messages" not in st.session_state:
                st.session_state.messages = []
            
            # ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            if st.session_state.chat_model_path != selected_model["path"]:
                if st.button("ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ", type="primary"):
                    with st.spinner("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ..."):
                        try:
                            from transformers import AutoTokenizer
                            from homellm.models.home_model import HomeForCausalLM, HomeConfig
                            from safetensors.torch import load_file
                            
                            model_path = Path(selected_model["path"])
                            device = "cuda" if torch.cuda.is_available() else "cpu"
                            
                            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°
                            config_json = model_path / "config.json"
                            model_safetensors = model_path / "model.safetensors"
                            tokenizer_json = model_path / "tokenizer.json"
                            tokenizer_config = model_path / "tokenizer_config.json"
                            
                            # HuggingFace Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ = ĞµÑÑ‚ÑŒ tokenizer Ñ„Ğ°Ğ¹Ğ»Ñ‹
                            is_hf_format = tokenizer_json.exists() or tokenizer_config.exists()
                            
                            if is_hf_format and config_json.exists():
                                # HuggingFace Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ (final_model Ñ tokenizer)
                                st.info("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ HuggingFace Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ...")
                                st.session_state.chat_tokenizer = AutoTokenizer.from_pretrained(
                                    str(model_path), 
                                    trust_remote_code=True
                                )
                                st.session_state.chat_model = HomeForCausalLM.from_pretrained(
                                    str(model_path)
                                ).to(device)
                            elif model_safetensors.exists():
                                # Accelerate checkpoint Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
                                st.info("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Accelerate Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚...")
                                
                                # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ GPT-2 (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ)
                                st.session_state.chat_tokenizer = AutoTokenizer.from_pretrained("gpt2")
                                if st.session_state.chat_tokenizer.pad_token is None:
                                    st.session_state.chat_tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
                                
                                # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°
                                if config_json.exists():
                                    config = HomeConfig.from_pretrained(str(model_path))
                                    st.info(f"ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½: hidden_size={config.hidden_size}, layers={config.num_hidden_layers}")
                                else:
                                    # Ğ˜Ñ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ² Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (run config)
                                    run_config_path = model_path.parent / "run_config.json"
                                    if run_config_path.exists():
                                        import json as json_module
                                        with open(run_config_path) as f:
                                            run_cfg = json_module.load(f)
                                        config = HomeConfig(
                                            vocab_size=len(st.session_state.chat_tokenizer),
                                            hidden_size=run_cfg.get("hidden_size", 512),
                                            num_hidden_layers=run_cfg.get("num_layers", 8),
                                            num_attention_heads=run_cfg.get("n_heads", 8),
                                            max_position_embeddings=run_cfg.get("seq_len", 512),
                                        )
                                        st.info(f"ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¸Ğ· run_config: hidden_size={config.hidden_size}")
                                    else:
                                        st.warning("âš ï¸ config.json Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹")
                                        config = HomeConfig(
                                            vocab_size=len(st.session_state.chat_tokenizer),
                                            hidden_size=512,
                                            num_hidden_layers=8,
                                            num_attention_heads=8,
                                            max_position_embeddings=512,
                                        )
                                
                                st.session_state.chat_model = HomeForCausalLM(config)
                                
                                # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ²ĞµÑĞ°
                                state_dict = load_file(str(model_safetensors))
                                st.session_state.chat_model.load_state_dict(state_dict)
                                st.session_state.chat_model = st.session_state.chat_model.to(device)
                            else:
                                raise ValueError(f"ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ config.json Ğ¸Ğ»Ğ¸ model.safetensors Ğ² {model_path}")
                            
                            st.session_state.chat_model.eval()
                            st.session_state.chat_model_path = str(model_path)
                            st.session_state.messages = []
                            st.success("âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°!")
                            st.rerun()
                        except Exception as e:
                            import traceback
                            st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
                            st.code(traceback.format_exc())
            else:
                st.success(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°: {selected_model_name}")
                
                # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ‡Ğ°Ñ‚Ğ°
                for message in st.session_state.messages:
                    with st.chat_message(message["role"]):
                        st.write(message["content"])
                
                # Ğ’Ğ²Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
                if prompt := st.chat_input("Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ..."):
                    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    
                    with st.chat_message("user"):
                        st.write(prompt)
                    
                    # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
                    with st.chat_message("assistant"):
                        with st.spinner("Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ..."):
                            try:
                                tokenizer = st.session_state.chat_tokenizer
                                model = st.session_state.chat_model
                                device = next(model.parameters()).device
                                
                                inputs = tokenizer(prompt, return_tensors="pt").to(device)
                                
                                with torch.no_grad():
                                    outputs = model.generate(
                                        **inputs,
                                        max_new_tokens=max_tokens,
                                        temperature=temperature,
                                        top_p=top_p,
                                        do_sample=True,
                                        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
                                        use_cache=False,  # ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ KV-cache Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
                                    )
                                
                                response = tokenizer.decode(
                                    outputs[0][inputs["input_ids"].shape[1]:], 
                                    skip_special_tokens=True
                                )
                                
                                st.write(response)
                                st.session_state.messages.append({"role": "assistant", "content": response})
                            except Exception as e:
                                st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: {e}")
                
                # ĞšĞ½Ğ¾Ğ¿ĞºĞ° Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ñ‡Ğ°Ñ‚Ğ°
                if st.session_state.messages:
                    if st.button("ğŸ—‘ï¸ ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ñ‡Ğ°Ñ‚"):
                        st.session_state.messages = []
                        st.rerun()
        else:
            st.info("ĞĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞµ 'Ğ—Ğ°Ğ¿ÑƒÑĞº'!")
            
            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ³Ğ´Ğµ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            st.markdown("""
            **ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸:**
            - `out/*/final_model/` â€” Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            - `out/*/checkpoint_*/` â€” Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹
            """)


if __name__ == "__main__":
    main()

