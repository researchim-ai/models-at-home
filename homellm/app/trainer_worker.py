"""
–§–æ–Ω–æ–≤—ã–π worker –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏.
–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON —Ñ–∞–π–ª –¥–ª—è —á—Ç–µ–Ω–∏—è Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º.
"""
from __future__ import annotations

import argparse
import json
import logging
import math
import os
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

import torch
from torch.utils.data import IterableDataset, DataLoader
from accelerate import Accelerator
from accelerate.utils import DataLoaderConfiguration
from transformers import (
    AutoTokenizer,
    get_cosine_schedule_with_warmup,
    get_scheduler,
    DataCollatorForLanguageModeling,
)

from homellm.models.adapters import resolve_adapter
from homellm.training.pretrain import StreamingTextDataset
from homellm.training.sft import SFTDataset

logger = logging.getLogger(__name__)


def get_gpu_stats():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É GPU —á–µ—Ä–µ–∑ nvidia-smi (—Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–∏ DDP).
    
    –í–ê–ñ–ù–û: –§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ —Ä–µ–º–∞–ø–∏—Ç GPU –ø–æ CUDA_VISIBLE_DEVICES, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å
    —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ GPU —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ (0..N-1).
    """
    gpu_stats = []
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–∏–¥–∏–º—ã—Ö GPU –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    visible = os.environ.get("CUDA_VISIBLE_DEVICES")
    visible_ids = None
    if visible:
        try:
            visible_ids = [int(x.strip()) for x in visible.split(",") if x.strip() != ""]
        except Exception:
            visible_ids = None
    
    try:
        import subprocess
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ—Ö GPU —á–µ—Ä–µ–∑ nvidia-smi
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', 
             '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=2
        )
        
        if result.returncode == 0:
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 4:
                        gpu_id = int(parts[0])
                        memory_used = float(parts[1]) / 1024  # MiB -> GiB
                        memory_total = float(parts[2]) / 1024
                        utilization = int(parts[3]) if parts[3].isdigit() else None
                        
                        gpu_stats.append({
                            "id": gpu_id,
                            "memory_used_gb": round(memory_used, 2),
                            "memory_total_gb": round(memory_total, 2),
                            "memory_percent": round(memory_used / memory_total * 100, 1) if memory_total > 0 else 0,
                            "utilization": utilization,
                        })
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ä–µ–º–∞–ø–∏–º –ø–æ CUDA_VISIBLE_DEVICES
            if visible_ids is not None:
                # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∏–¥–∏–º—ã–µ GPU
                gpu_stats = [g for g in gpu_stats if g["id"] in visible_ids]
                # –†–µ–º–∞–ø–∏–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ ID -> –ª–æ–≥–∏—á–µ—Å–∫–∏–µ (0..N-1)
                remap = {phys: i for i, phys in enumerate(visible_ids)}
                for g in gpu_stats:
                    g["id"] = remap.get(g["id"], g["id"])
    except Exception:
        # Fallback –Ω–∞ torch.cuda –µ—Å–ª–∏ nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                try:
                    memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)
                    memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    
                    gpu_stats.append({
                        "id": i,
                        "memory_used_gb": round(memory_allocated, 2),
                        "memory_total_gb": round(memory_total, 2),
                        "memory_percent": round(memory_allocated / memory_total * 100, 1),
                        "utilization": None,
                    })
                except:
                    pass
    
    return gpu_stats


class MetricsLogger:
    """–õ–æ–≥–≥–µ—Ä –º–µ—Ç—Ä–∏–∫ –≤ JSON —Ñ–∞–π–ª –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.
    
    –í–ê–ñ–ù–û: –í Multi-GPU —Ä–µ–∂–∏–º–µ –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ main process (rank 0),
    —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≥–æ–Ω–æ–∫ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª.
    """
    
    def __init__(self, log_path: Path, enabled: bool = True):
        self.log_path = log_path
        self.enabled = enabled
        self.start_timestamp = time.time()
        self.metrics = {
            "status": "initializing",
            "start_time": datetime.now().isoformat(),
            "current_step": 0,
            "total_steps": 0,
            "epoch": 0,
            "loss_history": [],
            "lr_history": [],
            "steps_history": [],
            "current_loss": 0.0,
            "current_lr": 0.0,
            "samples_per_second": 0.0,
            "eta_seconds": 0,
            "error": None,
            "checkpoints": [],
            "gpu_stats": [],
            "current_val_loss": None,
            "val_loss_history": [],
            "val_steps_history": [],
        }
        self._save()
    
    def _save(self):
        """–ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –≥–æ–Ω–æ–∫."""
        if not self.enabled:
            return
        
        # –°–æ–∑–¥–∞—ë–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        self.log_path.parent.mkdir(parents=True, exist_ok=True)
        
        # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        tmp_path = self.log_path.with_suffix(".tmp")
        try:
            with open(tmp_path, "w") as f:
                json.dump(self.metrics, f, indent=2)
            os.replace(tmp_path, self.log_path)
        except Exception as e:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å, –ø—ã—Ç–∞–µ–º—Å—è —É–¥–∞–ª–∏—Ç—å tmp —Ñ–∞–π–ª
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except:
                pass
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É, –Ω–æ –Ω–µ –ø–∞–¥–∞–µ–º
            logger.warning(f"Failed to save metrics: {e}")
    
    def update(self, **kwargs):
        if not self.enabled:
            return
        self.metrics.update(kwargs)
        self._save()
    
    def log_step(self, step: int, loss: float, lr: float, samples_per_sec: float = 0, step_time: float = 0):
        if not self.enabled:
            return
        
        self.metrics["current_step"] = step
        self.metrics["current_loss"] = loss
        self.metrics["current_lr"] = lr
        self.metrics["samples_per_second"] = samples_per_sec
        self.metrics["loss_history"].append(loss)
        self.metrics["lr_history"].append(lr)
        self.metrics["steps_history"].append(step)
        
        # Elapsed
        self.metrics["elapsed_seconds"] = time.time() - self.start_timestamp
        
        # GPU stats
        self.metrics["gpu_stats"] = get_gpu_stats()
        
        # ETA –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏ —à–∞–≥–∞
        if step > 0 and step_time > 0:
            remaining_steps = max(0, self.metrics["total_steps"] - step)
            self.metrics["eta_seconds"] = int(remaining_steps * step_time)
        
        self._save()
    
    def log_checkpoint(self, path: str, loss: float = None):
        if not self.enabled:
            return
        
        checkpoint_data = {
            "path": path,
            "step": self.metrics["current_step"],
            "time": datetime.now().isoformat()
        }
        # –î–æ–±–∞–≤–ª—è–µ–º loss –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
        if loss is not None:
            checkpoint_data["loss"] = float(loss)
        elif "current_loss" in self.metrics:
            checkpoint_data["loss"] = float(self.metrics["current_loss"])
        
        self.metrics["checkpoints"].append(checkpoint_data)
        self._save()
    
    def log_eval(self, step: int, val_loss: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
        if not self.enabled:
            return
        
        self.metrics["current_val_loss"] = val_loss
        self.metrics["val_loss_history"].append(val_loss)
        self.metrics["val_steps_history"].append(step)
        self._save()


def run_training(config: Dict[str, Any], metrics_path: Path):
    """–ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å –∑–∞–ø–∏—Å—å—é –º–µ—Ç—Ä–∏–∫."""
    
    # Mixed precision
    mixed_precision = config.get("mixed_precision", "no")
    stage = config.get("stage", "pretrain") # pretrain | continual_pretrain | sft
    fp16_pure = bool(config.get("fp16_pure", False))
    use_flash_attention = bool(config.get("use_flash_attention", True))
    
    # –í–ê–ñ–ù–û: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ accelerate/deepspeed config-–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç `gradient_accumulation_steps: auto`,
    # –Ω–æ Accelerator –æ–∂–∏–¥–∞–µ—Ç int –∏ —É–ø–∞–¥—ë—Ç –Ω–∞ int("auto"). –í –Ω–∞—à–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã ‚Äî config.json –∏–∑ UI.
    # –ü–æ—ç—Ç–æ–º—É –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º env –¥–ª—è accelerate.
    raw_ga = config.get("gradient_accumulation", 1)
    try:
        ga_steps = int(raw_ga)
    except Exception:
        if isinstance(raw_ga, str) and raw_ga.strip().lower() == "auto":
            ga_steps = 1
            logger.warning("gradient_accumulation='auto' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ runtime; –∏—Å–ø–æ–ª—å–∑—É—é 1. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ UI.")
        else:
            raise
    config["gradient_accumulation"] = ga_steps
    os.environ["ACCELERATE_GRADIENT_ACCUMULATION_STEPS"] = str(ga_steps)

    # "Pure fp16" (–≤–µ—Å–∞ fp16 –±–µ–∑ GradScaler) ‚Äî –¥–ª—è accelerate –Ω—É–∂–Ω–æ mixed_precision='no'
    # –∏–Ω–∞—á–µ –æ–Ω –≤–∫–ª—é—á–∏—Ç GradScaler –∏ —É–ø–∞–¥—ë—Ç –ø—Ä–∏ fp16 –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö.
    if str(mixed_precision).lower() == "fp16" and fp16_pure:
        mixed_precision = "no"
        logger.info("üß™ FP16 Pure —Ä–µ–∂–∏–º: Accelerator(mixed_precision='no') (–±–µ–∑ GradScaler)")
    
    # –í–ê–ñ–ù–û: –£–±–∏—Ä–∞–µ–º dispatch_batches=True, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å NoneType –ø—Ä–∏ broadcast
    # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ StreamingTextDataset (shard=True)
    dataloader_config = DataLoaderConfiguration(
        dispatch_batches=None,
        even_batches=False, 
    )
    
    accelerator = Accelerator(
        gradient_accumulation_steps=ga_steps,
        mixed_precision=mixed_precision,
        dataloader_config=dataloader_config,
    )

    # DeepSpeed: –Ω—É–∂–Ω–æ —è–≤–Ω–æ –∑–∞–¥–∞—Ç—å train_micro_batch_size_per_gpu
    # –µ—Å–ª–∏ –º—ã –Ω–µ –ø–µ—Ä–µ–¥–∞—ë–º DataLoader –≤ accelerator.prepare() (dataset-level sharding).
    # –ë–µ–∑ —ç—Ç–æ–≥–æ DeepSpeed –ø–∞–¥–∞–µ—Ç —Å "requires you to pass at least one dataloader with batch_size".
    if accelerator.state.deepspeed_plugin is not None:
        batch_size = int(config.get("batch_size", 1))
        ds_cfg = accelerator.state.deepspeed_plugin.deepspeed_config
        current_mbs = ds_cfg.get("train_micro_batch_size_per_gpu")
        if current_mbs in (None, "auto"):
            ds_cfg["train_micro_batch_size_per_gpu"] = batch_size
            logger.info(f"DeepSpeed: set train_micro_batch_size_per_gpu={batch_size}")
        # –¢–∞–∫–∂–µ –∑–∞–¥–∞–¥–∏–º train_batch_size –µ—Å–ª–∏ auto
        current_tbs = ds_cfg.get("train_batch_size")
        if current_tbs in (None, "auto"):
            # train_batch_size = micro_batch * grad_accum * world_size
            world_size = accelerator.num_processes
            ds_cfg["train_batch_size"] = batch_size * ga_steps * world_size
            logger.info(f"DeepSpeed: set train_batch_size={ds_cfg['train_batch_size']}")

    # HomeModel FlashAttention: –∏—Å–ø–æ–ª—å–∑—É–µ–º PyTorch SDPA (scaled_dot_product_attention).
    # –ß—Ç–æ–±—ã —Ä–µ–∞–ª—å–Ω–æ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å flash kernels, –≤–∫–ª—é—á–∞–µ–º CUDA SDPA backends (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã).
    if torch.cuda.is_available():
        try:
            if use_flash_attention:
                torch.backends.cuda.enable_flash_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(True)
                torch.backends.cuda.enable_math_sdp(True)
            else:
                # –Ø–≤–Ω–æ –∑–∞–ø—Ä–µ—â–∞–µ–º flash/mem_efficient kernels, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ math (eager/–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π)
                torch.backends.cuda.enable_flash_sdp(False)
                torch.backends.cuda.enable_mem_efficient_sdp(False)
                torch.backends.cuda.enable_math_sdp(True)
            logger.info(
                "SDPA kernels: flash=%s mem_efficient=%s math=%s (use_flash_attention=%s)",
                getattr(torch.backends.cuda, "flash_sdp_enabled", lambda: "N/A")(),
                getattr(torch.backends.cuda, "mem_efficient_sdp_enabled", lambda: "N/A")(),
                getattr(torch.backends.cuda, "math_sdp_enabled", lambda: "N/A")(),
                use_flash_attention,
            )
        except Exception as e:
            logger.warning(f"Could not configure CUDA SDPA kernels: {e}")
    
    # –í–ê–ñ–ù–û: MetricsLogger —Ç–æ–ª—å–∫–æ –Ω–∞ main process –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –≥–æ–Ω–æ–∫ –ø—Ä–∏ Multi-GPU
    metrics = MetricsLogger(metrics_path, enabled=accelerator.is_main_process)
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é
        adapter = resolve_adapter(config)
        logger.info(f"Using adapter: {adapter.__class__.__name__}")
        
        metrics.update(status="loading_tokenizer", stage=stage)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä
        tokenizer_path = config.get("tokenizer_path")
        if not tokenizer_path and stage in ("sft", "continual_pretrain") and config.get("base_model_path"):
            # –î–ª—è SFT/continual –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            tokenizer_path = config["base_model_path"]
        elif not tokenizer_path:
            # –î–ª—è pretrain –∏—Å–ø–æ–ª—å–∑—É–µ–º model_id –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω, –∏–Ω–∞—á–µ gpt2
            tokenizer_path = config.get("model_id", "gpt2")
        
        tokenizer = adapter.load_tokenizer(tokenizer_path, trust_remote_code=True)
        tokenizer = adapter.prepare_tokenizer(tokenizer)  # pad_token = eos_token
        
        metrics.update(status=f"loading_dataset ({stage})")

        # ---------------------------
        # Sharding mode
        # ---------------------------
        # auto: –¥–ª—è IterableDataset –≤—ã–±–∏—Ä–∞–µ–º dataset-level —à–∞—Ä–¥–∏–Ω–≥ (—Å—Ç—Ä–æ–≥–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ + —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å strict resume)
        # accelerate: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç accelerator.prepare(DataLoader)
        # dataset: —à–∞—Ä–¥–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç (shard=True) –∏ DataLoader –ù–ï –∑–∞–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ accelerate.prepare
        sharding_mode = str(config.get("sharding_mode", "auto")).lower().strip()
        if sharding_mode not in ("auto", "dataset", "accelerate"):
            raise ValueError(f"Invalid sharding_mode={sharding_mode}. Expected one of: auto|dataset|accelerate")
        
        # –í–ê–ñ–ù–û: –í—ã—á–∏—Å–ª—è–µ–º val_ratio –î–û —Å–æ–∑–¥–∞–Ω–∏—è train_dataset, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å data leakage
        val_ratio = float(config.get("val_ratio", 0.0))
        eval_every = int(config.get("eval_every", 0) or 0)
        eval_batches = int(config.get("eval_batches", 20) or 20)
        
        # –î–µ—Ä–∂–∏–º holdout —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º eval
        holdout_ratio = val_ratio if (val_ratio > 0.0 and eval_every > 0) else 0.0
        
        # Dataset Selection based on Stage
        # –í–ê–ñ–ù–û: –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–ª–∞–µ—Ç accelerate.prepare() —á–µ—Ä–µ–∑ DataLoaderShard/Dispatcher
        # –î–∞—Ç–∞—Å–µ—Ç —Ç–æ–ª—å–∫–æ –¥–µ–ª–∞–µ—Ç train/val split, –Ω–æ –ù–ï —à–∞—Ä–¥–∏—Ä—É–µ—Ç –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (shard=False)
        if stage == "sft":
            # SFTDataset ‚Äî IterableDataset, –ø–æ—ç—Ç–æ–º—É auto -> dataset-level
            effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
            ds_shard = (effective_shard_mode == "dataset")
            train_dataset = SFTDataset(
                config["data_path"],
                tokenizer,
                seq_len=config["seq_len"],
                sft_columns=config.get("sft_columns"),
                sft_template=config.get("sft_template"),
                num_replicas=accelerator.num_processes,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                rank=accelerator.process_index,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                split="train",
                val_ratio=holdout_ratio,
                shard=ds_shard,
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–¥–ª—è SFT)
            try:
                if hasattr(train_dataset, 'get_sample_prompt'):
                    sample_prompt = train_dataset.get_sample_prompt(max_samples=20)
                    if sample_prompt:
                        metrics.update(sample_prompt=sample_prompt)
                        logger.info("Sample prompt saved to metrics")
            except Exception as e:
                logger.warning(f"Failed to get sample prompt: {e}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∞)
            if hasattr(train_dataset, 'num_replicas'):
                logger.info(f"Dataset initialized: num_replicas={train_dataset.num_replicas}, rank={train_dataset.rank}")
            
            # –î–ª—è SFT data collator –Ω–µ –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π masking, 
            # –Ω–æ –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –ø–∞–¥–¥–∏—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã –≤ –±–∞—Ç—á–µ.
            # SFTDataset —É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã, DataCollatorForLanguageModeling —Å mlm=False –ø–æ–¥–æ–π–¥–µ—Ç
            # –∏–ª–∏ default_data_collator –µ—Å–ª–∏ labels —É–∂–µ –µ—Å—Ç—å (–∞ –æ–Ω–∏ –µ—Å—Ç—å –≤ SFTDataset)
            from transformers import default_data_collator
            collate_fn = default_data_collator
        else:
            # Pretrain –∏–ª–∏ Continual Pretrain - –∏—Å–ø–æ–ª—å–∑—É–µ–º StreamingTextDataset
            # Strict resume –¥–ª—è StreamingTextDataset (.jsonl): –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å byte_offset/global_idx –∏–∑ checkpoint
            resume_state = None
            resume_from_checkpoint_cfg = config.get("resume_from_checkpoint")
            strict_resume = bool(config.get("strict_dataloader_resume", True))
            effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
            if strict_resume and effective_shard_mode != "dataset":
                # –ò–Ω–∞—á–µ –º—ã –Ω–µ –º–æ–∂–µ–º –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–≥—É—é –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–æ—Ç–æ–∫–∞ –ø–æ rank
                raise ValueError("strict_dataloader_resume=True requires sharding_mode='dataset' for streaming datasets")
            if resume_from_checkpoint_cfg and strict_resume:
                try:
                    p = Path(resume_from_checkpoint_cfg)
                    # per-rank state
                    rs_path = p / f"dataloader_state_rank{accelerator.process_index}.json"
                    if rs_path.exists():
                        with open(rs_path, "r", encoding="utf-8") as f:
                            resume_state = json.load(f)
                        logger.info(
                            f"Loaded dataloader resume state for rank {accelerator.process_index}: "
                            f"byte_offset={resume_state.get('byte_offset')} global_idx={resume_state.get('global_idx')}"
                        )
                except Exception as e:
                    logger.warning(f"Failed to load dataloader resume state: {e}")

            train_dataset = StreamingTextDataset(
                config["data_path"], 
                tokenizer, 
                seq_len=config["seq_len"],
                num_replicas=accelerator.num_processes,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                rank=accelerator.process_index,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                split="train",
                val_ratio=holdout_ratio,
                shard=(effective_shard_mode == "dataset"),
                resume_byte_offset=(resume_state.get("byte_offset", 0) if resume_state else 0),
                resume_global_idx=(resume_state.get("global_idx", 0) if resume_state else 0),
                strict_resume=strict_resume,
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–¥–ª—è pretrain/continual_pretrain)
            try:
                if hasattr(train_dataset, 'get_sample_prompt'):
                    sample_prompt = train_dataset.get_sample_prompt(max_samples=20)
                    if sample_prompt:
                        metrics.update(sample_prompt=sample_prompt)
                        logger.info("Sample prompt saved to metrics")
            except Exception as e:
                logger.warning(f"Failed to get sample prompt: {e}")
            
            # –í–ê–ñ–ù–û: –î–ª—è pretrain –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π collator, –∫–æ—Ç–æ—Ä—ã–π –º–∞—Å–∫–∏—Ä—É–µ—Ç –ø–æ attention_mask,
            # –∞ –Ω–µ –ø–æ pad_token_id. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ, –µ—Å–ª–∏ pad_token = eos_token (EOS –Ω–µ –¥–æ–ª–∂–µ–Ω –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å—Å—è)
            def causal_lm_collator(batch):
                """–ö–∞—Å—Ç–æ–º–Ω—ã–π collator –¥–ª—è pretrain, –º–∞—Å–∫–∏—Ä—É–µ—Ç labels –ø–æ attention_mask."""
                # –§–∏–ª—å—Ç—Ä—É–µ–º –±–∏—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã (–µ—Å–ª–∏ dataset –≤–µ—Ä–Ω—É–ª None)
                batch = [x for x in batch if x is not None]
                if not batch:
                     # –ü—É—Å—Ç–æ–π –±–∞—Ç—á –Ω–µ –¥–æ–ª–∂–µ–Ω –∫—Ä–∞—à–∏—Ç—å accelerate, –≤–µ—Ä–Ω–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏–ª–∏ –ø—É—Å—Ç–æ–π dict
                     # –ù–æ accelerate –º–æ–∂–µ—Ç –Ω–µ –ø–µ—Ä–µ–≤–∞—Ä–∏—Ç—å –ø—É—Å—Ç–æ–π dict –ø—Ä–∏ broadcast, –ø–æ—ç—Ç–æ–º—É –≤–µ—Ä–Ω–µ–º dummy batch
                     # —á—Ç–æ–±—ã –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —à–∞–≥
                     dummy = torch.zeros((1, 1), dtype=torch.long)
                     return {"input_ids": dummy, "attention_mask": dummy, "labels": dummy}

                input_ids = torch.stack([x["input_ids"] for x in batch])
                attention_mask = torch.stack([x["attention_mask"] for x in batch])
                labels = input_ids.clone()
                labels[attention_mask == 0] = -100  # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ padding, –Ω–µ EOS
                return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
            
            collate_fn = causal_lm_collator
        
        # –°–æ–∑–¥–∞–µ–º validation loader –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        val_loader = None
        
        if holdout_ratio > 0.0:
            # –í–ê–ñ–ù–û: –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–ª–∞–µ—Ç accelerate.prepare() —á–µ—Ä–µ–∑ DataLoaderShard/Dispatcher
            # –î–∞—Ç–∞—Å–µ—Ç —Ç–æ–ª—å–∫–æ –¥–µ–ª–∞–µ—Ç train/val split, –Ω–æ –ù–ï —à–∞—Ä–¥–∏—Ä—É–µ—Ç –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (shard=False)
            if stage == "sft":
                # SFTDataset —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≥–ª–æ–±–∞–ª—å–Ω–æ
                # reuse effective_shard_mode from above if set, otherwise compute here
                effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
                ds_shard = (effective_shard_mode == "dataset")
                val_dataset = SFTDataset(
                    config["data_path"],
                    tokenizer,
                    seq_len=config["seq_len"],
                    sft_columns=config.get("sft_columns"),
                    sft_template=config.get("sft_template"),
                    num_replicas=accelerator.num_processes,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    rank=accelerator.process_index,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    split="val",
                    val_ratio=holdout_ratio,
                    shard=ds_shard,
                )
                from transformers import default_data_collator
                val_collate = default_data_collator
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π StreamingTextDataset
                effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
                val_dataset = StreamingTextDataset(
                    config["data_path"],
                    tokenizer,
                    seq_len=config["seq_len"],
                    num_replicas=accelerator.num_processes,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    rank=accelerator.process_index,  # ‚úÖ –Ø–≤–Ω–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    split="val",
                    val_ratio=holdout_ratio,
                    shard=(effective_shard_mode == "dataset"),
                    strict_resume=False,  # val –Ω–µ —Ä–µ–∑—é–º–∏–º —Å—Ç—Ä–æ–≥–æ
                )
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –∫–∞—Å—Ç–æ–º–Ω—ã–π collator –¥–ª—è val, —á—Ç–æ –∏ –¥–ª—è train
                def causal_lm_collator(batch):
                    """–ö–∞—Å—Ç–æ–º–Ω—ã–π collator –¥–ª—è pretrain, –º–∞—Å–∫–∏—Ä—É–µ—Ç labels –ø–æ attention_mask."""
                    batch = [x for x in batch if x is not None]
                    if not batch:
                        dummy = torch.zeros((1, 1), dtype=torch.long)
                        return {"input_ids": dummy, "attention_mask": dummy, "labels": dummy}

                    input_ids = torch.stack([x["input_ids"] for x in batch])
                    attention_mask = torch.stack([x["attention_mask"] for x in batch])
                    labels = input_ids.clone()
                    labels[attention_mask == 0] = -100  # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ padding, –Ω–µ EOS
                    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
                
                val_collate = causal_lm_collator
            
            # –í–ê–ñ–ù–û: num_workers=0 –¥–ª—è val, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–∏ shard=False
            val_loader = DataLoader(
                val_dataset,
                batch_size=config["batch_size"],
                collate_fn=val_collate,
                num_workers=0,  # –ë–µ–∑ workers –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            )
            
        # –í–ê–ñ–ù–û: num_workers=0 –¥–ª—è IterableDataset, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        # drop_last=True –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ DDP (–≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –≤—ã–ø–æ–ª–Ω—è—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π)
        train_loader = DataLoader(
            train_dataset,
            batch_size=config["batch_size"],
            collate_fn=collate_fn,
            num_workers=0,  # IterableDataset + num_workers>0 = –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            drop_last=True,  # ‚úÖ –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è DDP: –∏–∑–±–µ–≥–∞–µ–º —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∞ –ø–æ —á–∏—Å–ª—É —à–∞–≥–æ–≤ –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
        )
        
        metrics.update(status="building_model")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä
        resume_from_checkpoint = None
        base_model_path = config.get("base_model_path")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ accelerate checkpoint (–¥–ª—è resume)
        # –í–ê–ñ–ù–û: pytorch_model.bin.index.json –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ —É –æ–±—ã—á–Ω—ã—Ö —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö HF-—Å–µ–π–≤–æ–≤,
        # –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ accelerator_state.json - —ç—Ç–æ —Ç–æ—á–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ accelerate checkpoint
        def is_accelerate_checkpoint(p: Path) -> bool:
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—É—Ç—å accelerate checkpoint'–æ–º."""
            return (p / "accelerator_state.json").exists()
        
        if base_model_path:
            base_path = Path(base_model_path)
            is_checkpoint = is_accelerate_checkpoint(base_path)
            
            if is_checkpoint and stage == "continual_pretrain":
                resume_from_checkpoint = str(base_path)
                logger.info(f"Continual pretraining: will resume from accelerate checkpoint {base_path}")
            elif is_checkpoint and stage != "continual_pretrain":
                logger.warning(
                    f"Found accelerate checkpoint at {base_path}, but stage is {stage}. "
                    f"Resume from checkpoint is only supported for continual_pretrain. "
                    f"Will load as regular model instead."
                )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä
        model, model_config = adapter.load_for_training(
            base_model_path=base_model_path,
            stage=stage,
            tokenizer=tokenizer,
            config=config,
            trust_remote_code=True,
        )
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (resize, LoRA, use_cache, etc.)
        model = adapter.prepare_for_training(model, tokenizer, config)

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: dtype –≤–µ—Å–æ–≤ –∏ –≤–∫–ª—é—á—ë–Ω –ª–∏ SDPA/flash path —É HomeModel
        try:
            first = next(model.parameters())
            logger.info(f"üîé Model weights dtype (first param): {first.dtype}")
        except Exception:
            pass
        try:
            flash_mods = [m for m in model.modules() if hasattr(m, "flash")]
            if flash_mods:
                enabled = sum(1 for m in flash_mods if bool(getattr(m, "flash", False)))
                logger.info(f"üîé SDPA/flash modules: {enabled}/{len(flash_mods)} enabled")
        except Exception:
            pass
        
        # –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        num_params = sum(p.numel() for p in model.parameters())
        metrics.update(num_parameters=num_params)
        
        if config.get("grad_checkpoint", False):
            model.gradient_checkpointing_enable()
        
        # Scheduler - –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω max_steps - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if config.get("max_steps"):
            max_train_steps = config["max_steps"]
        else:
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–ª–∏–Ω—É –¥–∞—Ç–∞—Å–µ—Ç–∞
            # –í–ê–ñ–ù–û: len(train_dataset) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ split
            # –ù–æ –ø—Ä–∏ DDP/FSDP/ZeRO accelerate.prepare() —à–∞—Ä–¥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
            # –ö–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å —É–≤–∏–¥–∏—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ dataset_len / num_processes –ø—Ä–∏–º–µ—Ä–æ–≤
            try:
                dataset_len = len(train_dataset)  # –ü–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ split
                # –£—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ—Ü–µ—Å—Å–∞–º (–¥–ª—è IterableDataset —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ)
                per_proc_len = math.ceil(dataset_len / accelerator.num_processes)
                
                # batch_size –∑–¥–µ—Å—å per-device, gradient_accumulation —É–∂–µ —É—á—Ç–µ–Ω
                steps_per_epoch = math.ceil(per_proc_len / config["batch_size"])
                num_update_steps_per_epoch = math.ceil(steps_per_epoch / config["gradient_accumulation"])
                max_train_steps = config["epochs"] * num_update_steps_per_epoch
            except (TypeError, AttributeError):
                # –î–ª—è streaming dataset –±–µ–∑ __len__ - –∏—Å–ø–æ–ª—å–∑—É–µ–º save_every * 10 –∫–∞–∫ –æ—Ü–µ–Ω–∫—É
                max_train_steps = config.get("save_every", 5000) * 2
        
        planned_total_steps = int(max_train_steps)
        metrics.update(
            total_steps=planned_total_steps,
            planned_total_steps=planned_total_steps,
            max_steps_estimated=config.get("max_steps") is None,
        )
        
        # –í–ê–ñ–ù–û: –î–ª—è LoRA/QLoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è QLoRA, –≥–¥–µ –±–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã –∏ –æ–≥—Ä–æ–º–Ω—ã–µ
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        if len(trainable_params) == 0:
            raise ValueError("No trainable parameters found in model. Check LoRA/QLoRA configuration.")
        
        logger.info(f"Optimizing {len(trainable_params)} trainable parameters "
                   f"(total: {sum(p.numel() for p in model.parameters())})")
        
        optimizer = torch.optim.AdamW(
            trainable_params,  # ‚úÖ –¢–æ–ª—å–∫–æ trainable –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è LoRA/QLoRA)
            lr=config["learning_rate"], 
            betas=(0.9, 0.95), 
            eps=1e-8,
            weight_decay=config.get("weight_decay", 0.1)
        )
        # LR scheduler
        # –í–ê–ñ–ù–û: –º—ã —à–∞–≥–∞–µ–º scheduler –Ω–∞ UPDATE-step (–∫–æ–≥–¥–∞ accelerator.sync_gradients=True).
        # –î–ª—è resume –∏–∑ —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ: —Ä–∞–Ω—å—à–µ scheduler –º–æ–≥ —à–∞–≥–∞—Ç—å –ø–æ micro-step,
        # –∏ —Ç–æ–≥–¥–∞ –Ω–∞ resume LR —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è ~0.
        lr_schedule = str(config.get("lr_schedule", "cosine")).strip().lower()
        min_lr_ratio = float(config.get("min_lr_ratio", 0.0) or 0.0)
        warmup_steps = int(config.get("warmup_steps", 0))
        total_steps_for_sched = int(max_train_steps)

        if lr_schedule in ("cosine", "cosine_with_warmup") and min_lr_ratio > 0:
            import math as _math
            from torch.optim.lr_scheduler import LambdaLR

            def lr_lambda(step: int) -> float:
                if warmup_steps > 0 and step < warmup_steps:
                    return float(step) / float(max(1, warmup_steps))
                if total_steps_for_sched <= warmup_steps:
                    return 1.0
                progress = float(step - warmup_steps) / float(max(1, total_steps_for_sched - warmup_steps))
                progress = min(max(progress, 0.0), 1.0)
                cosine = 0.5 * (1.0 + _math.cos(_math.pi * progress))
                return float(min_lr_ratio + (1.0 - min_lr_ratio) * cosine)

            lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)
        else:
            # get_scheduler –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: linear/cosine/constant/cosine_with_restarts/...
            # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: —Å—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ "cosine_with_warmup" –º–∞–ø–ø–∏–º –≤ "cosine".
            sched_name = "cosine" if lr_schedule == "cosine_with_warmup" else lr_schedule
            lr_scheduler = get_scheduler(
                name=sched_name,
                optimizer=optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps_for_sched,
            )

        metrics.update(
            scheduler=str(lr_schedule),
            warmup_steps=int(warmup_steps),
            min_lr_ratio=float(min_lr_ratio),
        )
        
        # IMPORTANT (Sharding contract):
        # –£ –Ω–∞—Å –î–í–ê –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–ø–æ—Å–æ–±–∞ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö:
        #  1) dataset-level (StreamingTextDataset/SFTDataset —Å shard=True, —è–≤–Ω—ã–π num_replicas/rank)
        #  2) accelerate-level (accelerator.prepare(DataLoader) -> DataLoaderShard/Dispatcher)
        #
        # –ù–ï–õ–¨–ó–Ø –≤–∫–ª—é—á–∞—Ç—å –æ–±–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ ‚Äî –∏–Ω–∞—á–µ –±—É–¥–µ—Ç "–¥–≤–æ–π–Ω–æ–π —à–∞—Ä–¥–∏–Ω–≥" –∏ –¥–∞–Ω–Ω—ã–µ/—à–∞–≥–∏ —Å—Ç–∞–Ω—É—Ç –º–µ–Ω—å—à–µ –≤ N —Ä–∞–∑.
        # –î–ª—è IterableDataset –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º dataset-level —à–∞—Ä–¥–∏–Ω–≥ –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã.
        is_streaming_sharded = isinstance(train_dataset, IterableDataset) and getattr(train_dataset, "shard", False) is True
        effective_shard_mode = "dataset" if is_streaming_sharded else "accelerate"
        if accelerator.is_main_process:
            metrics.update(
                sharding_mode=effective_shard_mode,
                sharding_mode_requested=sharding_mode,
                num_processes=int(accelerator.num_processes),
            )
        if is_streaming_sharded:
            if val_loader is not None:
                model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
            else:
                model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
            logger.info("Using dataset-level sharding for IterableDataset -> skipping accelerator.prepare() for DataLoader(s)")
        else:
            if val_loader is not None:
                model, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(
                    model, optimizer, train_loader, val_loader, lr_scheduler
                )
            else:
                model, optimizer, train_loader, lr_scheduler = accelerator.prepare(
                    model, optimizer, train_loader, lr_scheduler
                )
        
        # Resume –∏–∑ checkpoint (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞–¥–∏–π)
        starting_step = 0
        resume_batches_to_skip = 0
        
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –∞—Ä–≥—É–º–µ–Ω—Ç resume_from_checkpoint —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥ (–æ—Ç CLI)
        if config.get("resume_from_checkpoint"):
            resume_from_checkpoint = config["resume_from_checkpoint"]
        # –ò–ª–∏ –µ—Å–ª–∏ —ç—Ç–æ continual_pretrain –∏ –º—ã –Ω–∞—à–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –≤ base_model_path
        elif stage == "continual_pretrain" and base_model_path:
             if is_accelerate_checkpoint(Path(base_model_path)):
                 resume_from_checkpoint = base_model_path

        if resume_from_checkpoint:
            try:
                logger.info(f"Resuming training from checkpoint: {resume_from_checkpoint}")
                accelerator.load_state(resume_from_checkpoint)
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å global_step –∏–∑ checkpoint
                checkpoint_meta = Path(resume_from_checkpoint) / "checkpoint_metadata.json"
                if checkpoint_meta.exists():
                    with open(checkpoint_meta) as f:
                        meta = json.load(f)
                        starting_step = meta.get("global_step", 0)
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä checkpoint_step1000)
                    import re
                    match = re.search(r'step(\d+)', str(resume_from_checkpoint))
                    if match:
                        starting_step = int(match.group(1))
                
                logger.info(f"Resumed from step {starting_step}")
                metrics.update(status="resumed", resumed_from_step=starting_step)
                
                # –í–ê–ñ–ù–û: –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–∞
                #
                # - –î–ª—è map-style –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ accelerate.load_state() –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç sampler state, –∏ –¥–æ–ø. skip –ù–ï –Ω—É–∂–µ–Ω.
                # - –î–ª—è IterableDataset sampler state –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç => –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç "—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å" ‚Äî —Ä–µ–∞–ª—å–Ω–æ
                #   –ø—Ä–æ–≥–Ω–∞—Ç—å N –±–∞—Ç—á–µ–π –∏ –≤—ã–±—Ä–æ—Å–∏—Ç—å. –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (–∏ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –∑–∞–≤–∏—Å–∞–Ω–∏–µ),
                #   –ø–æ—ç—Ç–æ–º—É –¥–µ–ª–∞–µ–º —ç—Ç–æ —è–≤–Ω–æ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–ª–æ–≥–∞–º–∏ –≤ –ø–µ—Ä–≤–æ–º epoch.
                if starting_step > 0 and isinstance(train_dataset, IterableDataset):
                    resume_skip_enabled = bool(config.get("resume_skip_batches", True))
                    # –ï—Å–ª–∏ —É StreamingTextDataset –≤–∫–ª—é—á—ë–Ω strict_resume –∏ –∑–∞–≥—Ä—É–∂–µ–Ω byte_offset, skip –Ω–µ –Ω—É–∂–µ–Ω.
                    try:
                        if hasattr(train_dataset, "get_resume_state"):
                            rs = train_dataset.get_resume_state()
                            if rs.get("byte_offset", 0) and bool(config.get("strict_dataloader_resume", True)):
                                logger.info("Strict dataloader resume active (byte_offset present) -> skipping batches is NOT required.")
                                resume_skip_enabled = False
                    except Exception:
                        pass
                    if resume_skip_enabled:
                        resume_batches_to_skip = int(starting_step) * int(config["gradient_accumulation"])
                        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç, —á—Ç–æ–±—ã –Ω–µ "–≤–∏—Å–µ—Ç—å" —á–∞—Å–∞–º–∏ –ø—Ä–∏ –±–æ–ª—å—à–æ–º starting_step
                        max_skip = config.get("resume_skip_batches_max")
                        if max_skip is not None:
                            max_skip = int(max_skip)
                            if resume_batches_to_skip > max_skip:
                                logger.warning(
                                    f"Requested to skip {resume_batches_to_skip} batches, but resume_skip_batches_max={max_skip}. "
                                    f"Capping skip to {max_skip}. (May slightly desync dataloader position.)"
                                )
                                resume_batches_to_skip = max_skip
                        logger.info(f"Will skip {resume_batches_to_skip} batches (IterableDataset) to sync dataloader...")
                        metrics.update(status="skipping", skipping_batches=resume_batches_to_skip)
                    else:
                        logger.warning("resume_skip_batches=False: will NOT skip dataloader batches for IterableDataset (data order may differ after resume).")
                        resume_batches_to_skip = 0
                    
            except Exception as e:
                logger.error(f"Failed to resume from checkpoint {resume_from_checkpoint}: {e}")
                # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ø–í–ù–û –ø—Ä–æ—Å–∏–ª resume (—á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥) - –º—ã –¥–æ–ª–∂–Ω—ã –ø–∞–¥–∞—Ç—å, –∞ –Ω–µ –º–æ–ª—á–∞ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å
                # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: auto-resume –ø—Ä–∏ continual_pretrain (base_model_path) - —Ç—É—Ç –º–æ–∂–Ω–æ –≤–∞—Ä–Ω–∏–Ω–≥
                is_explicit_resume = config.get("resume_from_checkpoint") is not None
                
                if is_explicit_resume:
                    metrics.update(status="error", resume_error=str(e), error=f"Resume failed: {e}")
                    raise RuntimeError(f"Could not resume from checkpoint {resume_from_checkpoint}: {e}")
                else:
                    logger.warning("Continuing without resume (starting from step 0)")
                    metrics.update(status="resume_failed", resume_error=str(e))

        
        output_dir = Path(config["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º run_config.json –≤ –ø–∞–ø–∫—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏ resume
        if accelerator.is_main_process:
            run_config_path = output_dir / "run_config.json"
            if not run_config_path.exists():  # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ø—Ä–∏ resume
                with open(run_config_path, "w", encoding="utf-8") as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                logger.info(f"Saved run_config.json to {run_config_path}")
        
        metrics.update(status="training")
        
        # –í–ê–ñ–ù–û: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º global_step —Å starting_step –ø–æ—Å–ª–µ resume
        # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã LR scheduler –∏ checkpointing
        global_step = starting_step
        # –í–ê–ñ–ù–û: —Ä–µ—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º scheduler –∫ update-step –∏–Ω–¥–µ–∫—Å—É.
        # –≠—Ç–æ –ª–µ—á–∏—Ç —Å–∏—Ç—É–∞—Ü–∏—é, –∫–æ–≥–¥–∞ scheduler –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ –±—ã–ª "–ø—Ä–æ–∫—Ä—É—á–µ–Ω" –ø–æ micro-step –∏ LR —É–ª–µ—Ç–∞–µ—Ç –≤ ~0.
        if starting_step > 0 and bool(config.get("scheduler_resync_on_resume", True)):
            try:
                lr_scheduler.step(int(global_step))
                if accelerator.is_main_process:
                    metrics.update(resume_scheduler_resynced=True, resume_scheduler_step=int(global_step))
            except TypeError:
                # fallback –¥–ª—è scheduler-–æ–≤ –±–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ step(epoch)
                try:
                    lr_scheduler.last_epoch = int(global_step)
                    lr_scheduler.step()
                    if accelerator.is_main_process:
                        metrics.update(resume_scheduler_resynced=True, resume_scheduler_step=int(global_step))
                except Exception as e:
                    logger.warning(f"Failed to resync LR scheduler on resume: {e}")
                    if accelerator.is_main_process:
                        metrics.update(resume_scheduler_resynced=False, resume_scheduler_error=str(e))
            except Exception as e:
                logger.warning(f"Failed to resync LR scheduler on resume: {e}")
                if accelerator.is_main_process:
                    metrics.update(resume_scheduler_resynced=False, resume_scheduler_error=str(e))
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç eval_batches –∏–∑ –∑–∞–º—ã–∫–∞–Ω–∏—è)
        def evaluate(val_loader):
            """
            –í—ã–ø–æ–ª–Ω–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ val_loader.
            
            –í–ê–ñ–ù–û: val_loader –∑–∞—à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –ø–æ—ç—Ç–æ–º—É –∫–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å
            –≤–∏–¥–∏—Ç —Å–≤–æ—é —á–∞—Å—Ç—å validation –¥–∞–Ω–Ω—ã—Ö. reduce() –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É—Å—Ä–µ–¥–Ω—è–µ—Ç loss –ø–æ –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–∞–º.
            """
            model.eval()
            losses = []
            with torch.no_grad():
                for i, batch in enumerate(val_loader):
                    if i >= eval_batches:
                        break
                    # –ï—Å–ª–∏ DataLoader –Ω–µ –±—ã–ª –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω accelerate'–æ–º ‚Äî –≤—Ä—É—á–Ω—É—é –∫–ª–∞–¥—ë–º –±–∞—Ç—á –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                    if is_streaming_sharded:
                        batch = {k: (v.to(accelerator.device) if hasattr(v, "to") else v) for k, v in batch.items()}
                    with accelerator.autocast():
                        out = model(**batch)
                    loss = out.loss.detach()
                    # –£—Å—Ä–µ–¥–Ω—è–µ–º loss –ø–æ –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–∞–º (–∫–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å –≤–∏–¥–∏—Ç —Å–≤–æ—é —á–∞—Å—Ç—å val –¥–∞–Ω–Ω—ã—Ö)
                    loss = accelerator.reduce(loss, reduction="mean")
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ main process, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                    if accelerator.is_main_process:
                        losses.append(loss.item())
            model.train()
            if not losses:
                return None
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π loss (—É–∂–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –ø–æ –ø—Ä–æ—Ü–µ—Å—Å–∞–º)
            return sum(losses) / len(losses) if losses else None
        
        # –í–ê–ñ–ù–û: global_step —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤—ã—à–µ –∫–∞–∫ starting_step (–¥–ª—è resume)
        # –ï—Å–ª–∏ resume –Ω–µ –±—ã–ª–æ, starting_step = 0, –ø–æ—ç—Ç–æ–º—É global_step = 0
        start_time = time.time()
        last_heartbeat = time.time()
        heartbeat_every = float(config.get("metrics_heartbeat_seconds", 20.0))
        
        # Debug: –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–µ–π –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (–ø–µ—Ä–≤—ã–µ N —à–∞–≥–æ–≤)
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –í–ö–õ –≤ multi-GPU (—ç—Ç–æ –Ω–∞—à "safety check", —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —à–∞—Ä–¥–∏–Ω–≥).
        if "debug_check_duplicates" in config:
            debug_check_duplicates = bool(config.get("debug_check_duplicates"))
        else:
            debug_check_duplicates = bool(accelerator.num_processes > 1)
        debug_sample_ids = []  # –°–æ–±–∏—Ä–∞–µ–º sample_id –¥–ª—è –ø–µ—Ä–≤—ã—Ö —à–∞–≥–æ–≤
        debug_max_samples = 20  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–∏–º–µ—Ä–æ–≤
        if debug_check_duplicates and accelerator.is_main_process:
            metrics.update(debug_check_duplicates=True, debug_max_samples=int(debug_max_samples))
        
        # Loss tracking:
        # - micro_* –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ divergence –Ω–∞ –ö–ê–ñ–î–û–ú update-step
        # - log_* –¥–ª—è —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∞ (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ update-step-–∞–º –º–µ–∂–¥—É –ª–æ–≥–∞–º–∏)
        micro_loss_sum = 0.0
        micro_count = 0
        log_loss_sum = 0.0
        log_updates = 0
        update_start_time = time.time()

        training_complete = False
        stop_reason = None
        
        for epoch in range(config["epochs"]):
            if training_complete:
                break
                
            metrics.update(epoch=epoch + 1)
            model.train()

            # –ï—Å–ª–∏ resume –∏–∑ IterableDataset: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞—Ç—á–∏ –Ø–í–ù–û –≤ –Ω–∞—á–∞–ª–µ –ø–µ—Ä–≤–æ–≥–æ epoch
            epoch_loader = train_loader
            if epoch == 0 and resume_batches_to_skip > 0:
                it = iter(train_loader)
                to_skip = int(resume_batches_to_skip)
                log_every = int(config.get("resume_skip_log_every", 500))
                t0 = time.time()
                skipped = 0
                logger.info(f"Skipping {to_skip} batches now... (logging every {log_every})")
                while skipped < to_skip:
                    try:
                        next(it)
                    except StopIteration:
                        logger.warning(f"Reached end of iterable dataloader while skipping at {skipped}/{to_skip}. Continuing from start of next epoch.")
                        break
                    skipped += 1
                    if skipped % log_every == 0 or skipped == to_skip:
                        dt = max(1e-6, time.time() - t0)
                        bps = skipped / dt
                        eta = (to_skip - skipped) / bps if bps > 0 else float("inf")
                        logger.info(f"Skipped {skipped}/{to_skip} batches ({bps:.2f} batches/s), ETA ~{eta:.0f}s")
                epoch_loader = it  # –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —ç—Ç–æ—Ç epoch —Å —Ç–µ–∫—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏
                if accelerator.is_main_process:
                    metrics.update(status="training", skipped_batches_done=int(skipped))
                resume_batches_to_skip = 0  # —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑

            for step, batch in enumerate(epoch_loader):
                # Heartbeat: –æ–±–Ω–æ–≤–ª—è–µ–º metrics.json –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –¥–∞–∂–µ –µ—Å–ª–∏ log_every –±–æ–ª—å—à–æ–π
                if accelerator.is_main_process and heartbeat_every > 0:
                    now = time.time()
                    if now - last_heartbeat >= heartbeat_every:
                        try:
                            metrics.update(last_heartbeat=datetime.now().isoformat())
                        except Exception:
                            pass
                        last_heartbeat = now
                # –û–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π –ª–æ–≥: —Ä–µ–∞–ª—å–Ω—ã–π per-process batch –∏ –æ—Ü–µ–Ω–∫–∞ "–ø–æ—á–µ–º—É –º–Ω–æ–≥–æ VRAM"
                if step == 0 and epoch == 0 and accelerator.is_main_process and global_step == starting_step:
                    try:
                        if "input_ids" in batch and hasattr(batch["input_ids"], "shape"):
                            bsz, seqlen = int(batch["input_ids"].shape[0]), int(batch["input_ids"].shape[1])
                        else:
                            bsz, seqlen = int(config.get("batch_size", 0)), int(config.get("seq_len", 0))

                        vocab_size = int(config.get("vocab_size", 50257))
                        mp = str(config.get("mixed_precision", "no")).lower()
                        bytes_per = 2 if mp in ("fp16", "bf16") else 4

                        # logits: (B,S,V) ‚Äî —ç—Ç–æ —á–∞—Å—Ç–æ –≥–ª–∞–≤–Ω—ã–π –ø–æ–∂–∏—Ä–∞—Ç–µ–ª—å –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö B –∏ –¥–ª–∏–Ω–Ω–æ–º S
                        logits_gb = (bsz * seqlen * vocab_size * bytes_per) / (1024**3)

                        num_gpus = int(accelerator.num_processes or 1)
                        grad_accum = int(config.get("gradient_accumulation", 1))
                        eff_per_gpu = bsz * grad_accum
                        global_batch = eff_per_gpu * num_gpus

                        logger.warning(
                            f"[BATCH CHECK] input_ids shape={getattr(batch.get('input_ids', None), 'shape', None)} | "
                            f"per-GPU microbatch={bsz}, grad_accum={grad_accum} -> effective per-GPU={eff_per_gpu}, global={global_batch} (x{num_gpus}). "
                            f"Estimated logits memory ~{logits_gb:.2f} GB (B*S*V, dtype={mp}). "
                        )
                    except Exception as e:
                        logger.warning(f"[BATCH CHECK] failed: {e}")
                # Debug: –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–µ–π (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –ø—Ä–∏–º–µ—Ä–æ–≤)
                # –í–ê–ñ–ù–û: –ü—Ä–∏ dispatch_batches=True –Ω—É–∂–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å —Ö—ç—à–∏ —Å–æ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —á–µ—Ä–µ–∑ gather
                if debug_check_duplicates and global_step < debug_max_samples:
                    # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º hash –æ—Ç input_ids
                    if "input_ids" in batch:
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ –±–∞—Ç—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                        sample_hash = hash(batch["input_ids"][0].cpu().numpy().tobytes())
                        debug_sample_ids.append((global_step, accelerator.process_index, sample_hash))
                
                with accelerator.accumulate(model):
                    if is_streaming_sharded:
                        batch = {k: (v.to(accelerator.device) if hasattr(v, "to") else v) for k, v in batch.items()}
                    with accelerator.autocast():
                        outputs = model(**batch)
                    loss = outputs.loss
                    
                    loss_val = loss.detach().float().item()
                    
                    # –ö–†–ò–¢–ò–ß–ù–û: –µ—Å–ª–∏ loss NaN/Inf - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —à–∞–≥ –∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                    if math.isnan(loss_val) or math.isinf(loss_val):
                        logger.error(f"Loss is NaN or Inf at step {global_step}. Stopping training to prevent bad model.")
                        metrics.update(status="error", error=f"Training Diverged: Loss is NaN or Infinity at step {global_step}. Try lowering learning rate or changing precision.")
                        raise ValueError(f"Training Diverged: Loss is NaN or Infinity at step {global_step}.")
                    
                    # micro-–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä: —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ update-step
                    micro_loss_sum += loss_val
                    micro_count += 1
                    
                    accelerator.backward(loss)
                    
                    # –í–ê–ñ–ù–û: —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ update-step
                    if accelerator.sync_gradients:
                        # –û–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π –ª–æ–≥ —Ä–µ–∞–ª—å–Ω–æ–≥–æ peak VRAM (allocator) –Ω–∞ –ø–µ—Ä–≤–æ–º update-step (–ø–æ –∫–∞–∂–¥–æ–º—É —Ä–∞–Ω–∫—É)
                        if epoch == 0 and global_step == starting_step and not metrics.metrics.get("logged_cuda_peak", False):
                            try:
                                if torch.cuda.is_available():
                                    dev = accelerator.device
                                    torch.cuda.synchronize(dev)
                                    peak_alloc = torch.cuda.max_memory_allocated(dev) / (1024**3)
                                    peak_res = torch.cuda.max_memory_reserved(dev) / (1024**3)
                                    cur_alloc = torch.cuda.memory_allocated(dev) / (1024**3)
                                    cur_res = torch.cuda.memory_reserved(dev) / (1024**3)
                                    logger.warning(
                                        f"[CUDA PEAK] rank={accelerator.process_index} device={dev} | "
                                        f"peak_alloc={peak_alloc:.2f}GB peak_reserved={peak_res:.2f}GB | "
                                        f"cur_alloc={cur_alloc:.2f}GB cur_reserved={cur_res:.2f}GB"
                                    )
                                # –ø–æ–º–µ—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ main process —á–µ—Ä–µ–∑ metrics-—Ñ–∞–π–ª (—á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –ø—Ä–∏ –¥–æ–ª–≥–æ–º –ø—Ä–æ–≥–æ–Ω–µ)
                                if accelerator.is_main_process:
                                    metrics.metrics["logged_cuda_peak"] = True
                                    metrics._save()
                            except Exception as e:
                                logger.warning(f"[CUDA PEAK] failed: {e}")

                        # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        max_grad_norm = config.get("max_grad_norm", 1.0)
                        if max_grad_norm > 0:
                            accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)
                        
                        optimizer.step()
                        lr_scheduler.step()
                        optimizer.zero_grad(set_to_none=True)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–æ–∏–∑–æ—à–µ–ª –ª–∏ —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
                if accelerator.sync_gradients:
                    global_step += 1
                    
                    # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (update step)
                    update_time = time.time() - update_start_time
                    update_start_time = time.time()
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –≤—Å–µ –º–∏–∫—Ä–æ-—à–∞–≥–∏ –±—ã–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã (–Ω–µ –¥–æ–ª–∂–Ω–æ —Å–ª—É—á–∏—Ç—å—Å—è –ø–æ—Å–ª–µ —Ñ–∏–∫—Å–∞ –≤—ã—à–µ)
                    if micro_count == 0:
                        logger.warning(f"No valid loss values at step {global_step}, skipping update")
                        continue

                    update_loss = micro_loss_sum / micro_count
                    # –í–ê–ñ–ù–û: —É—Å—Ä–µ–¥–Ω—è–µ–º loss –ø–æ –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–∞–º –≤ Multi-GPU
                    update_loss_t = torch.tensor(update_loss, device=accelerator.device)
                    update_loss = accelerator.reduce(update_loss_t, reduction="mean").item()
                    
                    micro_loss_sum = 0.0
                    micro_count = 0

                    # –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –¥–ª—è —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∞
                    log_loss_sum += update_loss
                    log_updates += 1

                    # Debug: –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–µ–π –ø–æ—Å–ª–µ –ø–µ—Ä–≤—ã—Ö N —à–∞–≥–æ–≤
                    # –í–ê–ñ–ù–û: gather() ‚Äî –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è, –¥–æ–ª–∂–Ω–∞ –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –Ω–∞ –í–°–ï–• –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö!
                    if debug_check_duplicates and global_step == debug_max_samples:
                        # 1) –°–æ–±–∏—Ä–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö—ç—à–µ–π —Å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ (–í–°–ï –ø—Ä–æ—Ü–µ—Å—Å—ã –≤—ã–∑—ã–≤–∞—é—Ç gather)
                        hash_counts = torch.tensor([len(debug_sample_ids)], device=accelerator.device, dtype=torch.long)
                        all_counts = accelerator.gather(hash_counts)  # collective op
                        
                        # 2) –ì–æ—Ç–æ–≤–∏–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ö—ç—à–∏ –¥–ª—è gather (–í–°–ï –ø—Ä–æ—Ü–µ—Å—Å—ã)
                        max_len = int(all_counts.max().item()) if all_counts.numel() > 0 else 0
                        if len(debug_sample_ids) > 0 and max_len > 0:
                            local_hashes = torch.tensor([h for _, _, h in debug_sample_ids], device=accelerator.device, dtype=torch.long)
                            # Pad –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –¥–ª—è gather
                            if len(local_hashes) < max_len:
                                padding = torch.full((max_len - len(local_hashes),), -1, device=accelerator.device, dtype=torch.long)
                                local_hashes = torch.cat([local_hashes, padding])
                        else:
                            # –ü—É—Å—Ç–æ–π —Ç–µ–Ω–∑–æ—Ä –µ—Å–ª–∏ –Ω–µ—Ç —Ö—ç—à–µ–π (–Ω–æ –≤—Å—ë —Ä–∞–≤–Ω–æ —É—á–∞—Å—Ç–≤—É–µ–º –≤ gather)
                            local_hashes = torch.full((max(1, max_len),), -1, device=accelerator.device, dtype=torch.long)
                        
                        # 3) –°–æ–±–∏—Ä–∞–µ–º —Ö—ç—à–∏ —Å–æ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–í–°–ï –ø—Ä–æ—Ü–µ—Å—Å—ã –≤—ã–∑—ã–≤–∞—é—Ç gather)
                        all_hashes = accelerator.gather(local_hashes.unsqueeze(0))  # collective op
                        
                        # 4) –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–µ–π ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞ main process (–ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è gather)
                        if accelerator.is_main_process:
                            total_hashes = int(all_counts.sum().item())
                            logger.info(f"Debug: collected {total_hashes} sample hashes across all processes")
                            
                            # –£–±–∏—Ä–∞–µ–º padding (-1) –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏
                            all_hashes_flat = all_hashes.flatten()
                            valid_hashes = all_hashes_flat[all_hashes_flat != -1].cpu().numpy()
                            
                            if len(valid_hashes) > 0:
                                unique_hashes = set(valid_hashes)
                                if len(valid_hashes) != len(unique_hashes):
                                    duplicates = len(valid_hashes) - len(unique_hashes)
                                    logger.warning(f"Debug: Found {duplicates} duplicate samples in first {debug_max_samples} steps across all processes!")
                                else:
                                    logger.info(f"Debug: No duplicates found in first {debug_max_samples} steps (all {len(valid_hashes)} samples unique across all processes)")
                            else:
                                logger.warning("Debug: No sample hashes collected")
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    if global_step % config.get("log_every", 10) == 0 or global_step == 1:
                        avg_loss = log_loss_sum / max(1, log_updates)
                        
                        # Samples per second (Effective Batch / Time)
                        # –£—á–∏—Ç—ã–≤–∞–µ–º multi-GPU: —Ä–µ–∞–ª—å–Ω—ã–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π batch = effective_batch * num_processes
                        effective_batch = config["batch_size"] * config["gradient_accumulation"]
                        global_batch = effective_batch * accelerator.num_processes
                        samples_per_sec = global_batch / update_time if update_time > 0 else 0
                        
                        metrics.log_step(
                            step=global_step,
                            loss=avg_loss,
                            lr=lr_scheduler.get_last_lr()[0],
                            samples_per_sec=samples_per_sec,
                            step_time=update_time
                        )
                        
                        log_loss_sum = 0.0
                        log_updates = 0
                    
                    # Checkpoint
                    if global_step % config.get("save_every", 5000) == 0:
                        ckpt_path = output_dir / f"checkpoint_step{global_step}"
                        accelerator.save_state(ckpt_path)
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–æ–≥–∏–π dataloader state (per-rank) —Ä—è–¥–æ–º —Å accelerate checkpoint
                        try:
                            if hasattr(train_dataset, "get_resume_state"):
                                ds_state = train_dataset.get_resume_state()
                                st_path = ckpt_path / f"dataloader_state_rank{accelerator.process_index}.json"
                                with open(st_path, "w", encoding="utf-8") as f:
                                    json.dump(ds_state, f, ensure_ascii=False, indent=2)
                        except Exception as e:
                            logger.warning(f"Failed to save dataloader state: {e}")
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (–¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ UI/–≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–ª–∞–Ω–æ–≤)
                        try:
                            if accelerator.is_main_process:
                                meta = {
                                    "global_step": int(global_step),
                                    "planned_total_steps": int(metrics.metrics.get("planned_total_steps", max_train_steps)),
                                    "learning_rate": float(config.get("learning_rate", 0.0)),
                                    "warmup_steps": int(config.get("warmup_steps", 0)),
                                    "min_lr_ratio": float(config.get("min_lr_ratio", 0.0) or 0.0),
                                    "scheduler": str(config.get("lr_schedule", "cosine")),
                                    "timestamp": datetime.now().isoformat(),
                                }
                                with open(ckpt_path / "checkpoint_metadata.json", "w", encoding="utf-8") as f:
                                    json.dump(meta, f, ensure_ascii=False, indent=2)
                        except Exception as e:
                            logger.warning(f"Failed to save checkpoint metadata: {e}")
                        accelerator.wait_for_everyone()
                        if accelerator.is_main_process:
                            model_config.save_pretrained(ckpt_path)
                            # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—É—â–∏–π loss –≤ —á–µ–∫–ø–æ–∏–Ω—Ç
                            current_loss = metrics.metrics.get("current_loss", 0.0)
                            metrics.log_checkpoint(str(ckpt_path), loss=current_loss)

                            # (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞ –ø—Ä–∏ –∫–∞–∂–¥–æ–º checkpoint.
                            # –≠—Ç–æ –ù–ï resume-state, –∞ –ø—Ä–æ—Å—Ç–æ —É–¥–æ–±–Ω—ã–π "latest final_model" –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.
                            if bool(config.get("export_on_checkpoint", True)):
                                try:
                                    import shutil
                                    tmp_dir = output_dir / "final_model.__tmp__"
                                    final_dir = output_dir / "final_model"
                                    if tmp_dir.exists():
                                        shutil.rmtree(tmp_dir, ignore_errors=True)
                                    tmp_dir.mkdir(parents=True, exist_ok=True)

                                    # SFT: —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ chat_template –ø–æ–ø–∞–¥–∞–µ—Ç –≤ tokenizer_config.json
                                    if stage == "sft" and config.get("sft_template"):
                                        tmpl = config["sft_template"]
                                        default_sys = tmpl.get("system", "You are a helpful assistant.")
                                        u_tag = tmpl.get("user_tag", "### User:")
                                        b_tag = tmpl.get("bot_tag", "### Assistant:")
                                        default_sys_escaped = default_sys.replace("'", "\\'")
                                        u_tag_escaped = u_tag.replace("'", "\\'")
                                        b_tag_escaped = b_tag.replace("'", "\\'")
                                        tokenizer.chat_template = (
                                            "{% if messages and messages[0]['role'] == 'system' %}"
                                            "{{ messages[0]['content'] }}\n\n"
                                            "{% else %}"
                                            f"{{{{ '{default_sys_escaped}' }}}}\n\n"
                                            "{% endif %}"
                                            "{% for message in messages %}"
                                            "{% if message['role'] == 'user' %}"
                                            f"{u_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                                            "{% elif message['role'] == 'assistant' %}"
                                            f"{b_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                                            "{% endif %}"
                                            "{% endfor %}"
                                            "{% if add_generation_prompt %}"
                                            f"{b_tag_escaped}\n"
                                            "{% endif %}"
                                        )

                                    adapter.save_final(accelerator, model, tokenizer, tmp_dir)

                                    # –ê—Ç–æ–º–∞—Ä–Ω–æ –∑–∞–º–µ–Ω—è–µ–º final_model
                                    if final_dir.exists():
                                        shutil.rmtree(final_dir, ignore_errors=True)
                                    tmp_dir.rename(final_dir)
                                    logger.info(f"Updated final_model at checkpoint step {global_step}")
                                except Exception as e:
                                    logger.warning(f"Failed to export final_model on checkpoint: {e}")
                    
                    # Validation
                    # –í–ê–ñ–ù–û: evaluate() –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö, —Ç.–∫. val_loader –∑–∞—à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω
                    # –ö–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å –≤–∏–¥–∏—Ç —Å–≤–æ—é —á–∞—Å—Ç—å val –¥–∞–Ω–Ω—ã—Ö, reduce() —É—Å—Ä–µ–¥–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    if val_loader is not None and eval_every > 0 and (global_step % eval_every == 0):
                        val_loss = evaluate(val_loader)  # –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö
                        if accelerator.is_main_process and val_loss is not None:
                            metrics.log_eval(global_step, float(val_loss))
                            logger.info(f"Validation at step {global_step}: val_loss={val_loss:.4f}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –ª–∏–º–∏—Ç–∞ —à–∞–≥–æ–≤
                    if global_step >= max_train_steps:
                        training_complete = True
                        stop_reason = "max_train_steps_reached"
                        break
        
        # –ï—Å–ª–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∏ –Ω–µ –ø–æ max_train_steps, —Ç–æ –ª–∏–±–æ —ç–ø–æ—Ö–∏ –∫–æ–Ω—á–∏–ª–∏—Å—å, –ª–∏–±–æ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä –∏—Å—á–µ—Ä–ø–∞–ª—Å—è.
        if stop_reason is None:
            if global_step >= max_train_steps:
                stop_reason = "max_train_steps_reached"
            else:
                # –î–ª—è IterableDataset –¥–ª–∏–Ω–∞ —á–∞—Å—Ç–æ –æ—Ü–µ–Ω–æ—á–Ω–∞—è (–ø—É—Å—Ç—ã–µ/–±–∏—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è),
                # –ø–æ—ç—Ç–æ–º—É –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ EOF –º–æ–∂–µ—Ç —Å–ª—É—á–∏—Ç—å—Å—è —Ä–∞–Ω—å—à–µ planned_total_steps.
                stop_reason = "epochs_completed_or_dataloader_exhausted"

        # –ï—Å–ª–∏ "–ø–ª–∞–Ω" –æ–∫–∞–∑–∞–ª—Å—è –±–æ–ª—å—à–µ —Ñ–∞–∫—Ç–∞ ‚Äî —Ñ–∏–∫—Å–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–∞–∫, —á—Ç–æ–±—ã –ø—Ä–æ–≥—Ä–µ—Å—Å –±—ã–ª 100%,
        # –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–ª–∞–Ω –æ—Ç–¥–µ–ª—å–Ω–æ (planned_total_steps).
        if accelerator.is_main_process:
            try:
                if metrics.metrics.get("planned_total_steps", 0) and global_step < int(metrics.metrics.get("planned_total_steps", 0)):
                    metrics.update(total_steps=int(global_step), stop_reason=stop_reason)
                else:
                    metrics.update(stop_reason=stop_reason)
            except Exception:
                pass

        # Final save - —Ç–æ–ª—å–∫–æ –Ω–∞ main process
        accelerator.wait_for_everyone()
        if accelerator.is_main_process:
            metrics.update(status="saving_model")
            final_dir = output_dir / "final_model"
            final_dir.mkdir(parents=True, exist_ok=True)
            
            # --- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Å–æ—Ö—Ä–∞–Ω—è–µ–º chat_template –¥–ª—è SFT ---
            if stage == "sft" and config.get("sft_template"):
                tmpl = config["sft_template"]
                
                default_sys = tmpl.get("system", "You are a helpful assistant.")
                u_tag = tmpl.get("user_tag", "### User:")
                b_tag = tmpl.get("bot_tag", "### Assistant:")
                
                default_sys_escaped = default_sys.replace("'", "\\'")
                u_tag_escaped = u_tag.replace("'", "\\'")
                b_tag_escaped = b_tag.replace("'", "\\'")
                
                tokenizer.chat_template = (
                    "{% if messages and messages[0]['role'] == 'system' %}"
                    "{{ messages[0]['content'] }}\n\n"
                    "{% else %}"
                    f"{{{{ '{default_sys_escaped}' }}}}\n\n"
                    "{% endif %}"
                    "{% for message in messages %}"
                    "{% if message['role'] == 'user' %}"
                    f"{u_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                    "{% elif message['role'] == 'assistant' %}"
                    f"{b_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                    "{% endif %}"
                    "{% endfor %}"
                    "{% if add_generation_prompt %}"
                    f"{b_tag_escaped}\n"
                    "{% endif %}"
                )
                logger.info(f"Saved chat_template: user_tag='{u_tag}', bot_tag='{b_tag}'")
            
            # --- –í–°–ï–ì–î–ê —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–∏ –¥–ª—è pretrain —Ç–æ–∂–µ) ---
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è Home –∏ HF –º–æ–¥–µ–ª–µ–π)
            adapter.save_final(accelerator, model, tokenizer, final_dir)
            
            total_time = time.time() - start_time
            hours, rem = divmod(total_time, 3600)
            minutes, seconds = divmod(rem, 60)
            duration_str = "{:0>2}:{:0>2}:{:05.2f}".format(int(hours), int(minutes), seconds)
            
            metrics.update(
                status="completed",
                total_time_seconds=total_time,
                training_duration=duration_str,
                final_model_path=str(final_dir),
            )
        
        accelerator.wait_for_everyone()
        
    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        logger.error(f"Training failed: {e}\n{tb}")
        metrics.update(status="error", error=f"{str(e)}\n\n{tb}")
        raise


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Path to JSON config")
    parser.add_argument("--metrics", type=str, required=True, help="Path to metrics JSON")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Path to checkpoint directory to resume from")
    args = parser.parse_args()
    
    with open(args.config) as f:
        config = json.load(f)
    
    # Merge CLI args into config
    if args.resume_from_checkpoint:
        config["resume_from_checkpoint"] = args.resume_from_checkpoint
    
    run_training(config, Path(args.metrics))


if __name__ == "__main__":
    main()
