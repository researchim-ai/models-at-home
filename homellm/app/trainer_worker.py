"""
Ð¤Ð¾Ð½Ð¾Ð²Ñ‹Ð¹ worker Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸.
Ð—Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð² JSON Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Streamlit Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÐµÐ¼.
"""
from __future__ import annotations

import argparse
import json
import logging
import math
import os
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

import torch
from torch.utils.data import IterableDataset, DataLoader
from accelerate import Accelerator
from accelerate.utils import DataLoaderConfiguration
from transformers import (
    AutoTokenizer,
    get_cosine_schedule_with_warmup,
    get_scheduler,
    DataCollatorForLanguageModeling,
)

from homellm.models.adapters import resolve_adapter
from homellm.training.pretrain import StreamingTextDataset
from homellm.training.sft import SFTDataset
from homellm.training.optimizers import MagmaAdamW, HybridMuonAdamW

# Liger Kernel Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Triton kernels)
try:
    from homellm.training.rl.liger_utils import (
        is_liger_available,
        apply_liger_patch_to_model,
        create_liger_fused_ce,
        LIGER_SUPPORTED_MODELS,
    )
    LIGER_UTILS_AVAILABLE = True
except ImportError:
    LIGER_UTILS_AVAILABLE = False
    LIGER_SUPPORTED_MODELS = set()

logger = logging.getLogger(__name__)


def get_gpu_stats():
    """
    ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ GPU Ñ‡ÐµÑ€ÐµÐ· nvidia-smi (Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¸ DDP).
    
    Ð’ÐÐ–ÐÐž: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÑ‚ Ð¸ Ñ€ÐµÐ¼Ð°Ð¿Ð¸Ñ‚ GPU Ð¿Ð¾ CUDA_VISIBLE_DEVICES, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ
    Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ GPU Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¸Ð½Ð´ÐµÐºÑÐ°Ð¼Ð¸ (0..N-1).
    """
    gpu_stats = []
    
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð²Ð¸Ð´Ð¸Ð¼Ñ‹Ñ… GPU Ð¸Ð· Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
    visible = os.environ.get("CUDA_VISIBLE_DEVICES")
    visible_ids = None
    if visible:
        try:
            visible_ids = [int(x.strip()) for x in visible.split(",") if x.strip() != ""]
        except Exception:
            visible_ids = None
    
    try:
        import subprocess
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾ Ð²ÑÐµÑ… GPU Ñ‡ÐµÑ€ÐµÐ· nvidia-smi
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', 
             '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=2
        )
        
        if result.returncode == 0:
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 4:
                        gpu_id = int(parts[0])
                        memory_used = float(parts[1]) / 1024  # MiB -> GiB
                        memory_total = float(parts[2]) / 1024
                        utilization = int(parts[3]) if parts[3].isdigit() else None
                        
                        gpu_stats.append({
                            "id": gpu_id,
                            "memory_used_gb": round(memory_used, 2),
                            "memory_total_gb": round(memory_total, 2),
                            "memory_percent": round(memory_used / memory_total * 100, 1) if memory_total > 0 else 0,
                            "utilization": utilization,
                        })
            
            # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¸ Ñ€ÐµÐ¼Ð°Ð¿Ð¸Ð¼ Ð¿Ð¾ CUDA_VISIBLE_DEVICES
            if visible_ids is not None:
                # ÐžÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ð¸Ð´Ð¸Ð¼Ñ‹Ðµ GPU
                gpu_stats = [g for g in gpu_stats if g["id"] in visible_ids]
                # Ð ÐµÐ¼Ð°Ð¿Ð¸Ð¼ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ID -> Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ (0..N-1)
                remap = {phys: i for i, phys in enumerate(visible_ids)}
                for g in gpu_stats:
                    g["id"] = remap.get(g["id"], g["id"])
    except Exception:
        # Fallback Ð½Ð° torch.cuda ÐµÑÐ»Ð¸ nvidia-smi Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                try:
                    memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)
                    memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    
                    gpu_stats.append({
                        "id": i,
                        "memory_used_gb": round(memory_allocated, 2),
                        "memory_total_gb": round(memory_total, 2),
                        "memory_percent": round(memory_allocated / memory_total * 100, 1),
                        "utilization": None,
                    })
                except:
                    pass
    
    return gpu_stats


class MetricsLogger:
    """Ð›Ð¾Ð³Ð³ÐµÑ€ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð² JSON Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸.
    
    Ð’ÐÐ–ÐÐž: Ð’ Multi-GPU Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main process (rank 0),
    Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð³Ð¾Ð½Ð¾Ðº Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² Ð¾Ð´Ð¸Ð½ Ñ„Ð°Ð¹Ð».
    """
    
    def __init__(self, log_path: Path, enabled: bool = True):
        self.log_path = log_path
        self.enabled = enabled
        self.start_timestamp = time.time()
        self.metrics = {
            "status": "initializing",
            "start_time": datetime.now().isoformat(),
            "current_step": 0,
            "total_steps": 0,
            "epoch": 0,
            "loss_history": [],
            "lr_history": [],
            "steps_history": [],
            "current_loss": 0.0,
            "current_lr": 0.0,
            "samples_per_second": 0.0,
            "eta_seconds": 0,
            "error": None,
            "checkpoints": [],
            "gpu_stats": [],
            "current_val_loss": None,
            "val_loss_history": [],
            "val_steps_history": [],
        }
        self._save()
    
    def _save(self):
        """ÐÑ‚Ð¾Ð¼Ð°Ñ€Ð½Ð°Ñ Ð·Ð°Ð¿Ð¸ÑÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ð¸Ð·Ð±ÐµÐ¶Ð°Ð½Ð¸Ñ Ð³Ð¾Ð½Ð¾Ðº."""
        if not self.enabled:
            return
        
        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ€Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÑÐºÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        self.log_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ÐÑ‚Ð¾Ð¼Ð°Ñ€Ð½Ð°Ñ Ð·Ð°Ð¿Ð¸ÑÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
        tmp_path = self.log_path.with_suffix(".tmp")
        try:
            with open(tmp_path, "w") as f:
                json.dump(self.metrics, f, indent=2)
            os.replace(tmp_path, self.log_path)
        except Exception as e:
            # Ð•ÑÐ»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ, Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ tmp Ñ„Ð°Ð¹Ð»
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except:
                pass
            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÑƒ, Ð½Ð¾ Ð½Ðµ Ð¿Ð°Ð´Ð°ÐµÐ¼
            logger.warning(f"Failed to save metrics: {e}")
    
    def update(self, **kwargs):
        if not self.enabled:
            return
        self.metrics.update(kwargs)
        self._save()
    
    def log_step(self, step: int, loss: float, lr: float, samples_per_sec: float = 0, step_time: float = 0):
        if not self.enabled:
            return
        
        self.metrics["current_step"] = step
        self.metrics["current_loss"] = loss
        self.metrics["current_lr"] = lr
        self.metrics["samples_per_second"] = samples_per_sec
        self.metrics["loss_history"].append(loss)
        self.metrics["lr_history"].append(lr)
        self.metrics["steps_history"].append(step)
        
        # Elapsed
        self.metrics["elapsed_seconds"] = time.time() - self.start_timestamp
        
        # GPU stats
        self.metrics["gpu_stats"] = get_gpu_stats()
        
        # ETA Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ ÑˆÐ°Ð³Ð°
        if step > 0 and step_time > 0:
            remaining_steps = max(0, self.metrics["total_steps"] - step)
            self.metrics["eta_seconds"] = int(remaining_steps * step_time)
        
        self._save()
    
    def log_checkpoint(self, path: str, loss: float = None):
        if not self.enabled:
            return
        
        checkpoint_data = {
            "path": path,
            "step": self.metrics["current_step"],
            "time": datetime.now().isoformat()
        }
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ loss ÐµÑÐ»Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½
        if loss is not None:
            checkpoint_data["loss"] = float(loss)
        elif "current_loss" in self.metrics:
            checkpoint_data["loss"] = float(self.metrics["current_loss"])
        
        self.metrics["checkpoints"].append(checkpoint_data)
        self._save()
    
    def log_eval(self, step: int, val_loss: float):
        """Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸."""
        if not self.enabled:
            return
        
        self.metrics["current_val_loss"] = val_loss
        self.metrics["val_loss_history"].append(val_loss)
        self.metrics["val_steps_history"].append(step)
        self._save()


def run_training(config: Dict[str, Any], metrics_path: Path):
    """Ð—Ð°Ð¿ÑƒÑÐº Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ñ Ð·Ð°Ð¿Ð¸ÑÑŒÑŽ Ð¼ÐµÑ‚Ñ€Ð¸Ðº."""
    
    # === Backend Selection ===
    training_backend = config.get("training_backend", "models-at-home")
    stage = config.get("stage", "pretrain")  # pretrain | continual_pretrain | sft | grpo
    
    # Unsloth backend Ð´Ð»Ñ SFT
    if training_backend == "unsloth" and stage == "sft":
        logger.info("ðŸ¦¥ Using Unsloth backend for SFT training")
        try:
            from homellm.training.unsloth_sft import run_unsloth_sft, is_unsloth_available
            
            if not is_unsloth_available():
                logger.warning("âš ï¸ Unsloth not available, falling back to models-at-home backend")
            else:
                # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ MetricsLogger
                metrics = MetricsLogger(metrics_path, enabled=True)
                metrics.update(
                    status="initializing",
                    backend="unsloth",
                    stage=stage,
                )
                
                try:
                    run_unsloth_sft(config, metrics)
                    return  # Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ Ñ‡ÐµÑ€ÐµÐ· unsloth
                except Exception as e:
                    import traceback
                    tb = traceback.format_exc()
                    logger.error(f"ðŸ¦¥ Unsloth training failed: {e}\n{tb}")
                    metrics.update(status="error", error=f"Unsloth error: {str(e)}\n\n{tb}")
                    raise
        except ImportError as e:
            logger.warning(f"âš ï¸ Could not import unsloth_sft: {e}. Falling back to models-at-home backend.")
    
    # === models-at-home backend (default) ===
    # Mixed precision
    mixed_precision = config.get("mixed_precision", "no")
    fp16_pure = bool(config.get("fp16_pure", False))
    use_flash_attention = bool(config.get("use_flash_attention", True))
    
    # Ð’ÐÐ–ÐÐž: Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ accelerate/deepspeed config-Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ `gradient_accumulation_steps: auto`,
    # Ð½Ð¾ Accelerator Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ int Ð¸ ÑƒÐ¿Ð°Ð´Ñ‘Ñ‚ Ð½Ð° int("auto"). Ð’ Ð½Ð°ÑˆÐµÐ¼ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¸ÑÑ‚Ð¸Ð½Ñ‹ â€” config.json Ð¸Ð· UI.
    # ÐŸÐ¾ÑÑ‚Ð¾Ð¼Ñƒ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¸ Ð¿Ñ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÐ¼ env Ð´Ð»Ñ accelerate.
    raw_ga = config.get("gradient_accumulation", 1)
    try:
        ga_steps = int(raw_ga)
    except Exception:
        if isinstance(raw_ga, str) and raw_ga.strip().lower() == "auto":
            ga_steps = 1
            logger.warning("gradient_accumulation='auto' Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð² runtime; Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽ 1. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹Ñ‚Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² UI.")
        else:
            raise
    config["gradient_accumulation"] = ga_steps
    os.environ["ACCELERATE_GRADIENT_ACCUMULATION_STEPS"] = str(ga_steps)

    # "Pure fp16" (Ð²ÐµÑÐ° fp16 Ð±ÐµÐ· GradScaler) â€” Ð´Ð»Ñ accelerate Ð½ÑƒÐ¶Ð½Ð¾ mixed_precision='no'
    # Ð¸Ð½Ð°Ñ‡Ðµ Ð¾Ð½ Ð²ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ GradScaler Ð¸ ÑƒÐ¿Ð°Ð´Ñ‘Ñ‚ Ð¿Ñ€Ð¸ fp16 Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ….
    if str(mixed_precision).lower() == "fp16" and fp16_pure:
        mixed_precision = "no"
        logger.info("ðŸ§ª FP16 Pure Ñ€ÐµÐ¶Ð¸Ð¼: Accelerator(mixed_precision='no') (Ð±ÐµÐ· GradScaler)")
    
    # Ð’ÐÐ–ÐÐž: Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ dispatch_batches=True, Ñ‚Ð°Ðº ÐºÐ°Ðº ÑÑ‚Ð¾ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ NoneType Ð¿Ñ€Ð¸ broadcast
    # Ð’Ð¼ÐµÑÑ‚Ð¾ ÑÑ‚Ð¾Ð³Ð¾ Ð±ÑƒÐ´ÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ StreamingTextDataset (shard=True)
    dataloader_config = DataLoaderConfiguration(
        dispatch_batches=None,
        even_batches=False, 
    )
    
    accelerator = Accelerator(
        gradient_accumulation_steps=ga_steps,
        mixed_precision=mixed_precision,
        dataloader_config=dataloader_config,
    )

    # DeepSpeed: Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ²Ð½Ð¾ Ð·Ð°Ð´Ð°Ñ‚ÑŒ train_micro_batch_size_per_gpu
    # ÐµÑÐ»Ð¸ Ð¼Ñ‹ Ð½Ðµ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‘Ð¼ DataLoader Ð² accelerator.prepare() (dataset-level sharding).
    # Ð‘ÐµÐ· ÑÑ‚Ð¾Ð³Ð¾ DeepSpeed Ð¿Ð°Ð´Ð°ÐµÑ‚ Ñ "requires you to pass at least one dataloader with batch_size".
    if accelerator.state.deepspeed_plugin is not None:
        batch_size = int(config.get("batch_size", 1))
        ds_cfg = accelerator.state.deepspeed_plugin.deepspeed_config
        current_mbs = ds_cfg.get("train_micro_batch_size_per_gpu")
        if current_mbs in (None, "auto"):
            ds_cfg["train_micro_batch_size_per_gpu"] = batch_size
            logger.info(f"DeepSpeed: set train_micro_batch_size_per_gpu={batch_size}")
        # Ð¢Ð°ÐºÐ¶Ðµ Ð·Ð°Ð´Ð°Ð´Ð¸Ð¼ train_batch_size ÐµÑÐ»Ð¸ auto
        current_tbs = ds_cfg.get("train_batch_size")
        if current_tbs in (None, "auto"):
            # train_batch_size = micro_batch * grad_accum * world_size
            world_size = accelerator.num_processes
            ds_cfg["train_batch_size"] = batch_size * ga_steps * world_size
            logger.info(f"DeepSpeed: set train_batch_size={ds_cfg['train_batch_size']}")

    # HomeModel FlashAttention: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ PyTorch SDPA (scaled_dot_product_attention).
    # Ð§Ñ‚Ð¾Ð±Ñ‹ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð·Ð°Ð´ÐµÐ¹ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ flash kernels, Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ CUDA SDPA backends (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹).
    if torch.cuda.is_available():
        try:
            if use_flash_attention:
                torch.backends.cuda.enable_flash_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(True)
                torch.backends.cuda.enable_math_sdp(True)
            else:
                # Ð¯Ð²Ð½Ð¾ Ð·Ð°Ð¿Ñ€ÐµÑ‰Ð°ÐµÐ¼ flash/mem_efficient kernels, Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ math (eager/Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹)
                torch.backends.cuda.enable_flash_sdp(False)
                torch.backends.cuda.enable_mem_efficient_sdp(False)
                torch.backends.cuda.enable_math_sdp(True)
            logger.info(
                "SDPA kernels: flash=%s mem_efficient=%s math=%s (use_flash_attention=%s)",
                getattr(torch.backends.cuda, "flash_sdp_enabled", lambda: "N/A")(),
                getattr(torch.backends.cuda, "mem_efficient_sdp_enabled", lambda: "N/A")(),
                getattr(torch.backends.cuda, "math_sdp_enabled", lambda: "N/A")(),
                use_flash_attention,
            )
        except Exception as e:
            logger.warning(f"Could not configure CUDA SDPA kernels: {e}")
    
    # Ð’ÐÐ–ÐÐž: MetricsLogger Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main process Ð´Ð»Ñ Ð¸Ð·Ð±ÐµÐ¶Ð°Ð½Ð¸Ñ Ð³Ð¾Ð½Ð¾Ðº Ð¿Ñ€Ð¸ Multi-GPU
    metrics = MetricsLogger(metrics_path, enabled=accelerator.is_main_process)
    
    try:
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ
        adapter = resolve_adapter(config)
        logger.info(f"Using adapter: {adapter.__class__.__name__}")
        
        metrics.update(status="loading_tokenizer", stage=stage)
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ñ‡ÐµÑ€ÐµÐ· Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€
        tokenizer_path = config.get("tokenizer_path")
        if not tokenizer_path and stage in ("sft", "continual_pretrain") and config.get("base_model_path"):
            # Ð”Ð»Ñ SFT/continual Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¸Ð· Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
            tokenizer_path = config["base_model_path"]
        elif not tokenizer_path:
            # Ð”Ð»Ñ pretrain Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ model_id ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½, Ð¸Ð½Ð°Ñ‡Ðµ gpt2
            tokenizer_path = config.get("model_id", "gpt2")
        
        tokenizer = adapter.load_tokenizer(tokenizer_path, trust_remote_code=True)
        tokenizer = adapter.prepare_tokenizer(tokenizer)  # pad_token = eos_token
        
        metrics.update(status=f"loading_dataset ({stage})")

        # ---------------------------
        # Sharding mode
        # ---------------------------
        # auto: Ð´Ð»Ñ IterableDataset Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ dataset-level ÑˆÐ°Ñ€Ð´Ð¸Ð½Ð³ (ÑÑ‚Ñ€Ð¾Ð³Ð¾Ðµ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ + ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ Ñ strict resume)
        # accelerate: ÑˆÐ°Ñ€Ð´Ð¸Ð½Ð³ Ð´ÐµÐ»Ð°ÐµÑ‚ accelerator.prepare(DataLoader)
        # dataset: ÑˆÐ°Ñ€Ð´Ð¸Ð½Ð³ Ð´ÐµÐ»Ð°ÐµÑ‚ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ (shard=True) Ð¸ DataLoader ÐÐ• Ð·Ð°Ð²Ð¾Ñ€Ð°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð² accelerate.prepare
        sharding_mode = str(config.get("sharding_mode", "auto")).lower().strip()
        if sharding_mode not in ("auto", "dataset", "accelerate"):
            raise ValueError(f"Invalid sharding_mode={sharding_mode}. Expected one of: auto|dataset|accelerate")
        
        # Ð’ÐÐ–ÐÐž: Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ val_ratio Ð”Ðž ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ train_dataset, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ data leakage
        val_ratio = float(config.get("val_ratio", 0.0))
        eval_every = int(config.get("eval_every", 0) or 0)
        eval_batches = int(config.get("eval_batches", 20) or 20)
        
        # Ð”ÐµÑ€Ð¶Ð¸Ð¼ holdout Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ eval
        holdout_ratio = val_ratio if (val_ratio > 0.0 and eval_every > 0) else 0.0
        
        # Dataset Selection based on Stage
        # Ð’ÐÐ–ÐÐž: Ð¨Ð°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ»Ð°ÐµÑ‚ accelerate.prepare() Ñ‡ÐµÑ€ÐµÐ· DataLoaderShard/Dispatcher
        # Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´ÐµÐ»Ð°ÐµÑ‚ train/val split, Ð½Ð¾ ÐÐ• ÑˆÐ°Ñ€Ð´Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÐµÐ¶Ð´Ñƒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼Ð¸ (shard=False)
        if stage == "sft":
            # SFTDataset â€” IterableDataset, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ auto -> dataset-level
            effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
            ds_shard = (effective_shard_mode == "dataset")
            train_dataset = SFTDataset(
                config["data_path"],
                tokenizer,
                seq_len=config["seq_len"],
                sft_columns=config.get("sft_columns"),
                sft_template=config.get("sft_template"),
                chat_template=config.get("chat_template"),  # âœ… Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ chat_template Ð¼Ð¾Ð´ÐµÐ»Ð¸
                num_replicas=accelerator.num_processes,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                rank=accelerator.process_index,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                split="train",
                val_ratio=holdout_ratio,
                shard=ds_shard,
            )
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ (Ð´Ð»Ñ SFT)
            try:
                if hasattr(train_dataset, 'get_sample_prompt'):
                    sample_prompt = train_dataset.get_sample_prompt(max_samples=20)
                    if sample_prompt:
                        metrics.update(sample_prompt=sample_prompt)
                        logger.info("Sample prompt saved to metrics")
            except Exception as e:
                logger.warning(f"Failed to get sample prompt: {e}")
            
            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° (Ð¿Ð¾ÑÐ»Ðµ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´Ð° Ð±ÑƒÐ´ÐµÑ‚ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð°)
            if hasattr(train_dataset, 'num_replicas'):
                logger.info(f"Dataset initialized: num_replicas={train_dataset.num_replicas}, rank={train_dataset.rank}")
            
            # Ð”Ð»Ñ SFT data collator Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ masking, 
            # Ð½Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ð°Ð´Ð´Ð¸Ñ‚ÑŒ Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ñ‹ Ð² Ð±Ð°Ñ‚Ñ‡Ðµ.
            # SFTDataset ÑƒÐ¶Ðµ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ñ‹, DataCollatorForLanguageModeling Ñ mlm=False Ð¿Ð¾Ð´Ð¾Ð¹Ð´ÐµÑ‚
            # Ð¸Ð»Ð¸ default_data_collator ÐµÑÐ»Ð¸ labels ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ (Ð° Ð¾Ð½Ð¸ ÐµÑÑ‚ÑŒ Ð² SFTDataset)
            from transformers import default_data_collator
            collate_fn = default_data_collator
        else:
            # Pretrain Ð¸Ð»Ð¸ Continual Pretrain - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ StreamingTextDataset
            # Strict resume Ð´Ð»Ñ StreamingTextDataset (.jsonl): Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ byte_offset/global_idx Ð¸Ð· checkpoint
            resume_state = None
            resume_from_checkpoint_cfg = config.get("resume_from_checkpoint")
            strict_resume = bool(config.get("strict_dataloader_resume", True))
            effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
            if strict_resume and effective_shard_mode != "dataset":
                # Ð˜Ð½Ð°Ñ‡Ðµ Ð¼Ñ‹ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÐ¼ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÑ‚Ñ€Ð¾Ð³ÑƒÑŽ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð¿Ð¾ rank
                raise ValueError("strict_dataloader_resume=True requires sharding_mode='dataset' for streaming datasets")
            if resume_from_checkpoint_cfg and strict_resume:
                try:
                    p = Path(resume_from_checkpoint_cfg)
                    # per-rank state
                    rs_path = p / f"dataloader_state_rank{accelerator.process_index}.json"
                    if rs_path.exists():
                        with open(rs_path, "r", encoding="utf-8") as f:
                            resume_state = json.load(f)
                        logger.info(
                            f"Loaded dataloader resume state for rank {accelerator.process_index}: "
                            f"byte_offset={resume_state.get('byte_offset')} global_idx={resume_state.get('global_idx')}"
                        )
                except Exception as e:
                    logger.warning(f"Failed to load dataloader resume state: {e}")

            train_dataset = StreamingTextDataset(
                config["data_path"], 
                tokenizer, 
                seq_len=config["seq_len"],
                num_replicas=accelerator.num_processes,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                rank=accelerator.process_index,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                split="train",
                val_ratio=holdout_ratio,
                shard=(effective_shard_mode == "dataset"),
                resume_byte_offset=(resume_state.get("byte_offset", 0) if resume_state else 0),
                resume_global_idx=(resume_state.get("global_idx", 0) if resume_state else 0),
                strict_resume=strict_resume,
            )
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ (Ð´Ð»Ñ pretrain/continual_pretrain)
            try:
                if hasattr(train_dataset, 'get_sample_prompt'):
                    sample_prompt = train_dataset.get_sample_prompt(max_samples=20)
                    if sample_prompt:
                        metrics.update(sample_prompt=sample_prompt)
                        logger.info("Sample prompt saved to metrics")
            except Exception as e:
                logger.warning(f"Failed to get sample prompt: {e}")
            
            # Ð’ÐÐ–ÐÐž: Ð”Ð»Ñ pretrain Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ collator, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð°ÑÐºÐ¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾ attention_mask,
            # Ð° Ð½Ðµ Ð¿Ð¾ pad_token_id. Ð­Ñ‚Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾, ÐµÑÐ»Ð¸ pad_token = eos_token (EOS Ð½Ðµ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¼Ð°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ)
            def causal_lm_collator(batch):
                """ÐšÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ collator Ð´Ð»Ñ pretrain, Ð¼Ð°ÑÐºÐ¸Ñ€ÑƒÐµÑ‚ labels Ð¿Ð¾ attention_mask."""
                # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð±Ð¸Ñ‚Ñ‹Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ (ÐµÑÐ»Ð¸ dataset Ð²ÐµÑ€Ð½ÑƒÐ» None)
                batch = [x for x in batch if x is not None]
                if not batch:
                     # ÐŸÑƒÑÑ‚Ð¾Ð¹ Ð±Ð°Ñ‚Ñ‡ Ð½Ðµ Ð´Ð¾Ð»Ð¶ÐµÐ½ ÐºÑ€Ð°ÑˆÐ¸Ñ‚ÑŒ accelerate, Ð²ÐµÑ€Ð½ÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ñ‹ Ð¸Ð»Ð¸ Ð¿ÑƒÑÑ‚Ð¾Ð¹ dict
                     # ÐÐ¾ accelerate Ð¼Ð¾Ð¶ÐµÑ‚ Ð½Ðµ Ð¿ÐµÑ€ÐµÐ²Ð°Ñ€Ð¸Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ð¾Ð¹ dict Ð¿Ñ€Ð¸ broadcast, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð²ÐµÑ€Ð½ÐµÐ¼ dummy batch
                     # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑˆÐ°Ð³
                     dummy = torch.zeros((1, 1), dtype=torch.long)
                     return {"input_ids": dummy, "attention_mask": dummy, "labels": dummy}

                input_ids = torch.stack([x["input_ids"] for x in batch])
                attention_mask = torch.stack([x["attention_mask"] for x in batch])
                labels = input_ids.clone()
                labels[attention_mask == 0] = -100  # ÐœÐ°ÑÐºÐ¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ padding, Ð½Ðµ EOS
                return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
            
            collate_fn = causal_lm_collator
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ validation loader ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        val_loader = None
        
        if holdout_ratio > 0.0:
            # Ð’ÐÐ–ÐÐž: Ð¨Ð°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ»Ð°ÐµÑ‚ accelerate.prepare() Ñ‡ÐµÑ€ÐµÐ· DataLoaderShard/Dispatcher
            # Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´ÐµÐ»Ð°ÐµÑ‚ train/val split, Ð½Ð¾ ÐÐ• ÑˆÐ°Ñ€Ð´Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÐµÐ¶Ð´Ñƒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼Ð¸ (shard=False)
            if stage == "sft":
                # SFTDataset ÑƒÐ¶Ðµ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾
                # reuse effective_shard_mode from above if set, otherwise compute here
                effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
                ds_shard = (effective_shard_mode == "dataset")
                val_dataset = SFTDataset(
                    config["data_path"],
                    tokenizer,
                    seq_len=config["seq_len"],
                    sft_columns=config.get("sft_columns"),
                    sft_template=config.get("sft_template"),
                    chat_template=config.get("chat_template"),  # âœ… Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ chat_template Ð¼Ð¾Ð´ÐµÐ»Ð¸
                    num_replicas=accelerator.num_processes,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                    rank=accelerator.process_index,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                    split="val",
                    val_ratio=holdout_ratio,
                    shard=ds_shard,
                )
                from transformers import default_data_collator
                val_collate = default_data_collator
            else:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑƒÐ¶Ðµ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ StreamingTextDataset
                effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
                val_dataset = StreamingTextDataset(
                    config["data_path"],
                    tokenizer,
                    seq_len=config["seq_len"],
                    num_replicas=accelerator.num_processes,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                    rank=accelerator.process_index,  # âœ… Ð¯Ð²Ð½Ð¾Ðµ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                    split="val",
                    val_ratio=holdout_ratio,
                    shard=(effective_shard_mode == "dataset"),
                    strict_resume=False,  # val Ð½Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ð¸Ð¼ ÑÑ‚Ñ€Ð¾Ð³Ð¾
                )
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾Ñ‚ Ð¶Ðµ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ collator Ð´Ð»Ñ val, Ñ‡Ñ‚Ð¾ Ð¸ Ð´Ð»Ñ train
                def causal_lm_collator(batch):
                    """ÐšÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ collator Ð´Ð»Ñ pretrain, Ð¼Ð°ÑÐºÐ¸Ñ€ÑƒÐµÑ‚ labels Ð¿Ð¾ attention_mask."""
                    batch = [x for x in batch if x is not None]
                    if not batch:
                        dummy = torch.zeros((1, 1), dtype=torch.long)
                        return {"input_ids": dummy, "attention_mask": dummy, "labels": dummy}

                    input_ids = torch.stack([x["input_ids"] for x in batch])
                    attention_mask = torch.stack([x["attention_mask"] for x in batch])
                    labels = input_ids.clone()
                    labels[attention_mask == 0] = -100  # ÐœÐ°ÑÐºÐ¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ padding, Ð½Ðµ EOS
                    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
                
                val_collate = causal_lm_collator
            
            # Ð’ÐÐ–ÐÐž: num_workers=0 Ð´Ð»Ñ val, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð¿Ñ€Ð¸ shard=False
            val_loader = DataLoader(
                val_dataset,
                batch_size=config["batch_size"],
                collate_fn=val_collate,
                num_workers=0,  # Ð‘ÐµÐ· workers Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²
            )
            
        # Ð’ÐÐ–ÐÐž: num_workers=0 Ð´Ð»Ñ IterableDataset, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        # drop_last=True Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ DDP (Ð²ÑÐµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÑ‚ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹)
        train_loader = DataLoader(
            train_dataset,
            batch_size=config["batch_size"],
            collate_fn=collate_fn,
            num_workers=0,  # IterableDataset + num_workers>0 = Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            drop_last=True,  # âœ… ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾ Ð´Ð»Ñ DDP: Ð¸Ð·Ð±ÐµÐ³Ð°ÐµÐ¼ Ñ€Ð°ÑÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð° Ð¿Ð¾ Ñ‡Ð¸ÑÐ»Ñƒ ÑˆÐ°Ð³Ð¾Ð² Ð¼ÐµÐ¶Ð´Ñƒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼Ð¸
        )
        
        metrics.update(status="building_model")
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€
        resume_from_checkpoint = None
        base_model_path = config.get("base_model_path")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ ÑÑ‚Ð¾ accelerate checkpoint (Ð´Ð»Ñ resume)
        # Ð’ÐÐ–ÐÐž: pytorch_model.bin.index.json Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¸ Ñƒ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ñ… ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… HF-ÑÐµÐ¹Ð²Ð¾Ð²,
        # Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ accelerator_state.json - ÑÑ‚Ð¾ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð·Ð½Ð°Ðº accelerate checkpoint
        def is_accelerate_checkpoint(p: Path) -> bool:
            """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð¿ÑƒÑ‚ÑŒ accelerate checkpoint'Ð¾Ð¼."""
            return (p / "accelerator_state.json").exists()
        
        if base_model_path:
            base_path = Path(base_model_path)
            is_checkpoint = is_accelerate_checkpoint(base_path)
            
            if is_checkpoint and stage == "continual_pretrain":
                resume_from_checkpoint = str(base_path)
                logger.info(f"Continual pretraining: will resume from accelerate checkpoint {base_path}")
            elif is_checkpoint and stage != "continual_pretrain":
                logger.warning(
                    f"Found accelerate checkpoint at {base_path}, but stage is {stage}. "
                    f"Resume from checkpoint is only supported for continual_pretrain. "
                    f"Will load as regular model instead."
                )
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€
        model, model_config = adapter.load_for_training(
            base_model_path=base_model_path,
            stage=stage,
            tokenizer=tokenizer,
            config=config,
            trust_remote_code=True,
        )
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ max_position_embeddings Ð¸ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ seq_len ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        model_max_pos = getattr(model.config, "max_position_embeddings", config.get("seq_len", 2048))
        user_seq_len = config.get("seq_len", 2048)
        if user_seq_len > model_max_pos:
            logger.warning(
                f"âš ï¸ seq_len ({user_seq_len}) > model max_position_embeddings ({model_max_pos}). "
                f"Truncating to {model_max_pos} to avoid index out of bounds errors."
            )
            config["seq_len"] = model_max_pos
        
        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (resize, LoRA, use_cache, etc.)
        model = adapter.prepare_for_training(model, tokenizer, config)
        
        # ============================================================
        # Liger Kernel Ð¿Ð°Ñ‚Ñ‡Ð¸Ð½Ð³ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ RMSNorm, RoPE, MLP, FusedCE)
        # ============================================================
        use_liger = config.get("use_liger", False)
        liger_fused_ce = config.get("liger_fused_ce", False)  # Fused lm_head + CE (ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‚ Ð¿Ð°Ð¼ÑÑ‚ÑŒ!)
        
        # ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð´Ð»Ñ fused CE loss (ÐµÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾)
        liger_fused_ce_loss = None
        
        if use_liger and LIGER_UTILS_AVAILABLE and is_liger_available():
            try:
                model_type = getattr(model.config, "model_type", "").lower()
                
                # ÐŸÐ°Ñ‚Ñ‡Ð¸Ð¼ RMSNorm, RoPE, MLP
                patched = apply_liger_patch_to_model(
                    model,
                    patch_rms_norm=True,
                    patch_rope=True,
                    patch_mlp=True,
                    patch_fused_linear_ce=False,  # Ð‘ÑƒÐ´ÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ loss module
                )
                if patched:
                    logger.info("âœ… Liger Kernel Ð¿Ð°Ñ‚Ñ‡Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ñ‹ (RMSNorm, RoPE, MLP)")
                else:
                    logger.info("â„¹ï¸ Liger: Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð´Ð»Ñ Ð¿Ð°Ñ‚Ñ‡Ð¸Ð½Ð³Ð° (home Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð´Ñ€.)")
                
                # ðŸ”¥ Fused Linear CrossEntropy â€” ÐÐ• Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·ÑƒÐµÑ‚ logits!
                # Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‚ Ð³Ð¸Ð³Ð°Ð±Ð°Ð¹Ñ‚Ñ‹ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¼ vocab_size
                # Ð’ÐÐ–ÐÐž: Fused CE Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð´Ð»Ñ Ð›Ð®Ð‘ÐžÐ™ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ lm_head, Ð½Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ HF Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹!
                # âš ï¸ Ð”Ð»Ñ ZeRO-3 Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‘Ð¼ accelerator Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð³Ð¾ ÑÐ±Ð¾Ñ€Ð° ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
                if liger_fused_ce:
                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐµÑÑ‚ÑŒ Ð»Ð¸ lm_head Ñƒ Ð¼Ð¾Ð´ÐµÐ»Ð¸
                    has_lm_head = hasattr(model, 'lm_head') and model.lm_head is not None
                    if has_lm_head:
                        try:
                            liger_fused_ce_loss = create_liger_fused_ce(
                                model,
                                ignore_index=-100,
                                label_smoothing=config.get("label_smoothing", 0.0),
                                accelerator=accelerator,  # âœ… Ð”Ð»Ñ ZeRO-3 Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸
                            )
                            if liger_fused_ce_loss:
                                logger.info("ðŸ¦ LigerFusedLinearCrossEntropyLoss Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ð½!")
                                logger.info("   âš¡ Logits ÐÐ• Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·ÑƒÑŽÑ‚ÑÑ â€” ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸!")
                            else:
                                logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ LigerFusedCELoss, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹")
                        except Exception as e:
                            logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ LigerFusedCELoss: {e}")
                    else:
                        logger.info(f"â„¹ï¸ LigerFusedCE: Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð¸Ð¼ÐµÐµÑ‚ lm_head, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Liger Ð¿Ð°Ñ‚Ñ‡Ð¸: {e}")
        elif use_liger and not LIGER_UTILS_AVAILABLE:
            logger.warning("âš ï¸ Liger Ð·Ð°Ð¿Ñ€Ð¾ÑˆÐµÐ½, Ð½Ð¾ liger_utils Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")
        elif use_liger and not is_liger_available():
            logger.warning("âš ï¸ Liger Ð·Ð°Ð¿Ñ€Ð¾ÑˆÐµÐ½, Ð½Ð¾ liger-kernel Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½")

        # Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°: dtype Ð²ÐµÑÐ¾Ð² Ð¸ Ð²ÐºÐ»ÑŽÑ‡Ñ‘Ð½ Ð»Ð¸ SDPA/flash path Ñƒ HomeModel
        try:
            first = next(model.parameters())
            logger.info(f"ðŸ”Ž Model weights dtype (first param): {first.dtype}")
        except Exception:
            pass
        try:
            flash_mods = [m for m in model.modules() if hasattr(m, "flash")]
            if flash_mods:
                enabled = sum(1 for m in flash_mods if bool(getattr(m, "flash", False)))
                logger.info(f"ðŸ”Ž SDPA/flash modules: {enabled}/{len(flash_mods)} enabled")
        except Exception:
            pass
        
        # ÐŸÐ¾Ð´ÑÑ‡Ñ‘Ñ‚ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
        num_params = sum(p.numel() for p in model.parameters())
        metrics.update(num_parameters=num_params)
        
        if config.get("grad_checkpoint", False):
            model.gradient_checkpointing_enable()
        
        # Scheduler - Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑˆÐ°Ð³Ð¾Ð²
        # Ð•ÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ max_steps - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾
        if config.get("max_steps"):
            max_train_steps = config["max_steps"]
        else:
            # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð»Ð¸Ð½Ñƒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°
            # Ð’ÐÐ–ÐÐž: len(train_dataset) Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ð´Ð»Ð¸Ð½Ñƒ Ð´Ð»Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð³Ð¾ split
            # ÐÐ¾ Ð¿Ñ€Ð¸ DDP/FSDP/ZeRO accelerate.prepare() ÑˆÐ°Ñ€Ð´Ð¸Ñ€ÑƒÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼Ð¸
            # ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ ÑƒÐ²Ð¸Ð´Ð¸Ñ‚ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾ dataset_len / num_processes Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²
            try:
                dataset_len = len(train_dataset)  # ÐŸÐ¾Ð»Ð½Ð°Ñ Ð´Ð»Ð¸Ð½Ð° Ð´Ð»Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð³Ð¾ split
                # Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼ (Ð´Ð»Ñ IterableDataset ÑÑ‚Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾)
                per_proc_len = math.ceil(dataset_len / accelerator.num_processes)
                
                # batch_size Ð·Ð´ÐµÑÑŒ per-device, gradient_accumulation ÑƒÐ¶Ðµ ÑƒÑ‡Ñ‚ÐµÐ½
                steps_per_epoch = math.ceil(per_proc_len / config["batch_size"])
                num_update_steps_per_epoch = math.ceil(steps_per_epoch / config["gradient_accumulation"])
                max_train_steps = config["epochs"] * num_update_steps_per_epoch
            except (TypeError, AttributeError):
                # Ð”Ð»Ñ streaming dataset Ð±ÐµÐ· __len__ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ save_every * 10 ÐºÐ°Ðº Ð¾Ñ†ÐµÐ½ÐºÑƒ
                max_train_steps = config.get("save_every", 5000) * 2
        
        planned_total_steps = int(max_train_steps)
        metrics.update(
            total_steps=planned_total_steps,
            planned_total_steps=planned_total_steps,
            max_steps_estimated=config.get("max_steps") is None,
        )
        
        # Ð’ÐÐ–ÐÐž: Ð”Ð»Ñ LoRA/QLoRA Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ€Ð°Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ trainable Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
        # Ð­Ñ‚Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾ Ð´Ð»Ñ QLoRA, Ð³Ð´Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð²ÐµÑÐ° Ð·Ð°Ð¼Ð¾Ñ€Ð¾Ð¶ÐµÐ½Ñ‹ Ð¸ Ð¾Ð³Ñ€Ð¾Ð¼Ð½Ñ‹Ðµ
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        if len(trainable_params) == 0:
            raise ValueError("No trainable parameters found in model. Check LoRA/QLoRA configuration.")
        
        logger.info(f"Optimizing {len(trainable_params)} trainable parameters "
                   f"(total: {sum(p.numel() for p in model.parameters())})")
        
        uses_deepspeed = getattr(accelerator.state, "deepspeed_plugin", None) is not None
        optimizer_name = str(config.get("optimizer", "adamw")).strip().lower()
        lr = float(config["learning_rate"])
        wd = float(config.get("weight_decay", 0.1))
        betas = tuple(config.get("betas", (0.9, 0.95)))
        eps = float(config.get("eps", 1e-8))

        if optimizer_name in ("adamw_8bit", "adamw8bit"):
            if uses_deepspeed:
                raise RuntimeError(
                    "Optimizer 'adamw_8bit' is not supported with DeepSpeed in this training path."
                )
            else:
                try:
                    from bitsandbytes.optim import AdamW8bit
                    optimizer = AdamW8bit(
                        trainable_params,
                        lr=lr,
                        betas=betas,
                        eps=eps,
                        weight_decay=wd,
                    )
                except ImportError:
                    raise RuntimeError(
                        "Optimizer 'adamw_8bit' requested, but bitsandbytes is not installed."
                    )
        elif optimizer_name in ("magma_adamw", "magma"):
            optimizer_name = "magma_adamw"
            optimizer = MagmaAdamW(
                trainable_params,
                lr=lr,
                betas=betas,
                eps=eps,
                weight_decay=wd,
                magma_prob=float(config.get("magma_prob", 0.5)),
                magma_tau=float(config.get("magma_tau", 2.0)),
                magma_ema_beta=float(config.get("magma_ema_beta", 0.9)),
                magma_cosine_eps=float(config.get("magma_cosine_eps", 1e-12)),
            )
        elif optimizer_name == "muon":
            if uses_deepspeed:
                raise RuntimeError(
                    "Optimizer 'muon' currently does not support DeepSpeed mode in this training path."
                )
            ns_coeff = config.get("muon_ns_coefficients", [3.4445, -4.775, 2.0315])
            if not isinstance(ns_coeff, (list, tuple)) or len(ns_coeff) != 3:
                raise ValueError(
                    f"muon_ns_coefficients must be a list/tuple of 3 floats, got: {ns_coeff}"
                )

            adjust_lr_fn = config.get("muon_adjust_lr_fn", None)
            if adjust_lr_fn not in (None, "original", "match_rms_adamw"):
                raise ValueError(
                    f"muon_adjust_lr_fn must be one of None|'original'|'match_rms_adamw', got: {adjust_lr_fn}"
                )

            optimizer = HybridMuonAdamW(
                trainable_params,
                lr=lr,
                weight_decay=wd,
                muon_momentum=float(config.get("muon_momentum", 0.95)),
                muon_nesterov=bool(config.get("muon_nesterov", True)),
                muon_ns_coefficients=(float(ns_coeff[0]), float(ns_coeff[1]), float(ns_coeff[2])),
                muon_eps=eps,
                muon_ns_steps=int(config.get("muon_ns_steps", 5)),
                muon_adjust_lr_fn=adjust_lr_fn,
                adamw_betas=betas,
                adamw_eps=eps,
            )
        else:
            optimizer_name = "adamw"
            optimizer = torch.optim.AdamW(
                trainable_params,  # âœ… Ð¢Ð¾Ð»ÑŒÐºÐ¾ trainable Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ (ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾ Ð´Ð»Ñ LoRA/QLoRA)
                lr=lr,
                betas=betas,
                eps=eps,
                weight_decay=wd,
            )

        logger.info(f"Using optimizer: {optimizer_name}")
        metrics.update(
            optimizer=optimizer_name,
            weight_decay=wd,
            betas=[float(betas[0]), float(betas[1])],
            eps=eps,
        )
        # LR scheduler
        # Ð’ÐÐ–ÐÐž: Ð¼Ñ‹ ÑˆÐ°Ð³Ð°ÐµÐ¼ scheduler Ð½Ð° UPDATE-step (ÐºÐ¾Ð³Ð´Ð° accelerator.sync_gradients=True).
        # Ð”Ð»Ñ resume Ð¸Ð· ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð¾Ð² ÑÑ‚Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾: Ñ€Ð°Ð½ÑŒÑˆÐµ scheduler Ð¼Ð¾Ð³ ÑˆÐ°Ð³Ð°Ñ‚ÑŒ Ð¿Ð¾ micro-step,
        # Ð¸ Ñ‚Ð¾Ð³Ð´Ð° Ð½Ð° resume LR ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ ~0.
        lr_schedule = str(config.get("lr_schedule", "cosine")).strip().lower()
        min_lr_ratio = float(config.get("min_lr_ratio", 0.0) or 0.0)
        warmup_steps = int(config.get("warmup_steps", 0))
        total_steps_for_sched = int(max_train_steps)

        if lr_schedule in ("cosine", "cosine_with_warmup") and min_lr_ratio > 0:
            import math as _math
            from torch.optim.lr_scheduler import LambdaLR

            def lr_lambda(step: int) -> float:
                if warmup_steps > 0 and step < warmup_steps:
                    return float(step) / float(max(1, warmup_steps))
                if total_steps_for_sched <= warmup_steps:
                    return 1.0
                progress = float(step - warmup_steps) / float(max(1, total_steps_for_sched - warmup_steps))
                progress = min(max(progress, 0.0), 1.0)
                cosine = 0.5 * (1.0 + _math.cos(_math.pi * progress))
                return float(min_lr_ratio + (1.0 - min_lr_ratio) * cosine)

            lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)
        else:
            # get_scheduler Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚: linear/cosine/constant/cosine_with_restarts/...
            # Ð”Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸: ÑÑ‚Ð°Ñ€Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ "cosine_with_warmup" Ð¼Ð°Ð¿Ð¿Ð¸Ð¼ Ð² "cosine".
            sched_name = "cosine" if lr_schedule == "cosine_with_warmup" else lr_schedule
            lr_scheduler = get_scheduler(
                name=sched_name,
                optimizer=optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps_for_sched,
            )

        metrics.update(
            scheduler=str(lr_schedule),
            warmup_steps=int(warmup_steps),
            min_lr_ratio=float(min_lr_ratio),
        )
        
        # IMPORTANT (Sharding contract):
        # Ð£ Ð½Ð°Ñ Ð”Ð’Ð Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ñ… ÑÐ¿Ð¾ÑÐ¾Ð±Ð° ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…:
        #  1) dataset-level (StreamingTextDataset/SFTDataset Ñ shard=True, ÑÐ²Ð½Ñ‹Ð¹ num_replicas/rank)
        #  2) accelerate-level (accelerator.prepare(DataLoader) -> DataLoaderShard/Dispatcher)
        #
        # ÐÐ•Ð›Ð¬Ð—Ð¯ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ‚ÑŒ Ð¾Ð±Ð° Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ â€” Ð¸Ð½Ð°Ñ‡Ðµ Ð±ÑƒÐ´ÐµÑ‚ "Ð´Ð²Ð¾Ð¹Ð½Ð¾Ð¹ ÑˆÐ°Ñ€Ð´Ð¸Ð½Ð³" Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ/ÑˆÐ°Ð³Ð¸ ÑÑ‚Ð°Ð½ÑƒÑ‚ Ð¼ÐµÐ½ÑŒÑˆÐµ Ð² N Ñ€Ð°Ð·.
        # Ð”Ð»Ñ IterableDataset Ð¼Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ dataset-level ÑˆÐ°Ñ€Ð´Ð¸Ð½Ð³ ÐºÐ°Ðº Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¿Ñ€Ð°Ð²Ð´Ñ‹.
        is_streaming_sharded = isinstance(train_dataset, IterableDataset) and getattr(train_dataset, "shard", False) is True
        effective_shard_mode = "dataset" if is_streaming_sharded else "accelerate"
        if accelerator.is_main_process:
            metrics.update(
                sharding_mode=effective_shard_mode,
                sharding_mode_requested=sharding_mode,
                num_processes=int(accelerator.num_processes),
            )

        # FSDP: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¹ transformer layer ÐºÐ»Ð°ÑÑ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ ÐµÑÑ‚ÑŒ Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸.
        # Ð”ÐµÐ»Ð°Ñ‚ÑŒ ÑÑ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ Ð”Ðž accelerator.prepare(), Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ Ñ€ÐµÐ¶Ð¸Ð¼Ð° ÑˆÐ°Ñ€Ð´Ð¸Ð½Ð³Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ….
        fsdp_plugin = getattr(accelerator.state, "fsdp_plugin", None)
        if fsdp_plugin is not None:
            def _model_has_layer_class(class_name: str) -> bool:
                for _m in model.modules():
                    if _m.__class__.__name__ == class_name:
                        return True
                return False

            def _infer_fsdp_wrap_cls():
                skip = {
                    "ModuleList", "ModuleDict", "Sequential", "Embedding", "Linear",
                    "LayerNorm", "RMSNorm", "Dropout", "SiLU", "GELU", "ReLU",
                }
                counts = {}
                cls_by_name = {}
                for _m in model.modules():
                    name = _m.__class__.__name__
                    if name in skip:
                        continue
                    if name.endswith("DecoderLayer") or name.endswith("Block") or name.endswith("Layer"):
                        counts[name] = counts.get(name, 0) + 1
                        cls_by_name[name] = _m.__class__
                if not counts:
                    return None
                best_name = max(counts, key=counts.get)
                if counts[best_name] < 2:
                    return None
                return cls_by_name[best_name]

            # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¸Ð· plugin (Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ set/frozenset)
            cfg_names = []
            current_wrap = getattr(fsdp_plugin, "transformer_cls_names_to_wrap", None)
            if current_wrap:
                if isinstance(current_wrap, (set, frozenset, list, tuple)):
                    cfg_names = [n for n in current_wrap if isinstance(n, str)]
                elif isinstance(current_wrap, str):
                    cfg_names = [current_wrap]
            # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð¸ "auto" Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
            cfg_names = [n for n in cfg_names if n and n.lower() != "auto"]

            # Ð•ÑÐ»Ð¸ ÐºÐ»Ð°ÑÑ Ð½Ðµ Ð·Ð°Ð´Ð°Ð½, "auto", Ð¸Ð»Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸ â€” Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸
            need_infer = not cfg_names or not any(_model_has_layer_class(n) for n in cfg_names)
            if need_infer:
                inferred_cls = _infer_fsdp_wrap_cls()
                if inferred_cls is not None:
                    inferred_name = inferred_cls.__name__
                    # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð² Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚ accelerate
                    fsdp_plugin.transformer_cls_names_to_wrap = {inferred_name}
                    logger.info(
                        f"âœ… FSDP: Ð°Ð²Ñ‚Ð¾Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½ transformer layer ÐºÐ»Ð°ÑÑ: {inferred_name}"
                    )
                else:
                    logger.warning(
                        "âš ï¸ FSDP: Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ transformer layer ÐºÐ»Ð°ÑÑ. "
                        "Ð•ÑÐ»Ð¸ prepare() ÑƒÐ¿Ð°Ð´Ñ‘Ñ‚, ÑƒÐºÐ°Ð¶Ð¸Ñ‚Ðµ fsdp_transformer_layer_cls_to_wrap Ð² ÐºÐ¾Ð½Ñ„Ð¸Ð³Ðµ."
                    )
        if is_streaming_sharded:
            if val_loader is not None:
                model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
            else:
                model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
            logger.info("Using dataset-level sharding for IterableDataset -> skipping accelerator.prepare() for DataLoader(s)")
        else:
            if val_loader is not None:
                model, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(
                    model, optimizer, train_loader, val_loader, lr_scheduler
                )
            else:
                model, optimizer, train_loader, lr_scheduler = accelerator.prepare(
                    model, optimizer, train_loader, lr_scheduler
                )

        # ÐŸÐ¾ÑÐ»Ðµ accelerator.prepare Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ð±Ñ‘Ñ€Ð½ÑƒÑ‚Ð° (FSDP/DeepSpeed).
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÑÑ‹Ð»ÐºÑƒ Ð² LigerFusedCEModule, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ ÑÐ¾Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹.
        if liger_fused_ce_loss is not None and hasattr(liger_fused_ce_loss, "set_model"):
            try:
                liger_fused_ce_loss.set_model(model)
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ (FSDP/DTensor Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ)
                logger.info("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ is_supported()...")
                if hasattr(liger_fused_ce_loss, "is_supported"):
                    is_supported = liger_fused_ce_loss.is_supported()
                    logger.info(f"ðŸ” is_supported() Ð²ÐµÑ€Ð½ÑƒÐ»: {is_supported}")
                    if not is_supported:
                        logger.info("ðŸ¦ Liger fused CE Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½ (Ð½ÐµÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ Ñ FSDP/DTensor), Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ path")
                        liger_fused_ce_loss = None  # ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ fused CE
                else:
                    logger.warning("âš ï¸ is_supported() Ð¼ÐµÑ‚Ð¾Ð´ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
            except Exception as e:
                logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð² set_model/is_supported: {e}")
                import traceback
                logger.warning(traceback.format_exc())
        
        # Resume Ð¸Ð· checkpoint (ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ð²ÑÐµÑ… ÑÑ‚Ð°Ð´Ð¸Ð¹)
        starting_step = 0
        resume_batches_to_skip = 0
        
        # Ð•ÑÐ»Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚ resume_from_checkpoint Ñ‡ÐµÑ€ÐµÐ· ÐºÐ¾Ð½Ñ„Ð¸Ð³ (Ð¾Ñ‚ CLI)
        if config.get("resume_from_checkpoint"):
            resume_from_checkpoint = config["resume_from_checkpoint"]
        # Ð˜Ð»Ð¸ ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ continual_pretrain Ð¸ Ð¼Ñ‹ Ð½Ð°ÑˆÐ»Ð¸ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚ Ð² base_model_path
        elif stage == "continual_pretrain" and base_model_path:
             if is_accelerate_checkpoint(Path(base_model_path)):
                 resume_from_checkpoint = base_model_path

        if resume_from_checkpoint:
            try:
                logger.info(f"Resuming training from checkpoint: {resume_from_checkpoint}")
                accelerator.load_state(resume_from_checkpoint)
                
                # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ global_step Ð¸Ð· checkpoint
                checkpoint_meta = Path(resume_from_checkpoint) / "checkpoint_metadata.json"
                if checkpoint_meta.exists():
                    with open(checkpoint_meta) as f:
                        meta = json.load(f)
                        starting_step = meta.get("global_step", 0)
                else:
                    # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¸Ð· Ð¸Ð¼ÐµÐ½Ð¸ Ð¿Ð°Ð¿ÐºÐ¸ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ checkpoint_step1000)
                    import re
                    match = re.search(r'step(\d+)', str(resume_from_checkpoint))
                    if match:
                        starting_step = int(match.group(1))
                
                logger.info(f"Resumed from step {starting_step}")
                metrics.update(status="resumed", resumed_from_step=starting_step)
                
                # Ð’ÐÐ–ÐÐž: Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð´Ð°Ñ‚Ð°Ð»Ð¾Ð°Ð´ÐµÑ€Ð°
                #
                # - Ð”Ð»Ñ map-style Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð² accelerate.load_state() Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ sampler state, Ð¸ Ð´Ð¾Ð¿. skip ÐÐ• Ð½ÑƒÐ¶ÐµÐ½.
                # - Ð”Ð»Ñ IterableDataset sampler state Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ => ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ "ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ" â€” Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾
                #   Ð¿Ñ€Ð¾Ð³Ð½Ð°Ñ‚ÑŒ N Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ Ð¸ Ð²Ñ‹Ð±Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ. Ð­Ñ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð¼Ð½Ð¾Ð³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ (Ð¸ Ð²Ñ‹Ð³Ð»ÑÐ´Ð¸Ñ‚ ÐºÐ°Ðº Ð·Ð°Ð²Ð¸ÑÐ°Ð½Ð¸Ðµ),
                #   Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð´ÐµÐ»Ð°ÐµÐ¼ ÑÑ‚Ð¾ ÑÐ²Ð½Ð¾ Ñ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð»Ð¾Ð³Ð°Ð¼Ð¸ Ð² Ð¿ÐµÑ€Ð²Ð¾Ð¼ epoch.
                if starting_step > 0 and isinstance(train_dataset, IterableDataset):
                    resume_skip_enabled = bool(config.get("resume_skip_batches", True))
                    # Ð•ÑÐ»Ð¸ Ñƒ StreamingTextDataset Ð²ÐºÐ»ÑŽÑ‡Ñ‘Ð½ strict_resume Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ byte_offset, skip Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½.
                    try:
                        if hasattr(train_dataset, "get_resume_state"):
                            rs = train_dataset.get_resume_state()
                            if rs.get("byte_offset", 0) and bool(config.get("strict_dataloader_resume", True)):
                                logger.info("Strict dataloader resume active (byte_offset present) -> skipping batches is NOT required.")
                                resume_skip_enabled = False
                    except Exception:
                        pass
                    if resume_skip_enabled:
                        resume_batches_to_skip = int(starting_step) * int(config["gradient_accumulation"])
                        # ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð»Ð¸Ð¼Ð¸Ñ‚, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ "Ð²Ð¸ÑÐµÑ‚ÑŒ" Ñ‡Ð°ÑÐ°Ð¼Ð¸ Ð¿Ñ€Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¼ starting_step
                        max_skip = config.get("resume_skip_batches_max")
                        if max_skip is not None:
                            max_skip = int(max_skip)
                            if resume_batches_to_skip > max_skip:
                                logger.warning(
                                    f"Requested to skip {resume_batches_to_skip} batches, but resume_skip_batches_max={max_skip}. "
                                    f"Capping skip to {max_skip}. (May slightly desync dataloader position.)"
                                )
                                resume_batches_to_skip = max_skip
                        logger.info(f"Will skip {resume_batches_to_skip} batches (IterableDataset) to sync dataloader...")
                        metrics.update(status="skipping", skipping_batches=resume_batches_to_skip)
                    else:
                        logger.warning("resume_skip_batches=False: will NOT skip dataloader batches for IterableDataset (data order may differ after resume).")
                        resume_batches_to_skip = 0
                    
            except Exception as e:
                logger.error(f"Failed to resume from checkpoint {resume_from_checkpoint}: {e}")
                # Ð•ÑÐ»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð¯Ð’ÐÐž Ð¿Ñ€Ð¾ÑÐ¸Ð» resume (Ñ‡ÐµÑ€ÐµÐ· Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚ Ð¸Ð»Ð¸ ÐºÐ¾Ð½Ñ„Ð¸Ð³) - Ð¼Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¿Ð°Ð´Ð°Ñ‚ÑŒ, Ð° Ð½Ðµ Ð¼Ð¾Ð»Ñ‡Ð° Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ
                # Ð˜ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ: auto-resume Ð¿Ñ€Ð¸ continual_pretrain (base_model_path) - Ñ‚ÑƒÑ‚ Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð°Ñ€Ð½Ð¸Ð½Ð³
                is_explicit_resume = config.get("resume_from_checkpoint") is not None
                
                if is_explicit_resume:
                    metrics.update(status="error", resume_error=str(e), error=f"Resume failed: {e}")
                    raise RuntimeError(f"Could not resume from checkpoint {resume_from_checkpoint}: {e}")
                else:
                    logger.warning("Continuing without resume (starting from step 0)")
                    metrics.update(status="resume_failed", resume_error=str(e))

        
        output_dir = Path(config["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ run_config.json Ð² Ð¿Ð°Ð¿ÐºÑƒ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð° Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¿Ñ€Ð¸ resume
        if accelerator.is_main_process:
            run_config_path = output_dir / "run_config.json"
            if not run_config_path.exists():  # ÐÐµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¸ resume
                with open(run_config_path, "w", encoding="utf-8") as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                logger.info(f"Saved run_config.json to {run_config_path}")
        
        metrics.update(status="training")
        
        # Ð’ÐÐ–ÐÐž: Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ global_step Ñ starting_step Ð¿Ð¾ÑÐ»Ðµ resume
        # Ð­Ñ‚Ð¾ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾ Ð´Ð»Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ LR scheduler Ð¸ checkpointing
        global_step = starting_step
        # Ð’ÐÐ–ÐÐž: Ñ€ÐµÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ scheduler Ðº update-step Ð¸Ð½Ð´ÐµÐºÑÑƒ.
        # Ð­Ñ‚Ð¾ Ð»ÐµÑ‡Ð¸Ñ‚ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÑŽ, ÐºÐ¾Ð³Ð´Ð° scheduler Ð² Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ðµ Ð±Ñ‹Ð» "Ð¿Ñ€Ð¾ÐºÑ€ÑƒÑ‡ÐµÐ½" Ð¿Ð¾ micro-step Ð¸ LR ÑƒÐ»ÐµÑ‚Ð°ÐµÑ‚ Ð² ~0.
        if starting_step > 0 and bool(config.get("scheduler_resync_on_resume", True)):
            try:
                lr_scheduler.step(int(global_step))
                if accelerator.is_main_process:
                    metrics.update(resume_scheduler_resynced=True, resume_scheduler_step=int(global_step))
            except TypeError:
                # fallback Ð´Ð»Ñ scheduler-Ð¾Ð² Ð±ÐµÐ· Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð° step(epoch)
                try:
                    lr_scheduler.last_epoch = int(global_step)
                    lr_scheduler.step()
                    if accelerator.is_main_process:
                        metrics.update(resume_scheduler_resynced=True, resume_scheduler_step=int(global_step))
                except Exception as e:
                    logger.warning(f"Failed to resync LR scheduler on resume: {e}")
                    if accelerator.is_main_process:
                        metrics.update(resume_scheduler_resynced=False, resume_scheduler_error=str(e))
            except Exception as e:
                logger.warning(f"Failed to resync LR scheduler on resume: {e}")
                if accelerator.is_main_process:
                    metrics.update(resume_scheduler_resynced=False, resume_scheduler_error=str(e))
        
        # Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ eval_batches Ð¸Ð· Ð·Ð°Ð¼Ñ‹ÐºÐ°Ð½Ð¸Ñ)
        def evaluate(val_loader):
            """
            Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ Ð½Ð° val_loader.
            
            Ð’ÐÐ–ÐÐž: val_loader Ð·Ð°ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð², Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ
            Ð²Ð¸Ð´Ð¸Ñ‚ ÑÐ²Ð¾ÑŽ Ñ‡Ð°ÑÑ‚ÑŒ validation Ð´Ð°Ð½Ð½Ñ‹Ñ…. reduce() ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ ÑƒÑÑ€ÐµÐ´Ð½ÑÐµÑ‚ loss Ð¿Ð¾ Ð²ÑÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼.
            
            ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸:
            - use_cache=False: Ð½Ðµ Ð½Ð°ÐºÐ°Ð¿Ð»Ð¸Ð²Ð°ÐµÐ¼ KV-cache
            - Ð•ÑÐ»Ð¸ Liger fused CE Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾ (Ð½Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ logits)
            - Ð˜Ð½Ð°Ñ‡Ðµ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ forward (Ð½Ð¾ logits Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·ÑƒÑŽÑ‚ÑÑ)
            """
            model.eval()
            losses = []
            with torch.no_grad():
                for i, batch in enumerate(val_loader):
                    if i >= eval_batches:
                        break
                    # Ð•ÑÐ»Ð¸ DataLoader Ð½Ðµ Ð±Ñ‹Ð» Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½ accelerate'Ð¾Ð¼ â€” Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ ÐºÐ»Ð°Ð´Ñ‘Ð¼ Ð±Ð°Ñ‚Ñ‡ Ð½Ð° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾
                    if is_streaming_sharded:
                        batch = {k: (v.to(accelerator.device) if hasattr(v, "to") else v) for k, v in batch.items()}
                    
                    with accelerator.autocast():
                        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Liger fused CE ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ (Ð½Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·ÑƒÐµÑ‚ logits)
                        if liger_fused_ce_loss is not None:
                            labels = batch.pop("labels", None)
                            outputs = model(**batch, output_hidden_states=True, use_cache=False)
                            batch["labels"] = labels
                            hidden_states = outputs.hidden_states[-1]
                            loss = liger_fused_ce_loss(hidden_states, labels).detach()
                        else:
                            # Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ forward â€” logits Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·ÑƒÑŽÑ‚ÑÑ, Ð½Ð¾ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´Ð°ÐµÑ‚ÑÑ ÑÑ€Ð°Ð·Ñƒ
                            out = model(**batch, use_cache=False)
                            loss = out.loss.detach()
                            del out  # Ð¯Ð²Ð½Ð¾ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´Ð°ÐµÐ¼ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð¾Ñ‚ logits
                        
                        # Ð£ÑÑ€ÐµÐ´Ð½ÑÐµÐ¼ loss Ð¿Ð¾ Ð²ÑÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼ (ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð²Ð¸Ð´Ð¸Ñ‚ ÑÐ²Ð¾ÑŽ Ñ‡Ð°ÑÑ‚ÑŒ val Ð´Ð°Ð½Ð½Ñ‹Ñ…)
                        loss = accelerator.reduce(loss, reduction="mean")
                        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main process, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
                        if accelerator.is_main_process:
                            losses.append(loss.item())
                        del loss  # ÐžÑÐ²Ð¾Ð±Ð¾Ð¶Ð´Ð°ÐµÐ¼ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
                
                # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ CUDA cache Ð¿Ð¾ÑÐ»Ðµ eval
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            model.train()
            if not losses:
                return None
            # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÑÑ€ÐµÐ´Ð½Ð¸Ð¹ loss (ÑƒÐ¶Ðµ ÑƒÑÑ€ÐµÐ´Ð½ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼)
            return sum(losses) / len(losses) if losses else None
        
        # Ð’ÐÐ–ÐÐž: global_step ÑƒÐ¶Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð²Ñ‹ÑˆÐµ ÐºÐ°Ðº starting_step (Ð´Ð»Ñ resume)
        # Ð•ÑÐ»Ð¸ resume Ð½Ðµ Ð±Ñ‹Ð»Ð¾, starting_step = 0, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ global_step = 0
        start_time = time.time()
        last_heartbeat = time.time()
        heartbeat_every = float(config.get("metrics_heartbeat_seconds", 20.0))
        
        # Debug: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´ÑƒÐ±Ð»ÐµÐ¹ Ð¼ÐµÐ¶Ð´Ñƒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼Ð¸ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ N ÑˆÐ°Ð³Ð¾Ð²)
        # ÐŸÐ¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ Ð’ÐšÐ› Ð² multi-GPU (ÑÑ‚Ð¾ Ð½Ð°Ñˆ "safety check", Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ ÑˆÐ°Ñ€Ð´Ð¸Ð½Ð³).
        if "debug_check_duplicates" in config:
            debug_check_duplicates = bool(config.get("debug_check_duplicates"))
        else:
            debug_check_duplicates = bool(accelerator.num_processes > 1)
        debug_sample_ids = []  # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ sample_id Ð´Ð»Ñ Ð¿ÐµÑ€Ð²Ñ‹Ñ… ÑˆÐ°Ð³Ð¾Ð²
        debug_max_samples = 20  # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 20 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²
        if debug_check_duplicates and accelerator.is_main_process:
            metrics.update(debug_check_duplicates=True, debug_max_samples=int(debug_max_samples))
        
        # Loss tracking:
        # - micro_* Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ divergence Ð½Ð° ÐšÐÐ–Ð”ÐžÐœ update-step
        # - log_* Ð´Ð»Ñ ÑÐ³Ð»Ð°Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð»Ð¾Ð³Ð° (ÑƒÑÑ€ÐµÐ´Ð½ÐµÐ½Ð¸Ðµ Ð¿Ð¾ update-step-Ð°Ð¼ Ð¼ÐµÐ¶Ð´Ñƒ Ð»Ð¾Ð³Ð°Ð¼Ð¸)
        micro_loss_sum = 0.0
        micro_count = 0
        log_loss_sum = 0.0
        log_updates = 0
        update_start_time = time.time()

        training_complete = False
        stop_reason = None
        
        for epoch in range(config["epochs"]):
            if training_complete:
                break
                
            metrics.update(epoch=epoch + 1)
            model.train()

            # Ð•ÑÐ»Ð¸ resume Ð¸Ð· IterableDataset: Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð±Ð°Ñ‚Ñ‡Ð¸ Ð¯Ð’ÐÐž Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ epoch
            epoch_loader = train_loader
            if epoch == 0 and resume_batches_to_skip > 0:
                it = iter(train_loader)
                to_skip = int(resume_batches_to_skip)
                log_every = int(config.get("resume_skip_log_every", 500))
                t0 = time.time()
                skipped = 0
                logger.info(f"Skipping {to_skip} batches now... (logging every {log_every})")
                while skipped < to_skip:
                    try:
                        next(it)
                    except StopIteration:
                        logger.warning(f"Reached end of iterable dataloader while skipping at {skipped}/{to_skip}. Continuing from start of next epoch.")
                        break
                    skipped += 1
                    if skipped % log_every == 0 or skipped == to_skip:
                        dt = max(1e-6, time.time() - t0)
                        bps = skipped / dt
                        eta = (to_skip - skipped) / bps if bps > 0 else float("inf")
                        logger.info(f"Skipped {skipped}/{to_skip} batches ({bps:.2f} batches/s), ETA ~{eta:.0f}s")
                epoch_loader = it  # Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ ÑÑ‚Ð¾Ñ‚ epoch Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸
                if accelerator.is_main_process:
                    metrics.update(status="training", skipped_batches_done=int(skipped))
                resume_batches_to_skip = 0  # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·

            for step, batch in enumerate(epoch_loader):
                # Heartbeat: Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ metrics.json Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸, Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ log_every Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹
                if accelerator.is_main_process and heartbeat_every > 0:
                    now = time.time()
                    if now - last_heartbeat >= heartbeat_every:
                        try:
                            metrics.update(last_heartbeat=datetime.now().isoformat())
                        except Exception:
                            pass
                        last_heartbeat = now
                # ÐžÐ´Ð½Ð¾Ñ€Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð»Ð¾Ð³: Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ per-process batch Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ° "Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ Ð¼Ð½Ð¾Ð³Ð¾ VRAM"
                if step == 0 and epoch == 0 and accelerator.is_main_process and global_step == starting_step:
                    try:
                        if "input_ids" in batch and hasattr(batch["input_ids"], "shape"):
                            bsz, seqlen = int(batch["input_ids"].shape[0]), int(batch["input_ids"].shape[1])
                        else:
                            bsz, seqlen = int(config.get("batch_size", 0)), int(config.get("seq_len", 0))

                        vocab_size = int(config.get("vocab_size", 50257))
                        mp = str(config.get("mixed_precision", "no")).lower()
                        bytes_per = 2 if mp in ("fp16", "bf16") else 4

                        # logits: (B,S,V) â€” ÑÑ‚Ð¾ Ñ‡Ð°ÑÑ‚Ð¾ Ð³Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¶Ð¸Ñ€Ð°Ñ‚ÐµÐ»ÑŒ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… B Ð¸ Ð´Ð»Ð¸Ð½Ð½Ð¾Ð¼ S
                        logits_gb = (bsz * seqlen * vocab_size * bytes_per) / (1024**3)

                        num_gpus = int(accelerator.num_processes or 1)
                        grad_accum = int(config.get("gradient_accumulation", 1))
                        eff_per_gpu = bsz * grad_accum
                        global_batch = eff_per_gpu * num_gpus

                        logger.warning(
                            f"[BATCH CHECK] input_ids shape={getattr(batch.get('input_ids', None), 'shape', None)} | "
                            f"per-GPU microbatch={bsz}, grad_accum={grad_accum} -> effective per-GPU={eff_per_gpu}, global={global_batch} (x{num_gpus}). "
                            f"Estimated logits memory ~{logits_gb:.2f} GB (B*S*V, dtype={mp}). "
                        )
                    except Exception as e:
                        logger.warning(f"[BATCH CHECK] failed: {e}")
                # Debug: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´ÑƒÐ±Ð»ÐµÐ¹ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€Ð²Ñ‹Ðµ N Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²)
                # Ð’ÐÐ–ÐÐž: ÐŸÑ€Ð¸ dispatch_batches=True Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ¾Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ Ñ…ÑÑˆÐ¸ ÑÐ¾ Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ñ‡ÐµÑ€ÐµÐ· gather
                if debug_check_duplicates and global_step < debug_max_samples:
                    # Ð”Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ hash Ð¾Ñ‚ input_ids
                    if "input_ids" in batch:
                        # Ð‘ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¸Ð· Ð±Ð°Ñ‚Ñ‡Ð° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
                        sample_hash = hash(batch["input_ids"][0].cpu().numpy().tobytes())
                        debug_sample_ids.append((global_step, accelerator.process_index, sample_hash))
                
                with accelerator.accumulate(model):
                    if is_streaming_sharded:
                        batch = {k: (v.to(accelerator.device) if hasattr(v, "to") else v) for k, v in batch.items()}
                    with accelerator.autocast():
                        # ðŸ¦ Liger Fused CE path vs standard path
                        if liger_fused_ce_loss is not None:
                            # LIGER FUSED PATH: hidden_states -> fused loss (ÐÐ• Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ logits!)
                            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ labels Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÐÐ• Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐ»Ð° loss
                            labels = batch.pop("labels", None)
                            outputs = model(**batch, output_hidden_states=True, use_cache=False)
                            batch["labels"] = labels  # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸
                            
                            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ hidden state
                            hidden_states = outputs.hidden_states[-1]
                            
                            # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ loss Ñ‡ÐµÑ€ÐµÐ· Liger Fused CE
                            loss = liger_fused_ce_loss(hidden_states, labels)
                        else:
                            # STANDARD PATH
                            outputs = model(**batch)
                            loss = outputs.loss
                        
                        loss_val = loss.detach().float().item()
                    
                    # ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: ÐµÑÐ»Ð¸ loss NaN/Inf - Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÑ‚Ð¾Ñ‚ ÑˆÐ°Ð³ Ð¸ Ð¾ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ÑÑ
                    if math.isnan(loss_val) or math.isinf(loss_val):
                        logger.error(f"Loss is NaN or Inf at step {global_step}. Stopping training to prevent bad model.")
                        metrics.update(status="error", error=f"Training Diverged: Loss is NaN or Infinity at step {global_step}. Try lowering learning rate or changing precision.")
                        raise ValueError(f"Training Diverged: Loss is NaN or Infinity at step {global_step}.")
                    
                    # micro-Ð°ÐºÐºÑƒÐ¼ÑƒÐ»ÑÑ‚Ð¾Ñ€: Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ update-step
                    micro_loss_sum += loss_val
                    micro_count += 1
                    
                    accelerator.backward(loss)
                    
                    # Ð’ÐÐ–ÐÐž: ÑˆÐ°Ð³ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° update-step
                    if accelerator.sync_gradients:
                        # ÐžÐ´Ð½Ð¾Ñ€Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð»Ð¾Ð³ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ peak VRAM (allocator) Ð½Ð° Ð¿ÐµÑ€Ð²Ð¾Ð¼ update-step (Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ñ€Ð°Ð½ÐºÑƒ)
                        if epoch == 0 and global_step == starting_step and not metrics.metrics.get("logged_cuda_peak", False):
                            try:
                                if torch.cuda.is_available():
                                    dev = accelerator.device
                                    torch.cuda.synchronize(dev)
                                    peak_alloc = torch.cuda.max_memory_allocated(dev) / (1024**3)
                                    peak_res = torch.cuda.max_memory_reserved(dev) / (1024**3)
                                    cur_alloc = torch.cuda.memory_allocated(dev) / (1024**3)
                                    cur_res = torch.cuda.memory_reserved(dev) / (1024**3)
                                    logger.warning(
                                        f"[CUDA PEAK] rank={accelerator.process_index} device={dev} | "
                                        f"peak_alloc={peak_alloc:.2f}GB peak_reserved={peak_res:.2f}GB | "
                                        f"cur_alloc={cur_alloc:.2f}GB cur_reserved={cur_res:.2f}GB"
                                    )
                                # Ð¿Ð¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main process Ñ‡ÐµÑ€ÐµÐ· metrics-Ñ„Ð°Ð¹Ð» (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÑÐ¿Ð°Ð¼Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸ Ð´Ð¾Ð»Ð³Ð¾Ð¼ Ð¿Ñ€Ð¾Ð³Ð¾Ð½Ðµ)
                                if accelerator.is_main_process:
                                    metrics.metrics["logged_cuda_peak"] = True
                                    metrics._save()
                            except Exception as e:
                                logger.warning(f"[CUDA PEAK] failed: {e}")

                        # Gradient clipping Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
                        max_grad_norm = config.get("max_grad_norm", 1.0)
                        if max_grad_norm > 0:
                            accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)
                        
                        optimizer.step()
                        lr_scheduler.step()
                        optimizer.zero_grad(set_to_none=True)
                
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐµÐ» Ð»Ð¸ ÑˆÐ°Ð³ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° (ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð²)
                if accelerator.sync_gradients:
                    global_step += 1
                    
                    # Ð’Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑˆÐ°Ð³Ð° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ (update step)
                    update_time = time.time() - update_start_time
                    update_start_time = time.time()
                    
                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° ÑÐ»ÑƒÑ‡Ð°Ð¹ ÐµÑÐ»Ð¸ Ð²ÑÐµ Ð¼Ð¸ÐºÑ€Ð¾-ÑˆÐ°Ð³Ð¸ Ð±Ñ‹Ð»Ð¸ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ñ‹ (Ð½Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ ÑÐ»ÑƒÑ‡Ð¸Ñ‚ÑŒÑÑ Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸ÐºÑÐ° Ð²Ñ‹ÑˆÐµ)
                    if micro_count == 0:
                        logger.warning(f"No valid loss values at step {global_step}, skipping update")
                        continue

                    update_loss = micro_loss_sum / micro_count
                    # Ð’ÐÐ–ÐÐž: ÑƒÑÑ€ÐµÐ´Ð½ÑÐµÐ¼ loss Ð¿Ð¾ Ð²ÑÐµÐ¼ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼ Ð² Multi-GPU
                    update_loss_t = torch.tensor(update_loss, device=accelerator.device)
                    update_loss = accelerator.reduce(update_loss_t, reduction="mean").item()
                    
                    micro_loss_sum = 0.0
                    micro_count = 0

                    # Ð½Ð°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐ³Ð»Ð°Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð»Ð¾Ð³Ð°
                    log_loss_sum += update_loss
                    log_updates += 1

                    # Debug: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´ÑƒÐ±Ð»ÐµÐ¹ Ð¿Ð¾ÑÐ»Ðµ Ð¿ÐµÑ€Ð²Ñ‹Ñ… N ÑˆÐ°Ð³Ð¾Ð²
                    # Ð’ÐÐ–ÐÐž: gather() â€” ÐºÐ¾Ð»Ð»ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ, Ð´Ð¾Ð»Ð¶Ð½Ð° Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒÑÑ Ð½Ð° Ð’Ð¡Ð•Ð¥ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ…!
                    if debug_check_duplicates and global_step == debug_max_samples:
                        # 1) Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ…ÑÑˆÐµÐ¹ Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° (Ð’Ð¡Ð• Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÑŽÑ‚ gather)
                        hash_counts = torch.tensor([len(debug_sample_ids)], device=accelerator.device, dtype=torch.long)
                        all_counts = accelerator.gather(hash_counts)  # collective op
                        
                        # 2) Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ…ÑÑˆÐ¸ Ð´Ð»Ñ gather (Ð’Ð¡Ð• Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹)
                        max_len = int(all_counts.max().item()) if all_counts.numel() > 0 else 0
                        if len(debug_sample_ids) > 0 and max_len > 0:
                            local_hashes = torch.tensor([h for _, _, h in debug_sample_ids], device=accelerator.device, dtype=torch.long)
                            # Pad Ð´Ð¾ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð´Ð»Ð¸Ð½Ñ‹ Ð´Ð»Ñ gather
                            if len(local_hashes) < max_len:
                                padding = torch.full((max_len - len(local_hashes),), -1, device=accelerator.device, dtype=torch.long)
                                local_hashes = torch.cat([local_hashes, padding])
                        else:
                            # ÐŸÑƒÑÑ‚Ð¾Ð¹ Ñ‚ÐµÐ½Ð·Ð¾Ñ€ ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ Ñ…ÑÑˆÐµÐ¹ (Ð½Ð¾ Ð²ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾ ÑƒÑ‡Ð°ÑÑ‚Ð²ÑƒÐµÐ¼ Ð² gather)
                            local_hashes = torch.full((max(1, max_len),), -1, device=accelerator.device, dtype=torch.long)
                        
                        # 3) Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ…ÑÑˆÐ¸ ÑÐ¾ Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² (Ð’Ð¡Ð• Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÑŽÑ‚ gather)
                        all_hashes = accelerator.gather(local_hashes.unsqueeze(0))  # collective op
                        
                        # 4) ÐÐ½Ð°Ð»Ð¸Ð· Ð´ÑƒÐ±Ð»ÐµÐ¹ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main process (Ð¿Ð¾ÑÐ»Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ gather)
                        if accelerator.is_main_process:
                            total_hashes = int(all_counts.sum().item())
                            logger.info(f"Debug: collected {total_hashes} sample hashes across all processes")
                            
                            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ padding (-1) Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸
                            all_hashes_flat = all_hashes.flatten()
                            valid_hashes = all_hashes_flat[all_hashes_flat != -1].cpu().numpy()
                            
                            if len(valid_hashes) > 0:
                                unique_hashes = set(valid_hashes)
                                if len(valid_hashes) != len(unique_hashes):
                                    duplicates = len(valid_hashes) - len(unique_hashes)
                                    logger.warning(f"Debug: Found {duplicates} duplicate samples in first {debug_max_samples} steps across all processes!")
                                else:
                                    logger.info(f"Debug: No duplicates found in first {debug_max_samples} steps (all {len(valid_hashes)} samples unique across all processes)")
                            else:
                                logger.warning("Debug: No sample hashes collected")
                    
                    # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                    if global_step % config.get("log_every", 10) == 0 or global_step == 1:
                        avg_loss = log_loss_sum / max(1, log_updates)
                        
                        # Samples per second (Effective Batch / Time)
                        # Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ multi-GPU: Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ batch = effective_batch * num_processes
                        effective_batch = config["batch_size"] * config["gradient_accumulation"]
                        global_batch = effective_batch * accelerator.num_processes
                        samples_per_sec = global_batch / update_time if update_time > 0 else 0
                        
                        metrics.log_step(
                            step=global_step,
                            loss=avg_loss,
                            lr=lr_scheduler.get_last_lr()[0],
                            samples_per_sec=samples_per_sec,
                            step_time=update_time
                        )
                        
                        log_loss_sum = 0.0
                        log_updates = 0
                    
                    # Checkpoint
                    if global_step % config.get("save_every", 5000) == 0:
                        ckpt_path = output_dir / f"checkpoint_step{global_step}"
                        accelerator.save_state(ckpt_path)
                        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾Ð³Ð¸Ð¹ dataloader state (per-rank) Ñ€ÑÐ´Ð¾Ð¼ Ñ accelerate checkpoint
                        try:
                            if hasattr(train_dataset, "get_resume_state"):
                                ds_state = train_dataset.get_resume_state()
                                st_path = ckpt_path / f"dataloader_state_rank{accelerator.process_index}.json"
                                with open(st_path, "w", encoding="utf-8") as f:
                                    json.dump(ds_state, f, ensure_ascii=False, indent=2)
                        except Exception as e:
                            logger.warning(f"Failed to save dataloader state: {e}")
                        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð° (Ð´Ð»Ñ Ñ‡ÐµÑÑ‚Ð½Ð¾Ð³Ð¾ UI/Ð²Ð¾Ð·Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð»Ð°Ð½Ð¾Ð²)
                        try:
                            if accelerator.is_main_process:
                                meta = {
                                    "global_step": int(global_step),
                                    "planned_total_steps": int(metrics.metrics.get("planned_total_steps", max_train_steps)),
                                    "learning_rate": float(config.get("learning_rate", 0.0)),
                                    "warmup_steps": int(config.get("warmup_steps", 0)),
                                    "min_lr_ratio": float(config.get("min_lr_ratio", 0.0) or 0.0),
                                    "scheduler": str(config.get("lr_schedule", "cosine")),
                                    "timestamp": datetime.now().isoformat(),
                                }
                                with open(ckpt_path / "checkpoint_metadata.json", "w", encoding="utf-8") as f:
                                    json.dump(meta, f, ensure_ascii=False, indent=2)
                        except Exception as e:
                            logger.warning(f"Failed to save checkpoint metadata: {e}")
                        accelerator.wait_for_everyone()
                        
                        # ÐžÐ¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main process
                        if accelerator.is_main_process:
                            model_config.save_pretrained(ckpt_path)
                            # ÐŸÐµÑ€ÐµÐ´Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ loss Ð² Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚
                            current_loss = metrics.metrics.get("current_loss", 0.0)
                            metrics.log_checkpoint(str(ckpt_path), loss=current_loss)

                        # (ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾) Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½Ñ-Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° Ð¿Ñ€Ð¸ ÐºÐ°Ð¶Ð´Ð¾Ð¼ checkpoint.
                        # Ð­Ñ‚Ð¾ ÐÐ• resume-state, Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑƒÐ´Ð¾Ð±Ð½Ñ‹Ð¹ "latest final_model" Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸.
                        # Ð’ÐÐ–ÐÐž: Ð”Ð»Ñ ZeRO-3/FSDP adapter.save_final Ð”ÐžÐ›Ð–Ð•Ð Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒÑÑ Ð½Ð° Ð’Ð¡Ð•Ð¥ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ…!
                        if bool(config.get("export_on_checkpoint", True)):
                            try:
                                import shutil
                                tmp_dir = output_dir / "final_model.__tmp__"
                                final_dir = output_dir / "final_model"
                                
                                # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð½Ð° main process
                                if accelerator.is_main_process:
                                    if tmp_dir.exists():
                                        shutil.rmtree(tmp_dir, ignore_errors=True)
                                    tmp_dir.mkdir(parents=True, exist_ok=True)

                                    # SFT: ÑƒÐ±ÐµÐ´Ð¸Ð¼ÑÑ, Ñ‡Ñ‚Ð¾ chat_template Ð¿Ð¾Ð¿Ð°Ð´Ð°ÐµÑ‚ Ð² tokenizer_config.json
                                    if stage == "sft":
                                        user_chat_template = config.get("chat_template")
                                        if user_chat_template:
                                            tokenizer.chat_template = user_chat_template
                                            logger.info("Using user-provided chat_template")
                                        elif config.get("sft_template"):
                                            tmpl = config["sft_template"]
                                            default_sys = tmpl.get("system", "You are a helpful assistant.")
                                            u_tag = tmpl.get("user_tag", "### User:")
                                            b_tag = tmpl.get("bot_tag", "### Assistant:")
                                            default_sys_escaped = default_sys.replace("'", "\\'")
                                            u_tag_escaped = u_tag.replace("'", "\\'")
                                            b_tag_escaped = b_tag.replace("'", "\\'")
                                            tokenizer.chat_template = (
                                                "{% if messages and messages[0]['role'] == 'system' %}"
                                                "{{ messages[0]['content'] }}\n\n"
                                                "{% else %}"
                                                f"{{{{ '{default_sys_escaped}' }}}}\n\n"
                                                "{% endif %}"
                                                "{% for message in messages %}"
                                                "{% if message['role'] == 'user' %}"
                                                f"{u_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                                                "{% elif message['role'] == 'assistant' %}"
                                                f"{b_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                                                "{% endif %}"
                                                "{% endfor %}"
                                                "{% if add_generation_prompt %}"
                                                f"{b_tag_escaped}\n"
                                                "{% endif %}"
                                            )
                                            logger.info("Generated chat_template from sft_template")
                                
                                accelerator.wait_for_everyone()
                                
                                # save_final Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð’Ð¡Ð•Ð¥ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ… (Ð´Ð»Ñ ZeRO-3/FSDP)
                                adapter.save_final(accelerator, model, tokenizer, tmp_dir)
                                
                                accelerator.wait_for_everyone()
                                
                                # ÐÑ‚Ð¾Ð¼Ð°Ñ€Ð½Ð¾ Ð·Ð°Ð¼ÐµÐ½ÑÐµÐ¼ final_model (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main)
                                if accelerator.is_main_process:
                                    if final_dir.exists():
                                        shutil.rmtree(final_dir, ignore_errors=True)
                                    tmp_dir.rename(final_dir)
                                    logger.info(f"Updated final_model at checkpoint step {global_step}")
                                
                                accelerator.wait_for_everyone()
                            except Exception as e:
                                if accelerator.is_main_process:
                                    logger.warning(f"Failed to export final_model on checkpoint: {e}")
                    
                    # Validation
                    # Ð’ÐÐ–ÐÐž: evaluate() Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ…, Ñ‚.Ðº. val_loader Ð·Ð°ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½
                    # ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð²Ð¸Ð´Ð¸Ñ‚ ÑÐ²Ð¾ÑŽ Ñ‡Ð°ÑÑ‚ÑŒ val Ð´Ð°Ð½Ð½Ñ‹Ñ…, reduce() ÑƒÑÑ€ÐµÐ´Ð½ÑÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
                    if val_loader is not None and eval_every > 0 and (global_step % eval_every == 0):
                        val_loss = evaluate(val_loader)  # Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ…
                        if accelerator.is_main_process and val_loss is not None:
                            metrics.log_eval(global_step, float(val_loss))
                            logger.info(f"Validation at step {global_step}: val_loss={val_loss:.4f}")
                    
                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð»Ð¸ Ð»Ð¸ Ð»Ð¸Ð¼Ð¸Ñ‚Ð° ÑˆÐ°Ð³Ð¾Ð²
                    if global_step >= max_train_steps:
                        training_complete = True
                        stop_reason = "max_train_steps_reached"
                        break
        
        # Ð•ÑÐ»Ð¸ Ð·Ð°ÐºÐ¾Ð½Ñ‡Ð¸Ð»Ð¸ Ð½Ðµ Ð¿Ð¾ max_train_steps, Ñ‚Ð¾ Ð»Ð¸Ð±Ð¾ ÑÐ¿Ð¾Ñ…Ð¸ ÐºÐ¾Ð½Ñ‡Ð¸Ð»Ð¸ÑÑŒ, Ð»Ð¸Ð±Ð¾ Ð´Ð°Ñ‚Ð°Ð»Ð¾Ð°Ð´ÐµÑ€ Ð¸ÑÑ‡ÐµÑ€Ð¿Ð°Ð»ÑÑ.
        if stop_reason is None:
            if global_step >= max_train_steps:
                stop_reason = "max_train_steps_reached"
            else:
                # Ð”Ð»Ñ IterableDataset Ð´Ð»Ð¸Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¾ Ð¾Ñ†ÐµÐ½Ð¾Ñ‡Ð½Ð°Ñ (Ð¿ÑƒÑÑ‚Ñ‹Ðµ/Ð±Ð¸Ñ‚Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð¾Ñ‚Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÑŽÑ‚ÑÑ),
                # Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ðµ EOF Ð¼Ð¾Ð¶ÐµÑ‚ ÑÐ»ÑƒÑ‡Ð¸Ñ‚ÑŒÑÑ Ñ€Ð°Ð½ÑŒÑˆÐµ planned_total_steps.
                stop_reason = "epochs_completed_or_dataloader_exhausted"

        # Ð•ÑÐ»Ð¸ "Ð¿Ð»Ð°Ð½" Ð¾ÐºÐ°Ð·Ð°Ð»ÑÑ Ð±Ð¾Ð»ÑŒÑˆÐµ Ñ„Ð°ÐºÑ‚Ð° â€” Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ñ‚Ð°Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð±Ñ‹Ð» 100%,
        # Ð½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ (planned_total_steps).
        if accelerator.is_main_process:
            try:
                if metrics.metrics.get("planned_total_steps", 0) and global_step < int(metrics.metrics.get("planned_total_steps", 0)):
                    metrics.update(total_steps=int(global_step), stop_reason=stop_reason)
                else:
                    metrics.update(stop_reason=stop_reason)
            except Exception:
                pass

        # Final save - Ð´Ð»Ñ ZeRO-3/FSDP Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ ÑƒÑ‡Ð°ÑÑ‚Ð¸Ðµ Ð’Ð¡Ð•Ð¥ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²
        accelerator.wait_for_everyone()
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð½Ð° main
        if accelerator.is_main_process:
            metrics.update(status="saving_model")
        
        final_dir = output_dir / "final_model"
        
        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð½Ð° main process
        if accelerator.is_main_process:
            final_dir.mkdir(parents=True, exist_ok=True)
        accelerator.wait_for_everyone()
        
        # --- (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾) ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ chat_template Ð´Ð»Ñ SFT ---
        # ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° main, Ð½Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð±ÑƒÐ´ÐµÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ adapter.save_final
        if accelerator.is_main_process and stage == "sft":
            # ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚: Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ chat_template > Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¸Ð· sft_template
            user_chat_template = config.get("chat_template")
            if user_chat_template:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ chat_template
                tokenizer.chat_template = user_chat_template
                logger.info("Final save: using user-provided chat_template")
            elif config.get("sft_template"):
                # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Qwen-style chat_template Ð¸Ð· sft_template
                tmpl = config["sft_template"]
                
                default_sys = tmpl.get("system", "You are a helpful assistant.")
                im_start = tmpl.get("im_start", "<|im_start|>")
                im_end = tmpl.get("im_end", "<|im_end|>")
                sep = tmpl.get("separator", "\n")
                
                default_sys_escaped = default_sys.replace("'", "\\'")
                
                # Qwen-style chat template
                tokenizer.chat_template = (
                    "{%- if messages[0]['role'] == 'system' -%}"
                    f"{{{{ '{im_start}system{sep}' + messages[0]['content'] + '{im_end}{sep}' }}}}"
                    "{%- else -%}"
                    f"{{{{ '{im_start}system{sep}{default_sys_escaped}{im_end}{sep}' }}}}"
                    "{%- endif -%}"
                    "{%- for message in messages -%}"
                    "{%- if message.role == 'user' or (message.role == 'system' and not loop.first) -%}"
                    f"{{{{ '{im_start}' + message.role + '{sep}' + message.content + '{im_end}{sep}' }}}}"
                    "{%- elif message.role == 'assistant' -%}"
                    f"{{{{ '{im_start}assistant{sep}' + message.content + '{im_end}{sep}' }}}}"
                    "{%- endif -%}"
                    "{%- endfor -%}"
                    "{%- if add_generation_prompt -%}"
                    f"{{{{ '{im_start}assistant{sep}' }}}}"
                    "{%- endif -%}"
                )
                logger.info(f"Final save: generated Qwen-style chat_template (im_start='{im_start}', im_end='{im_end}')")
        
        # Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ tokenizer.model_max_length Ñ max_position_embeddings Ð¼Ð¾Ð´ÐµÐ»Ð¸
        # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ confusing warnings Ð¿Ñ€Ð¸ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐµ
        if accelerator.is_main_process:
            unwrapped = accelerator.unwrap_model(model)
            model_max_pos = getattr(unwrapped.config, "max_position_embeddings", None)
            if model_max_pos and tokenizer.model_max_length != model_max_pos:
                logger.info(f"Syncing tokenizer.model_max_length: {tokenizer.model_max_length} -> {model_max_pos}")
                tokenizer.model_max_length = model_max_pos
        
        # --- Ð’Ð¡Ð•Ð“Ð”Ð ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (Ð¸ Ð´Ð»Ñ pretrain Ñ‚Ð¾Ð¶Ðµ) ---
        # Ð’ÐÐ–ÐÐž: Ð”Ð»Ñ ZeRO-3/FSDP adapter.save_final Ð”ÐžÐ›Ð–Ð•Ð Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒÑÑ Ð½Ð° Ð’Ð¡Ð•Ð¥ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ…
        # Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ ÑÐ¾Ð±Ñ€Ð°Ñ‚ÑŒ ÑˆÐ°Ñ€Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð²ÐµÑÐ° Ñ‡ÐµÑ€ÐµÐ· accelerator.save_model
        adapter.save_final(accelerator, model, tokenizer, final_dir)
        
        # Ð–Ð´Ñ‘Ð¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð½Ð° Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ…
        accelerator.wait_for_everyone()
        
        # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° main
        if accelerator.is_main_process:
            total_time = time.time() - start_time
            hours, rem = divmod(total_time, 3600)
            minutes, seconds = divmod(rem, 60)
            duration_str = "{:0>2}:{:0>2}:{:05.2f}".format(int(hours), int(minutes), seconds)
            
            metrics.update(
                status="completed",
                total_time_seconds=total_time,
                training_duration=duration_str,
                final_model_path=str(final_dir),
            )
        
        accelerator.wait_for_everyone()
        
    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        logger.error(f"Training failed: {e}\n{tb}")
        metrics.update(status="error", error=f"{str(e)}\n\n{tb}")
        raise


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Path to JSON config")
    parser.add_argument("--metrics", type=str, required=True, help="Path to metrics JSON")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Path to checkpoint directory to resume from")
    args = parser.parse_args()
    
    with open(args.config) as f:
        config = json.load(f)
    
    # Merge CLI args into config
    if args.resume_from_checkpoint:
        config["resume_from_checkpoint"] = args.resume_from_checkpoint
    
    run_training(config, Path(args.metrics))


if __name__ == "__main__":
    main()
