"""
Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ğ¹ worker Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ² JSON Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Streamlit Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼.
"""
from __future__ import annotations

import argparse
import json
import logging
import math
import os
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

import torch
from torch.utils.data import IterableDataset, DataLoader
from accelerate import Accelerator
from accelerate.utils import DataLoaderConfiguration
from transformers import (
    AutoTokenizer,
    get_cosine_schedule_with_warmup,
    get_scheduler,
    DataCollatorForLanguageModeling,
)

from homellm.models.adapters import resolve_adapter
from homellm.training.pretrain import StreamingTextDataset
from homellm.training.sft import SFTDataset
from homellm.training.optimizers import MagmaAdamW, MuonWithAuxAdam

# Liger Kernel Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Triton kernels)
try:
    from homellm.training.rl.liger_utils import (
        is_liger_available,
        apply_liger_patch_to_model,
        create_liger_fused_ce,
        LIGER_SUPPORTED_MODELS,
    )
    LIGER_UTILS_AVAILABLE = True
except ImportError:
    LIGER_UTILS_AVAILABLE = False
    LIGER_SUPPORTED_MODELS = set()

logger = logging.getLogger(__name__)


def get_gpu_stats():
    """
    ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ GPU Ñ‡ĞµÑ€ĞµĞ· nvidia-smi (Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸ DDP).
    
    Ğ’ĞĞ–ĞĞ: Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ¸ Ñ€ĞµĞ¼Ğ°Ğ¿Ğ¸Ñ‚ GPU Ğ¿Ğ¾ CUDA_VISIBLE_DEVICES, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ
    Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ GPU Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ğ¼Ğ¸ (0..N-1).
    """
    gpu_stats = []
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… GPU Ğ¸Ğ· Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
    visible = os.environ.get("CUDA_VISIBLE_DEVICES")
    visible_ids = None
    if visible:
        try:
            visible_ids = [int(x.strip()) for x in visible.split(",") if x.strip() != ""]
        except Exception:
            visible_ids = None
    
    try:
        import subprocess
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾ Ğ²ÑĞµÑ… GPU Ñ‡ĞµÑ€ĞµĞ· nvidia-smi
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', 
             '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=2
        )
        
        if result.returncode == 0:
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 4:
                        gpu_id = int(parts[0])
                        memory_used = float(parts[1]) / 1024  # MiB -> GiB
                        memory_total = float(parts[2]) / 1024
                        utilization = int(parts[3]) if parts[3].isdigit() else None
                        
                        gpu_stats.append({
                            "id": gpu_id,
                            "memory_used_gb": round(memory_used, 2),
                            "memory_total_gb": round(memory_total, 2),
                            "memory_percent": round(memory_used / memory_total * 100, 1) if memory_total > 0 else 0,
                            "utilization": utilization,
                        })
            
            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¸ Ñ€ĞµĞ¼Ğ°Ğ¿Ğ¸Ğ¼ Ğ¿Ğ¾ CUDA_VISIBLE_DEVICES
            if visible_ids is not None:
                # ĞÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ GPU
                gpu_stats = [g for g in gpu_stats if g["id"] in visible_ids]
                # Ğ ĞµĞ¼Ğ°Ğ¿Ğ¸Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ID -> Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ (0..N-1)
                remap = {phys: i for i, phys in enumerate(visible_ids)}
                for g in gpu_stats:
                    g["id"] = remap.get(g["id"], g["id"])
    except Exception:
        # Fallback Ğ½Ğ° torch.cuda ĞµÑĞ»Ğ¸ nvidia-smi Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                try:
                    memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)
                    memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    
                    gpu_stats.append({
                        "id": i,
                        "memory_used_gb": round(memory_allocated, 2),
                        "memory_total_gb": round(memory_total, 2),
                        "memory_percent": round(memory_allocated / memory_total * 100, 1),
                        "utilization": None,
                    })
                except:
                    pass
    
    return gpu_stats


class MetricsLogger:
    """Ğ›Ğ¾Ğ³Ğ³ĞµÑ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ² JSON Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.
    
    Ğ’ĞĞ–ĞĞ: Ğ’ Multi-GPU Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main process (rank 0),
    Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ³Ğ¾Ğ½Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ñ„Ğ°Ğ¹Ğ».
    """
    
    def __init__(self, log_path: Path, enabled: bool = True):
        self.log_path = log_path
        self.enabled = enabled
        self.start_timestamp = time.time()
        self.metrics = {
            "status": "initializing",
            "start_time": datetime.now().isoformat(),
            "current_step": 0,
            "total_steps": 0,
            "epoch": 0,
            "loss_history": [],
            "lr_history": [],
            "steps_history": [],
            "current_loss": 0.0,
            "current_lr": 0.0,
            "samples_per_second": 0.0,
            "eta_seconds": 0,
            "error": None,
            "checkpoints": [],
            "gpu_stats": [],
            "current_val_loss": None,
            "val_loss_history": [],
            "val_steps_history": [],
        }
        self._save()
    
    def _save(self):
        """ĞÑ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ½Ğ¾Ğº."""
        if not self.enabled:
            return
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
        self.log_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ĞÑ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
        tmp_path = self.log_path.with_suffix(".tmp")
        try:
            with open(tmp_path, "w") as f:
                json.dump(self.metrics, f, indent=2)
            os.replace(tmp_path, self.log_path)
        except Exception as e:
            # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ, Ğ¿Ñ‹Ñ‚Ğ°ĞµĞ¼ÑÑ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ tmp Ñ„Ğ°Ğ¹Ğ»
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except:
                pass
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ°ĞµĞ¼
            logger.warning(f"Failed to save metrics: {e}")
    
    def update(self, **kwargs):
        if not self.enabled:
            return
        self.metrics.update(kwargs)
        self._save()
    
    def log_step(self, step: int, loss: float, lr: float, samples_per_sec: float = 0, step_time: float = 0):
        if not self.enabled:
            return
        
        self.metrics["current_step"] = step
        self.metrics["current_loss"] = loss
        self.metrics["current_lr"] = lr
        self.metrics["samples_per_second"] = samples_per_sec
        self.metrics["loss_history"].append(loss)
        self.metrics["lr_history"].append(lr)
        self.metrics["steps_history"].append(step)
        
        # Elapsed
        self.metrics["elapsed_seconds"] = time.time() - self.start_timestamp
        
        # GPU stats
        self.metrics["gpu_stats"] = get_gpu_stats()
        
        # ETA Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ÑˆĞ°Ğ³Ğ°
        if step > 0 and step_time > 0:
            remaining_steps = max(0, self.metrics["total_steps"] - step)
            self.metrics["eta_seconds"] = int(remaining_steps * step_time)
        
        self._save()
    
    def log_checkpoint(self, path: str, loss: float = None):
        if not self.enabled:
            return
        
        checkpoint_data = {
            "path": path,
            "step": self.metrics["current_step"],
            "time": datetime.now().isoformat()
        }
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ loss ĞµÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½
        if loss is not None:
            checkpoint_data["loss"] = float(loss)
        elif "current_loss" in self.metrics:
            checkpoint_data["loss"] = float(self.metrics["current_loss"])
        
        self.metrics["checkpoints"].append(checkpoint_data)
        self._save()
    
    def log_eval(self, step: int, val_loss: float):
        """Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸."""
        if not self.enabled:
            return
        
        self.metrics["current_val_loss"] = val_loss
        self.metrics["val_loss_history"].append(val_loss)
        self.metrics["val_steps_history"].append(step)
        self._save()


def run_training(config: Dict[str, Any], metrics_path: Path):
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº."""
    
    # === Backend Selection ===
    training_backend = config.get("training_backend", "models-at-home")
    stage = config.get("stage", "pretrain")  # pretrain | continual_pretrain | sft | grpo
    
    # Unsloth backend Ğ´Ğ»Ñ SFT
    if training_backend == "unsloth" and stage == "sft":
        logger.info("ğŸ¦¥ Using Unsloth backend for SFT training")
        try:
            from homellm.training.unsloth_sft import run_unsloth_sft, is_unsloth_available
            
            if not is_unsloth_available():
                logger.warning("âš ï¸ Unsloth not available, falling back to models-at-home backend")
            else:
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ MetricsLogger
                metrics = MetricsLogger(metrics_path, enabled=True)
                metrics.update(
                    status="initializing",
                    backend="unsloth",
                    stage=stage,
                )
                
                try:
                    run_unsloth_sft(config, metrics)
                    return  # Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· unsloth
                except Exception as e:
                    import traceback
                    tb = traceback.format_exc()
                    logger.error(f"ğŸ¦¥ Unsloth training failed: {e}\n{tb}")
                    metrics.update(status="error", error=f"Unsloth error: {str(e)}\n\n{tb}")
                    raise
        except ImportError as e:
            logger.warning(f"âš ï¸ Could not import unsloth_sft: {e}. Falling back to models-at-home backend.")
    
    # === models-at-home backend (default) ===
    # Mixed precision
    mixed_precision = config.get("mixed_precision", "no")
    fp16_pure = bool(config.get("fp16_pure", False))
    use_flash_attention = bool(config.get("use_flash_attention", True))
    
    # Ğ’ĞĞ–ĞĞ: Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ accelerate/deepspeed config-Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ `gradient_accumulation_steps: auto`,
    # Ğ½Ğ¾ Accelerator Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ int Ğ¸ ÑƒĞ¿Ğ°Ğ´Ñ‘Ñ‚ Ğ½Ğ° int("auto"). Ğ’ Ğ½Ğ°ÑˆĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¸ÑÑ‚Ğ¸Ğ½Ñ‹ â€” config.json Ğ¸Ğ· UI.
    # ĞŸĞ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ env Ğ´Ğ»Ñ accelerate.
    raw_ga = config.get("gradient_accumulation", 1)
    try:
        ga_steps = int(raw_ga)
    except Exception:
        if isinstance(raw_ga, str) and raw_ga.strip().lower() == "auto":
            ga_steps = 1
            logger.warning("gradient_accumulation='auto' Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² runtime; Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 1. ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹Ñ‚Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² UI.")
        else:
            raise
    config["gradient_accumulation"] = ga_steps
    os.environ["ACCELERATE_GRADIENT_ACCUMULATION_STEPS"] = str(ga_steps)

    # "Pure fp16" (Ğ²ĞµÑĞ° fp16 Ğ±ĞµĞ· GradScaler) â€” Ğ´Ğ»Ñ accelerate Ğ½ÑƒĞ¶Ğ½Ğ¾ mixed_precision='no'
    # Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ¾Ğ½ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ GradScaler Ğ¸ ÑƒĞ¿Ğ°Ğ´Ñ‘Ñ‚ Ğ¿Ñ€Ğ¸ fp16 Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ….
    if str(mixed_precision).lower() == "fp16" and fp16_pure:
        mixed_precision = "no"
        logger.info("ğŸ§ª FP16 Pure Ñ€ĞµĞ¶Ğ¸Ğ¼: Accelerator(mixed_precision='no') (Ğ±ĞµĞ· GradScaler)")
    
    # Ğ’ĞĞ–ĞĞ: Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ dispatch_batches=True, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑÑ‚Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ NoneType Ğ¿Ñ€Ğ¸ broadcast
    # Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±ÑƒĞ´ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ StreamingTextDataset (shard=True)
    dataloader_config = DataLoaderConfiguration(
        dispatch_batches=None,
        even_batches=False, 
    )
    
    accelerator = Accelerator(
        gradient_accumulation_steps=ga_steps,
        mixed_precision=mixed_precision,
        dataloader_config=dataloader_config,
    )

    # DeepSpeed: Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‚ÑŒ train_micro_batch_size_per_gpu
    # ĞµÑĞ»Ğ¸ Ğ¼Ñ‹ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼ DataLoader Ğ² accelerator.prepare() (dataset-level sharding).
    # Ğ‘ĞµĞ· ÑÑ‚Ğ¾Ğ³Ğ¾ DeepSpeed Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ "requires you to pass at least one dataloader with batch_size".
    if accelerator.state.deepspeed_plugin is not None:
        batch_size = int(config.get("batch_size", 1))
        ds_cfg = accelerator.state.deepspeed_plugin.deepspeed_config
        current_mbs = ds_cfg.get("train_micro_batch_size_per_gpu")
        if current_mbs in (None, "auto"):
            ds_cfg["train_micro_batch_size_per_gpu"] = batch_size
            logger.info(f"DeepSpeed: set train_micro_batch_size_per_gpu={batch_size}")
        # Ğ¢Ğ°ĞºĞ¶Ğµ Ğ·Ğ°Ğ´Ğ°Ğ´Ğ¸Ğ¼ train_batch_size ĞµÑĞ»Ğ¸ auto
        current_tbs = ds_cfg.get("train_batch_size")
        if current_tbs in (None, "auto"):
            # train_batch_size = micro_batch * grad_accum * world_size
            world_size = accelerator.num_processes
            ds_cfg["train_batch_size"] = batch_size * ga_steps * world_size
            logger.info(f"DeepSpeed: set train_batch_size={ds_cfg['train_batch_size']}")

    # HomeModel FlashAttention: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ PyTorch SDPA (scaled_dot_product_attention).
    # Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ flash kernels, Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ CUDA SDPA backends (ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹).
    if torch.cuda.is_available():
        try:
            if use_flash_attention:
                torch.backends.cuda.enable_flash_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(True)
                torch.backends.cuda.enable_math_sdp(True)
            else:
                # Ğ¯Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰Ğ°ĞµĞ¼ flash/mem_efficient kernels, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ math (eager/Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹)
                torch.backends.cuda.enable_flash_sdp(False)
                torch.backends.cuda.enable_mem_efficient_sdp(False)
                torch.backends.cuda.enable_math_sdp(True)
            logger.info(
                "SDPA kernels: flash=%s mem_efficient=%s math=%s (use_flash_attention=%s)",
                getattr(torch.backends.cuda, "flash_sdp_enabled", lambda: "N/A")(),
                getattr(torch.backends.cuda, "mem_efficient_sdp_enabled", lambda: "N/A")(),
                getattr(torch.backends.cuda, "math_sdp_enabled", lambda: "N/A")(),
                use_flash_attention,
            )
        except Exception as e:
            logger.warning(f"Could not configure CUDA SDPA kernels: {e}")
    
    # Ğ’ĞĞ–ĞĞ: MetricsLogger Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main process Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ½Ğ¾Ğº Ğ¿Ñ€Ğ¸ Multi-GPU
    metrics = MetricsLogger(metrics_path, enabled=accelerator.is_main_process)
    
    try:
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ
        adapter = resolve_adapter(config)
        logger.info(f"Using adapter: {adapter.__class__.__name__}")
        
        metrics.update(status="loading_tokenizer", stage=stage)
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€
        tokenizer_path = config.get("tokenizer_path")
        if not tokenizer_path and stage in ("sft", "continual_pretrain") and config.get("base_model_path"):
            # Ğ”Ğ»Ñ SFT/continual Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ· Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            tokenizer_path = config["base_model_path"]
        elif not tokenizer_path:
            # Ğ”Ğ»Ñ pretrain Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ model_id ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½, Ğ¸Ğ½Ğ°Ñ‡Ğµ gpt2
            tokenizer_path = config.get("model_id", "gpt2")
        
        tokenizer = adapter.load_tokenizer(tokenizer_path, trust_remote_code=True)
        tokenizer = adapter.prepare_tokenizer(tokenizer)  # pad_token = eos_token
        
        metrics.update(status=f"loading_dataset ({stage})")

        # ---------------------------
        # Sharding mode
        # ---------------------------
        # auto: Ğ´Ğ»Ñ IterableDataset Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ dataset-level ÑˆĞ°Ñ€Ğ´Ğ¸Ğ½Ğ³ (ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ + ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ Ñ strict resume)
        # accelerate: ÑˆĞ°Ñ€Ğ´Ğ¸Ğ½Ğ³ Ğ´ĞµĞ»Ğ°ĞµÑ‚ accelerator.prepare(DataLoader)
        # dataset: ÑˆĞ°Ñ€Ğ´Ğ¸Ğ½Ğ³ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ (shard=True) Ğ¸ DataLoader ĞĞ• Ğ·Ğ°Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ² accelerate.prepare
        sharding_mode = str(config.get("sharding_mode", "auto")).lower().strip()
        if sharding_mode not in ("auto", "dataset", "accelerate"):
            raise ValueError(f"Invalid sharding_mode={sharding_mode}. Expected one of: auto|dataset|accelerate")
        
        # Ğ’ĞĞ–ĞĞ: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ val_ratio Ğ”Ğ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ train_dataset, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ data leakage
        val_ratio = float(config.get("val_ratio", 0.0))
        eval_every = int(config.get("eval_every", 0) or 0)
        eval_batches = int(config.get("eval_batches", 20) or 20)
        
        # Ğ”ĞµÑ€Ğ¶Ğ¸Ğ¼ holdout Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ eval
        holdout_ratio = val_ratio if (val_ratio > 0.0 and eval_every > 0) else 0.0
        
        # Dataset Selection based on Stage
        # Ğ’ĞĞ–ĞĞ: Ğ¨Ğ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ accelerate.prepare() Ñ‡ĞµÑ€ĞµĞ· DataLoaderShard/Dispatcher
        # Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ train/val split, Ğ½Ğ¾ ĞĞ• ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ (shard=False)
        if stage == "sft":
            # SFTDataset â€” IterableDataset, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ auto -> dataset-level
            effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
            ds_shard = (effective_shard_mode == "dataset")
            train_dataset = SFTDataset(
                config["data_path"],
                tokenizer,
                seq_len=config["seq_len"],
                sft_columns=config.get("sft_columns"),
                sft_template=config.get("sft_template"),
                chat_template=config.get("chat_template"),  # âœ… Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ chat_template Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                num_replicas=accelerator.num_processes,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                rank=accelerator.process_index,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                split="train",
                val_ratio=holdout_ratio,
                shard=ds_shard,
            )
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ´Ğ»Ñ SFT)
            try:
                if hasattr(train_dataset, 'get_sample_prompt'):
                    sample_prompt = train_dataset.get_sample_prompt(max_samples=20)
                    if sample_prompt:
                        metrics.update(sample_prompt=sample_prompt)
                        logger.info("Sample prompt saved to metrics")
            except Exception as e:
                logger.warning(f"Failed to get sample prompt: {e}")
            
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° (Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ° Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ°)
            if hasattr(train_dataset, 'num_replicas'):
                logger.info(f"Dataset initialized: num_replicas={train_dataset.num_replicas}, rank={train_dataset.rank}")
            
            # Ğ”Ğ»Ñ SFT data collator Ğ½Ğµ Ğ½ÑƒĞ¶ĞµĞ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ masking, 
            # Ğ½Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ°Ğ´Ğ´Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ñ‹ Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğµ.
            # SFTDataset ÑƒĞ¶Ğµ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ñ‹, DataCollatorForLanguageModeling Ñ mlm=False Ğ¿Ğ¾Ğ´Ğ¾Ğ¹Ğ´ĞµÑ‚
            # Ğ¸Ğ»Ğ¸ default_data_collator ĞµÑĞ»Ğ¸ labels ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ (Ğ° Ğ¾Ğ½Ğ¸ ĞµÑÑ‚ÑŒ Ğ² SFTDataset)
            from transformers import default_data_collator
            collate_fn = default_data_collator
        else:
            # Pretrain Ğ¸Ğ»Ğ¸ Continual Pretrain - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ StreamingTextDataset
            # Strict resume Ğ´Ğ»Ñ StreamingTextDataset (.jsonl): Ğ¿Ñ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ byte_offset/global_idx Ğ¸Ğ· checkpoint
            resume_state = None
            resume_from_checkpoint_cfg = config.get("resume_from_checkpoint")
            strict_resume = bool(config.get("strict_dataloader_resume", True))
            effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
            if strict_resume and effective_shard_mode != "dataset":
                # Ğ˜Ğ½Ğ°Ñ‡Ğµ Ğ¼Ñ‹ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ¾Ğ³ÑƒÑ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¿Ğ¾ rank
                raise ValueError("strict_dataloader_resume=True requires sharding_mode='dataset' for streaming datasets")
            if resume_from_checkpoint_cfg and strict_resume:
                try:
                    p = Path(resume_from_checkpoint_cfg)
                    # per-rank state
                    rs_path = p / f"dataloader_state_rank{accelerator.process_index}.json"
                    if rs_path.exists():
                        with open(rs_path, "r", encoding="utf-8") as f:
                            resume_state = json.load(f)
                        logger.info(
                            f"Loaded dataloader resume state for rank {accelerator.process_index}: "
                            f"byte_offset={resume_state.get('byte_offset')} global_idx={resume_state.get('global_idx')}"
                        )
                except Exception as e:
                    logger.warning(f"Failed to load dataloader resume state: {e}")

            train_dataset = StreamingTextDataset(
                config["data_path"], 
                tokenizer, 
                seq_len=config["seq_len"],
                num_replicas=accelerator.num_processes,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                rank=accelerator.process_index,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                split="train",
                val_ratio=holdout_ratio,
                shard=(effective_shard_mode == "dataset"),
                resume_byte_offset=(resume_state.get("byte_offset", 0) if resume_state else 0),
                resume_global_idx=(resume_state.get("global_idx", 0) if resume_state else 0),
                strict_resume=strict_resume,
            )
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ´Ğ»Ñ pretrain/continual_pretrain)
            try:
                if hasattr(train_dataset, 'get_sample_prompt'):
                    sample_prompt = train_dataset.get_sample_prompt(max_samples=20)
                    if sample_prompt:
                        metrics.update(sample_prompt=sample_prompt)
                        logger.info("Sample prompt saved to metrics")
            except Exception as e:
                logger.warning(f"Failed to get sample prompt: {e}")
            
            # Ğ’ĞĞ–ĞĞ: Ğ”Ğ»Ñ pretrain Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ collator, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ attention_mask,
            # Ğ° Ğ½Ğµ Ğ¿Ğ¾ pad_token_id. Ğ­Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾, ĞµÑĞ»Ğ¸ pad_token = eos_token (EOS Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ)
            def causal_lm_collator(batch):
                """ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ collator Ğ´Ğ»Ñ pretrain, Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ labels Ğ¿Ğ¾ attention_mask."""
                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ±Ğ¸Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ (ĞµÑĞ»Ğ¸ dataset Ğ²ĞµÑ€Ğ½ÑƒĞ» None)
                batch = [x for x in batch if x is not None]
                if not batch:
                     # ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ ĞºÑ€Ğ°ÑˆĞ¸Ñ‚ÑŒ accelerate, Ğ²ĞµÑ€Ğ½ĞµĞ¼ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ dict
                     # ĞĞ¾ accelerate Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ°Ñ€Ğ¸Ñ‚ÑŒ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ dict Ğ¿Ñ€Ğ¸ broadcast, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ²ĞµÑ€Ğ½ĞµĞ¼ dummy batch
                     # Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑˆĞ°Ğ³
                     dummy = torch.zeros((1, 1), dtype=torch.long)
                     return {"input_ids": dummy, "attention_mask": dummy, "labels": dummy}

                input_ids = torch.stack([x["input_ids"] for x in batch])
                attention_mask = torch.stack([x["attention_mask"] for x in batch])
                labels = input_ids.clone()
                labels[attention_mask == 0] = -100  # ĞœĞ°ÑĞºĞ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ padding, Ğ½Ğµ EOS
                return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
            
            collate_fn = causal_lm_collator
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ validation loader ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
        val_loader = None
        
        if holdout_ratio > 0.0:
            # Ğ’ĞĞ–ĞĞ: Ğ¨Ğ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ accelerate.prepare() Ñ‡ĞµÑ€ĞµĞ· DataLoaderShard/Dispatcher
            # Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ train/val split, Ğ½Ğ¾ ĞĞ• ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ (shard=False)
            if stage == "sft":
                # SFTDataset ÑƒĞ¶Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾
                # reuse effective_shard_mode from above if set, otherwise compute here
                effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
                ds_shard = (effective_shard_mode == "dataset")
                val_dataset = SFTDataset(
                    config["data_path"],
                    tokenizer,
                    seq_len=config["seq_len"],
                    sft_columns=config.get("sft_columns"),
                    sft_template=config.get("sft_template"),
                    chat_template=config.get("chat_template"),  # âœ… Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ chat_template Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                    num_replicas=accelerator.num_processes,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                    rank=accelerator.process_index,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                    split="val",
                    val_ratio=holdout_ratio,
                    shard=ds_shard,
                )
                from transformers import default_data_collator
                val_collate = default_data_collator
            else:
                # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒĞ¶Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ StreamingTextDataset
                effective_shard_mode = "dataset" if sharding_mode == "auto" else sharding_mode
                val_dataset = StreamingTextDataset(
                    config["data_path"],
                    tokenizer,
                    seq_len=config["seq_len"],
                    num_replicas=accelerator.num_processes,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                    rank=accelerator.process_index,  # âœ… Ğ¯Ğ²Ğ½Ğ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                    split="val",
                    val_ratio=holdout_ratio,
                    shard=(effective_shard_mode == "dataset"),
                    strict_resume=False,  # val Ğ½Ğµ Ñ€ĞµĞ·ÑĞ¼Ğ¸Ğ¼ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾
                )
                # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ collator Ğ´Ğ»Ñ val, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ´Ğ»Ñ train
                def causal_lm_collator(batch):
                    """ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ collator Ğ´Ğ»Ñ pretrain, Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒĞµÑ‚ labels Ğ¿Ğ¾ attention_mask."""
                    batch = [x for x in batch if x is not None]
                    if not batch:
                        dummy = torch.zeros((1, 1), dtype=torch.long)
                        return {"input_ids": dummy, "attention_mask": dummy, "labels": dummy}

                    input_ids = torch.stack([x["input_ids"] for x in batch])
                    attention_mask = torch.stack([x["attention_mask"] for x in batch])
                    labels = input_ids.clone()
                    labels[attention_mask == 0] = -100  # ĞœĞ°ÑĞºĞ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ padding, Ğ½Ğµ EOS
                    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
                
                val_collate = causal_lm_collator
            
            # Ğ’ĞĞ–ĞĞ: num_workers=0 Ğ´Ğ»Ñ val, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ shard=False
            val_loader = DataLoader(
                val_dataset,
                batch_size=config["batch_size"],
                collate_fn=val_collate,
                num_workers=0,  # Ğ‘ĞµĞ· workers Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²
            )
            
        # Ğ’ĞĞ–ĞĞ: num_workers=0 Ğ´Ğ»Ñ IterableDataset, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        # drop_last=True Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ DDP (Ğ²ÑĞµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹)
        train_loader = DataLoader(
            train_dataset,
            batch_size=config["batch_size"],
            collate_fn=collate_fn,
            num_workers=0,  # IterableDataset + num_workers>0 = Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            drop_last=True,  # âœ… ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ DDP: Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµĞ¼ Ñ€Ğ°ÑÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ° Ğ¿Ğ¾ Ñ‡Ğ¸ÑĞ»Ñƒ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸
        )
        
        metrics.update(status="building_model")
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€
        resume_from_checkpoint = None
        base_model_path = config.get("base_model_path")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ ÑÑ‚Ğ¾ accelerate checkpoint (Ğ´Ğ»Ñ resume)
        # Ğ’ĞĞ–ĞĞ: pytorch_model.bin.index.json Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ Ñƒ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… HF-ÑĞµĞ¹Ğ²Ğ¾Ğ²,
        # Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ accelerator_state.json - ÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğº accelerate checkpoint
        def is_accelerate_checkpoint(p: Path) -> bool:
            """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¿ÑƒÑ‚ÑŒ accelerate checkpoint'Ğ¾Ğ¼."""
            return (p / "accelerator_state.json").exists()
        
        if base_model_path:
            base_path = Path(base_model_path)
            is_checkpoint = is_accelerate_checkpoint(base_path)
            
            if is_checkpoint and stage == "continual_pretrain":
                resume_from_checkpoint = str(base_path)
                logger.info(f"Continual pretraining: will resume from accelerate checkpoint {base_path}")
            elif is_checkpoint and stage != "continual_pretrain":
                logger.warning(
                    f"Found accelerate checkpoint at {base_path}, but stage is {stage}. "
                    f"Resume from checkpoint is only supported for continual_pretrain. "
                    f"Will load as regular model instead."
                )
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€
        model, model_config = adapter.load_for_training(
            base_model_path=base_model_path,
            stage=stage,
            tokenizer=tokenizer,
            config=config,
            trust_remote_code=True,
        )
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ max_position_embeddings Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ seq_len ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
        model_max_pos = getattr(model.config, "max_position_embeddings", config.get("seq_len", 2048))
        user_seq_len = config.get("seq_len", 2048)
        if user_seq_len > model_max_pos:
            logger.warning(
                f"âš ï¸ seq_len ({user_seq_len}) > model max_position_embeddings ({model_max_pos}). "
                f"Truncating to {model_max_pos} to avoid index out of bounds errors."
            )
            config["seq_len"] = model_max_pos
        
        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (resize, LoRA, use_cache, etc.)
        model = adapter.prepare_for_training(model, tokenizer, config)
        
        # ============================================================
        # Liger Kernel Ğ¿Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³ (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ RMSNorm, RoPE, MLP, FusedCE)
        # ============================================================
        use_liger = config.get("use_liger", False)
        liger_fused_ce = config.get("liger_fused_ce", False)  # Fused lm_head + CE (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ!)
        
        # ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ fused CE loss (ĞµÑĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾)
        liger_fused_ce_loss = None
        
        if use_liger and LIGER_UTILS_AVAILABLE and is_liger_available():
            try:
                model_type = getattr(model.config, "model_type", "").lower()
                
                # ĞŸĞ°Ñ‚Ñ‡Ğ¸Ğ¼ RMSNorm, RoPE, MLP
                patched = apply_liger_patch_to_model(
                    model,
                    patch_rms_norm=True,
                    patch_rope=True,
                    patch_mlp=True,
                    patch_fused_linear_ce=False,  # Ğ‘ÑƒĞ´ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ loss module
                )
                if patched:
                    logger.info("âœ… Liger Kernel Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ñ‹ (RMSNorm, RoPE, MLP)")
                else:
                    logger.info("â„¹ï¸ Liger: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³Ğ° (home Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ñ€.)")
                
                # ğŸ”¥ Fused Linear CrossEntropy â€” ĞĞ• Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ logits!
                # Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ğ³Ğ¸Ğ³Ğ°Ğ±Ğ°Ğ¹Ñ‚Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ vocab_size
                # Ğ’ĞĞ–ĞĞ: Fused CE Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ğ›Ğ®Ğ‘ĞĞ™ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ lm_head, Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ HF Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹!
                # âš ï¸ Ğ”Ğ»Ñ ZeRO-3 Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼ accelerator Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
                if liger_fused_ce:
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞµÑÑ‚ÑŒ Ğ»Ğ¸ lm_head Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                    has_lm_head = hasattr(model, 'lm_head') and model.lm_head is not None
                    if has_lm_head:
                        try:
                            liger_fused_ce_loss = create_liger_fused_ce(
                                model,
                                ignore_index=-100,
                                label_smoothing=config.get("label_smoothing", 0.0),
                                accelerator=accelerator,  # âœ… Ğ”Ğ»Ñ ZeRO-3 Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸
                            )
                            if liger_fused_ce_loss:
                                logger.info("ğŸ¦ LigerFusedLinearCrossEntropyLoss Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½!")
                                logger.info("   âš¡ Logits ĞĞ• Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ â€” ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸!")
                            else:
                                logger.warning("âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ LigerFusedCELoss, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹")
                        except Exception as e:
                            logger.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ LigerFusedCELoss: {e}")
                    else:
                        logger.info(f"â„¹ï¸ LigerFusedCE: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ lm_head, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Liger Ğ¿Ğ°Ñ‚Ñ‡Ğ¸: {e}")
        elif use_liger and not LIGER_UTILS_AVAILABLE:
            logger.warning("âš ï¸ Liger Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑˆĞµĞ½, Ğ½Ğ¾ liger_utils Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½")
        elif use_liger and not is_liger_available():
            logger.warning("âš ï¸ Liger Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑˆĞµĞ½, Ğ½Ğ¾ liger-kernel Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")

        # Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ°: dtype Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ñ‘Ğ½ Ğ»Ğ¸ SDPA/flash path Ñƒ HomeModel
        try:
            first = next(model.parameters())
            logger.info(f"ğŸ” Model weights dtype (first param): {first.dtype}")
        except Exception:
            pass
        try:
            flash_mods = [m for m in model.modules() if hasattr(m, "flash")]
            if flash_mods:
                enabled = sum(1 for m in flash_mods if bool(getattr(m, "flash", False)))
                logger.info(f"ğŸ” SDPA/flash modules: {enabled}/{len(flash_mods)} enabled")
        except Exception:
            pass
        
        # ĞŸĞ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
        num_params = sum(p.numel() for p in model.parameters())
        metrics.update(num_parameters=num_params)
        
        if config.get("grad_checkpoint", False):
            model.gradient_checkpointing_enable()
        
        # Scheduler - Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ²
        # Ğ•ÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ max_steps - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞµĞ³Ğ¾
        if config.get("max_steps"):
            max_train_steps = config["max_steps"]
        else:
            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°
            # Ğ’ĞĞ–ĞĞ: len(train_dataset) Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ split
            # ĞĞ¾ Ğ¿Ñ€Ğ¸ DDP/FSDP/ZeRO accelerate.prepare() ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸
            # ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑƒĞ²Ğ¸Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ dataset_len / num_processes Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²
            try:
                dataset_len = len(train_dataset)  # ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ split
                # Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼ (Ğ´Ğ»Ñ IterableDataset ÑÑ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾)
                per_proc_len = math.ceil(dataset_len / accelerator.num_processes)
                
                # batch_size Ğ·Ğ´ĞµÑÑŒ per-device, gradient_accumulation ÑƒĞ¶Ğµ ÑƒÑ‡Ñ‚ĞµĞ½
                steps_per_epoch = math.ceil(per_proc_len / config["batch_size"])
                num_update_steps_per_epoch = math.ceil(steps_per_epoch / config["gradient_accumulation"])
                max_train_steps = config["epochs"] * num_update_steps_per_epoch
            except (TypeError, AttributeError):
                # Ğ”Ğ»Ñ streaming dataset Ğ±ĞµĞ· __len__ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ save_every * 10 ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºÑƒ
                max_train_steps = config.get("save_every", 5000) * 2
        
        planned_total_steps = int(max_train_steps)
        metrics.update(
            total_steps=planned_total_steps,
            planned_total_steps=planned_total_steps,
            max_steps_estimated=config.get("max_steps") is None,
        )
        
        # Ğ’ĞĞ–ĞĞ: Ğ”Ğ»Ñ LoRA/QLoRA Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ€Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ trainable Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
        # Ğ­Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ QLoRA, Ğ³Ğ´Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ
        trainable_named_params = [(n, p) for n, p in model.named_parameters() if p.requires_grad]
        trainable_params = [p for _, p in trainable_named_params]
        if len(trainable_params) == 0:
            raise ValueError("No trainable parameters found in model. Check LoRA/QLoRA configuration.")
        
        logger.info(f"Optimizing {len(trainable_params)} trainable parameters "
                   f"(total: {sum(p.numel() for p in model.parameters())})")
        
        uses_deepspeed = getattr(accelerator.state, "deepspeed_plugin", None) is not None
        optimizer_name = str(config.get("optimizer", "adamw")).strip().lower()
        lr = float(config["learning_rate"])
        wd = float(config.get("weight_decay", 0.1))
        betas = tuple(config.get("betas", (0.9, 0.95)))
        eps = float(config.get("eps", 1e-8))

        if optimizer_name in ("adamw_8bit", "adamw8bit"):
            if uses_deepspeed:
                raise RuntimeError(
                    "Optimizer 'adamw_8bit' is not supported with DeepSpeed in this training path."
                )
            else:
                try:
                    from bitsandbytes.optim import AdamW8bit
                    optimizer = AdamW8bit(
                        trainable_params,
                        lr=lr,
                        betas=betas,
                        eps=eps,
                        weight_decay=wd,
                    )
                except ImportError:
                    raise RuntimeError(
                        "Optimizer 'adamw_8bit' requested, but bitsandbytes is not installed."
                    )
        elif optimizer_name in ("magma_adamw", "magma"):
            optimizer_name = "magma_adamw"
            optimizer = MagmaAdamW(
                trainable_params,
                lr=lr,
                betas=betas,
                eps=eps,
                weight_decay=wd,
                magma_prob=float(config.get("magma_prob", 0.5)),
                magma_tau=float(config.get("magma_tau", 2.0)),
                magma_ema_beta=float(config.get("magma_ema_beta", 0.9)),
                magma_cosine_eps=float(config.get("magma_cosine_eps", 1e-12)),
            )
        elif optimizer_name == "muon":
            if uses_deepspeed:
                raise RuntimeError(
                    "Optimizer 'muon' currently does not support DeepSpeed mode in this training path."
                )
            ns_coeff = config.get("muon_ns_coefficients", [3.4445, -4.775, 2.0315])
            if not isinstance(ns_coeff, (list, tuple)) or len(ns_coeff) != 3:
                raise ValueError(
                    f"muon_ns_coefficients must be a list/tuple of 3 floats, got: {ns_coeff}"
                )

            adjust_lr_fn = config.get("muon_adjust_lr_fn", "original")
            if adjust_lr_fn not in (None, "original", "match_rms_adamw"):
                raise ValueError(
                    f"muon_adjust_lr_fn must be one of None|'original'|'match_rms_adamw', got: {adjust_lr_fn}"
                )

            muon_exclude_patterns = config.get(
                "muon_exclude_patterns",
                [
                    r"(^|\.)(embed|embeddings|embed_tokens|tok_embeddings|token_embeddings|wte|wpe|word_embeddings|position_embeddings)(\.|$)",
                    r"(^|\.)(lm_head|output|classifier|head)(\.|$)",
                    r"(^|\.)(lora(_[AaBb])?|lora[AaBb]?|adapter|adapters|ia3|prompt|prefix)(\.|$)",
                ],
            )
            muon_hidden_patterns = config.get(
                "muon_hidden_patterns",
                [
                    r"(^|\.)(layers|h|blocks)(\.|$)",
                ],
            )
            if not isinstance(muon_exclude_patterns, (list, tuple)) or not all(
                isinstance(x, str) for x in muon_exclude_patterns
            ):
                raise ValueError(
                    f"muon_exclude_patterns must be a list[str], got: {muon_exclude_patterns}"
                )
            if not isinstance(muon_hidden_patterns, (list, tuple)) or not all(
                isinstance(x, str) for x in muon_hidden_patterns
            ):
                raise ValueError(
                    f"muon_hidden_patterns must be a list[str], got: {muon_hidden_patterns}"
                )

            muon_lr = float(config.get("muon_lr", 0.02))
            muon_weight_decay = float(config.get("muon_weight_decay", 0.01))
            muon_aux_adamw_lr = float(
                config.get("muon_aux_adamw_lr", config.get("muon_adamw_lr", 3e-4))
            )
            muon_aux_adamw_eps = float(
                config.get("muon_aux_adamw_eps", config.get("muon_adamw_eps", 1e-10))
            )
            muon_aux_adamw_weight_decay = float(
                config.get(
                    "muon_aux_adamw_weight_decay",
                    config.get("muon_adamw_weight_decay", 0.01),
                )
            )

            optimizer = MuonWithAuxAdam(
                trainable_params,
                named_params=trainable_named_params,
                lr=lr,
                weight_decay=wd,
                muon_lr=muon_lr,
                muon_weight_decay=muon_weight_decay,
                adamw_lr=muon_aux_adamw_lr,
                adamw_weight_decay=muon_aux_adamw_weight_decay,
                muon_momentum=float(config.get("muon_momentum", 0.95)),
                muon_nesterov=bool(config.get("muon_nesterov", True)),
                muon_ns_coefficients=(float(ns_coeff[0]), float(ns_coeff[1]), float(ns_coeff[2])),
                muon_eps=eps,
                muon_ns_steps=int(config.get("muon_ns_steps", 5)),
                muon_adjust_lr_fn=adjust_lr_fn,
                muon_hidden_patterns=tuple(muon_hidden_patterns),
                muon_exclude_patterns=tuple(muon_exclude_patterns),
                adamw_betas=betas,
                adamw_eps=muon_aux_adamw_eps,
            )
        else:
            optimizer_name = "adamw"
            optimizer = torch.optim.AdamW(
                trainable_params,  # âœ… Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ trainable Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ (ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ LoRA/QLoRA)
                lr=lr,
                betas=betas,
                eps=eps,
                weight_decay=wd,
            )

        logger.info(f"Using optimizer: {optimizer_name}")
        metrics.update(
            optimizer=optimizer_name,
            weight_decay=wd,
            betas=[float(betas[0]), float(betas[1])],
            eps=eps,
        )
        if optimizer_name == "muon" and isinstance(optimizer, MuonWithAuxAdam):
            logger.info(
                "Muon split: muon_params=%s, adamw_params=%s, muon_lr=%.3e, aux_adamw_lr=%.3e, muon_wd=%.4g, aux_adamw_wd=%.4g",
                f"{optimizer.muon_param_count:,}",
                f"{optimizer.adamw_param_count:,}",
                optimizer.muon_lr,
                optimizer.adamw_lr,
                optimizer.muon_weight_decay,
                optimizer.adamw_weight_decay,
            )
            logger.info(
                "Muon split detail: total_2d_tensors=%s, nonhidden_2d=%s, excluded_2d_by_pattern=%s, unnamed_2d=%s",
                optimizer.total_2d_param_tensors,
                optimizer.nonhidden_2d_param_tensors,
                optimizer.excluded_2d_param_tensors,
                optimizer.unnamed_2d_param_tensors,
            )
            logger.info("Muon sample param names: %s", optimizer.muon_param_names_sample)
            logger.info("Aux AdamW sample param names: %s", optimizer.adamw_param_names_sample)
            metrics.update(
                muon_impl="muon_with_aux_adam",
                muon_param_count=int(optimizer.muon_param_count),
                muon_aux_adamw_param_count=int(optimizer.adamw_param_count),
                muon_total_2d_param_tensors=int(optimizer.total_2d_param_tensors),
                muon_nonhidden_2d_param_tensors=int(optimizer.nonhidden_2d_param_tensors),
                muon_excluded_2d_param_tensors=int(optimizer.excluded_2d_param_tensors),
                muon_unnamed_2d_param_tensors=int(optimizer.unnamed_2d_param_tensors),
                muon_adjust_lr_fn=adjust_lr_fn,
                muon_hidden_patterns=list(muon_hidden_patterns),
                muon_exclude_patterns=list(muon_exclude_patterns),
                muon_lr=float(optimizer.muon_lr),
                muon_weight_decay=float(optimizer.muon_weight_decay),
                muon_aux_adamw_lr=float(optimizer.adamw_lr),
                muon_aux_adamw_weight_decay=float(optimizer.adamw_weight_decay),
                muon_param_names_sample=list(optimizer.muon_param_names_sample),
                muon_aux_adamw_param_names_sample=list(optimizer.adamw_param_names_sample),
            )
        # LR scheduler
        # Ğ’ĞĞ–ĞĞ: Ğ¼Ñ‹ ÑˆĞ°Ğ³Ğ°ĞµĞ¼ scheduler Ğ½Ğ° UPDATE-step (ĞºĞ¾Ğ³Ğ´Ğ° accelerator.sync_gradients=True).
        # Ğ”Ğ»Ñ resume Ğ¸Ğ· ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ² ÑÑ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾: Ñ€Ğ°Ğ½ÑŒÑˆĞµ scheduler Ğ¼Ğ¾Ğ³ ÑˆĞ°Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ micro-step,
        # Ğ¸ Ñ‚Ğ¾Ğ³Ğ´Ğ° Ğ½Ğ° resume LR ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ~0.
        lr_schedule = str(config.get("lr_schedule", "cosine")).strip().lower()
        min_lr_ratio = float(config.get("min_lr_ratio", 0.0) or 0.0)
        warmup_steps = int(config.get("warmup_steps", 0))
        total_steps_for_sched = int(max_train_steps)

        if lr_schedule in ("cosine", "cosine_with_warmup") and min_lr_ratio > 0:
            import math as _math
            from torch.optim.lr_scheduler import LambdaLR

            def lr_lambda(step: int) -> float:
                if warmup_steps > 0 and step < warmup_steps:
                    return float(step) / float(max(1, warmup_steps))
                if total_steps_for_sched <= warmup_steps:
                    return 1.0
                progress = float(step - warmup_steps) / float(max(1, total_steps_for_sched - warmup_steps))
                progress = min(max(progress, 0.0), 1.0)
                cosine = 0.5 * (1.0 + _math.cos(_math.pi * progress))
                return float(min_lr_ratio + (1.0 - min_lr_ratio) * cosine)

            lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)
        else:
            # get_scheduler Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚: linear/cosine/constant/cosine_with_restarts/...
            # Ğ”Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸: ÑÑ‚Ğ°Ñ€Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ "cosine_with_warmup" Ğ¼Ğ°Ğ¿Ğ¿Ğ¸Ğ¼ Ğ² "cosine".
            sched_name = "cosine" if lr_schedule == "cosine_with_warmup" else lr_schedule
            lr_scheduler = get_scheduler(
                name=sched_name,
                optimizer=optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps_for_sched,
            )

        metrics.update(
            scheduler=str(lr_schedule),
            warmup_steps=int(warmup_steps),
            min_lr_ratio=float(min_lr_ratio),
        )
        
        # IMPORTANT (Sharding contract):
        # Ğ£ Ğ½Ğ°Ñ Ğ”Ğ’Ğ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ° ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:
        #  1) dataset-level (StreamingTextDataset/SFTDataset Ñ shard=True, ÑĞ²Ğ½Ñ‹Ğ¹ num_replicas/rank)
        #  2) accelerate-level (accelerator.prepare(DataLoader) -> DataLoaderShard/Dispatcher)
        #
        # ĞĞ•Ğ›Ğ¬Ğ—Ğ¯ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ â€” Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ±ÑƒĞ´ĞµÑ‚ "Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑˆĞ°Ñ€Ğ´Ğ¸Ğ½Ğ³" Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ/ÑˆĞ°Ğ³Ğ¸ ÑÑ‚Ğ°Ğ½ÑƒÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ² N Ñ€Ğ°Ğ·.
        # Ğ”Ğ»Ñ IterableDataset Ğ¼Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ dataset-level ÑˆĞ°Ñ€Ğ´Ğ¸Ğ½Ğ³ ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹.
        is_streaming_sharded = isinstance(train_dataset, IterableDataset) and getattr(train_dataset, "shard", False) is True
        effective_shard_mode = "dataset" if is_streaming_sharded else "accelerate"
        if accelerator.is_main_process:
            metrics.update(
                sharding_mode=effective_shard_mode,
                sharding_mode_requested=sharding_mode,
                num_processes=int(accelerator.num_processes),
            )

        # FSDP: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ transformer layer ĞºĞ»Ğ°ÑÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ ĞµÑÑ‚ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
        # Ğ”ĞµĞ»Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ”Ğ accelerator.prepare(), Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° ÑˆĞ°Ñ€Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….
        fsdp_plugin = getattr(accelerator.state, "fsdp_plugin", None)
        if fsdp_plugin is not None:
            def _model_has_layer_class(class_name: str) -> bool:
                for _m in model.modules():
                    if _m.__class__.__name__ == class_name:
                        return True
                return False

            def _infer_fsdp_wrap_cls():
                skip = {
                    "ModuleList", "ModuleDict", "Sequential", "Embedding", "Linear",
                    "LayerNorm", "RMSNorm", "Dropout", "SiLU", "GELU", "ReLU",
                }
                counts = {}
                cls_by_name = {}
                for _m in model.modules():
                    name = _m.__class__.__name__
                    if name in skip:
                        continue
                    if name.endswith("DecoderLayer") or name.endswith("Block") or name.endswith("Layer"):
                        counts[name] = counts.get(name, 0) + 1
                        cls_by_name[name] = _m.__class__
                if not counts:
                    return None
                best_name = max(counts, key=counts.get)
                if counts[best_name] < 2:
                    return None
                return cls_by_name[best_name]

            # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· plugin (Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ set/frozenset)
            cfg_names = []
            current_wrap = getattr(fsdp_plugin, "transformer_cls_names_to_wrap", None)
            if current_wrap:
                if isinstance(current_wrap, (set, frozenset, list, tuple)):
                    cfg_names = [n for n in current_wrap if isinstance(n, str)]
                elif isinstance(current_wrap, str):
                    cfg_names = [current_wrap]
            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ Ğ¸ "auto" Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
            cfg_names = [n for n in cfg_names if n and n.lower() != "auto"]

            # Ğ•ÑĞ»Ğ¸ ĞºĞ»Ğ°ÑÑ Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½, "auto", Ğ¸Ğ»Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸
            need_infer = not cfg_names or not any(_model_has_layer_class(n) for n in cfg_names)
            if need_infer:
                inferred_cls = _infer_fsdp_wrap_cls()
                if inferred_cls is not None:
                    inferred_name = inferred_cls.__name__
                    # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚ accelerate
                    fsdp_plugin.transformer_cls_names_to_wrap = {inferred_name}
                    logger.info(
                        f"âœ… FSDP: Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½ transformer layer ĞºĞ»Ğ°ÑÑ: {inferred_name}"
                    )
                else:
                    logger.warning(
                        "âš ï¸ FSDP: Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ transformer layer ĞºĞ»Ğ°ÑÑ. "
                        "Ğ•ÑĞ»Ğ¸ prepare() ÑƒĞ¿Ğ°Ğ´Ñ‘Ñ‚, ÑƒĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ fsdp_transformer_layer_cls_to_wrap Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğµ."
                    )
        if is_streaming_sharded:
            if val_loader is not None:
                model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
            else:
                model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
            logger.info("Using dataset-level sharding for IterableDataset -> skipping accelerator.prepare() for DataLoader(s)")
        else:
            if val_loader is not None:
                model, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(
                    model, optimizer, train_loader, val_loader, lr_scheduler
                )
            else:
                model, optimizer, train_loader, lr_scheduler = accelerator.prepare(
                    model, optimizer, train_loader, lr_scheduler
                )

        # ĞŸĞ¾ÑĞ»Ğµ accelerator.prepare Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±Ñ‘Ñ€Ğ½ÑƒÑ‚Ğ° (FSDP/DeepSpeed).
        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑÑÑ‹Ğ»ĞºÑƒ Ğ² LigerFusedCEModule, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹.
        if liger_fused_ce_loss is not None and hasattr(liger_fused_ce_loss, "set_model"):
            try:
                liger_fused_ce_loss.set_model(model)
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ (FSDP/DTensor Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ)
                logger.info("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ is_supported()...")
                if hasattr(liger_fused_ce_loss, "is_supported"):
                    is_supported = liger_fused_ce_loss.is_supported()
                    logger.info(f"ğŸ” is_supported() Ğ²ĞµÑ€Ğ½ÑƒĞ»: {is_supported}")
                    if not is_supported:
                        logger.info("ğŸ¦ Liger fused CE Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ñ‘Ğ½ (Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ Ñ FSDP/DTensor), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ path")
                        liger_fused_ce_loss = None  # ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ fused CE
                else:
                    logger.warning("âš ï¸ is_supported() Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
            except Exception as e:
                logger.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² set_model/is_supported: {e}")
                import traceback
                logger.warning(traceback.format_exc())
        
        # Resume Ğ¸Ğ· checkpoint (ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ´Ğ¸Ğ¹)
        starting_step = 0
        resume_batches_to_skip = 0
        
        # Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚ resume_from_checkpoint Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ (Ğ¾Ñ‚ CLI)
        if config.get("resume_from_checkpoint"):
            resume_from_checkpoint = config["resume_from_checkpoint"]
        # Ğ˜Ğ»Ğ¸ ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ continual_pretrain Ğ¸ Ğ¼Ñ‹ Ğ½Ğ°ÑˆĞ»Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚ Ğ² base_model_path
        elif stage == "continual_pretrain" and base_model_path:
             if is_accelerate_checkpoint(Path(base_model_path)):
                 resume_from_checkpoint = base_model_path

        if resume_from_checkpoint:
            try:
                logger.info(f"Resuming training from checkpoint: {resume_from_checkpoint}")
                accelerator.load_state(resume_from_checkpoint)
                
                # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ global_step Ğ¸Ğ· checkpoint
                checkpoint_meta = Path(resume_from_checkpoint) / "checkpoint_metadata.json"
                if checkpoint_meta.exists():
                    with open(checkpoint_meta) as f:
                        meta = json.load(f)
                        starting_step = meta.get("global_step", 0)
                else:
                    # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ¸Ğ· Ğ¸Ğ¼ĞµĞ½Ğ¸ Ğ¿Ğ°Ğ¿ĞºĞ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ checkpoint_step1000)
                    import re
                    match = re.search(r'step(\d+)', str(resume_from_checkpoint))
                    if match:
                        starting_step = int(match.group(1))
                
                logger.info(f"Resumed from step {starting_step}")
                metrics.update(status="resumed", resumed_from_step=starting_step)
                
                # Ğ’ĞĞ–ĞĞ: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°Ğ»Ğ¾Ğ°Ğ´ĞµÑ€Ğ°
                #
                # - Ğ”Ğ»Ñ map-style Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² accelerate.load_state() Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ sampler state, Ğ¸ Ğ´Ğ¾Ğ¿. skip ĞĞ• Ğ½ÑƒĞ¶ĞµĞ½.
                # - Ğ”Ğ»Ñ IterableDataset sampler state Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ => ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ "ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ" â€” Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾
                #   Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ°Ñ‚ÑŒ N Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ. Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (Ğ¸ Ğ²Ñ‹Ğ³Ğ»ÑĞ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑĞ°Ğ½Ğ¸Ğµ),
                #   Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ´ĞµĞ»Ğ°ĞµĞ¼ ÑÑ‚Ğ¾ ÑĞ²Ğ½Ğ¾ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸ Ğ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ epoch.
                if starting_step > 0 and isinstance(train_dataset, IterableDataset):
                    resume_skip_enabled = bool(config.get("resume_skip_batches", True))
                    # Ğ•ÑĞ»Ğ¸ Ñƒ StreamingTextDataset Ğ²ĞºĞ»ÑÑ‡Ñ‘Ğ½ strict_resume Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ byte_offset, skip Ğ½Ğµ Ğ½ÑƒĞ¶ĞµĞ½.
                    try:
                        if hasattr(train_dataset, "get_resume_state"):
                            rs = train_dataset.get_resume_state()
                            if rs.get("byte_offset", 0) and bool(config.get("strict_dataloader_resume", True)):
                                logger.info("Strict dataloader resume active (byte_offset present) -> skipping batches is NOT required.")
                                resume_skip_enabled = False
                    except Exception:
                        pass
                    if resume_skip_enabled:
                        resume_batches_to_skip = int(starting_step) * int(config["gradient_accumulation"])
                        # ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ "Ğ²Ğ¸ÑĞµÑ‚ÑŒ" Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ starting_step
                        max_skip = config.get("resume_skip_batches_max")
                        if max_skip is not None:
                            max_skip = int(max_skip)
                            if resume_batches_to_skip > max_skip:
                                logger.warning(
                                    f"Requested to skip {resume_batches_to_skip} batches, but resume_skip_batches_max={max_skip}. "
                                    f"Capping skip to {max_skip}. (May slightly desync dataloader position.)"
                                )
                                resume_batches_to_skip = max_skip
                        logger.info(f"Will skip {resume_batches_to_skip} batches (IterableDataset) to sync dataloader...")
                        metrics.update(status="skipping", skipping_batches=resume_batches_to_skip)
                    else:
                        logger.warning("resume_skip_batches=False: will NOT skip dataloader batches for IterableDataset (data order may differ after resume).")
                        resume_batches_to_skip = 0
                    
            except Exception as e:
                logger.error(f"Failed to resume from checkpoint {resume_from_checkpoint}: {e}")
                # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¯Ğ’ĞĞ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» resume (Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³) - Ğ¼Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ, Ğ° Ğ½Ğµ Ğ¼Ğ¾Ğ»Ñ‡Ğ° Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
                # Ğ˜ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ: auto-resume Ğ¿Ñ€Ğ¸ continual_pretrain (base_model_path) - Ñ‚ÑƒÑ‚ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ°Ñ€Ğ½Ğ¸Ğ½Ğ³
                is_explicit_resume = config.get("resume_from_checkpoint") is not None
                
                if is_explicit_resume:
                    metrics.update(status="error", resume_error=str(e), error=f"Resume failed: {e}")
                    raise RuntimeError(f"Could not resume from checkpoint {resume_from_checkpoint}: {e}")
                else:
                    logger.warning("Continuing without resume (starting from step 0)")
                    metrics.update(status="resume_failed", resume_error=str(e))

        
        output_dir = Path(config["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ run_config.json Ğ² Ğ¿Ğ°Ğ¿ĞºÑƒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¿Ñ€Ğ¸ resume
        if accelerator.is_main_process:
            run_config_path = output_dir / "run_config.json"
            if not run_config_path.exists():  # ĞĞµ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸ resume
                with open(run_config_path, "w", encoding="utf-8") as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                logger.info(f"Saved run_config.json to {run_config_path}")
        
        metrics.update(status="training")
        
        # Ğ’ĞĞ–ĞĞ: Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ global_step Ñ starting_step Ğ¿Ğ¾ÑĞ»Ğµ resume
        # Ğ­Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LR scheduler Ğ¸ checkpointing
        global_step = starting_step
        # Ğ’ĞĞ–ĞĞ: Ñ€ĞµÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ scheduler Ğº update-step Ğ¸Ğ½Ğ´ĞµĞºÑÑƒ.
        # Ğ­Ñ‚Ğ¾ Ğ»ĞµÑ‡Ğ¸Ñ‚ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° scheduler Ğ² Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğµ Ğ±Ñ‹Ğ» "Ğ¿Ñ€Ğ¾ĞºÑ€ÑƒÑ‡ĞµĞ½" Ğ¿Ğ¾ micro-step Ğ¸ LR ÑƒĞ»ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ~0.
        if starting_step > 0 and bool(config.get("scheduler_resync_on_resume", True)):
            try:
                lr_scheduler.step(int(global_step))
                if accelerator.is_main_process:
                    metrics.update(resume_scheduler_resynced=True, resume_scheduler_step=int(global_step))
            except TypeError:
                # fallback Ğ´Ğ»Ñ scheduler-Ğ¾Ğ² Ğ±ĞµĞ· Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ° step(epoch)
                try:
                    lr_scheduler.last_epoch = int(global_step)
                    lr_scheduler.step()
                    if accelerator.is_main_process:
                        metrics.update(resume_scheduler_resynced=True, resume_scheduler_step=int(global_step))
                except Exception as e:
                    logger.warning(f"Failed to resync LR scheduler on resume: {e}")
                    if accelerator.is_main_process:
                        metrics.update(resume_scheduler_resynced=False, resume_scheduler_error=str(e))
            except Exception as e:
                logger.warning(f"Failed to resync LR scheduler on resume: {e}")
                if accelerator.is_main_process:
                    metrics.update(resume_scheduler_resynced=False, resume_scheduler_error=str(e))
        
        # Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ eval_batches Ğ¸Ğ· Ğ·Ğ°Ğ¼Ñ‹ĞºĞ°Ğ½Ğ¸Ñ)
        def evaluate(val_loader):
            """
            Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° val_loader.
            
            Ğ’ĞĞ–ĞĞ: val_loader Ğ·Ğ°ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
            Ğ²Ğ¸Ğ´Ğ¸Ñ‚ ÑĞ²Ğ¾Ñ Ñ‡Ğ°ÑÑ‚ÑŒ validation Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. reduce() ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ ÑƒÑÑ€ĞµĞ´Ğ½ÑĞµÑ‚ loss Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼.
            
            ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸:
            - use_cache=False: Ğ½Ğµ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ KV-cache
            - Ğ•ÑĞ»Ğ¸ Liger fused CE Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞµĞ³Ğ¾ (Ğ½Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ logits)
            - Ğ˜Ğ½Ğ°Ñ‡Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ forward (Ğ½Ğ¾ logits Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ)
            """
            model.eval()
            losses = []
            with torch.no_grad():
                for i, batch in enumerate(val_loader):
                    if i >= eval_batches:
                        break
                    # Ğ•ÑĞ»Ğ¸ DataLoader Ğ½Ğµ Ğ±Ñ‹Ğ» Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½ accelerate'Ğ¾Ğ¼ â€” Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ ĞºĞ»Ğ°Ğ´Ñ‘Ğ¼ Ğ±Ğ°Ñ‚Ñ‡ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾
                    if is_streaming_sharded:
                        batch = {k: (v.to(accelerator.device) if hasattr(v, "to") else v) for k, v in batch.items()}
                    
                    with accelerator.autocast():
                        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Liger fused CE ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ (Ğ½Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ logits)
                        if liger_fused_ce_loss is not None:
                            labels = batch.pop("labels", None)
                            outputs = model(**batch, output_hidden_states=True, use_cache=False)
                            batch["labels"] = labels
                            hidden_states = outputs.hidden_states[-1]
                            loss = liger_fused_ce_loss(hidden_states, labels).detach()
                        else:
                            # Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ forward â€” logits Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ, Ğ½Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑÑ€Ğ°Ğ·Ñƒ
                            out = model(**batch, use_cache=False)
                            loss = out.loss.detach()
                            del out  # Ğ¯Ğ²Ğ½Ğ¾ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ñ‚ logits
                        
                        # Ğ£ÑÑ€ĞµĞ´Ğ½ÑĞµĞ¼ loss Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼ (ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ ÑĞ²Ğ¾Ñ Ñ‡Ğ°ÑÑ‚ÑŒ val Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
                        loss = accelerator.reduce(loss, reduction="mean")
                        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main process, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
                        if accelerator.is_main_process:
                            losses.append(loss.item())
                        del loss  # ĞÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ
                
                # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ CUDA cache Ğ¿Ğ¾ÑĞ»Ğµ eval
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            model.train()
            if not losses:
                return None
            # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ loss (ÑƒĞ¶Ğµ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼)
            return sum(losses) / len(losses) if losses else None
        
        # Ğ’ĞĞ–ĞĞ: global_step ÑƒĞ¶Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ²Ñ‹ÑˆĞµ ĞºĞ°Ğº starting_step (Ğ´Ğ»Ñ resume)
        # Ğ•ÑĞ»Ğ¸ resume Ğ½Ğµ Ğ±Ñ‹Ğ»Ğ¾, starting_step = 0, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ global_step = 0
        start_time = time.time()
        last_heartbeat = time.time()
        heartbeat_every = float(config.get("metrics_heartbeat_seconds", 20.0))
        
        # Debug: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´ÑƒĞ±Ğ»ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ N ÑˆĞ°Ğ³Ğ¾Ğ²)
        # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ’ĞšĞ› Ğ² multi-GPU (ÑÑ‚Ğ¾ Ğ½Ğ°Ñˆ "safety check", Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ ÑˆĞ°Ñ€Ğ´Ğ¸Ğ½Ğ³).
        if "debug_check_duplicates" in config:
            debug_check_duplicates = bool(config.get("debug_check_duplicates"))
        else:
            debug_check_duplicates = bool(accelerator.num_processes > 1)
        debug_sample_ids = []  # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ sample_id Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ²
        debug_max_samples = 20  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 20 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²
        if debug_check_duplicates and accelerator.is_main_process:
            metrics.update(debug_check_duplicates=True, debug_max_samples=int(debug_max_samples))
        
        # Loss tracking:
        # - micro_* Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ divergence Ğ½Ğ° ĞšĞĞ–Ğ”ĞĞœ update-step
        # - log_* Ğ´Ğ»Ñ ÑĞ³Ğ»Ğ°Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ° (ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ update-step-Ğ°Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸)
        micro_loss_sum = 0.0
        micro_count = 0
        log_loss_sum = 0.0
        log_updates = 0
        update_start_time = time.time()

        training_complete = False
        stop_reason = None
        
        for epoch in range(config["epochs"]):
            if training_complete:
                break
                
            metrics.update(epoch=epoch + 1)
            model.train()

            # Ğ•ÑĞ»Ğ¸ resume Ğ¸Ğ· IterableDataset: Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ¸ Ğ¯Ğ’ĞĞ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ epoch
            epoch_loader = train_loader
            if epoch == 0 and resume_batches_to_skip > 0:
                it = iter(train_loader)
                to_skip = int(resume_batches_to_skip)
                log_every = int(config.get("resume_skip_log_every", 500))
                t0 = time.time()
                skipped = 0
                logger.info(f"Skipping {to_skip} batches now... (logging every {log_every})")
                while skipped < to_skip:
                    try:
                        next(it)
                    except StopIteration:
                        logger.warning(f"Reached end of iterable dataloader while skipping at {skipped}/{to_skip}. Continuing from start of next epoch.")
                        break
                    skipped += 1
                    if skipped % log_every == 0 or skipped == to_skip:
                        dt = max(1e-6, time.time() - t0)
                        bps = skipped / dt
                        eta = (to_skip - skipped) / bps if bps > 0 else float("inf")
                        logger.info(f"Skipped {skipped}/{to_skip} batches ({bps:.2f} batches/s), ETA ~{eta:.0f}s")
                epoch_loader = it  # Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ ÑÑ‚Ğ¾Ñ‚ epoch Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸
                if accelerator.is_main_process:
                    metrics.update(status="training", skipped_batches_done=int(skipped))
                resume_batches_to_skip = 0  # Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·

            for step, batch in enumerate(epoch_loader):
                # Heartbeat: Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ metrics.json Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ log_every Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹
                if accelerator.is_main_process and heartbeat_every > 0:
                    now = time.time()
                    if now - last_heartbeat >= heartbeat_every:
                        try:
                            metrics.update(last_heartbeat=datetime.now().isoformat())
                        except Exception:
                            pass
                        last_heartbeat = now
                # ĞĞ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ»Ğ¾Ğ³: Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ per-process batch Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° "Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ VRAM"
                if step == 0 and epoch == 0 and accelerator.is_main_process and global_step == starting_step:
                    try:
                        if "input_ids" in batch and hasattr(batch["input_ids"], "shape"):
                            bsz, seqlen = int(batch["input_ids"].shape[0]), int(batch["input_ids"].shape[1])
                        else:
                            bsz, seqlen = int(config.get("batch_size", 0)), int(config.get("seq_len", 0))

                        vocab_size = int(config.get("vocab_size", 50257))
                        mp = str(config.get("mixed_precision", "no")).lower()
                        bytes_per = 2 if mp in ("fp16", "bf16") else 4

                        # logits: (B,S,V) â€” ÑÑ‚Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¶Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… B Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ S
                        logits_gb = (bsz * seqlen * vocab_size * bytes_per) / (1024**3)

                        num_gpus = int(accelerator.num_processes or 1)
                        grad_accum = int(config.get("gradient_accumulation", 1))
                        eff_per_gpu = bsz * grad_accum
                        global_batch = eff_per_gpu * num_gpus

                        logger.warning(
                            f"[BATCH CHECK] input_ids shape={getattr(batch.get('input_ids', None), 'shape', None)} | "
                            f"per-GPU microbatch={bsz}, grad_accum={grad_accum} -> effective per-GPU={eff_per_gpu}, global={global_batch} (x{num_gpus}). "
                            f"Estimated logits memory ~{logits_gb:.2f} GB (B*S*V, dtype={mp}). "
                        )
                    except Exception as e:
                        logger.warning(f"[BATCH CHECK] failed: {e}")
                # Debug: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´ÑƒĞ±Ğ»ĞµĞ¹ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ N Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²)
                # Ğ’ĞĞ–ĞĞ: ĞŸÑ€Ğ¸ dispatch_batches=True Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ…ÑÑˆĞ¸ ÑĞ¾ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· gather
                if debug_check_duplicates and global_step < debug_max_samples:
                    # Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ hash Ğ¾Ñ‚ input_ids
                    if "input_ids" in batch:
                        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¸Ğ· Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
                        sample_hash = hash(batch["input_ids"][0].cpu().numpy().tobytes())
                        debug_sample_ids.append((global_step, accelerator.process_index, sample_hash))
                
                with accelerator.accumulate(model):
                    if is_streaming_sharded:
                        batch = {k: (v.to(accelerator.device) if hasattr(v, "to") else v) for k, v in batch.items()}
                    with accelerator.autocast():
                        # ğŸ¦ Liger Fused CE path vs standard path
                        if liger_fused_ce_loss is not None:
                            # LIGER FUSED PATH: hidden_states -> fused loss (ĞĞ• Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ logits!)
                            # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ labels Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞĞ• Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞ»Ğ° loss
                            labels = batch.pop("labels", None)
                            outputs = model(**batch, output_hidden_states=True, use_cache=False)
                            batch["labels"] = labels  # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
                            
                            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ hidden state
                            hidden_states = outputs.hidden_states[-1]
                            
                            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ loss Ñ‡ĞµÑ€ĞµĞ· Liger Fused CE
                            loss = liger_fused_ce_loss(hidden_states, labels)
                        else:
                            # STANDARD PATH
                            outputs = model(**batch)
                            loss = outputs.loss
                        
                        loss_val = loss.detach().float().item()
                    
                    # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: ĞµÑĞ»Ğ¸ loss NaN/Inf - Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑÑ‚Ğ¾Ñ‚ ÑˆĞ°Ğ³ Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ÑÑ
                    if math.isnan(loss_val) or math.isinf(loss_val):
                        logger.error(f"Loss is NaN or Inf at step {global_step}. Stopping training to prevent bad model.")
                        metrics.update(status="error", error=f"Training Diverged: Loss is NaN or Infinity at step {global_step}. Try lowering learning rate or changing precision.")
                        raise ValueError(f"Training Diverged: Loss is NaN or Infinity at step {global_step}.")
                    
                    # micro-Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€: Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ update-step
                    micro_loss_sum += loss_val
                    micro_count += 1
                    
                    accelerator.backward(loss)
                    
                    # Ğ’ĞĞ–ĞĞ: ÑˆĞ°Ğ³ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° update-step
                    if accelerator.sync_gradients:
                        # ĞĞ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ»Ğ¾Ğ³ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ peak VRAM (allocator) Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ update-step (Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ½ĞºÑƒ)
                        if epoch == 0 and global_step == starting_step and not metrics.metrics.get("logged_cuda_peak", False):
                            try:
                                if torch.cuda.is_available():
                                    dev = accelerator.device
                                    torch.cuda.synchronize(dev)
                                    peak_alloc = torch.cuda.max_memory_allocated(dev) / (1024**3)
                                    peak_res = torch.cuda.max_memory_reserved(dev) / (1024**3)
                                    cur_alloc = torch.cuda.memory_allocated(dev) / (1024**3)
                                    cur_res = torch.cuda.memory_reserved(dev) / (1024**3)
                                    logger.warning(
                                        f"[CUDA PEAK] rank={accelerator.process_index} device={dev} | "
                                        f"peak_alloc={peak_alloc:.2f}GB peak_reserved={peak_res:.2f}GB | "
                                        f"cur_alloc={cur_alloc:.2f}GB cur_reserved={cur_res:.2f}GB"
                                    )
                                # Ğ¿Ğ¾Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main process Ñ‡ĞµÑ€ĞµĞ· metrics-Ñ„Ğ°Ğ¹Ğ» (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ ÑĞ¿Ğ°Ğ¼Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğµ)
                                if accelerator.is_main_process:
                                    metrics.metrics["logged_cuda_peak"] = True
                                    metrics._save()
                            except Exception as e:
                                logger.warning(f"[CUDA PEAK] failed: {e}")

                        # Gradient clipping Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
                        # Muon Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ¼ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ clip_grad_norm=1.0.
                        max_grad_norm_default = 0.0 if optimizer_name == "muon" else 1.0
                        max_grad_norm = config.get("max_grad_norm", max_grad_norm_default)
                        if max_grad_norm > 0:
                            accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)
                        
                        optimizer.step()
                        lr_scheduler.step()
                        optimizer.zero_grad(set_to_none=True)
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞµĞ» Ğ»Ğ¸ ÑˆĞ°Ğ³ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° (ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²)
                if accelerator.sync_gradients:
                    global_step += 1
                    
                    # Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ (update step)
                    update_time = time.time() - update_start_time
                    update_start_time = time.time()
                    
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹ ĞµÑĞ»Ğ¸ Ğ²ÑĞµ Ğ¼Ğ¸ĞºÑ€Ğ¾-ÑˆĞ°Ğ³Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ñ‹ (Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ ÑĞ»ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ¾ÑĞ»Ğµ Ñ„Ğ¸ĞºÑĞ° Ğ²Ñ‹ÑˆĞµ)
                    if micro_count == 0:
                        logger.warning(f"No valid loss values at step {global_step}, skipping update")
                        continue

                    update_loss = micro_loss_sum / micro_count
                    # Ğ’ĞĞ–ĞĞ: ÑƒÑÑ€ĞµĞ´Ğ½ÑĞµĞ¼ loss Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼ Ğ² Multi-GPU
                    update_loss_t = torch.tensor(update_loss, device=accelerator.device)
                    update_loss = accelerator.reduce(update_loss_t, reduction="mean").item()
                    
                    micro_loss_sum = 0.0
                    micro_count = 0

                    # Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ³Ğ»Ğ°Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ°
                    log_loss_sum += update_loss
                    log_updates += 1

                    # Debug: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´ÑƒĞ±Ğ»ĞµĞ¹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… N ÑˆĞ°Ğ³Ğ¾Ğ²
                    # Ğ’ĞĞ–ĞĞ: gather() â€” ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ’Ğ¡Ğ•Ğ¥ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…!
                    if debug_check_duplicates and global_step == debug_max_samples:
                        # 1) Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ…ÑÑˆĞµĞ¹ Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (Ğ’Ğ¡Ğ• Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ gather)
                        hash_counts = torch.tensor([len(debug_sample_ids)], device=accelerator.device, dtype=torch.long)
                        all_counts = accelerator.gather(hash_counts)  # collective op
                        
                        # 2) Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…ÑÑˆĞ¸ Ğ´Ğ»Ñ gather (Ğ’Ğ¡Ğ• Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹)
                        max_len = int(all_counts.max().item()) if all_counts.numel() > 0 else 0
                        if len(debug_sample_ids) > 0 and max_len > 0:
                            local_hashes = torch.tensor([h for _, _, h in debug_sample_ids], device=accelerator.device, dtype=torch.long)
                            # Pad Ğ´Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ gather
                            if len(local_hashes) < max_len:
                                padding = torch.full((max_len - len(local_hashes),), -1, device=accelerator.device, dtype=torch.long)
                                local_hashes = torch.cat([local_hashes, padding])
                        else:
                            # ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€ ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ Ñ…ÑÑˆĞµĞ¹ (Ğ½Ğ¾ Ğ²ÑÑ‘ Ñ€Ğ°Ğ²Ğ½Ğ¾ ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒĞµĞ¼ Ğ² gather)
                            local_hashes = torch.full((max(1, max_len),), -1, device=accelerator.device, dtype=torch.long)
                        
                        # 3) Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ…ÑÑˆĞ¸ ÑĞ¾ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² (Ğ’Ğ¡Ğ• Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ gather)
                        all_hashes = accelerator.gather(local_hashes.unsqueeze(0))  # collective op
                        
                        # 4) ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´ÑƒĞ±Ğ»ĞµĞ¹ â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main process (Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ gather)
                        if accelerator.is_main_process:
                            total_hashes = int(all_counts.sum().item())
                            logger.info(f"Debug: collected {total_hashes} sample hashes across all processes")
                            
                            # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ padding (-1) Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸
                            all_hashes_flat = all_hashes.flatten()
                            valid_hashes = all_hashes_flat[all_hashes_flat != -1].cpu().numpy()
                            
                            if len(valid_hashes) > 0:
                                unique_hashes = set(valid_hashes)
                                if len(valid_hashes) != len(unique_hashes):
                                    duplicates = len(valid_hashes) - len(unique_hashes)
                                    logger.warning(f"Debug: Found {duplicates} duplicate samples in first {debug_max_samples} steps across all processes!")
                                else:
                                    logger.info(f"Debug: No duplicates found in first {debug_max_samples} steps (all {len(valid_hashes)} samples unique across all processes)")
                            else:
                                logger.warning("Debug: No sample hashes collected")
                    
                    # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                    if global_step % config.get("log_every", 10) == 0 or global_step == 1:
                        avg_loss = log_loss_sum / max(1, log_updates)
                        
                        # Samples per second (Effective Batch / Time)
                        # Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ multi-GPU: Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ batch = effective_batch * num_processes
                        effective_batch = config["batch_size"] * config["gradient_accumulation"]
                        global_batch = effective_batch * accelerator.num_processes
                        samples_per_sec = global_batch / update_time if update_time > 0 else 0
                        
                        metrics.log_step(
                            step=global_step,
                            loss=avg_loss,
                            lr=lr_scheduler.get_last_lr()[0],
                            samples_per_sec=samples_per_sec,
                            step_time=update_time
                        )
                        
                        log_loss_sum = 0.0
                        log_updates = 0
                    
                    # Checkpoint
                    if global_step % config.get("save_every", 5000) == 0:
                        ckpt_path = output_dir / f"checkpoint_step{global_step}"
                        accelerator.save_state(ckpt_path)
                        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ dataloader state (per-rank) Ñ€ÑĞ´Ğ¾Ğ¼ Ñ accelerate checkpoint
                        try:
                            if hasattr(train_dataset, "get_resume_state"):
                                ds_state = train_dataset.get_resume_state()
                                st_path = ckpt_path / f"dataloader_state_rank{accelerator.process_index}.json"
                                with open(st_path, "w", encoding="utf-8") as f:
                                    json.dump(ds_state, f, ensure_ascii=False, indent=2)
                        except Exception as e:
                            logger.warning(f"Failed to save dataloader state: {e}")
                        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ° (Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ UI/Ğ²Ğ¾Ğ·Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ²)
                        try:
                            if accelerator.is_main_process:
                                meta = {
                                    "global_step": int(global_step),
                                    "planned_total_steps": int(metrics.metrics.get("planned_total_steps", max_train_steps)),
                                    "learning_rate": float(config.get("learning_rate", 0.0)),
                                    "warmup_steps": int(config.get("warmup_steps", 0)),
                                    "min_lr_ratio": float(config.get("min_lr_ratio", 0.0) or 0.0),
                                    "scheduler": str(config.get("lr_schedule", "cosine")),
                                    "timestamp": datetime.now().isoformat(),
                                }
                                with open(ckpt_path / "checkpoint_metadata.json", "w", encoding="utf-8") as f:
                                    json.dump(meta, f, ensure_ascii=False, indent=2)
                        except Exception as e:
                            logger.warning(f"Failed to save checkpoint metadata: {e}")
                        accelerator.wait_for_everyone()
                        
                        # ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main process
                        if accelerator.is_main_process:
                            model_config.save_pretrained(ckpt_path)
                            # ĞŸĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ loss Ğ² Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚
                            current_loss = metrics.metrics.get("current_loss", 0.0)
                            metrics.log_checkpoint(str(ckpt_path), loss=current_loss)

                        # (ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾) Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ checkpoint.
                        # Ğ­Ñ‚Ğ¾ ĞĞ• resume-state, Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ "latest final_model" Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸.
                        # Ğ’ĞĞ–ĞĞ: Ğ”Ğ»Ñ ZeRO-3/FSDP adapter.save_final Ğ”ĞĞ›Ğ–Ğ•Ğ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ’Ğ¡Ğ•Ğ¥ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…!
                        if bool(config.get("export_on_checkpoint", True)):
                            try:
                                import shutil
                                tmp_dir = output_dir / "final_model.__tmp__"
                                final_dir = output_dir / "final_model"
                                
                                # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ½Ğ° main process
                                if accelerator.is_main_process:
                                    if tmp_dir.exists():
                                        shutil.rmtree(tmp_dir, ignore_errors=True)
                                    tmp_dir.mkdir(parents=True, exist_ok=True)

                                    # SFT: ÑƒĞ±ĞµĞ´Ğ¸Ğ¼ÑÑ, Ñ‡Ñ‚Ğ¾ chat_template Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ² tokenizer_config.json
                                    if stage == "sft":
                                        user_chat_template = config.get("chat_template")
                                        if user_chat_template:
                                            tokenizer.chat_template = user_chat_template
                                            logger.info("Using user-provided chat_template")
                                        elif config.get("sft_template"):
                                            tmpl = config["sft_template"]
                                            default_sys = tmpl.get("system", "You are a helpful assistant.")
                                            u_tag = tmpl.get("user_tag", "### User:")
                                            b_tag = tmpl.get("bot_tag", "### Assistant:")
                                            default_sys_escaped = default_sys.replace("'", "\\'")
                                            u_tag_escaped = u_tag.replace("'", "\\'")
                                            b_tag_escaped = b_tag.replace("'", "\\'")
                                            tokenizer.chat_template = (
                                                "{% if messages and messages[0]['role'] == 'system' %}"
                                                "{{ messages[0]['content'] }}\n\n"
                                                "{% else %}"
                                                f"{{{{ '{default_sys_escaped}' }}}}\n\n"
                                                "{% endif %}"
                                                "{% for message in messages %}"
                                                "{% if message['role'] == 'user' %}"
                                                f"{u_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                                                "{% elif message['role'] == 'assistant' %}"
                                                f"{b_tag_escaped}\n{{{{ message['content'] }}}}\n\n"
                                                "{% endif %}"
                                                "{% endfor %}"
                                                "{% if add_generation_prompt %}"
                                                f"{b_tag_escaped}\n"
                                                "{% endif %}"
                                            )
                                            logger.info("Generated chat_template from sft_template")
                                
                                accelerator.wait_for_everyone()
                                
                                # save_final Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ’Ğ¡Ğ•Ğ¥ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… (Ğ´Ğ»Ñ ZeRO-3/FSDP)
                                adapter.save_final(accelerator, model, tokenizer, tmp_dir)
                                
                                accelerator.wait_for_everyone()
                                
                                # ĞÑ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ final_model (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main)
                                if accelerator.is_main_process:
                                    if final_dir.exists():
                                        shutil.rmtree(final_dir, ignore_errors=True)
                                    tmp_dir.rename(final_dir)
                                    logger.info(f"Updated final_model at checkpoint step {global_step}")
                                
                                accelerator.wait_for_everyone()
                            except Exception as e:
                                if accelerator.is_main_process:
                                    logger.warning(f"Failed to export final_model on checkpoint: {e}")
                    
                    # Validation
                    # Ğ’ĞĞ–ĞĞ: evaluate() Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…, Ñ‚.Ğº. val_loader Ğ·Ğ°ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½
                    # ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ¸Ğ´Ğ¸Ñ‚ ÑĞ²Ğ¾Ñ Ñ‡Ğ°ÑÑ‚ÑŒ val Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, reduce() ÑƒÑÑ€ĞµĞ´Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
                    if val_loader is not None and eval_every > 0 and (global_step % eval_every == 0):
                        val_loss = evaluate(val_loader)  # Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…
                        if accelerator.is_main_process and val_loss is not None:
                            metrics.log_eval(global_step, float(val_loss))
                            logger.info(f"Validation at step {global_step}: val_loss={val_loss:.4f}")
                    
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ»Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ° ÑˆĞ°Ğ³Ğ¾Ğ²
                    if global_step >= max_train_steps:
                        training_complete = True
                        stop_reason = "max_train_steps_reached"
                        break
        
        # Ğ•ÑĞ»Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ğ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾ max_train_steps, Ñ‚Ğ¾ Ğ»Ğ¸Ğ±Ğ¾ ÑĞ¿Ğ¾Ñ…Ğ¸ ĞºĞ¾Ğ½Ñ‡Ğ¸Ğ»Ğ¸ÑÑŒ, Ğ»Ğ¸Ğ±Ğ¾ Ğ´Ğ°Ñ‚Ğ°Ğ»Ğ¾Ğ°Ğ´ĞµÑ€ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ»ÑÑ.
        if stop_reason is None:
            if global_step >= max_train_steps:
                stop_reason = "max_train_steps_reached"
            else:
                # Ğ”Ğ»Ñ IterableDataset Ğ´Ğ»Ğ¸Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ğ°Ñ (Ğ¿ÑƒÑÑ‚Ñ‹Ğµ/Ğ±Ğ¸Ñ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ),
                # Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ EOF Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ñ€Ğ°Ğ½ÑŒÑˆĞµ planned_total_steps.
                stop_reason = "epochs_completed_or_dataloader_exhausted"

        # Ğ•ÑĞ»Ğ¸ "Ğ¿Ğ»Ğ°Ğ½" Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ„Ğ°ĞºÑ‚Ğ° â€” Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ‚Ğ°Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ±Ñ‹Ğ» 100%,
        # Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ (planned_total_steps).
        if accelerator.is_main_process:
            try:
                if metrics.metrics.get("planned_total_steps", 0) and global_step < int(metrics.metrics.get("planned_total_steps", 0)):
                    metrics.update(total_steps=int(global_step), stop_reason=stop_reason)
                else:
                    metrics.update(stop_reason=stop_reason)
            except Exception:
                pass

        # Final save - Ğ´Ğ»Ñ ZeRO-3/FSDP Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğµ Ğ’Ğ¡Ğ•Ğ¥ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²
        accelerator.wait_for_everyone()
        
        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ½Ğ° main
        if accelerator.is_main_process:
            metrics.update(status="saving_model")
        
        final_dir = output_dir / "final_model"
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ½Ğ° main process
        if accelerator.is_main_process:
            final_dir.mkdir(parents=True, exist_ok=True)
        accelerator.wait_for_everyone()
        
        # --- (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾) ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ chat_template Ğ´Ğ»Ñ SFT ---
        # ĞĞ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ° main, Ğ½Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ adapter.save_final
        if accelerator.is_main_process and stage == "sft":
            # ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚: Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ chat_template > Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· sft_template
            user_chat_template = config.get("chat_template")
            if user_chat_template:
                # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ chat_template
                tokenizer.chat_template = user_chat_template
                logger.info("Final save: using user-provided chat_template")
            elif config.get("sft_template"):
                # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Qwen-style chat_template Ğ¸Ğ· sft_template
                tmpl = config["sft_template"]
                
                default_sys = tmpl.get("system", "You are a helpful assistant.")
                im_start = tmpl.get("im_start", "<|im_start|>")
                im_end = tmpl.get("im_end", "<|im_end|>")
                sep = tmpl.get("separator", "\n")
                
                default_sys_escaped = default_sys.replace("'", "\\'")
                
                # Qwen-style chat template
                tokenizer.chat_template = (
                    "{%- if messages[0]['role'] == 'system' -%}"
                    f"{{{{ '{im_start}system{sep}' + messages[0]['content'] + '{im_end}{sep}' }}}}"
                    "{%- else -%}"
                    f"{{{{ '{im_start}system{sep}{default_sys_escaped}{im_end}{sep}' }}}}"
                    "{%- endif -%}"
                    "{%- for message in messages -%}"
                    "{%- if message.role == 'user' or (message.role == 'system' and not loop.first) -%}"
                    f"{{{{ '{im_start}' + message.role + '{sep}' + message.content + '{im_end}{sep}' }}}}"
                    "{%- elif message.role == 'assistant' -%}"
                    f"{{{{ '{im_start}assistant{sep}' + message.content + '{im_end}{sep}' }}}}"
                    "{%- endif -%}"
                    "{%- endfor -%}"
                    "{%- if add_generation_prompt -%}"
                    f"{{{{ '{im_start}assistant{sep}' }}}}"
                    "{%- endif -%}"
                )
                logger.info(f"Final save: generated Qwen-style chat_template (im_start='{im_start}', im_end='{im_end}')")
        
        # Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ tokenizer.model_max_length Ñ max_position_embeddings Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        # Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ confusing warnings Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ
        if accelerator.is_main_process:
            unwrapped = accelerator.unwrap_model(model)
            model_max_pos = getattr(unwrapped.config, "max_position_embeddings", None)
            if model_max_pos and tokenizer.model_max_length != model_max_pos:
                logger.info(f"Syncing tokenizer.model_max_length: {tokenizer.model_max_length} -> {model_max_pos}")
                tokenizer.model_max_length = model_max_pos
        
        # --- Ğ’Ğ¡Ğ•Ğ“Ğ”Ğ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Ğ¸ Ğ´Ğ»Ñ pretrain Ñ‚Ğ¾Ğ¶Ğµ) ---
        # Ğ’ĞĞ–ĞĞ: Ğ”Ğ»Ñ ZeRO-3/FSDP adapter.save_final Ğ”ĞĞ›Ğ–Ğ•Ğ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ’Ğ¡Ğ•Ğ¥ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…
        # Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ñ‡ĞµÑ€ĞµĞ· accelerator.save_model
        adapter.save_final(accelerator, model, tokenizer, final_dir)
        
        # Ğ–Ğ´Ñ‘Ğ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…
        accelerator.wait_for_everyone()
        
        # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° main
        if accelerator.is_main_process:
            total_time = time.time() - start_time
            hours, rem = divmod(total_time, 3600)
            minutes, seconds = divmod(rem, 60)
            duration_str = "{:0>2}:{:0>2}:{:05.2f}".format(int(hours), int(minutes), seconds)
            
            metrics.update(
                status="completed",
                total_time_seconds=total_time,
                training_duration=duration_str,
                final_model_path=str(final_dir),
            )
        
        accelerator.wait_for_everyone()
        
    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        logger.error(f"Training failed: {e}\n{tb}")
        metrics.update(status="error", error=f"{str(e)}\n\n{tb}")
        raise


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Path to JSON config")
    parser.add_argument("--metrics", type=str, required=True, help="Path to metrics JSON")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Path to checkpoint directory to resume from")
    args = parser.parse_args()
    
    with open(args.config) as f:
        config = json.load(f)
    
    # Merge CLI args into config
    if args.resume_from_checkpoint:
        config["resume_from_checkpoint"] = args.resume_from_checkpoint
    
    run_training(config, Path(args.metrics))


if __name__ == "__main__":
    main()
