"""
homellm.models.adapters
-----------------------
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –º–æ–¥–µ–ª–µ–π.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- Home –º–æ–¥–µ–ª–∏ (HomeForCausalLM)
- –õ—é–±—ã–µ HuggingFace –º–æ–¥–µ–ª–∏ (AutoModelForCausalLM)
- LoRA/QLoRA —Ç—é–Ω–∏–Ω–≥ —á–µ—Ä–µ–∑ PEFT
"""
from __future__ import annotations

import json
import logging
import os
from pathlib import Path
from typing import Optional, Dict, Any, Tuple

import torch
from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForCausalLM,
    PreTrainedTokenizer,
    PreTrainedModel,
)

from homellm.models.home_model import HomeConfig, HomeForCausalLM
from homellm.models.blueprint import Blueprint
from homellm.models.blueprint_model import BlueprintLMConfig, BlueprintForCausalLM

logger = logging.getLogger(__name__)

# PEFT –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ (–¥–ª—è LoRA/QLoRA)
try:
    from peft import (
        LoraConfig,
        get_peft_model,
        prepare_model_for_kbit_training,
        TaskType,
        PeftModel,
    )
    from transformers import BitsAndBytesConfig
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    PeftModel = None
    logger.warning("PEFT not available. LoRA/QLoRA features will be disabled.")


def detect_model_type(model_path: Path) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ config.json.
    
    Returns:
        "home" | "hf" | "unknown"
    """
    config_path = model_path / "config.json"
    if not config_path.exists():
        return "unknown"
    
    try:
        with open(config_path) as f:
            config = json.load(f)
        
        model_type = config.get("model_type", "")
        architectures = config.get("architectures", [])
        
        if model_type == "homellm" or "HomeForCausalLM" in architectures:
            return "home"
        
        # –õ—é–±–∞—è –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å —Å model_type - —ç—Ç–æ HF –º–æ–¥–µ–ª—å
        if model_type:
            return "hf"
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å architectures - —Ç–æ–∂–µ HF
        if architectures:
            return "hf"
        
        return "unknown"
    except Exception as e:
        logger.warning(f"Failed to detect model type: {e}")
        return "unknown"


def resolve_adapter(config: Dict[str, Any]) -> "ModelAdapter":
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥–∞.
    
    –õ–æ–≥–∏–∫–∞:
    1. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω base_model_path - –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ config.json
    2. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω model_id (–¥–ª—è pretrain) - –ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø
    3. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - HomeAdapter
    """
    base_model_path = config.get("base_model_path")
    model_id = config.get("model_id")  # –î–ª—è pretrain from scratch
    
    if base_model_path:
        model_path = Path(base_model_path)
        model_type = detect_model_type(model_path)
        
        if model_type == "home":
            return HomeAdapter()
        elif model_type == "hf":
            return HFAdapter()
        else:
            logger.warning(f"Could not detect model type for {base_model_path}, using HomeAdapter")
            return HomeAdapter()
    
    if model_id:
        # –î–ª—è pretrain from scratch - –ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ model_id
        try:
            cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=True)
            if cfg.model_type == "homellm":
                return HomeAdapter()
            else:
                return HFAdapter()
        except Exception:
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º Home –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return HomeAdapter()
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - Home
    return HomeAdapter()


class ModelAdapter:
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π."""
    
    def load_tokenizer(
        self,
        source: str | Path,
        trust_remote_code: bool = True,
    ) -> PreTrainedTokenizer:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
        
        Args:
            source: –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∏–ª–∏ model_id
            trust_remote_code: —Ä–∞–∑—Ä–µ—à–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ –∏–∑ –º–æ–¥–µ–ª–∏
        
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        """
        raise NotImplementedError
    
    def prepare_tokenizer(self, tokenizer: PreTrainedTokenizer) -> PreTrainedTokenizer:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        
        –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º pad_token = eos_token –≤–º–µ—Å—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.
        –≠—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å vocab —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.
        """
        if tokenizer.pad_token is None:
            if tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
                logger.info(f"Set pad_token = eos_token ({tokenizer.eos_token})")
            else:
                # Fallback: –¥–æ–±–∞–≤–ª—è–µ–º pad —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç eos
                tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
                logger.warning("Added new pad_token (no eos_token found)")
        return tokenizer
    
    def load_for_training(
        self,
        base_model_path: Optional[str | Path],
        stage: str,
        tokenizer: PreTrainedTokenizer,
        config: Dict[str, Any],
        trust_remote_code: bool = True,
    ) -> Tuple[PreTrainedModel, Any]:  # (model, model_config)
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            base_model_path: –ø—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (None –¥–ª—è pretrain from scratch)
            stage: "pretrain" | "continual_pretrain" | "sft"
            tokenizer: –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            config: –∫–æ–Ω—Ñ–∏–≥ –æ–±—É—á–µ–Ω–∏—è
            trust_remote_code: —Ä–∞–∑—Ä–µ—à–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞
        
        Returns:
            (model, model_config)
        """
        raise NotImplementedError
    
    def init_from_scratch(
        self,
        model_id_or_config: str | Dict[str, Any],
        tokenizer: PreTrainedTokenizer,
        config: Dict[str, Any],
        trust_remote_code: bool = True,
    ) -> Tuple[PreTrainedModel, Any]:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è (–¥–ª—è pretrain).
        
        Args:
            model_id_or_config: model_id –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥–∞
            tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            config: –∫–æ–Ω—Ñ–∏–≥ –æ–±—É—á–µ–Ω–∏—è
            trust_remote_code: —Ä–∞–∑—Ä–µ—à–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞
        
        Returns:
            (model, model_config)
        """
        raise NotImplementedError
    
    def prepare_for_training(
        self,
        model: PreTrainedModel,
        tokenizer: PreTrainedTokenizer,
        config: Dict[str, Any],
    ) -> PreTrainedModel:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        
        –í—ã–ø–æ–ª–Ω—è–µ—Ç:
        - resize_token_embeddings –µ—Å–ª–∏ vocab –∏–∑–º–µ–Ω–∏–ª—Å—è
        - tie_weights –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        - use_cache = False
        - gradient_checkpointing –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        - LoRA/QLoRA –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤ config
        """
        tuning_method = config.get("tuning_method", "full")
        
        # QLoRA: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è 4-bit (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ 4-bit)
        if tuning_method == "qlora" and PEFT_AVAILABLE:
            # –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ 4-bit —á–µ—Ä–µ–∑ quantization_config –≤ load_for_training()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤ 4-bit
            is_4bit = (
                getattr(model, "is_loaded_in_4bit", False) or
                getattr(model, "is_quantized", False) or
                getattr(model.config, "quantization_config", None) is not None
            )
            
            if is_4bit:
                logger.info("Preparing 4-bit model for QLoRA training...")
                model = prepare_model_for_kbit_training(model)
                logger.info("Model prepared for 4-bit training")
            else:
                logger.warning(
                    "QLoRA requires 4-bit quantization, but model was not loaded in 4-bit. "
                    "This may cause issues. Ensure quantization_config is applied during model loading. "
                    f"Model attributes: is_loaded_in_4bit={getattr(model, 'is_loaded_in_4bit', 'N/A')}, "
                    f"is_quantized={getattr(model, 'is_quantized', 'N/A')}, "
                    f"quantization_config={getattr(model.config, 'quantization_config', 'N/A')}"
                )
        
        # Resize embeddings –µ—Å–ª–∏ vocab –∏–∑–º–µ–Ω–∏–ª—Å—è (–¥–æ LoRA)
        if hasattr(model.config, 'vocab_size') and model.config.vocab_size != len(tokenizer):
            logger.info(f"Resizing token embeddings: {model.config.vocab_size} -> {len(tokenizer)}")
            model.resize_token_embeddings(len(tokenizer))
            model.config.vocab_size = len(tokenizer)
            if hasattr(model, "tie_weights"):
                model.tie_weights()
        
        # LoRA/QLoRA: –ø—Ä–∏–º–µ–Ω—è–µ–º PEFT
        if tuning_method in ("lora", "qlora") and PEFT_AVAILABLE:
            target_modules = config.get("lora_target_modules")
            if not target_modules:
                # –ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç target_modules –ø–æ —Ç–∏–ø—É –º–æ–¥–µ–ª–∏
                target_modules = self._detect_target_modules(model)
            
            lora_config = LoraConfig(
                r=config.get("lora_r", 16),
                lora_alpha=config.get("lora_alpha", 32),
                target_modules=target_modules,
                lora_dropout=config.get("lora_dropout", 0.1),
                bias="none",
                task_type=TaskType.CAUSAL_LM,
            )
            
            model = get_peft_model(model, lora_config)
            logger.info(f"Applied LoRA with r={lora_config.r}, target_modules={target_modules}")
        
        # –û—Ç–∫–ª—é—á–∞–µ–º cache –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = False
            logger.info("Model use_cache set to False for training")
        
        # Gradient checkpointing
        if config.get("grad_checkpoint", False):
            if hasattr(model, "gradient_checkpointing_enable"):
                model.gradient_checkpointing_enable()
                logger.info("Gradient checkpointing enabled")
        
        return model
    
    def _detect_target_modules(self, model: PreTrainedModel) -> list[str]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç target_modules –¥–ª—è LoRA –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω –º–æ–¥—É–ª–µ–π –¥–ª—è LoRA
        """
        model_type = getattr(model.config, "model_type", "").lower()
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
        if model_type in ("llama", "mistral", "mixtral", "qwen"):
            return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        elif model_type in ("gpt2", "gpt_neox", "gptj"):
            return ["c_attn", "c_proj", "c_fc"]
        elif model_type == "opt":
            return ["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2"]
        elif model_type == "bloom":
            return ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
        elif model_type == "homellm":
            # –î–ª—è Home –º–æ–¥–µ–ª–∏
            return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        else:
            # Fallback: –∏—â–µ–º –≤—Å–µ linear —Å–ª–æ–∏
            logger.warning(f"Unknown model_type {model_type}, using fallback target_modules detection")
            target_modules = []
            for name, module in model.named_modules():
                if isinstance(module, torch.nn.Linear) and "embed" not in name.lower():
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å –∏–º–µ–Ω–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä "q_proj" –∏–∑ "model.layers.0.q_proj")
                    target_modules.append(name.split(".")[-1])
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞
            return list(set(target_modules))[:8]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 8 –º–æ–¥—É–ª–µ–π
    
    def save_final(
        self,
        accelerator,
        model: PreTrainedModel,
        tokenizer: PreTrainedTokenizer,
        output_dir: Path,
    ):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ.
        
        –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LoRA/QLoRA, –º–µ—Ä–¥–∂–∏–º –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É,
        —á—Ç–æ–±—ã —á–∞—Ç –º–æ–≥ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∫–∞–∫ –æ–±—ã—á–Ω—É—é.
        
        Args:
            accelerator: Accelerator instance
            model: –º–æ–¥–µ–ª—å (–º–æ–∂–µ—Ç –±—ã—Ç—å wrapped)
            tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ main process (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Å—Ç–æ –¥–æ–∂–¥—É—Ç—Å—è barrier –≤—ã—à–µ –ø–æ —Å—Ç–µ–∫—É)
        if hasattr(accelerator, "is_main_process") and not accelerator.is_main_process:
            return

        # Unwrap –º–æ–¥–µ–ª—å –ë–ï–ó accelerate.unwrap_model():
        # accelerate.unwrap_model() –≤–Ω—É—Ç—Ä–∏ –ø—ã—Ç–∞–µ—Ç—Å—è `import deepspeed`, –¥–∞–∂–µ –µ—Å–ª–∏ –≤—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ DeepSpeed.
        # –í –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö –±–µ–∑ `distutils` —ç—Ç–æ –ø–∞–¥–∞–µ—Ç –Ω–∞ —ç—Ç–∞–ø–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
        unwrapped_model = model
        # DDP / DataParallel / –¥—Ä—É–≥–∏–µ –æ–±—ë—Ä—Ç–∫–∏
        while hasattr(unwrapped_model, "module"):
            unwrapped_model = unwrapped_model.module
        
        # –ï—Å–ª–∏ —ç—Ç–æ PEFT-–º–æ–¥–µ–ª—å (LoRA/QLoRA) ‚Äî –º–µ—Ä–¥–∂–∏–º –∞–¥–∞–ø—Ç–µ—Ä –≤ –±–∞–∑—É
        if PEFT_AVAILABLE and PeftModel is not None:
            try:
                if isinstance(unwrapped_model, PeftModel):
                    logger.info("Merging LoRA adapter into base model for final save...")
                    unwrapped_model = unwrapped_model.merge_and_unload()
                    logger.info("LoRA adapter merged successfully")
            except Exception as e:
                logger.warning(f"LoRA merge failed, saving as-is: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ –ë–ï–ó –≤—ã–∑–æ–≤–∞ transformers.save_pretrained(),
        # –ø–æ—Ç–æ–º—É —á—Ç–æ transformers –≤–Ω—É—Ç—Ä–∏ –¥–µ–ª–∞–µ—Ç unwrap_model() -> accelerate -> import deepspeed,
        # –∞ deepspeed –º–æ–∂–µ—Ç –ø–∞–¥–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤ runtime –Ω–µ—Ç nvcc).
        #
        # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º:
        # - config.json
        # - model.safetensors
        # - (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) generation_config.json
        try:
            unwrapped_model.config.save_pretrained(str(output_dir))
        except Exception as e:
            logger.warning(f"Failed to save config.json: {e}")

        try:
            if getattr(unwrapped_model, "generation_config", None) is not None:
                unwrapped_model.generation_config.save_pretrained(str(output_dir))
        except Exception as e:
            logger.warning(f"Failed to save generation_config.json: {e}")

        try:
            from safetensors.torch import save_file as _save_safetensors

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º state_dict –Ω–∞ CPU (–¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ —Å–µ–π–≤–∞)
            state_dict = unwrapped_model.state_dict()
            cpu_state = {k: v.detach().cpu() for k, v in state_dict.items()}
            _save_safetensors(cpu_state, str(output_dir / "model.safetensors"))
        except Exception as e:
            logger.error(f"Failed to save model.safetensors: {e}")
            raise
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –ø–æ blueprint ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º blueprint —Ä—è–¥–æ–º
        bp_dict = getattr(unwrapped_model.config, "blueprint", None)
        if bp_dict:
            try:
                blueprint_path = output_dir / "blueprint.json"
                import json as _json

                blueprint_path.write_text(_json.dumps(bp_dict, indent=2, ensure_ascii=False), encoding="utf-8")
                logger.info(f"Saved blueprint to {blueprint_path}")
            except Exception as e:
                logger.warning(f"Failed to save blueprint: {e}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer.save_pretrained(str(output_dir))
        
        logger.info(f"Model and tokenizer saved to {output_dir}")


class HomeAdapter(ModelAdapter):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è Home –º–æ–¥–µ–ª–µ–π."""
    
    def load_tokenizer(
        self,
        source: str | Path,
        trust_remote_code: bool = True,
    ) -> PreTrainedTokenizer:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è Home –º–æ–¥–µ–ª–∏."""
        source_str = str(source)
        source = Path(source) if isinstance(source, str) else source
        
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä HF –∏–ª–∏ –ø—É—Ç—å
        # 1. –ü—Ä–æ–±—É–µ–º HF from_pretrained (—Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è gpt2, –∏ –¥–ª—è –ø—É—Ç–µ–π)
        try:
            return AutoTokenizer.from_pretrained(source_str, trust_remote_code=trust_remote_code)
        except Exception:
            pass

        # 2. –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –ø–∞–ø–∫–∏ –º–æ–¥–µ–ª–∏ (–Ω–∞—à —Ñ–æ—Ä–º–∞—Ç)
        if source.exists() and (source / "tokenizer.json").exists():
            try:
                return AutoTokenizer.from_pretrained(str(source), trust_remote_code=trust_remote_code)
            except Exception as e:
                logger.warning(f"Failed to load tokenizer from {source}: {e}")
        
        # Fallback
        logger.warning(f"Could not load tokenizer from {source_str}, falling back to gpt2")
        return AutoTokenizer.from_pretrained("gpt2", trust_remote_code=trust_remote_code)
    
    def load_for_training(
        self,
        base_model_path: Optional[str | Path],
        stage: str,
        tokenizer: PreTrainedTokenizer,
        config: Dict[str, Any],
        trust_remote_code: bool = True,
    ) -> Tuple[PreTrainedModel, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç Home –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        def _set_home_sdpa_enabled(m: PreTrainedModel, enabled: bool) -> None:
            # home_model.Attention —Ö—Ä–∞–Ω–∏—Ç —Ñ–ª–∞–≥ self.flash, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏.
            try:
                import torch.nn.functional as F
                can_sdpa = bool(enabled) and hasattr(F, "scaled_dot_product_attention")
                for mod in m.modules():
                    if hasattr(mod, "flash"):
                        try:
                            mod.flash = bool(can_sdpa)
                        except Exception:
                            pass
                if hasattr(m, "config"):
                    try:
                        m.config.use_sdpa = bool(enabled)
                    except Exception:
                        pass
            except Exception:
                pass

        # QLoRA –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è Home –º–æ–¥–µ–ª–µ–π (bitsandbytes –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º)
        tuning_method = config.get("tuning_method", "full")
        if tuning_method == "qlora":
            logger.warning("QLoRA is not supported for Home models. Falling back to LoRA.")
            config["tuning_method"] = "lora"  # –ü–æ–Ω–∏–∂–∞–µ–º –¥–æ LoRA

        # Mixed precision / dtype –¥–ª—è Home –º–æ–¥–µ–ª–∏
        mp = str(config.get("mixed_precision", "no")).lower()
        fp16_pure = bool(config.get("fp16_pure", False))
        if mp == "bf16":
            torch_dtype = torch.bfloat16
        elif mp == "fp16":
            # AMP fp16: –≤–µ—Å–∞ –æ–±—ã—á–Ω–æ fp32; pure fp16: –≤–µ—Å–∞ fp16 (–±–µ–∑ GradScaler)
            torch_dtype = torch.float16 if fp16_pure else torch.float32
        else:
            torch_dtype = torch.float32

        # Blueprint —Ä–µ–∂–∏–º (—Å–±–æ—Ä–∫–∞ —Å –Ω—É–ª—è –ø–æ —Å—Ö–µ–º–µ)
        if config.get("model_blueprint") or config.get("blueprint_path"):
            bp_path = Path(config.get("blueprint_path") or config.get("model_blueprint"))
            if not bp_path.exists():
                raise ValueError(f"Blueprint file not found: {bp_path}")
                
            bp = Blueprint.load(bp_path)
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è vocab_size
            if bp.vocab_size != len(tokenizer):
                logger.info(f"Blueprint vocab_size ({bp.vocab_size}) != tokenizer len ({len(tokenizer)}). Updating blueprint.")
                bp = bp.copy(update={"vocab_size": len(tokenizer)})
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
            bp_cfg = BlueprintLMConfig(
                vocab_size=bp.vocab_size,
                hidden_size=bp.hidden_size,
                max_position_embeddings=bp.max_position_embeddings,
                auto_project=bp.auto_project,
                blueprint=bp.dict(),
            )
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            model = BlueprintForCausalLM(bp_cfg)
            logger.info(f"Loaded blueprint model from {bp_path} (hash={bp.hash()})")
            logger.info(f"Model structure: {model}")
            
            return model, bp_cfg
        
        base_model_path = Path(base_model_path) if base_model_path else None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ accelerate checkpoint (–¥–ª—è resume)
        # –í–ê–ñ–ù–û: pytorch_model.bin.index.json –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ —É –æ–±—ã—á–Ω—ã—Ö —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö HF-—Å–µ–π–≤–æ–≤,
        # –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ accelerator_state.json - —ç—Ç–æ —Ç–æ—á–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ accelerate checkpoint
        def is_accelerate_checkpoint(p: Path) -> bool:
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—É—Ç—å accelerate checkpoint'–æ–º."""
            return (p / "accelerator_state.json").exists()
        
        is_checkpoint = False
        if base_model_path:
            is_checkpoint = is_accelerate_checkpoint(base_model_path)
        
        if is_checkpoint and stage == "continual_pretrain":
            # –î–ª—è resume - –∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥, –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–¥–∏–º –ø–æ–∑–∂–µ
            if (base_model_path / "config.json").exists():
                model_config = HomeConfig.from_pretrained(str(base_model_path))
            else:
                parent_config = base_model_path.parent / "run_config.json"
                if parent_config.exists():
                    with open(parent_config) as f:
                        run_cfg = json.load(f)
                    model_config = HomeConfig(
                        vocab_size=len(tokenizer),
                        hidden_size=run_cfg.get("hidden_size", 512),
                        num_hidden_layers=run_cfg.get("num_layers", 8),
                        num_attention_heads=run_cfg.get("n_heads", 8),
                        max_position_embeddings=run_cfg.get("seq_len", 512),
                        use_liger=config.get("use_liger", True),  # ü¶Å Liger –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                    )
                else:
                    raise ValueError(f"Cannot find config.json in {base_model_path}")
            
            model = HomeForCausalLM(model_config)
            if torch_dtype != torch.float32:
                model = model.to(dtype=torch_dtype)
            _set_home_sdpa_enabled(model, bool(config.get("use_flash_attention", True)))
            logger.info(f"Home model initialized for resume from accelerate checkpoint")
            logger.warning(
                "Model initialized with random weights. "
                "Weights will be loaded from checkpoint via accelerator.load_state(). "
                "If resume fails, training will continue with random weights (this is likely incorrect)."
            )
            return model, model_config
        
        elif base_model_path:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ final_model
            if not (base_model_path / "config.json").exists():
                raise ValueError(
                    f"Base model path {base_model_path} does not contain config.json. "
                    f"For continual_pretrain, please use a final_model directory."
                )
            
            try:
                logger.info(f"Loading Home model from {base_model_path} using from_pretrained...")
                model = HomeForCausalLM.from_pretrained(
                    str(base_model_path),
                    torch_dtype=torch_dtype,
                    trust_remote_code=trust_remote_code,
                )
                model_config = model.config
                _set_home_sdpa_enabled(model, bool(config.get("use_flash_attention", True)))
                logger.info(f"‚úÖ Successfully loaded Home model from {base_model_path}")
                return model, model_config
            except Exception as e:
                logger.error(f"Failed to load Home model using from_pretrained: {e}")
                # Fallback –Ω–∞ —Ä—É—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
                logger.warning("Falling back to manual weight loading...")
                model_config = HomeConfig.from_pretrained(str(base_model_path))
                model = HomeForCausalLM(model_config)
                if torch_dtype != torch.float32:
                    model = model.to(dtype=torch_dtype)
                _set_home_sdpa_enabled(model, bool(config.get("use_flash_attention", True)))
                
                from safetensors.torch import load_file
                if (base_model_path / "model.safetensors").exists():
                    state_dict = load_file(str(base_model_path / "model.safetensors"))
                    model.load_state_dict(state_dict, strict=False)
                elif (base_model_path / "pytorch_model.bin").exists():
                    state_dict = torch.load(base_model_path / "pytorch_model.bin", map_location="cpu")
                    model.load_state_dict(state_dict, strict=False)
                else:
                    raise ValueError(f"No weights found in {base_model_path}")
                
                logger.info(f"Loaded Home model from {base_model_path} (fallback method)")
                return model, model_config
        
        else:
            # Pretrain from scratch
            return self.init_from_scratch(None, tokenizer, config, trust_remote_code)
    
    def init_from_scratch(
        self,
        model_id_or_config: str | Dict[str, Any],
        tokenizer: PreTrainedTokenizer,
        config: Dict[str, Any],
        trust_remote_code: bool = True,
    ) -> Tuple[PreTrainedModel, Any]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Home –º–æ–¥–µ–ª—å –∏–ª–∏ blueprint-–º–æ–¥–µ–ª—å —Å –Ω—É–ª—è."""
        mp = str(config.get("mixed_precision", "no")).lower()
        fp16_pure = bool(config.get("fp16_pure", False))
        if mp == "bf16":
            torch_dtype = torch.bfloat16
        elif mp == "fp16":
            torch_dtype = torch.float16 if fp16_pure else torch.float32
        else:
            torch_dtype = torch.float32

        use_flash_attention = bool(config.get("use_flash_attention", True))
        blueprint_path = config.get("model_blueprint")
        if blueprint_path:
            bp = Blueprint.load(blueprint_path)
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º vocab_size —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
            if bp.vocab_size != len(tokenizer):
                bp = bp.copy(update={"vocab_size": len(tokenizer)})
            bp_cfg = BlueprintLMConfig(
                vocab_size=bp.vocab_size,
                hidden_size=bp.hidden_size,
                max_position_embeddings=bp.max_position_embeddings,
                auto_project=bp.auto_project,
                blueprint=bp.dict(),
            )
            model = BlueprintForCausalLM(bp_cfg)
            logger.info(f"Initialized blueprint model from {blueprint_path} (hash={bp.hash()})")
            return model, bp_cfg

        model_config = HomeConfig(
            vocab_size=len(tokenizer),
            hidden_size=config["hidden_size"],
            num_hidden_layers=config["num_layers"],
            num_attention_heads=config["n_heads"],
            max_position_embeddings=config["seq_len"],
            dropout=config.get("dropout", 0.0),
            use_sdpa=use_flash_attention,
            use_liger=config.get("use_liger", True),  # ü¶Å Liger –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        )
        model = HomeForCausalLM(model_config)
        if torch_dtype != torch.float32:
            model = model.to(dtype=torch_dtype)
        return model, model_config


class HFAdapter(ModelAdapter):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è HuggingFace –º–æ–¥–µ–ª–µ–π."""
    
    def load_tokenizer(
        self,
        source: str | Path,
        trust_remote_code: bool = True,
    ) -> PreTrainedTokenizer:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è HF –º–æ–¥–µ–ª–∏."""
        try:
            return AutoTokenizer.from_pretrained(
                str(source),
                trust_remote_code=trust_remote_code,
            )
        except Exception as e:
            logger.warning(f"Failed to load tokenizer from {source}: {e}")
            # Fallback –Ω–∞ gpt2
            return AutoTokenizer.from_pretrained("gpt2", trust_remote_code=trust_remote_code)
    
    def load_for_training(
        self,
        base_model_path: Optional[str | Path],
        stage: str,
        tokenizer: PreTrainedTokenizer,
        config: Dict[str, Any],
        trust_remote_code: bool = True,
    ) -> Tuple[PreTrainedModel, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HF –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è QLoRA –∏ mixed precision
        tuning_method = config.get("tuning_method", "full")
        mp = str(config.get("mixed_precision", "no")).lower()
        fp16_pure = bool(config.get("fp16_pure", False))
        use_flash_attention = bool(config.get("use_flash_attention", True))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º torch_dtype
        # –í–ê–ñ–ù–û:
        # - bf16: –º–æ–∂–Ω–æ –≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –≤ bf16 (–Ω–µ—Ç GradScaler)
        # - fp16: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —ç—Ç–æ AMP fp16 (GradScaler) -> –≤–µ—Å–∞ –¥–µ—Ä–∂–∏–º fp32
        # - fp16_pure=True: –≤–µ—Å–∞ –≥—Ä—É–∑–∏–º –≤ fp16, –∞ Accelerator –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å mixed_precision='no'
        if mp == "bf16":
            torch_dtype = torch.bfloat16
        elif mp == "fp16":
            torch_dtype = torch.float16 if fp16_pure else torch.float32
        else:
            torch_dtype = torch.float32
        
        quantization_config = None
        device_map = None
        
        if tuning_method == "qlora":
            if not PEFT_AVAILABLE:
                raise ValueError("QLoRA –≤—ã–±—Ä–∞–Ω, –Ω–æ peft/bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
            
            # QLoRA —Ç—Ä–µ–±—É–µ—Ç 4-bit quantization –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
            # –î–ª—è multi-GPU —á–µ—Ä–µ–∑ accelerate –Ω—É–∂–µ–Ω device_map
            if torch.cuda.is_available():
                local_rank = int(os.environ.get("LOCAL_RANK", "0"))
                device_map = {"": local_rank}
            
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch_dtype if torch_dtype != torch.float32 else torch.float16,
            )
            
            logger.info(f"QLoRA: loading model in 4-bit with compute_dtype={quantization_config.bnb_4bit_compute_dtype}")
        
        if base_model_path:
            base_model_path = Path(base_model_path)
            
            if not (base_model_path / "config.json").exists():
                raise ValueError(
                    f"Base model path {base_model_path} does not contain config.json. "
                    f"For HF models, please use a model directory with config.json."
                )
            
            try:
                logger.info(f"Loading HF model from {base_model_path} using from_pretrained...")
                # FlashAttention 2: –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–µ—Å–∞ –≤ fp16/bf16 –∏ –º–æ–¥–µ–ª—å –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (QLoRA).
                extra_kwargs: Dict[str, Any] = {}
                try:
                    use_flash = (
                        use_flash_attention
                        and (torch_dtype in (torch.float16, torch.bfloat16))
                        and tuning_method != "qlora"
                    )
                    if use_flash:
                        import flash_attn  # noqa: F401
                        extra_kwargs["attn_implementation"] = "flash_attention_2"
                        logger.info(f"‚úÖ FlashAttention2 –≤–∫–ª—é—á–µ–Ω (attn_implementation=flash_attention_2, dtype={torch_dtype})")
                    elif not use_flash_attention:
                        # –Ø–≤–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º SDPA/flash –¥–ª—è HF –º–æ–¥–µ–ª–µ–π
                        extra_kwargs["attn_implementation"] = "eager"
                except Exception:
                    # –ï—Å–ª–∏ flash_attn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –º–æ–ª—á–∞ –æ—Å—Ç–∞—ë–º—Å—è –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º attention
                    pass

                model = AutoModelForCausalLM.from_pretrained(
                    str(base_model_path),
                    torch_dtype=torch_dtype if tuning_method != "qlora" else None,  # dtype –≤ qlora –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è bnb
                    trust_remote_code=trust_remote_code,
                    quantization_config=quantization_config,
                    device_map=device_map,
                    **extra_kwargs,
                )
                model_config = model.config
                logger.info(f"‚úÖ Successfully loaded HF model from {base_model_path}")
                return model, model_config
            except Exception as e:
                logger.error(f"Failed to load HF model: {e}")
                raise
        
        else:
            # Pretrain from scratch –¥–ª—è HF –º–æ–¥–µ–ª–µ–π
            return self.init_from_scratch(None, tokenizer, config, trust_remote_code)
    
    def init_from_scratch(
        self,
        model_id_or_config: str | Dict[str, Any],
        tokenizer: PreTrainedTokenizer,
        config: Dict[str, Any],
        trust_remote_code: bool = True,
    ) -> Tuple[PreTrainedModel, Any]:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç HF –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è.
        
        –î–ª—è HF –º–æ–¥–µ–ª–µ–π —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω model_id,
        –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥.
        
        –í–ê–ñ–ù–û: QLoRA –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è pretrain from scratch (–Ω—É–∂–Ω–∞ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å).
        """
        model_id = config.get("model_id")
        if not model_id:
            raise ValueError(
                "For HF models, pretrain from scratch requires 'model_id' in config. "
                "Please specify a HuggingFace model ID (e.g., 'gpt2', 'microsoft/DialoGPT-small')."
            )
        
        tuning_method = config.get("tuning_method", "full")
        if tuning_method == "qlora":
            raise ValueError(
                "QLoRA is not supported for pretrain from scratch. "
                "Please use a base model for QLoRA training, or use LoRA/full fine-tuning."
            )
        
        try:
            logger.info(f"Initializing HF model from scratch using {model_id}...")
            model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=trust_remote_code)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º vocab_size –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if model_config.vocab_size != len(tokenizer):
                logger.info(f"Updating vocab_size: {model_config.vocab_size} -> {len(tokenizer)}")
                model_config.vocab_size = len(tokenizer)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º torch_dtype
            mp = config.get("mixed_precision", "no")
            if mp in ("fp16", "bf16"):
                torch_dtype = torch.float16 if mp == "fp16" else torch.bfloat16
            else:
                torch_dtype = torch.float32
            
            # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            model = AutoModelForCausalLM.from_config(
                model_config,
                trust_remote_code=trust_remote_code,
                torch_dtype=torch_dtype,
            )
            
            logger.info(f"‚úÖ Initialized HF model from {model_id}")
            return model, model_config
        
        except Exception as e:
            logger.error(f"Failed to initialize HF model from {model_id}: {e}")
            raise ValueError(
                f"Cannot initialize HF model from {model_id}. "
                f"Some models require trust_remote_code=True or cannot be initialized from config. "
                f"Error: {e}"
            )

