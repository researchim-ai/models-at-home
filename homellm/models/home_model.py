"""
homellm.models.home_model
-------------------------
–ù–µ–±–æ–ª—å—à–∞—è –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformer-—è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ —Å—Ç–∏–ª–µ MiniMind,
–Ω–æ —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –∏ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, –∫—Ä–æ–º–µ PyTorch / HF-Transformers.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
‚Ä¢ RMSNorm –≤–º–µ—Å—Ç–æ LayerNorm (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ LigerRMSNorm –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω);
‚Ä¢ Rotary Positional Embedding (RoPE);
‚Ä¢ –°–≤—ë—Ä—Ç–∫–∞ QK-KV –Ω–∞ –º–æ–¥—É–ª—å FlashAttention (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏–ª–∏ –æ–±—ã—á–Ω–æ–µ scaled-dot-prod;
‚Ä¢ Feed-Forward –Ω–∞ SiLU (GELU-–ø–æ–¥–æ–±–Ω–∞—è);
‚Ä¢ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ —á–∏—Å–ª–∞ –≥–æ–ª–æ–≤, —Å–ª–æ—ë–≤, hidden_size —á–µ—Ä–µ–∑ HomeConfig.
‚Ä¢ ü¶Å Liger Kernel –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω liger-kernel).

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã generate (—á–µ—Ä–µ–∑ GenerationMixin).
"""
from __future__ import annotations

import logging
import math
from typing import List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
from torch import nn
from transformers import PretrainedConfig, PreTrainedModel, GenerationMixin
from transformers.modeling_outputs import CausalLMOutputWithPast

logger = logging.getLogger(__name__)

# ============================================================
# LIGER KERNEL ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
# ============================================================
_LIGER_RMSNORM = None
_LIGER_SILUMUL = None
_LIGER_CHECKED = False


def _check_liger_available():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Liger –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""
    global _LIGER_RMSNORM, _LIGER_SILUMUL, _LIGER_CHECKED
    
    if _LIGER_CHECKED:
        return
    
    _LIGER_CHECKED = True
    
    # LigerRMSNorm
    try:
        from liger_kernel.transformers import LigerRMSNorm
        _LIGER_RMSNORM = LigerRMSNorm
        logger.debug("‚úÖ LigerRMSNorm –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è Home –º–æ–¥–µ–ª–µ–π")
    except ImportError:
        _LIGER_RMSNORM = None
    
    # LigerSiLUMulFunction (fused SwiGLU)
    try:
        from liger_kernel.ops.swiglu import LigerSiLUMulFunction
        _LIGER_SILUMUL = LigerSiLUMulFunction
        logger.debug("‚úÖ LigerSiLUMulFunction –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è Home –º–æ–¥–µ–ª–µ–π (fused SwiGLU)")
    except ImportError:
        _LIGER_SILUMUL = None


def _get_liger_rmsnorm():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç LigerRMSNorm –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω (–∫—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç)."""
    _check_liger_available()
    return _LIGER_RMSNORM


def _get_liger_silumul():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç LigerSiLUMulFunction –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω (fused silu * mul)."""
    _check_liger_available()
    return _LIGER_SILUMUL

# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------


class HomeConfig(PretrainedConfig):
    model_type = "homellm"

    def __init__(
        self,
        vocab_size: int = 50257,
        hidden_size: int = 512,
        num_hidden_layers: int = 8,
        num_attention_heads: int = 8,
        intermediate_size: Optional[int] = None,
        dropout: float = 0.0,
        bos_token_id: int = 50256,
        eos_token_id: int = 50256,
        max_position_embeddings: int = 4096,
        rope_theta: float = 1e4,
        use_sdpa: bool = True,
        use_liger: bool = True,  # ü¶Å –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Liger –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        **kwargs,
    ):
        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = (
            intermediate_size if intermediate_size is not None else int(hidden_size * 4)
        )
        self.dropout = dropout
        self.max_position_embeddings = max_position_embeddings
        self.rope_theta = rope_theta
        self.use_sdpa = bool(use_sdpa)
        self.use_liger = bool(use_liger)


# -----------------------------------------------------------------------------
# Building blocks
# -----------------------------------------------------------------------------


class RMSNorm(nn.Module):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RMSNorm (fallback –µ—Å–ª–∏ Liger –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)."""
    
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)


def create_rmsnorm(dim: int, eps: float = 1e-5, use_liger: bool = True) -> nn.Module:
    """
    –°–æ–∑–¥–∞—ë—Ç RMSNorm ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ LigerRMSNorm –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    
    Args:
        dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        eps: Epsilon –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        use_liger: –ü—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Liger (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
    
    Returns:
        RMSNorm –∏–ª–∏ LigerRMSNorm
    """
    if use_liger:
        LigerRMSNorm = _get_liger_rmsnorm()
        if LigerRMSNorm is not None:
            return LigerRMSNorm(dim, eps=eps)
    return RMSNorm(dim, eps=eps)


def precompute_freqs_cis(dim: int, end: int, theta: float):
    """–í—ã—á–∏—Å–ª—è–µ—Ç cos/sin –¥–ª—è RoPE –æ–¥–∏–Ω —Ä–∞–∑ –∏ –∫—ç—à–∏—Ä—É–µ—Ç."""
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
    t = torch.arange(end)
    freqs = torch.outer(t, freqs)
    freqs_cos = torch.cos(freqs)
    freqs_sin = torch.sin(freqs)
    emb = torch.cat([freqs_cos, freqs_cos], dim=-1), torch.cat([freqs_sin, freqs_sin], dim=-1)
    return emb


def rotate_half(x):
    return torch.cat((-x[..., x.shape[-1] // 2 :], x[..., : x.shape[-1] // 2]), dim=-1)


def apply_rope(q, k, cos, sin, position_ids):
    cos = cos[position_ids][:, None, :, :]
    sin = sin[position_ids][:, None, :, :]
    q_embed = q * cos + rotate_half(q) * sin
    k_embed = k * cos + rotate_half(k) * sin
    return q_embed, k_embed


class Attention(nn.Module):
    def __init__(self, config: HomeConfig):
        super().__init__()
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.num_heads = config.num_attention_heads
        self.scale = self.head_dim ** -0.5

        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.dropout = nn.Dropout(config.dropout)

        self.flash = bool(getattr(config, "use_sdpa", True)) and hasattr(F, "scaled_dot_product_attention")

    def forward(
        self,
        x: torch.Tensor,
        cos_sin: Tuple[torch.Tensor, torch.Tensor],
        position_ids: torch.Tensor,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False,
    ):
        bsz, seq_len, _ = x.size()
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        q = q.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        cos, sin = cos_sin
        q, k = apply_rope(q, k, cos, sin, position_ids)

        # Append past
        if past_key_value is not None:
            pk, pv = past_key_value
            k = torch.cat([pk, k], dim=2)
            v = torch.cat([pv, v], dim=2)
        present = (k, v) if use_cache else None

        kv_len = k.size(2)  # –ü–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞ KV (—Å —É—á—ë—Ç–æ–º past_key_value)
        
        if self.flash:
            # PyTorch SDPA —Å–∞–º –≤—ã–±–µ—Ä–µ—Ç –ª—É—á—à–∏–π backend (flash/mem_efficient/math) –ø—Ä–∏ fp16/bf16.
            # –í–∞–∂–Ω–æ: –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å KV-–∫—ç—à–µ–º –æ–±—ã—á–Ω–æ seq_len=1, –∏ –º—ã –º–æ–∂–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ —Å—á–∏—Ç–∞—Ç—å attention –±–µ–∑ causal-–º–∞—Å–∫–∏:
            # –≤ k/v —É–∂–µ —Ç–æ–ª—å–∫–æ "–ø—Ä–æ—à–ª–æ–µ", –±—É–¥—É—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Ç–∞–º –Ω–µ—Ç.
            if past_key_value is None:
                output = F.scaled_dot_product_attention(
                    q, k, v,
                    dropout_p=self.dropout.p if self.training else 0.0,
                    is_causal=True,
                )
            elif seq_len == 1:
                output = F.scaled_dot_product_attention(
                    q, k, v,
                    dropout_p=0.0,
                    is_causal=False,
                )
            else:
                # –†–µ–¥–∫–∏–π —Å–ª—É—á–∞–π: past_key_value –µ—Å—Ç—å, –Ω–æ seq_len > 1 (chunked decode) ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø—É—Ç—å –Ω–∏–∂–µ.
                attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale
                causal_mask = torch.triu(
                    torch.ones(seq_len, kv_len, device=x.device, dtype=torch.bool),
                    diagonal=kv_len - seq_len + 1
                )
                attn_weights = attn_weights.masked_fill(causal_mask, -1e4)
                attn_probs = F.softmax(attn_weights, dim=-1)
                attn_probs = self.dropout(attn_probs)
                output = torch.matmul(attn_probs, v)
        else:
            attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale
            # –°–æ–∑–¥–∞—ë–º –∫–∞—É–∑–∞–ª—å–Ω—É—é –º–∞—Å–∫—É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (seq_len √ó kv_len)
            causal_mask = torch.triu(
                torch.ones(seq_len, kv_len, device=x.device, dtype=torch.bool),
                diagonal=kv_len - seq_len + 1
            )
            attn_weights = attn_weights.masked_fill(causal_mask, -1e4)
            attn_probs = F.softmax(attn_weights, dim=-1)
            attn_probs = self.dropout(attn_probs)
            output = torch.matmul(attn_probs, v)

        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)
        return self.out_proj(output), present


class FeedForward(nn.Module):
    """
    SwiGLU FeedForward —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π Liger –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.
    
    –ï—Å–ª–∏ Liger –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LigerSiLUMulFunction –¥–ª—è fused silu(gate) * up:
    - –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å (–Ω–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞)
    - –ë—ã—Å—Ç—Ä–µ–µ (–æ–¥–∏–Ω kernel –≤–º–µ—Å—Ç–æ –¥–≤—É—Ö)
    """
    def __init__(self, config: HomeConfig):
        super().__init__()
        hidden = config.intermediate_size
        self.w1 = nn.Linear(config.hidden_size, hidden, bias=False)  # gate_proj
        self.w2 = nn.Linear(hidden, config.hidden_size, bias=False)  # down_proj
        self.w3 = nn.Linear(config.hidden_size, hidden, bias=False)  # up_proj
        self.act = nn.SiLU()
        self.dropout = nn.Dropout(config.dropout)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Liger SiLUMul
        use_liger = getattr(config, 'use_liger', True)
        self._liger_silumul = _get_liger_silumul() if use_liger else None
        
        if self._liger_silumul is not None:
            logger.debug("ü¶Å FeedForward: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LigerSiLUMulFunction (fused SwiGLU)")

    def forward(self, x):
        gate = self.w1(x)  # gate_proj
        up = self.w3(x)    # up_proj
        
        if self._liger_silumul is not None:
            # ü¶Å Liger fused path: silu(gate) * up –≤ –æ–¥–Ω–æ–º kernel
            hidden = self._liger_silumul.apply(gate, up)
        else:
            # Standard path
            hidden = self.act(gate) * up
        
        return self.dropout(self.w2(hidden))


class HomeBlock(nn.Module):
    def __init__(self, config: HomeConfig):
        super().__init__()
        use_liger = getattr(config, 'use_liger', True)
        self.attn_norm = create_rmsnorm(config.hidden_size, use_liger=use_liger)
        self.ffn_norm = create_rmsnorm(config.hidden_size, use_liger=use_liger)
        self.attn = Attention(config)
        self.mlp = FeedForward(config)

    def forward(self, x, cos_sin, position_ids, past_key_value=None, use_cache=False):
        attn_out, present = self.attn(
            self.attn_norm(x), cos_sin, position_ids, past_key_value, use_cache
        )
        x = x + attn_out
        x = x + self.mlp(self.ffn_norm(x))
        return x, present


# -----------------------------------------------------------------------------
# Model
# -----------------------------------------------------------------------------


class HomeModel(nn.Module):
    def __init__(self, config: HomeConfig):
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = nn.ModuleList([HomeBlock(config) for _ in range(config.num_hidden_layers)])
        use_liger = getattr(config, 'use_liger', True)
        self.final_norm = create_rmsnorm(config.hidden_size, use_liger=use_liger)
        self.gradient_checkpointing = False  # –§–ª–∞–≥ –¥–ª—è checkpointing

        # Precompute RoPE
        cos, sin = precompute_freqs_cis(
            config.hidden_size // config.num_attention_heads,
            config.max_position_embeddings,
            config.rope_theta,
        )
        self.register_buffer("rope_cos", cos, persistent=False)
        self.register_buffer("rope_sin", sin, persistent=False)
        
        # –õ–æ–≥–∏—Ä—É–µ–º Liger –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if use_liger:
            liger_opts = []
            if _get_liger_rmsnorm() is not None:
                liger_opts.append("RMSNorm")
            if _get_liger_silumul() is not None:
                liger_opts.append("SwiGLU (fused SiLU*Mul)")
            if liger_opts:
                logger.info(f"ü¶Å HomeModel: Liger –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–∫—Ç–∏–≤–Ω—ã: {', '.join(liger_opts)}")

    def forward(
        self,
        input_ids: torch.Tensor,
        position_ids: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, List[Tuple[torch.Tensor, torch.Tensor]]]:
        if position_ids is None:
            past_len = 0
            if past_key_values is not None and len(past_key_values) > 0 and past_key_values[0] is not None:
                pk, _ = past_key_values[0]
                # pk: [bs, heads, past_len, head_dim]
                past_len = int(pk.size(2))
            position_ids = torch.arange(
                past_len, past_len + input_ids.shape[1],
                device=input_ids.device
            ).unsqueeze(0)

        hidden_states = self.embed_tokens(input_ids)
        
        presents = [] if use_cache else None
        cos_sin = (self.rope_cos, self.rope_sin)
        
        for i, layer in enumerate(self.layers):
            past = past_key_values[i] if past_key_values is not None else None
            
            if self.gradient_checkpointing and self.training and not use_cache:
                # –ü—Ä–æ—Å—Ç–∞—è –æ–±—ë—Ä—Ç–∫–∞ - –ø–µ—Ä–µ–¥–∞—ë–º layer —á–µ—Ä–µ–∑ –∑–∞–º—ã–∫–∞–Ω–∏–µ —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º –∑–∞—Ö–≤–∞—Ç–æ–º
                def make_ckpt_fn(block, cs):
                    def fn(hidden, pos_ids):
                        out, _ = block(hidden, cs, pos_ids, None, False)
                        return out
                    return fn
                
                # –°–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é —Å –∑–∞—Ö–≤–∞—Ç–æ–º —Ç–µ–∫—É—â–µ–≥–æ layer
                ckpt_forward = make_ckpt_fn(layer, cos_sin)
                
                # –í—ã–∑—ã–≤–∞–µ–º checkpoint
                hidden_states = torch.utils.checkpoint.checkpoint(
                    ckpt_forward, 
                    hidden_states, 
                    position_ids,
                    use_reentrant=False,
                )
                present = None
            else:
                hidden_states, present = layer(
                    hidden_states,
                    cos_sin,
                    position_ids,
                    past,
                    use_cache,
                )
            
            if use_cache:
                presents.append(present)

        hidden_states = self.final_norm(hidden_states)
        return hidden_states, presents


class HomeForCausalLM(PreTrainedModel, GenerationMixin):
    config_class = HomeConfig
    base_model_prefix = "home_model"
    _tied_weights_keys = ["lm_head.weight"]
    _supports_gradient_checkpointing = True  # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç

    def __init__(self, config: HomeConfig):
        super().__init__(config)
        self.home_model = HomeModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Weight tying
        self.lm_head.weight = self.home_model.embed_tokens.weight

        self.post_init()
    
    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
        """–í–∫–ª—é—á–∞–µ—Ç gradient checkpointing –¥–ª—è –º–æ–¥–µ–ª–∏."""
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –Ω–∞ HomeModel
        self.home_model.gradient_checkpointing = True
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º kwargs –¥–ª—è checkpointing
        if gradient_checkpointing_kwargs is None:
            gradient_checkpointing_kwargs = {"use_reentrant": False}
        
        def ckpt_func(fn, *args):
            return torch.utils.checkpoint.checkpoint(fn, *args, **gradient_checkpointing_kwargs)
        
        self.home_model._gradient_checkpointing_func = ckpt_func
    
    def gradient_checkpointing_disable(self):
        """–í—ã–∫–ª—é—á–∞–µ—Ç gradient checkpointing."""
        self.home_model.gradient_checkpointing = False
        if hasattr(self.home_model, '_gradient_checkpointing_func'):
            delattr(self.home_model, '_gradient_checkpointing_func')
    
    def tie_weights(self):
        """–ü—Ä–∏–≤—è–∑–∞—Ç—å –≤–µ—Å–∞ lm_head –∫ embed_tokens."""
        self.lm_head.weight = self.home_model.embed_tokens.weight

    def get_input_embeddings(self):
        return self.home_model.embed_tokens

    def set_input_embeddings(self, value):
        self.home_model.embed_tokens = value

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
        use_cache: bool = False,
        labels: Optional[torch.Tensor] = None,
        output_hidden_states: bool = False,
        **kwargs,  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ transformers (cache_position, etc.)
    ) -> CausalLMOutputWithPast:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DynamicCache –≤ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        legacy_past = None
        if past_key_values is not None:
            if hasattr(past_key_values, 'to_legacy_cache'):
                # –≠—Ç–æ DynamicCache –∏–∑ –Ω–æ–≤—ã—Ö transformers
                legacy_past = past_key_values.to_legacy_cache()
            elif isinstance(past_key_values, (list, tuple)):
                legacy_past = past_key_values
        
        hidden_states, presents = self.home_model(
            input_ids,
            past_key_values=legacy_past,
            use_cache=use_cache,
        )

        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # –°–¥–≤–∏–≥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss = F.cross_entropy(
                shift_logits.view(-1, self.config.vocab_size),
                shift_labels.view(-1),
                ignore_index=-100,
            )

        # –§–æ—Ä–º–∏—Ä—É–µ–º hidden_states –¥–ª—è output –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        # CausalLMOutputWithPast –æ–∂–∏–¥–∞–µ—Ç tuple of (hidden_states_per_layer,)
        # –î–ª—è Liger Fused CE –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ hidden state
        all_hidden_states = None
        if output_hidden_states:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º tuple —Å –æ–¥–Ω–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state (–¥–æ lm_head)
            all_hidden_states = (hidden_states,)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=presents,
            hidden_states=all_hidden_states,
        )
